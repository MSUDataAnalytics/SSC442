---
title: "Project 1"
linktitle: "Project 1"
date: "2022-01-05"
due_date: "2022-02-25"
due_time: "11:59 PM"
output:
  blogdown::html_page:
    toc: true
menu:
  assignment:
    parent: Projects
    weight: 1
type: docs
editor_options:
  chunk_output_type: console
---

Updated 4-4-2022 (no longer applicable for Sp22)

:::fyi

One member of each group should turn in the group's work on D2L. In addition to the requirements below, the project should have the group member's names. All other group members should turn in a text document stating the group members (D2L requires something be turned in to give a grade and feedback, but I do not want to accidentally grade your projects 2-3 times over.)

Turn in your copies by **11:59 on the date listed on the Schedule** 

:::


## Part 1: Rats, rats, rats.

New York City is full of urban wildlife, and rats are one of the city's most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they're useful too.

`r blogdown::shortcode("youtube", "PeJUqcbool4")`

NYC keeps incredibly detailed data regarding animal sightings, including rats, and [it makes this data publicly available](https://www.kaggle.com/new-york-city/nyc-rat-sightings/data).

For this first project, pretend that you are an analyst with the NYC Mayor's office, and you have been tasked with getting a better understanding of the rat problem. Your job is to use **R and ggplot2** to tell an interesting story hidden in the data. You must create a story by looking carefully at the data, finding some insight into where or when rat sightings occur, and describing to the Mayor how this insight may inform a strategy to address the rats. Your assignment will take the form of a memo that presents the data and insights along with appropriate visualizations that communicate the story in-line. Your memo should be approximately 2-4 pages (including plots) and, of course, will use Rmarkdown rendered to PDF using LaTeX.

### Instructions

Here's what you need to do:

1. **Download** New York City's database of rat sightings since 2010:

    - [<i class="fas fa-file-csv"></i> `Rat_sightings.csv`](/data/Rat_sightings.csv)

2. **Summarize** the data somehow. The raw data has more than 100,000 rows, which means you'll need to aggregate the data (`filter()`, `group_by()`, and `summarize()` will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2016.

3. **Create** an appropriate visualization based on the data you summarized.

4. **Write** a polished, professional memo presenting your analysis. We are specifically looking for a discussion of the following:

    - What story are you telling with your new graphic?
    - How have you applied reasonable standards in visual storytelling?
    - What policy implication is there (if any)?
  

5. **Upload** the following outputs to D2L:

    - A PDF file of your memo with your graphics embedded in it.^[You can approach this in a couple different waysâ€”you can write the memo and then include the full figure and code at the end, [similar to this blog post](https://rud.is/b/2017/09/18/mapping-fall-foliage-with-sf/), or you can write the memo in an incremental way, describing the different steps of creating the figure, ultimately arriving at a clean final figure, [like this blog post](https://rudeboybert.github.io/fivethirtyeight/articles/bechdel.html).] This means you'll need to do all your coding in an `R` Markdown file and embed your code in chunks. 
    - Note that Part 2 of this project should be included in this PDF in it's own section (see below).
    - Nothing else. No .Rmd, no code, nothing but your clean, polished memo with Part 1 and Part 2.
    
Some important notes on your assignment

- Your assignment should be clean and polished as if you were a city employee and you were turning in a work product. It should should flow nicely and use subsections (using ```###``` at the start of the line) as appropriate.
- Do **not** "annotate" your thought process (e.g. do not write "we tried X but there were too many NA's, so we did Y instead"). This should be a polished memo suitable for turning in as a work product.
- Your code should **not** appear in your output - it should be only your plots and memo writing. 
  - To turn off code echoing, add `echo = FALSE` in each of your code chunk options (e.g. `{r setup, echo = FALSE}`), or set it globally in the first code chunk inside the `knitr::opts_chunk$set` function.
    
    
### Starter code

I've provided some starter code below. A couple comments about it:

- By default, `read_csv()` treats cells that are empty or "NA" as missing values. This rat dataset uses "N/A" to mark missing values, so we need to add that as a possible marker of missingness (hence `na = c("", "NA", "N/A")`)
- To make life easier, I've renamed some of the key variables you might work with. You can rename others if you want.
- I've also created a few date-related variables (`sighting_year`, `sighting_month`, `sighting_day`, and `sighting_weekday`). You don't have to use them, but they're there if you need them. The functions that create these, like `year()` and `wday()` are part of the **lubridate** library.
- The date/time variables are formatted like `04/03/2017 12:00:00 AM`, which R is not able to automatically parse as a date when reading the CSV file. You can use the `mdy_hms()` function in the **lubridate** library to parse dates that are structured as "month-day-year-hour-minute". There are also a bunch of other iterations of this function, like `ymd()`, `dmy()`, etc., for other date formats.
- There's one row with an unspecified borough, so I filter that out.

```{r load-clean-data, eval=FALSE}
library(tidyverse)
library(lubridate)
rats_raw <- read_csv("data/Rat_Sightings.csv", na = c("", "NA", "N/A"))
# If you get an error that says "All formats failed to parse. No formats
# found", it's because the mdy_hms function couldn't parse the date. The date
# variable *should* be in this format: "04/03/2017 12:00:00 AM", but in some
# rare instances, it might load without the seconds as "04/03/2017 12:00 AM".
# If there are no seconds, use mdy_hm() instead of mdy_hms().
rats_clean <- rats_raw %>%
  rename(created_date = `Created Date`,
         location_type = `Location Type`,
         borough = Borough) %>%
  mutate(created_date = mdy_hms(created_date)) %>%
  mutate(sighting_year = year(created_date),
         sighting_month = month(created_date),
         sighting_day = day(created_date),
         sighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %>%
  filter(borough != "Unspecified")
```

You'll summarize the data with functions from **dplyr**, including stuff like `count()`, `arrange()`, `filter()`, `group_by()`, `summarize()`, and `mutate()`. Here are some examples of ways to summarize the data:

```{r dplyr-examples, eval=FALSE}
# See the count of rat sightings by weekday
rats_clean %>%
  count(sighting_weekday)
# Assign a summarized data frame to an object to use it in a plot
rats_by_weekday <- rats_clean %>%
  count(sighting_weekday, sighting_year)
ggplot(rats_by_weekday, aes(x = fct_rev(sighting_weekday), y = n)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ sighting_year)
# See the count of rat sightings by weekday and borough
rats_clean %>%
  count(sighting_weekday, borough, sighting_year)
# An alternative to count() is to specify the groups with group_by() and then
# be explicit about how you're summarizing the groups, such as calculating the
# mean, standard deviation, or number of observations (we do that here with
# `n()`).
rats_clean %>%
  group_by(sighting_weekday, borough) %>%
  summarize(n = n())
```

## Part 2: Data Hunting

For the second part of the project, your task is simple. Your group must identify three different data sources^[The three different sources need not be different websites or from different organizations. For example, three different tables from the US Census would be sufficient] for potential use in your final project. You are not bound to this decision.

Do **not** use Kaggle data. While Kaggle is useful for learning data science, part of this assignment is learning how to find actual data in the wild. 

For each dataset, you must write a single short paragraph about what about this data interests you. Add this to the memo from Part 1, using `##` to make a new header in Rmarkdown for this section. 



## Evaluations

I will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:

__Part 1__

1. Technical difficulty: Does the final project show mastery of the skills we've discussed thus far? Are axes and legends appropriately labeled? (12 points)

2. Appropriateness of visuals: Do the visualizations tell a clear story? Have we learned something? (10 points)

3. Storytelling: Does your memo clearly convey what you're doing and why? (10 points)

__Part 2__

Each piece of data (and description) is worth 6 points. (18 points total)

.small[Updated 4-4-2022]
