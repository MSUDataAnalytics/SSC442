---
title: "Illustrating Bias vs. Variance"
linktitle: "9: Bias vs. Variance"
output:
  blogdown::html_page:
    toc: true
menu:
  example:
    parent: Examples
    weight: 1
type: docs
weight: 1
editor_options:
  chunk_output_type: console
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#review-and-clarify" id="toc-review-and-clarify">Review and Clarify</a>
<ul>
<li><a href="#illustration-of-bias-vs.-variance" id="toc-illustration-of-bias-vs.-variance">Illustration of Bias vs. Variance</a></li>
</ul></li>
<li><a href="#a-quick-bit-of-r-code-to-help-with-todays-example" id="toc-a-quick-bit-of-r-code-to-help-with-todays-example">A quick bit of R code to help with todays example</a></li>
<li><a href="#todays-example" id="toc-todays-example">Today’s Example</a>
<ul>
<li><a href="#simulation" id="toc-simulation">Simulation</a></li>
<li><a href="#heres-the-code" id="toc-heres-the-code">Here’s the code</a></li>
</ul></li>
</ul>
</div>

<div id="review-and-clarify" class="section level2">
<h2>Review and Clarify</h2>
<p>Bias and Variance are tricky subjects. Hopefully the illustrations from yesterday are helpful. Let’s talk through a few things based on questions some of you have asked since Content 9.</p>
<div id="illustration-of-bias-vs.-variance" class="section level3">
<h3>Illustration of Bias vs. Variance</h3>
<p>Bias is about how close you are on average to the correct answer. Variance is about how scattered your estimates would be if you repeated your experiment with new data.</p>
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="https://www.machinelearningplus.com/wp-content/uploads/2020/10/output_31_0.png" alt="Image from MachineLearningPlus.com"  />
<p class="caption">
Figure 1: Image from MachineLearningPlus.com
</p>
</div>
<p>We care about these things because we usually only have our one dataset (when we’re not creating simulated data, that is), but need to know something about how bias and variance tend to look when we change our model complexity.</p>
<div id="deriving-bias-and-variance" class="section level4">
<h4>Deriving Bias and Variance</h4>
<p>For this section, recall our model:</p>
<p><span class="math display">\[
y = f(x) + \epsilon
\]</span></p>
<p>This tells us that some of <span class="math inline">\(y\)</span> can be predicted by the <strong>true</strong> <span class="math inline">\(f(x)\)</span>, and some is just noise <span class="math inline">\(\epsilon\)</span>. In our simulation from last week, <span class="math inline">\(f(x) = x^2\)</span>, so <span class="math inline">\(y = x^2 + \epsilon\)</span>.</p>
<ul>
<li><p>We want to predict <span class="math inline">\(y\)</span>. We call our prediction <span class="math inline">\(\hat{y}\)</span>.</p></li>
<li><p>Our best guess for <span class="math inline">\(f(x)\)</span> is <span class="math inline">\(\hat{f}(x)\)</span>, where <span class="math inline">\(\hat{f}(x)\)</span> is our model. It might be from a linear regression with 1, 2, 9, 15, etc. predictors or interactions of predictors. It might be from a k-nearest-neighbors estimation with <code>k = 4</code>. It might be from a regression tree with <code>cp = .1</code> and <code>minsplit=2</code>.</p></li>
<li><p>Even when we really nail <span class="math inline">\(\hat{f}(x)\)</span> (which means <span class="math inline">\(\hat{f}(x) = f(x)\)</span>), there is <em>still</em> error in our prediction because of <span class="math inline">\(\epsilon\)</span></p>
<ul>
<li><span class="math inline">\(y \neq \hat{y}\)</span></li>
</ul></li>
</ul>
<p>So we think of two different measures of error:</p>
<p><span class="math display">\[
EPE = E[(y - \hat{y})^2] =
\underbrace{\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]}_\textrm{reducible error - MSE from imperfect model} +
\underbrace{\mathbb{V}_{Y \mid X} \left[ Y \mid X = x \right]}_{\textrm{irreducible error from }\epsilon}
\]</span></p>
<p>And:</p>
<p><span class="math display">\[
MSE(f(x), \hat{f}(x)) = \mathbb{E}_{\mathcal{D}}\left[\left(f(x) - \hat{f}(x)\right)^2\right]
\]</span></p>
<p>Some of you asked about this equation from last time that decomposed our MSE:</p>
<p><span class="math display">\[
\text{MSE}\left(f(x), \hat{f}(x)\right) =
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] =
\underbrace{\left(f(x) - \mathbb{E} \left[ \hat{f}(x) \right]  \right)^2}_{\text{bias}^2 \left(\hat{f}(x) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}(x) - \mathbb{E} \left[ \hat{f}(x) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}(x) \right)}
\]</span>
This can be derived by:</p>
<p><span class="math display">\[
\begin{eqnarray*}
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] &amp;=&amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)] + E[\hat{f}(x)] - \hat{f}(x)\right)^2 \right] \\
&amp;=&amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)]\right)^2 + \left(E[\hat{f}(x)] - \hat{f}(x)\right)^2 + 2\left(f(x) - E[\hat{f}(x)]\right) \left(E[\hat{f}(x)] - \hat{f}(x)\right) \right] \\
&amp;=&amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)]\right)^2\right] + \mathbb{E}_{\mathcal{D}} \left[\left(E[\hat{f}(x)] - \hat{f}(x)\right)^2\right] +  \mathbb{E}_{\mathcal{D}} \left[2\left(f(x) - E[\hat{f}(x)]\right) \left(E[\hat{f}(x)] - \hat{f}(x)\right) \right] \\
&amp;=&amp; \left(f(x) - E[\hat{f}(x)]\right)^2 + Var\left(\hat{f}(x)\right) + 0
\end{eqnarray*}
\]</span>
Let’s talk about what’s in this equation:</p>
<ul>
<li>MSE is the Mean Squared Error between <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(\hat{f}(x)\)</span>
<ul>
<li>It does not have the <span class="math inline">\(\epsilon\)</span> in it</li>
</ul></li>
<li>It is an expectation over all the possible <span class="math inline">\(\mathcal{D}\)</span> draws of the data we could have
<ul>
<li>Because of this <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(E\left[\hat{f}(x)\right]\)</span> can move out of the expectation. This lets us cancel that last term with the “2” in it.</li>
</ul></li>
</ul>
<p>The main takeaway is that, even given the error, <span class="math inline">\(\epsilon\)</span>, we <em>still</em> have additional error coming from our inability to perfectly get <span class="math inline">\(\hat{f}(x) = f(x)\)</span>.</p>
</div>
</div>
</div>
<div id="a-quick-bit-of-r-code-to-help-with-todays-example" class="section level2">
<h2>A quick bit of R code to help with todays example</h2>
<p>We saw before the usefulness of having a list</p>
<pre class="r"><code>myList = list()
myList[[&#39;thisThing&#39;]] = c(1,2,3)
myList[[&#39;thisOtherThing&#39;]] = c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;)
myList</code></pre>
<pre><code>## $thisThing
## [1] 1 2 3
## 
## $thisOtherThing
## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot;</code></pre>
<p>It worked really well for holding results from models since we could name the things in the list. But if we put it into a loop:</p>
<pre class="r"><code>for(i in c(&#39;G&#39;,&#39;H&#39;,&#39;I&#39;)){
  myList = list()
  myList[[i]] = paste0(&#39;This loop is on &#39;,i)
}

print(myList)</code></pre>
<pre><code>## $I
## [1] &quot;This loop is on I&quot;</code></pre>
<p>What happened? Every time we used the loop, it re-initiated the list, so we only get the last result!</p>
<p>So what we want is to create the list <strong>if</strong> it doesn’t exist, and add to it afterwards. We can do that with <code>exists('myList')</code></p>
<pre class="r"><code>for(i in c(&#39;G&#39;,&#39;H&#39;,&#39;I&#39;)){
  if(!exists(&#39;myList&#39;)) myList = list()
  myList[[i]] = paste0(&#39;This loop is on &#39;,i)
}
print(myList)</code></pre>
<pre><code>## $I
## [1] &quot;This loop is on I&quot;
## 
## $G
## [1] &quot;This loop is on G&quot;
## 
## $H
## [1] &quot;This loop is on H&quot;</code></pre>
<p>We’re almost there. It turns out, we have our original <span class="math inline">\(I\)</span> in there left over from the previous creation of the list. That’s why its out of order. What we want to do is start with a fresh, clean list. If we run <code>rm(myList)</code>, the old list will no longer exist, and <em>our code will create a fresh one when we run it again!</em></p>
<pre class="r"><code>rm(myList)
for(i in c(&#39;G&#39;,&#39;H&#39;,&#39;I&#39;)){
  if(!exists(&#39;myList&#39;)) myList = list()
  myList[[i]] = paste0(&#39;This loop is on &#39;,i)
}
print(myList)</code></pre>
<pre><code>## $G
## [1] &quot;This loop is on G&quot;
## 
## $H
## [1] &quot;This loop is on H&quot;
## 
## $I
## [1] &quot;This loop is on I&quot;</code></pre>
<p>You’re going to use this in your breakout rooms today. You’re going to be asked to run some code that stores a plot in a list. To reset the list that stores things, just use <code>rm(listName)</code> (where <code>listName</code> is the name of the list).</p>
<p>Alternatively, you can make sure you run code (outside of the loop) that makes a new list. This will re-set the list to be empty going into the loop:</p>
<pre class="r"><code>myList = list()
for(i in c(&#39;G&#39;,&#39;H&#39;,&#39;I&#39;)){
  if(!exists(&#39;myList&#39;)) myList = list()
  myList[[i]] = paste0(&#39;This loop is on &#39;,i)
}
print(myList)</code></pre>
<pre><code>## $G
## [1] &quot;This loop is on G&quot;
## 
## $H
## [1] &quot;This loop is on H&quot;
## 
## $I
## [1] &quot;This loop is on I&quot;</code></pre>
</div>
<div id="todays-example" class="section level2">
<h2>Today’s Example</h2>
<p>Our goal today is to</p>
<ul>
<li><p>See the code that produced this week’s Content</p>
<ul>
<li>Why? Because it helps to illustrate the <em>true</em> sources of noise in the data</li>
</ul></li>
<li><p>See what larger sample sizes and higher/lower irreducible error does to our Bias vs. Variance tradeoff.</p></li>
</ul>
<div id="simulation" class="section level3">
<h3>Simulation</h3>
<p>We will use the exact code from Content 9, which I have reproduced here. I have removed the in-between parts with notation so we can focus on the example. I have <strong>copied all of the relevant code into one chunk down at the bottom as well</strong></p>
<p>We’ll need the following libraries:</p>
<pre class="r"><code>library(ggplot2)
library(patchwork)
library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──</code></pre>
<pre><code>## ✓ tibble  3.1.3     ✓ dplyr   1.0.7
## ✓ tidyr   1.1.3     ✓ stringr 1.4.0
## ✓ readr   2.0.0     ✓ forcats 0.5.1
## ✓ purrr   0.3.4</code></pre>
<pre><code>## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<p>And here, I’ve made a little change to Content 9’s code so we can play with sample size <code>NN</code> and the SD of the irreducible Bayes error.</p>
<pre class="r"><code>NN = 100   #----&gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = .75   #-----&gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.</code></pre>
<div id="begin-content-9-code-here" class="section level4">
<h4>Begin Content 9 code here:</h4>
<p>We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function <span class="math inline">\(f(x) = x^2\)</span>.</p>
<pre class="r"><code>f = function(x) {
  x ^ 2
}</code></pre>
<p>To carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following <code>R</code> code.</p>
<pre class="r"><code>get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  tibble(x, y)
}</code></pre>
<p>To completely specify the data generating process, we have made more model assumptions than simply <span class="math inline">\(\mathbb{E}[Y \mid X = x] = x^2\)</span> and <span class="math inline">\(\mathbb{V}[Y \mid X = x] = \sigma ^ 2\)</span>. In particular,</p>
<ul>
<li>The <span class="math inline">\(x_i\)</span> in <span class="math inline">\(\mathcal{D}\)</span> are sampled from a uniform distribution over <span class="math inline">\([0, 1]\)</span>.</li>
<li>The <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\epsilon\)</span> are independent.</li>
<li>The <span class="math inline">\(y_i\)</span> in <span class="math inline">\(\mathcal{D}\)</span> are sampled from the conditional normal distribution.</li>
</ul>
<p>Using this setup, we will generate datasets, <span class="math inline">\(\mathcal{D}\)</span>, with a sample size <span class="math inline">\(NN\)</span> and fit four models.</p>
<p><span class="math display">\[
\begin{aligned}
\texttt{predict(fit0, x)} &amp;= \hat{f}_0(x) = \hat{\beta}_0\\
\texttt{predict(fit1, x)} &amp;= \hat{f}_1(x) = \hat{\beta}_0 + \hat{\beta}_1 x \\
\texttt{predict(fit2, x)} &amp;= \hat{f}_2(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \\
\texttt{predict(fit9, x)} &amp;= \hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\end{aligned}
\]</span></p>
<p>To get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.</p>
<pre class="r"><code>set.seed(1)
sim_data = get_sim_data(f)</code></pre>
<pre class="r"><code>fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)</code></pre>
<p>Plotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.</p>
<p><img src="/example/09-example_files/figure-html/unnamed-chunk-13-1.png" width="864" /></p>
<p>…</p>
<p>We will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates for <span class="math inline">\(f(x)\)</span> given by these four models at the point <span class="math inline">\(x = 0.90\)</span>. We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.</p>
<pre class="r"><code>set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)</code></pre>
<pre class="r"><code>for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}</code></pre>
<p>Compile all of the results:</p>
<p><img src="/example/09-example_files/figure-html/unnamed-chunk-16-1.png" width="864" /></p>
<div class="fyi">
<p>In your breakout room and using the code below:</p>
<p><strong>First Breakout</strong>: set <code>NN</code> = 100, the value we used in our Content 9 lecture. The value is set in one of the first code chunks. Step through the code to get your finalPlot and make sure it looks like the plot in Content 9. I changed the code to use ggplot (easier to save output), so the formatting and colors will be different - that’s OK, we want to get the same results, not copy the layout of the plot. Note that at the end of the code, a list is created that will hold all of your results. In case you need to clear this list, <code>rm(FinalResults)</code> will do so and the code will initate a new blank list to hold subsequent results.</p>
<p><strong>Second Breakout</strong>: set NN to a larger number. Usually, more data means more precise predictions. Run your code again stepping through it, until you get to this plot. Note that at the end of the code provided, there is a list that aggregates your results. <strong>Repeat this</strong> with a 3rd, even larger value for NN. Don’t go much beyond 50,000 or it’ll take too long. Your <code>FinalResults</code> list should have 3 elements in it. Use <code>patchwork::wrap_plots(FinalResults, nrow = 1)</code> to see all 3 side-by-side.</p>
<p><strong>Third Breakout</strong>: Finally, change the <code>SD.of.Bayes.Error</code> value to make it higher or lower. Remember, this is the <em>irreducible</em> error. Run your code again with your first, second, and third different value for sample size <code>NN</code>. You should have 6 plots in your <code>FinalResults</code> list - 3 from before, and 3 more with the new SD of Bayes Error. Use <code>wrap_plots</code> with the right number of rows to see a 2x3 grid of the results.</p>
<ul>
<li>Usually we think larger sample sizes and lower error lead to better overall prediction. Do we see any change in the bias vs. tradeoff relationship with lower/higher sample size <code>NN</code> and lower/higher SD of Bayes Error?</li>
</ul>
</div>
</div>
</div>
<div id="heres-the-code" class="section level3">
<h3>Here’s the code</h3>
<p>I’ve merged all of the code together for you here. Copy this into a new .R script - you don’t need to use a full Markdown.</p>
<pre class="r"><code>library(ggplot2)
library(patchwork)
library(tidyverse)



NN = 100   #----&gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = 0.75   #-----&gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.


f = function(x) {
  x ^ 2
}



get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  tibble(x, y)
}






## See the fit of the four models (viz only)
set.seed(1)
sim_data = get_sim_data(f)



fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)


plot(y ~ x, data = sim_data, col = &quot;grey&quot;, pch = 20,
     main = &quot;Four Polynomial Models fit to a Simulated Dataset&quot;)

grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = &quot;black&quot;, lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = &quot;dodgerblue&quot;,  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &quot;firebrick&quot;,   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &quot;springgreen&quot;, lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = &quot;darkorange&quot;,  lwd = 2, lty = 5)

legend(&quot;topleft&quot;, 
       c(&quot;y ~ 1&quot;, &quot;y ~ poly(x, 1)&quot;, &quot;y ~ poly(x, 2)&quot;,  &quot;y ~ poly(x, 9)&quot;, &quot;truth&quot;), 
       col = c(&quot;dodgerblue&quot;, &quot;firebrick&quot;, &quot;springgreen&quot;, &quot;darkorange&quot;, &quot;black&quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)













set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)



for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}



predictions.proc = (predictions)
colnames(predictions.proc) = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;, &quot;9&quot;)
predictions.proc = as.data.frame(predictions.proc)

tall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)

## Here, you can save your ggplot output 
FinalPlot &lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + 
                      geom_boxplot() +
                      geom_jitter(alpha = .5) +
                      geom_hline(yintercept = f(x = .90)) +
                      labs(col = &#39;Model&#39;, x = &#39;Model&#39;, y = &#39;Prediction&#39;, title = paste0(&#39;Bias v Var - Sample Size: &#39;,NN), subtitle = paste0(&#39;SD of Bayes Err: &#39;,SD.of.Bayes.Error)) +
                      theme_bw()



FinalPlot


## This is going to aggregate your results for you:
if(!exists(&#39;FinalResults&#39;)) FinalResults = list()

FinalResults[[paste0(&#39;finalPlot.NN.&#39;,NN,&#39;.SDBayes.&#39;,SD.of.Bayes.Error)]] = FinalPlot

# 
# boxplot(value ~ key, data = tall_predictions, border = &quot;darkgrey&quot;, xlab = &quot;Polynomial Degree&quot;, ylab = &quot;Predictions&quot;,
#         main = &quot;Simulated Predictions for Polynomial Models&quot;)
# grid()
# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = &quot;jitter&quot;, jitter = 0.15, pch = 1, col = c(&quot;dodgerblue&quot;, &quot;firebrick&quot;, &quot;springgreen&quot;, &quot;darkorange&quot;))
# abline(h = f(x = 0.90), lwd = 2)</code></pre>
<p>And to output <em>whatever is in your list of ggplot objects</em>:</p>
<pre class="r"><code>## This not run automatically.
## To plot the whole list of ggplot objects in FinalResults:
wrap_plots(FinalResults, nrow = 2, guides = &#39;collect&#39;)</code></pre>
</div>
</div>
