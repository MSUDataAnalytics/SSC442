---
title: "Linear Regression III"
linktitle: "7: Linear Regression III"
read_date: "2022-03-01"
menu:
  content:
    parent: Course content
    weight: 1
type: docs
output:
  blogdown::html_page:
    toc: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#required-reading" id="toc-required-reading">Required Reading</a>
<ul>
<li><a href="#guiding-question" id="toc-guiding-question">Guiding Question</a></li>
</ul></li>
<li><a href="#association-is-not-causation" id="toc-association-is-not-causation">Association is not causation</a>
<ul>
<li><a href="#spurious-correlation" id="toc-spurious-correlation">Spurious correlation</a></li>
<li><a href="#outliers" id="toc-outliers">Outliers</a></li>
<li><a href="#reversing-cause-and-effect" id="toc-reversing-cause-and-effect">Reversing cause and effect</a></li>
<li><a href="#confounders" id="toc-confounders">Confounders</a>
<ul>
<li><a href="#example-uc-berkeley-admissions" id="toc-example-uc-berkeley-admissions">Example: UC Berkeley admissions</a></li>
<li><a href="#confounding-explained-graphically" id="toc-confounding-explained-graphically">Confounding explained graphically</a></li>
<li><a href="#average-after-stratifying" id="toc-average-after-stratifying">Average after stratifying</a></li>
</ul></li>
<li><a href="#simpsons-paradox" id="toc-simpsons-paradox">Simpson’s paradox</a></li>
<li><a href="#lecture-video" id="toc-lecture-video">Lecture Video</a></li>
</ul></li>
</ul>
</div>

<pre><code>## Loading required package: knitr</code></pre>
<div id="required-reading" class="section level2">
<h2>Required Reading</h2>
<ul>
<li>This page.</li>
</ul>
<div id="guiding-question" class="section level3">
<h3>Guiding Question</h3>
<ul>
<li>When can we make causal claims about the relationship between variables?</li>
</ul>
<p>As with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into <code>R</code>.</p>
<ul>
<li><a href="/projects/data/ames.csv"><i class="fas fa-file-csv"></i> <code>Ames.csv</code></a></li>
</ul>
</div>
</div>
<div id="association-is-not-causation" class="section level1">
<h1>Association is not causation</h1>
<p><em>Association is not causation</em> is perhaps the most important lesson one learns in a statistics class. <em>Correlation is not causation</em> is another way to say this. Throughout the previous parts of this class, we have described tools useful for quantifying associations between variables. However, we must be careful not to over-interpret these associations.</p>
<p>There are many reasons that a variable <span class="math inline">\(X\)</span> can be correlated with a variable <span class="math inline">\(Y\)</span> without having any direct effect on <span class="math inline">\(Y\)</span>. Here we examine four common ways that can lead to misinterpreting data.</p>
<div id="spurious-correlation" class="section level2">
<h2>Spurious correlation</h2>
<p>The following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.</p>
<p><img src="/content/07-content_files/figure-html/divorce-versus-margarine-1.png" width="672" /></p>
<p>Does this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a <em>spurious correlation</em>.</p>
<p>You can see many more absurd examples on the Spurious Correlations website<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p>The cases presented in the spurious correlation site are all instances of what is generally called <em>data dredging</em>, <em>data fishing</em>, or <em>data snooping</em>. It’s basically a form of what in the US they call <em>cherry picking</em>. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.</p>
<p>A Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:</p>
<pre class="r"><code>N &lt;- 25
g &lt;- 1000000
sim_data &lt;- tibble(group = rep(1:g, each=N),
                   x = rnorm(N * g),
                   y = rnorm(N * g))</code></pre>
<p>The first column denotes group. We created groups and for each one we generated a pair of independent vectors, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not correlated.</p>
<p>Next, we compute the correlation between <code>X</code> and <code>Y</code> for each group and look at the max:</p>
<pre class="r"><code>res &lt;- sim_data %&gt;%
  group_by(group) %&gt;%
  summarize(r = cor(x, y)) %&gt;%
  arrange(desc(r))
res</code></pre>
<pre><code>## # A tibble: 1,000,000 × 2
##     group     r
##     &lt;int&gt; &lt;dbl&gt;
##  1  16926 0.820
##  2 570226 0.808
##  3 883447 0.791
##  4 460882 0.776
##  5 163151 0.762
##  6 805411 0.757
##  7 934609 0.754
##  8 501726 0.753
##  9 119597 0.753
## 10 522332 0.751
## # … with 999,990 more rows</code></pre>
<p>We see a maximum correlation of 0.82 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are in fact correlated:</p>
<pre class="r"><code>sim_data %&gt;% filter(group == res$group[which.max(res$r)]) %&gt;%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;)</code></pre>
<p><img src="/content/07-content_files/figure-html/dredging-1.png" width="672" /></p>
<p>Remember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:</p>
<pre class="r"><code>res %&gt;% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = &quot;black&quot;)</code></pre>
<p><img src="/content/07-content_files/figure-html/null-corr-hist-1.png" width="672" /></p>
<p>It’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.2042812, the largest one will be close to 1.</p>
<p>If we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:</p>
<pre class="r"><code>library(broom)
sim_data %&gt;%
  filter(group == res$group[which.max(res$r)]) %&gt;%
  do(tidy(lm(y ~ x, data = .))) %&gt;%
  filter(term == &quot;x&quot;)</code></pre>
<pre><code>## # A tibble: 1 × 5
##   term  estimate std.error statistic     p.value
##   &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;
## 1 x        0.884     0.129      6.87 0.000000525</code></pre>
<p>Now, imagine that instead of a whole lot of simulated data, you had a whole lot of actual data and waded through enough of it to find two unrelated variables that happened to show up as correlated (like divorce rates and pounds of margarine consumed). This particular form of data dredging is referred to as <em>p-hacking</em>. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and a lot of different variables that represent exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you can learn methods to adjust for these multiple comparisons.</p>
</div>
<div id="outliers" class="section level2">
<h2>Outliers</h2>
<p>Suppose we take measurements from two independent outcomes, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:</p>
<pre class="r"><code>set.seed(1985)
x &lt;- rnorm(100,100,1)
y &lt;- rnorm(100,84,1)
x[-23] &lt;- scale(x[-23])
y[-23] &lt;- scale(y[-23])</code></pre>
<p>The data look like this:</p>
<pre class="r"><code>qplot(x, y)</code></pre>
<p><img src="/content/07-content_files/figure-html/outlier-1.png" width="672" /></p>
<p>Not surprisingly, the correlation is very high:</p>
<pre class="r"><code>cor(x,y)</code></pre>
<pre><code>## [1] 0.9878382</code></pre>
<p>But this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:</p>
<pre class="r"><code>cor(x[-23], y[-23])</code></pre>
<pre><code>## [1] -0.04419032</code></pre>
<p>Previously, we (briefly) described alternatives to the average and standard deviation that are robust to outliers. There is also an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called <em>Spearman correlation</em>. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:</p>
<pre class="r"><code>qplot(rank(x), rank(y))</code></pre>
<p><img src="/content/07-content_files/figure-html/scatter-plot-of-ranks-1.png" width="672" /></p>
<p>The outlier is no longer associated with a very large value and the correlation comes way down:</p>
<pre class="r"><code>cor(rank(x), rank(y))</code></pre>
<pre><code>## [1] 0.002508251</code></pre>
<p>Spearman correlation can also be calculated like this:</p>
<pre class="r"><code>cor(x, y, method = &quot;spearman&quot;)</code></pre>
<pre><code>## [1] 0.002508251</code></pre>
<p>There are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber &amp; Elvezio M. Ronchetti.</p>
</div>
<div id="reversing-cause-and-effect" class="section level2">
<h2>Reversing cause and effect</h2>
<p>Another way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.</p>
<p>A form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. Consider this quote from the article:</p>
<blockquote>
<p>When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.</p>
</blockquote>
<p>A very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.</p>
<p>We can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model:</p>
<p><span class="math display">\[X_i = \beta_0 + \beta_1 y_i + \varepsilon_i, i=1, \dots, N\]</span></p>
<p>to the father and son height data, with <span class="math inline">\(X_i\)</span> the father height and <span class="math inline">\(y_i\)</span> the son height, we do get a statistically significant result:</p>
<pre class="r"><code>library(HistData)
data(&quot;GaltonFamilies&quot;)
GaltonFamilies %&gt;%
  filter(childNum == 1 &amp; gender == &quot;male&quot;) %&gt;%
  select(father, childHeight) %&gt;%
  rename(son = childHeight) %&gt;%
  do(tidy(lm(father ~ son, data = .)))</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   34.0      4.57        7.44 4.31e-12
## 2 son            0.499    0.0648      7.70 9.47e-13</code></pre>
<p>The model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation.</p>
</div>
<div id="confounders" class="section level2">
<h2>Confounders</h2>
<p>Confounders are perhaps the most common reason that leads to associations begin misinterpreted.</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are correlated, we call <span class="math inline">\(Z\)</span> a <em>confounder</em> if changes in <span class="math inline">\(Z\)</span> causes changes in both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.</p>
<p>Incorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.</p>
<div id="example-uc-berkeley-admissions" class="section level3">
<h3>Example: UC Berkeley admissions</h3>
<p>Admission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and calculate the “headline” number:</p>
<pre class="r"><code>data(admissions)
admissions %&gt;% group_by(gender) %&gt;%
  summarize(percentage =
              round(sum(admitted*applicants)/sum(applicants),1))</code></pre>
<pre><code>## # A tibble: 2 × 2
##   gender percentage
##   &lt;chr&gt;       &lt;dbl&gt;
## 1 men          44.5
## 2 women        30.3</code></pre>
<p>The chi-squared test compares two groups with binary outcomes (like “admit” and “nonadmit”). The null hypothesis is that the groups are not differently distributed between the outcomes. A low p-value rejects this hypothesis. Here, the test clearly rejects the hypothesis that gender and admission are independent:</p>
<pre class="r"><code>admissions %&gt;% group_by(gender) %&gt;%
  summarize(total_admitted = round(sum(admitted / 100 * applicants)),
            not_admitted = sum(applicants) - sum(total_admitted)) %&gt;%
  select(-gender) %&gt;%
  do(tidy(chisq.test(.))) %&gt;% .$p.value</code></pre>
<pre><code>## [1] 1.055797e-21</code></pre>
<p>But closer inspection shows a paradoxical result. Here are the percent admissions by major:</p>
<pre class="r"><code>admissions %&gt;% select(major, gender, admitted) %&gt;%
  spread(gender, admitted) %&gt;%
  mutate(women_minus_men = women - men)</code></pre>
<pre><code>##   major men women women_minus_men
## 1     A  62    82              20
## 2     B  63    68               5
## 3     C  37    34              -3
## 4     D  33    35               2
## 5     E  28    24              -4
## 6     F   6     7               1</code></pre>
<p>Four out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.</p>
<p>The paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.</p>
<p>So let’s define three variables: <span class="math inline">\(X\)</span> is 1 for men and 0 for women, <span class="math inline">\(Y\)</span> is 1 for those admitted and 0 otherwise, and <span class="math inline">\(Z\)</span> quantifies the selectivity of the major. A gender bias claim would be based on the fact that <span class="math inline">\(\mbox{Pr}(Y=1 | X = x)\)</span> is higher for <span class="math inline">\(x=1\)</span> than <span class="math inline">\(x=0\)</span>. However, <span class="math inline">\(Z\)</span> is an important confounder to consider. Clearly <span class="math inline">\(Z\)</span> is associated with <span class="math inline">\(Y\)</span>, as the more selective a major, the lower <span class="math inline">\(\mbox{Pr}(Y=1 | Z = z)\)</span>. But is major selectivity <span class="math inline">\(Z\)</span> associated with gender <span class="math inline">\(X\)</span>?</p>
<p>One way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:</p>
<pre class="r"><code>admissions %&gt;%
  group_by(major) %&gt;%
  summarize(major_selectivity = sum(admitted * applicants)/sum(applicants),
            percent_women_applicants = sum(applicants * (gender==&quot;women&quot;)) /
                                             sum(applicants) * 100) %&gt;%
  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +
  geom_text()</code></pre>
<p><img src="/content/07-content_files/figure-html/uc-berkeley-majors-1.png" width="672" /></p>
<p>There seems to be association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women.</p>
</div>
<div id="confounding-explained-graphically" class="section level3">
<h3>Confounding explained graphically</h3>
<p>The following plot shows the number of applicants that were admitted and those that were not by:</p>
<p><img src="/content/07-content_files/figure-html/confounding-1.png" width="672" /></p>
<!--

```r
admissions %>%
  mutate(percent_admitted = admitted * applicants/sum(applicants)) %>%
  ggplot(aes(gender, y = percent_admitted, fill = major)) +
  geom_bar(stat = "identity", position = "stack")
```

<img src="/content/07-content_files/figure-html/confounding-2-1.png" width="672" />
-->
<p>It also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.</p>
</div>
<div id="average-after-stratifying" class="section level3">
<h3>Average after stratifying</h3>
<p>In this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:</p>
<pre class="r"><code>admissions %&gt;%
  ggplot(aes(major, admitted, col = gender, size = applicants)) +
  geom_point()</code></pre>
<p><img src="/content/07-content_files/figure-html/admission-by-major-1.png" width="672" /></p>
<p>Now we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.</p>
<p>If we average the difference by major, we find that the percent is actually 3.5% higher for women.</p>
<pre class="r"><code>admissions %&gt;%  group_by(gender) %&gt;% summarize(average = mean(admitted))</code></pre>
<pre><code>## # A tibble: 2 × 2
##   gender average
##   &lt;chr&gt;    &lt;dbl&gt;
## 1 men       38.2
## 2 women     41.7</code></pre>
</div>
</div>
<div id="simpsons-paradox" class="section level2">
<h2>Simpson’s paradox</h2>
<p>The case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(Z\)</span> and that we observe realizations of these. Here is a plot of simulated observations for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> along with the sample correlation:</p>
<p><img src="/content/07-content_files/figure-html/simpsons-paradox-1.png" width="672" /></p>
<p>You can see that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are negatively correlated. However, once we stratify by <span class="math inline">\(Z\)</span> (shown in different colors below) another pattern emerges:</p>
<p><img src="/content/07-content_files/figure-html/simpsons-paradox-explained-1.png" width="672" /></p>
<p>It is really <span class="math inline">\(Z\)</span> that is negatively correlated with <span class="math inline">\(X\)</span>. If we stratify by <span class="math inline">\(Z\)</span>, the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are actually positively correlated as seen in the plot above.</p>
</div>
<div id="lecture-video" class="section level2">
<h2>Lecture Video</h2>
<p>The lecture video for Tuesday (this material, but we only made it about 2/3rds of the way) is <a href="https://mediaspace.msu.edu/media/SSC442+-+Content+07+-+March+1st/1_p3zx59rx">available here <i class="fas fa-film"></i></a></p>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p><a href="http://tylervigen.com/spurious-correlations" class="uri">http://tylervigen.com/spurious-correlations</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated" class="uri">https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
