---
title: "Applied Scraping"
linktitle: "*13: Scraping"
date: "2020-12-01"
read_date: "2020-12-01"
menu:
  content:
    parent: Course content
    weight: 1
type: docs
output:
  blogdown::html_page:
    toc: true
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#required-reading">Required Reading</a></li>
<li><a href="#text-mining">Text mining</a>
<ul>
<li><a href="#case-study-trump-tweets">Case study: Trump tweets</a></li>
<li><a href="#text-as-data">Text as data</a></li>
<li><a href="#sentiment-analysis">Sentiment analysis</a></li>
</ul></li>
</ul>
</div>

<div id="required-reading" class="section level2">
<h2>Required Reading</h2>
<ul>
<li>This page.</li>
</ul>
</div>
<div id="text-mining" class="section level1">
<h1>Text mining</h1>
<p>With the exception of labels used to represent categorical data, we have focused on numerical data. But in many applications, data starts as text. Well-known examples are spam filtering, cyber-crime prevention, counter-terrorism and sentiment analysis. In all these cases, the raw data is composed of free form text. Our task is to extract insights from these data. In this section, we learn how to generate useful numerical summaries from text data to which we can apply some of the powerful data visualization and analysis techniques we have learned.</p>
<div id="case-study-trump-tweets" class="section level2">
<h2>Case study: Trump tweets</h2>
<p>During the 2016 US presidential election, then candidate Donald J. Trump used his twitter account as a way to communicate with potential voters. On August 6, 2016, Todd Vaziri tweeted<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> about Trump that “Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him).”</p>
<p>Data scientist David Robinson conducted an analysis<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> to determine if data supported this assertion. Here, we go through David’s analysis to learn some of the basics of text mining. To learn more about text mining in R, we recommend the Text Mining with R book<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> by Julia Silge and David Robinson.</p>
<p>We will use the following libraries:</p>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(scales)
library(dslabs)
library(rtweet)
library(jsonlite)</code></pre>
<p>In general, we can extract data directly from Twitter using the <strong>rtweet</strong> package. However, in this case, a group has already compiled data for us and made it available at <a href="http://www.trumptwitterarchive.com">http://www.trumptwitterarchive.com</a>. We can get the data from their JSON API using a script like this:</p>
<pre class="r"><code>url &lt;- &#39;https://www.thetrumparchive.com/latest-tweets&#39;
trump_tweets &lt;- map(2020:2020, ~sprintf(url, .x)) %&gt;%
  map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %&gt;%
  filter(!isRetweet &amp; !str_detect(text, &#39;^&quot;&#39;)) %&gt;%
  mutate(created_at = as.Date(date))</code></pre>
<p>You can see the data frame with information about the tweets by typing</p>
<pre class="r"><code>data(trump_tweets)
head(trump_tweets)</code></pre>
<pre><code>##               source     id_str
## 1 Twitter Web Client 6971079756
## 2 Twitter Web Client 6312794445
## 3 Twitter Web Client 6090839867
## 4 Twitter Web Client 5775731054
## 5 Twitter Web Client 5364614040
## 6 Twitter Web Client 5203117820
##                                                                                                                                         text
## 1       From Donald Trump: Wishing everyone a wonderful holiday &amp; a happy, healthy, prosperous New Year. Let’s think like champions in 2010!
## 2 Trump International Tower in Chicago ranked 6th tallest building in world by Council on Tall Buildings &amp; Urban Habitat http://bit.ly/sqvQq
## 3                                                                             Wishing you and yours a very Happy and Bountiful Thanksgiving!
## 4                       Donald Trump Partners with TV1 on New Reality Series Entitled, Omarosa&#39;s Ultimate Merger: http://tinyurl.com/yk5m3lc
## 5                         --Work has begun, ahead of schedule, to build the greatest golf course in history: Trump International – Scotland.
## 6              --From Donald Trump: &quot;Ivanka and Jared’s wedding was spectacular, and they make a beautiful couple. I’m a very proud father.&quot;
##            created_at retweet_count in_reply_to_user_id_str favorite_count
## 1 2009-12-23 12:38:18            28                    &lt;NA&gt;             12
## 2 2009-12-03 14:39:09            33                    &lt;NA&gt;              6
## 3 2009-11-26 14:55:38            13                    &lt;NA&gt;             11
## 4 2009-11-16 16:06:10             5                    &lt;NA&gt;              3
## 5 2009-11-02 09:57:56             7                    &lt;NA&gt;              6
## 6 2009-10-27 10:31:48             4                    &lt;NA&gt;              5
##   is_retweet
## 1      FALSE
## 2      FALSE
## 3      FALSE
## 4      FALSE
## 5      FALSE
## 6      FALSE</code></pre>
<p>with the following variables included:</p>
<pre class="r"><code>names(trump_tweets)</code></pre>
<pre><code>## [1] &quot;source&quot;                  &quot;id_str&quot;                 
## [3] &quot;text&quot;                    &quot;created_at&quot;             
## [5] &quot;retweet_count&quot;           &quot;in_reply_to_user_id_str&quot;
## [7] &quot;favorite_count&quot;          &quot;is_retweet&quot;</code></pre>
<p>The help file <code>?trump_tweets</code> provides details on what each variable represents. The tweets are represented by the <code>text</code> variable:</p>
<pre class="r"><code>trump_tweets$text[16413] %&gt;% str_wrap(width = options()$width) %&gt;% cat</code></pre>
<pre><code>## Great to be back in Iowa! #TBT with @JerryJrFalwell joining me in Davenport-
## this past winter. #MAGA https://t.co/A5IF0QHnic</code></pre>
<p>and the source variable tells us which device was used to compose and upload each tweet:</p>
<pre class="r"><code>trump_tweets %&gt;% count(source) %&gt;% arrange(desc(n)) %&gt;% head(5)</code></pre>
<pre><code>##                source     n
## 1  Twitter Web Client 10718
## 2 Twitter for Android  4652
## 3  Twitter for iPhone  3962
## 4           TweetDeck   468
## 5     TwitLonger Beta   288</code></pre>
<p>We are interested in what happened during the campaign, so for this analysis we will focus on what was tweeted between the day Trump announced his campaign and election day. We define the following table containing just the tweets from that time period. Note that we use <code>extract</code> to remove the <code>Twitter for</code> part of the source and filter out retweets.</p>
<pre class="r"><code>campaign_tweets &lt;- trump_tweets %&gt;%
  extract(source, &quot;source&quot;, &quot;Twitter for (.*)&quot;) %&gt;%
  filter(source %in% c(&quot;Android&quot;, &quot;iPhone&quot;) &amp;
           created_at &gt;= ymd(&quot;2015-06-17&quot;) &amp;
           created_at &lt; ymd(&quot;2016-11-08&quot;)) %&gt;%
  filter(!is_retweet) %&gt;%
  arrange(created_at)</code></pre>
<p>We can now use data visualization to explore the possibility that two different groups were tweeting from these devices. For each tweet, we will extract the hour, East Coast time (EST), it was tweeted and then compute the proportion of tweets tweeted at each hour for each device:</p>
<pre class="r"><code>ds_theme_set()
campaign_tweets %&gt;%
  mutate(hour = hour(with_tz(created_at, &quot;EST&quot;))) %&gt;%
  count(source, hour) %&gt;%
  group_by(source) %&gt;%
  mutate(percent = n / sum(n)) %&gt;%
  ungroup %&gt;%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = &quot;Hour of day (EST)&quot;, y = &quot;% of tweets&quot;, color = &quot;&quot;)</code></pre>
<p><img src="/content/13-content_files/figure-html/tweets-by-time-by-device-1.png" width="672" /></p>
<p>We notice a big peak for the Android in the early hours of the morning, between 6 and 8 AM. There seems to be a clear difference in these patterns. We will therefore assume that two different entities are using these two devices.</p>
<p>We will now study how the tweets differ when we compare Android to iPhone. To do this, we introduce the <strong>tidytext</strong> package.</p>
</div>
<div id="text-as-data" class="section level2">
<h2>Text as data</h2>
<p>The <strong>tidytext</strong> package helps us convert free form text into a tidy table. Having the data in this format greatly facilitates data visualization and the use of statistical techniques.</p>
<pre class="r"><code>library(tidytext)</code></pre>
<p>The main function needed to achieve this is <code>unnest_tokens</code>. A <em>token</em> refers to a unit that we are considering to be a data point. The most common <em>token</em> will be words, but they can also be single characters, ngrams, sentences, lines, or a pattern defined by a regex. The functions will take a vector of strings and extract the tokens so that each one gets a row in the new table. Here is a simple example:</p>
<pre class="r"><code>poem &lt;- c(&quot;Roses are red,&quot;, &quot;Violets are blue,&quot;,
          &quot;Sugar is sweet,&quot;, &quot;And so are you.&quot;)
example &lt;- tibble(line = c(1, 2, 3, 4),
                      text = poem)
example</code></pre>
<pre><code>## # A tibble: 4 x 2
##    line text             
##   &lt;dbl&gt; &lt;chr&gt;            
## 1     1 Roses are red,   
## 2     2 Violets are blue,
## 3     3 Sugar is sweet,  
## 4     4 And so are you.</code></pre>
<pre class="r"><code>example %&gt;% unnest_tokens(word, text)</code></pre>
<pre><code>## # A tibble: 13 x 2
##     line word   
##    &lt;dbl&gt; &lt;chr&gt;  
##  1     1 roses  
##  2     1 are    
##  3     1 red    
##  4     2 violets
##  5     2 are    
##  6     2 blue   
##  7     3 sugar  
##  8     3 is     
##  9     3 sweet  
## 10     4 and    
## 11     4 so     
## 12     4 are    
## 13     4 you</code></pre>
<p>Now let’s look at an example from the tweets. We will look at tweet number 3008 because it will later permit us to illustrate a couple of points:</p>
<pre class="r"><code>i &lt;- 3008
campaign_tweets$text[i] %&gt;% str_wrap(width = 65) %&gt;% cat()</code></pre>
<pre><code>## Great to be back in Iowa! #TBT with @JerryJrFalwell joining me in
## Davenport- this past winter. #MAGA https://t.co/A5IF0QHnic</code></pre>
<pre class="r"><code>campaign_tweets[i,] %&gt;%
  unnest_tokens(word, text) %&gt;%
  pull(word)</code></pre>
<pre><code>##  [1] &quot;great&quot;          &quot;to&quot;             &quot;be&quot;             &quot;back&quot;          
##  [5] &quot;in&quot;             &quot;iowa&quot;           &quot;tbt&quot;            &quot;with&quot;          
##  [9] &quot;jerryjrfalwell&quot; &quot;joining&quot;        &quot;me&quot;             &quot;in&quot;            
## [13] &quot;davenport&quot;      &quot;this&quot;           &quot;past&quot;           &quot;winter&quot;        
## [17] &quot;maga&quot;           &quot;https&quot;          &quot;t.co&quot;           &quot;a5if0qhnic&quot;</code></pre>
<p>Note that the function tries to convert tokens into words. To do this, however, it strips characters that are important in the context of twitter. Namely, the function removes all the <code>#</code> and <code>@</code>. A <em>token</em> in the context of Twitter is not the same as in the context of spoken or written English. For this reason, instead of using the default, words, we use the <code>tweets</code> token includes patterns that start with @ and #:</p>
<pre class="r"><code>campaign_tweets[i,] %&gt;%
  unnest_tokens(word, text, token = &quot;tweets&quot;) %&gt;%
  pull(word)</code></pre>
<pre><code>##  [1] &quot;great&quot;                   &quot;to&quot;                     
##  [3] &quot;be&quot;                      &quot;back&quot;                   
##  [5] &quot;in&quot;                      &quot;iowa&quot;                   
##  [7] &quot;#tbt&quot;                    &quot;with&quot;                   
##  [9] &quot;@jerryjrfalwell&quot;         &quot;joining&quot;                
## [11] &quot;me&quot;                      &quot;in&quot;                     
## [13] &quot;davenport&quot;               &quot;this&quot;                   
## [15] &quot;past&quot;                    &quot;winter&quot;                 
## [17] &quot;#maga&quot;                   &quot;https://t.co/a5if0qhnic&quot;</code></pre>
<p>Another minor adjustment we want to make is to remove the links to pictures:</p>
<pre class="r"><code>links &lt;- &quot;https://t.co/[A-Za-z\\d]+|&amp;amp;&quot;
campaign_tweets[i,] %&gt;%
  mutate(text = str_replace_all(text, links, &quot;&quot;))  %&gt;%
  unnest_tokens(word, text, token = &quot;tweets&quot;) %&gt;%
  pull(word)</code></pre>
<pre><code>##  [1] &quot;great&quot;           &quot;to&quot;              &quot;be&quot;              &quot;back&quot;           
##  [5] &quot;in&quot;              &quot;iowa&quot;            &quot;#tbt&quot;            &quot;with&quot;           
##  [9] &quot;@jerryjrfalwell&quot; &quot;joining&quot;         &quot;me&quot;              &quot;in&quot;             
## [13] &quot;davenport&quot;       &quot;this&quot;            &quot;past&quot;            &quot;winter&quot;         
## [17] &quot;#maga&quot;</code></pre>
<p>Now we are now ready to extract the words for all our tweets.</p>
<pre class="r"><code>tweet_words &lt;- campaign_tweets %&gt;%
  mutate(text = str_replace_all(text, links, &quot;&quot;))  %&gt;%
  unnest_tokens(word, text, token = &quot;tweets&quot;)</code></pre>
<p>And we can now answer questions such as “what are the most commonly used words?”:</p>
<pre class="r"><code>tweet_words %&gt;%
  count(word) %&gt;%
  arrange(desc(n))</code></pre>
<p>Running this, we find a boring answer. But it is not surprising that these are the top words. The top words are not informative. The <em>tidytext</em> package has a database of these commonly used words, referred to as <em>stop words</em>, in text mining:</p>
<pre class="r"><code>stop_words</code></pre>
<pre><code>## # A tibble: 1,149 x 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           SMART  
##  2 a&#39;s         SMART  
##  3 able        SMART  
##  4 about       SMART  
##  5 above       SMART  
##  6 according   SMART  
##  7 accordingly SMART  
##  8 across      SMART  
##  9 actually    SMART  
## 10 after       SMART  
## # … with 1,139 more rows</code></pre>
<p>If we filter out rows representing stop words with <code>filter(!word %in% stop_words$word)</code>:</p>
<pre class="r"><code>tweet_words &lt;- campaign_tweets %&gt;%
  mutate(text = str_replace_all(text, links, &quot;&quot;))  %&gt;%
  unnest_tokens(word, text, token = &quot;tweets&quot;) %&gt;%
  filter(!word %in% stop_words$word )</code></pre>
<p>we end up with a much more informative set of top 10 tweeted words:</p>
<pre class="r"><code>tweet_words %&gt;%
  count(word) %&gt;%
  top_n(10, n) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  arrange(desc(n))</code></pre>
<pre><code>##                      word   n
## 1              #trump2016 414
## 2                 hillary 405
## 3                  people 303
## 4  #makeamericagreatagain 294
## 5                 america 254
## 6                 clinton 237
## 7                    poll 217
## 8                 crooked 205
## 9                   trump 195
## 10                   cruz 159</code></pre>
<p>Some exploration of the resulting words (not shown here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years, for example). We want to remove these and we can find them using the regex <code>^\d+$</code>. Second, some of our tokens come from a quote and they start with <code>'</code>. We want to remove the <code>'</code> when it is at the start of a word so we will just <code>str_replace</code>. We add these two lines to the code above to generate our final table:</p>
<pre class="r"><code>tweet_words &lt;- campaign_tweets %&gt;%
  mutate(text = str_replace_all(text, links, &quot;&quot;))  %&gt;%
  unnest_tokens(word, text, token = &quot;tweets&quot;) %&gt;%
  filter(!word %in% stop_words$word &amp;
           !str_detect(word, &quot;^\\d+$&quot;)) %&gt;%
  mutate(word = str_replace(word, &quot;^&#39;&quot;, &quot;&quot;))</code></pre>
<p>Now that we have all our words in a table, along with information about what device was used to compose the tweet they came from, we can start exploring which words are more common when comparing Android to iPhone.</p>
<p>For each word, we want to know if it is more likely to come from an Android tweet or an iPhone tweet. In Section <a href="#association-tests"><strong>??</strong></a> we introduced the odds ratio as a summary statistic useful for quantifying these differences. For each device and a given word, let’s call it <code>y</code>, we compute the odds or the ratio between the proportion of words that are <code>y</code> and not <code>y</code> and compute the ratio of those odds. Here we will have many proportions that are 0, so we use the 0.5 correction described in Section <a href="#association-tests"><strong>??</strong></a>.</p>
<pre class="r"><code>android_iphone_or &lt;- tweet_words %&gt;%
  count(word, source) %&gt;%
  spread(source, n, fill = 0) %&gt;%
  mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) /
           ( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))</code></pre>
<p>Here are the highest odds ratios for Android</p>
<pre class="r"><code>android_iphone_or %&gt;% arrange(desc(or))</code></pre>
<p>and the top for iPhone:</p>
<pre class="r"><code>android_iphone_or %&gt;% arrange(or)</code></pre>
<p>Given that several of these words are overall low frequency words, we can impose a filter based on the total frequency like this:</p>
<pre class="r"><code>android_iphone_or %&gt;% filter(Android+iPhone &gt; 100) %&gt;%
  arrange(desc(or))

android_iphone_or %&gt;% filter(Android+iPhone &gt; 100) %&gt;%
  arrange(or)</code></pre>
<p>We already see somewhat of a pattern in the types of words that are being tweeted more from one device versus the other. However, we are not interested in specific words but rather in the tone. Vaziri’s assertion is that the Android tweets are more hyperbolic. So how can we check this with data? <em>Hyperbolic</em> is a hard sentiment to extract from words as it relies on interpreting phrases. However, words can be associated to more basic sentiment such as anger, fear, joy, and surprise. In the next section, we demonstrate basic sentiment analysis.</p>
</div>
<div id="sentiment-analysis" class="section level2">
<h2>Sentiment analysis</h2>
<p>In sentiment analysis, we assign a word to one or more “sentiments”. Although this approach will miss context-dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.</p>
<p>The first step in sentiment analysis is to assign a sentiment to each word. As we demonstrate, the <strong>tidytext</strong> package includes several maps or lexicons. We will also be using the <strong>textdata</strong> package.</p>
<pre class="r"><code>library(tidytext)
library(textdata)</code></pre>
<p>The <code>bing</code> lexicon divides words into <code>positive</code> and <code>negative</code> sentiments. We can see this using the <em>tidytext</em> function <code>get_sentiments</code>:</p>
<pre class="r"><code>get_sentiments(&quot;bing&quot;)</code></pre>
<p>The <code>AFINN</code> lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive. Note that this lexicon needs to be downloaded the first time you call the function <code>get_sentiment</code>:</p>
<pre class="r"><code>get_sentiments(&quot;afinn&quot;)</code></pre>
<p>The <code>loughran</code> and <code>nrc</code> lexicons provide several different sentiments. Note that these also have to be downloaded the first time you use them.</p>
<pre class="r"><code>get_sentiments(&quot;loughran&quot;) %&gt;% count(sentiment)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   sentiment        n
##   &lt;chr&gt;        &lt;int&gt;
## 1 constraining   184
## 2 litigious      904
## 3 negative      2355
## 4 positive       354
## 5 superfluous     56
## 6 uncertainty    297</code></pre>
<pre class="r"><code>get_sentiments(&quot;nrc&quot;) %&gt;% count(sentiment)</code></pre>
<pre><code>## # A tibble: 10 x 2
##    sentiment        n
##    &lt;chr&gt;        &lt;int&gt;
##  1 anger         1247
##  2 anticipation   839
##  3 disgust       1058
##  4 fear          1476
##  5 joy            689
##  6 negative      3324
##  7 positive      2312
##  8 sadness       1191
##  9 surprise       534
## 10 trust         1231</code></pre>
<p>For our analysis, we are interested in exploring the different sentiments of each tweet so we will use the <code>nrc</code> lexicon:</p>
<pre class="r"><code>nrc &lt;- get_sentiments(&quot;nrc&quot;) %&gt;%
  select(word, sentiment)</code></pre>
<p>We can combine the words and sentiments using <code>inner_join</code>, which will only keep words associated with a sentiment. Here are 10 random words extracted from the tweets:</p>
<pre class="r"><code>tweet_words %&gt;% inner_join(nrc, by = &quot;word&quot;) %&gt;%
  select(source, word, sentiment) %&gt;%
  sample_n(5)</code></pre>
<pre><code>##    source     word    sentiment
## 1  iPhone  failing         fear
## 2 Android    proud        trust
## 3 Android     time anticipation
## 4  iPhone horrible      disgust
## 5 Android  failing        anger</code></pre>
<p>Now we are ready to perform a quantitative analysis comparing Android and iPhone by comparing the sentiments of the tweets posted from each device. Here we could perform a tweet-by-tweet analysis, assigning a sentiment to each tweet. However, this will be challenging since each tweet will have several sentiments attached to it, one for each word appearing in the lexicon. For illustrative purposes, we will perform a much simpler analysis: we will count and compare the frequencies of each sentiment appearing in each device.</p>
<pre class="r"><code>sentiment_counts &lt;- tweet_words %&gt;%
  left_join(nrc, by = &quot;word&quot;) %&gt;%
  count(source, sentiment) %&gt;%
  spread(source, n) %&gt;%
  mutate(sentiment = replace_na(sentiment, replace = &quot;none&quot;))</code></pre>
<p>For each sentiment, we can compute the odds of being in the device: proportion of words with sentiment versus proportion of words without, and then compute the odds ratio comparing the two devices.</p>
<pre class="r"><code>sentiment_counts %&gt;%
  mutate(Android = Android / (sum(Android) - Android) ,
         iPhone = iPhone / (sum(iPhone) - iPhone),
         or = Android/iPhone) %&gt;%
  arrange(desc(or))</code></pre>
<p>So we do see some differences and the order is interesting: the largest three sentiments are disgust, anger, and negative! But are these differences just due to chance? How does this compare if we are just assigning sentiments at random? To answer this question we can compute, for each sentiment, an odds ratio and a confidence interval, as defined in Section <a href="#association-tests"><strong>??</strong></a>. We will add the two values we need to form a two-by-two table and the odds ratio:</p>
<pre class="r"><code>library(broom)
log_or &lt;- sentiment_counts %&gt;%
  mutate(log_or = log((Android / (sum(Android) - Android)) /
      (iPhone / (sum(iPhone) - iPhone))),
          se = sqrt(1/Android + 1/(sum(Android) - Android) +
                      1/iPhone + 1/(sum(iPhone) - iPhone)),
          conf.low = log_or - qnorm(0.975)*se,
          conf.high = log_or + qnorm(0.975)*se) %&gt;%
  arrange(desc(log_or))

log_or</code></pre>
<pre><code>##       sentiment Android iPhone        log_or         se     conf.low
## 1       disgust     638    322  0.4735570999 0.06912035  0.338083695
## 2         anger     958    528  0.3886460226 0.05517087  0.280513101
## 3      negative    1641    929  0.3711281448 0.04236574  0.288092813
## 4       sadness     894    515  0.3421548755 0.05626874  0.231870166
## 5          fear     795    486  0.2797607725 0.05848940  0.165123653
## 6      surprise     518    365  0.1317550233 0.06909953 -0.003677571
## 7           joy     688    535  0.0312500000 0.05855009 -0.083506063
## 8  anticipation     910    715  0.0207782389 0.05103024 -0.079239194
## 9         trust    1236    990  0.0007368608 0.04390294 -0.085311327
## 10     positive    1806   1473 -0.0189958333 0.03666144 -0.090850937
## 11         none   11904  10766 -0.2850526973 0.02053953 -0.325309446
##      conf.high
## 1   0.60903050
## 2   0.49677894
## 3   0.45416348
## 4   0.45243958
## 5   0.39439789
## 6   0.26718762
## 7   0.14600606
## 8   0.12079567
## 9   0.08678505
## 10  0.05285927
## 11 -0.24479595</code></pre>
<p>A graphical visualization shows some sentiments that are clearly overrepresented:</p>
<pre class="r"><code>log_or %&gt;%
  mutate(sentiment = reorder(sentiment, log_or)) %&gt;%
  ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point(aes(sentiment, log_or)) +
  ylab(&quot;Log odds ratio for association between Android and sentiment&quot;) +
  coord_flip()</code></pre>
<p><img src="/content/13-content_files/figure-html/tweets-log-odds-ratio-1.png" width="672" /></p>
<p>We see that the disgust, anger, negative, sadness, and fear sentiments are associated with the Android in a way that is hard to explain by chance alone. Words not associated to a sentiment were strongly associated with the iPhone source, which is in agreement with the original claim about hyperbolic tweets.</p>
<p>If we are interested in exploring which specific words are driving these differences, we can refer back to our <code>android_iphone_or</code> object:</p>
<pre class="r"><code>android_iphone_or %&gt;% inner_join(nrc) %&gt;%
  filter(sentiment == &quot;disgust&quot; &amp; Android + iPhone &gt; 10) %&gt;%
  arrange(desc(or))</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre><code>##         word Android iPhone        or sentiment
## 1       mess      13      2 4.6193174   disgust
## 2    finally      12      2 4.2768806   disgust
## 3     unfair      12      2 4.2768806   disgust
## 4        bad     104     26 3.3865340   disgust
## 5   terrible      31      8 3.1722813   disgust
## 6        lie      12      3 3.0546933   disgust
## 7      lying       9      3 2.3211353   disgust
## 8      waste      12      5 1.9436140   disgust
## 9    illegal      32     14 1.9179329   disgust
## 10     phony      20      9 1.8457906   disgust
## 11  pathetic      11      5 1.7880140   disgust
## 12     nasty      13      6 1.7761455   disgust
## 13  horrible      14      7 1.6533329   disgust
## 14  disaster      21     11 1.5990305   disgust
## 15   winning      14      9 1.3050736   disgust
## 16      liar       6      5 1.0103035   disgust
## 17      john      24     21 0.9741134   disgust
## 18 dishonest      36     32 0.9599928   disgust
## 19     dying       6      6 0.8548102   disgust
## 20 terrorism       9      9 0.8547832   disgust</code></pre>
<p>and we can make a graph:</p>
<pre class="r"><code>android_iphone_or %&gt;% inner_join(nrc, by = &quot;word&quot;) %&gt;%
  mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %&gt;%
  mutate(log_or = log(or)) %&gt;%
  filter(Android + iPhone &gt; 10 &amp; abs(log_or)&gt;1) %&gt;%
  mutate(word = reorder(word, log_or)) %&gt;%
  ggplot(aes(word, log_or, fill = log_or &lt; 0)) +
  facet_wrap(~sentiment, scales = &quot;free_x&quot;, nrow = 2) +
  geom_bar(stat=&quot;identity&quot;, show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))</code></pre>
<p><img src="/content/13-content_files/figure-html/log-odds-by-word-1.png" width="100%" /></p>
<p>This is just a simple example of the many analyses one can perform with tidytext.
To learn more, we again recommend the Tidy Text Mining book<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
<div class="fyi">
<p><strong>TRY IT</strong></p>
<p>Project Gutenberg is a digital archive of public domain books. The R package <strong>gutenbergr</strong> facilitates the importation of these texts into R.</p>
<p>You can install and load by typing:</p>
<pre class="r"><code>install.packages(&quot;gutenbergr&quot;)
library(gutenbergr)</code></pre>
<p>You can see the books that are available like this:</p>
<pre class="r"><code>gutenberg_metadata</code></pre>
<ol style="list-style-type: decimal">
<li><p>Use <code>str_detect</code> to find the ID of the novel <em>Pride and Prejudice</em>.</p></li>
<li><p>We notice that there are several versions. The <code>gutenberg_works()</code> function filters this table to remove replicates and include only English language works. Read the help file and use this function to find the ID for <em>Pride and Prejudice</em>.</p></li>
<li><p>Use the <code>gutenberg_download</code> function to download the text for Pride and Prejudice. Save it to an object called <code>book</code>.</p></li>
<li><p>Use the <strong>tidytext</strong> package to create a tidy table with all the words in the text. Save the table in an object called <code>words</code></p></li>
<li><p>We will later make a plot of sentiment versus location in the book. For this, it will be useful to add a column with the word number to the table.</p></li>
<li><p>Remove the stop words and numbers from the <code>words</code> object. Hint: use the <code>anti_join</code>.</p></li>
<li><p>Now use the <code>AFINN</code> lexicon to assign a sentiment value to each word.</p></li>
<li><p>Make a plot of sentiment score versus location in the book and add a smoother.</p></li>
<li><p>Assume there are 300 words per page. Convert the locations to pages and then compute the average sentiment in each page. Plot that average score by page. Add a smoother that appears to go through data.</p></li>
</ol>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://twitter.com/tvaziri/status/762005541388378112/photo/1" class="uri">https://twitter.com/tvaziri/status/762005541388378112/photo/1</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p><a href="http://varianceexplained.org/r/trump-tweets/" class="uri">http://varianceexplained.org/r/trump-tweets/</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><a href="https://www.tidytextmining.com/" class="uri">https://www.tidytextmining.com/</a><a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://www.tidytextmining.com/" class="uri">https://www.tidytextmining.com/</a><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
