<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Course materials | Data Analytics</title>
    <link>https://ssc442.netlify.app/</link>
      <atom:link href="https://ssc442.netlify.app/index.xml" rel="self" type="application/rss+xml" />
    <description>Course materials</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <image>
      <url>https://ssc442.netlify.app/img/social-image.png</url>
      <title>Course materials</title>
      <link>https://ssc442.netlify.app/</link>
    </image>
    
    <item>
      <title>Applied Data Wrangling</title>
      <link>https://ssc442.netlify.app/assignment/12-assignment/</link>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/12-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#this-is-an-extra-lab&#34;&gt;This is an extra lab&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-wrangling-continued&#34;&gt;Data Wrangling Continued…&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;this-is-an-extra-lab&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;This is an extra lab&lt;/h1&gt;
&lt;p&gt;You have already had 11 labs. As mentioned in class numerous times, you will be evaluated based solely on your top &lt;em&gt;ten&lt;/em&gt; scores. Accordingly, many of you will not complete this. There are no bonus points, so only complete this if you need to.&lt;/p&gt;
&lt;div id=&#34;data-wrangling-continued&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Wrangling Continued…&lt;/h2&gt;
&lt;p&gt;You still work for a travel booking website as a data analyst. The hotel has once again asked your company for data on corporate bookings at the hotel via your site. Specifically, they have five corporations that are frequent customers of the hotel, and they want to know who spends the most with them. They’ve asked you to help out. Most of the corporate spending is in the form of room reservations, but there are also parking fees that the hotel wants included in the analysis. Your goal: total up spending by corporation and report the biggest and smallest spenders inclusive of rooms and parking.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You did this already in class, but your boss now has some different data. It’s similar, and your code from before will help, but it has some new wrinkles in it to tackle&lt;/strong&gt;. Here’s your data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Lab11_booking.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;booking.csv&lt;/code&gt;&lt;/a&gt; - Contains the corporation name, the room type, and the dates someone from the corporation stayed at the hotel. It was pulled by an intern who doesn’t understand date-time stamps.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Lab11_roomrates.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;roomrates.csv&lt;/code&gt;&lt;/a&gt; - Contains the price of each room on each day. The Lab 11 version of this data is from our German affiliate, so pay attention to the date format!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Lab11_parking.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;parking.csv&lt;/code&gt;&lt;/a&gt; - Contains the corporations who negotiated free parking for employees. It has been updated.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Parking at the hotel is $60 if you don’t have free parking. This hotel is in California, so everyone drives and parks when they stay.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 1&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;As you did in class, but with your new set of data, total up spending by corporation and report the biggest and smallest spenders inclusive of rooms and parking&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visualize (using &lt;code&gt;ggplot&lt;/code&gt;) each corporation’s spending at the hotel over time and by roomtype. Make one plot with &lt;code&gt;ggplot&lt;/code&gt; that shows this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visualize (using &lt;code&gt;ggplot&lt;/code&gt;) the room rates over time by room type. Can you tell what factors determine room price? Note that we know each corporation gets the same room rate as the others on the same day, so this is about room rates, not corporate spending. Make &lt;strong&gt;two&lt;/strong&gt; total plots, the first showing the room rates over time by room type, and the second explaining some feature of one of the room rates (e.g. when is the double room price high? When is it low?). Using the &lt;code&gt;month(...)&lt;/code&gt; and &lt;code&gt;day(...)&lt;/code&gt; functions from &lt;code&gt;lubridate&lt;/code&gt; will help with figuring out the patterns.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;Congrats on finishing your last lab!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Classification</title>
      <link>https://ssc442.netlify.app/example/11-example/</link>
      <pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/11-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#spam-example&#34;&gt;&lt;code&gt;spam&lt;/code&gt; Example&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#evaluating-classifiers&#34;&gt;Evaluating Classifiers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;This week’s weekly writing is to generate an ROC curve with data from the &lt;code&gt;SAHeart&lt;/code&gt; dataset. It is available as a part of the &lt;code&gt;bestglm&lt;/code&gt; package. We will build toward this exercise in class using a different example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spam-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;code&gt;spam&lt;/code&gt; Example&lt;/h1&gt;
&lt;p&gt;To illustrate the use of logistic regression as a classifier, we will use the &lt;code&gt;spam&lt;/code&gt; dataset from the &lt;code&gt;kernlab&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;kernlab&amp;quot;)
library(kernlab)
data(&amp;quot;spam&amp;quot;)
tibble::as.tibble(spam)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `as.tibble()` was deprecated in tibble 2.0.0.
## Please use `as_tibble()` instead.
## The signature and semantics have changed, see `?as_tibble`.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4,601 × 58
##     make address   all num3d   our  over remove internet order  mail receive
##    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1  0       0.64  0.64     0  0.32  0      0        0     0     0       0   
##  2  0.21    0.28  0.5      0  0.14  0.28   0.21     0.07  0     0.94    0.21
##  3  0.06    0     0.71     0  1.23  0.19   0.19     0.12  0.64  0.25    0.38
##  4  0       0     0        0  0.63  0      0.31     0.63  0.31  0.63    0.31
##  5  0       0     0        0  0.63  0      0.31     0.63  0.31  0.63    0.31
##  6  0       0     0        0  1.85  0      0        1.85  0     0       0   
##  7  0       0     0        0  1.92  0      0        0     0     0.64    0.96
##  8  0       0     0        0  1.88  0      0        1.88  0     0       0   
##  9  0.15    0     0.46     0  0.61  0      0.3      0     0.92  0.76    0.76
## 10  0.06    0.12  0.77     0  0.19  0.32   0.38     0     0.06  0       0   
## # … with 4,591 more rows, and 47 more variables: will &amp;lt;dbl&amp;gt;, people &amp;lt;dbl&amp;gt;,
## #   report &amp;lt;dbl&amp;gt;, addresses &amp;lt;dbl&amp;gt;, free &amp;lt;dbl&amp;gt;, business &amp;lt;dbl&amp;gt;, email &amp;lt;dbl&amp;gt;,
## #   you &amp;lt;dbl&amp;gt;, credit &amp;lt;dbl&amp;gt;, your &amp;lt;dbl&amp;gt;, font &amp;lt;dbl&amp;gt;, num000 &amp;lt;dbl&amp;gt;, money &amp;lt;dbl&amp;gt;,
## #   hp &amp;lt;dbl&amp;gt;, hpl &amp;lt;dbl&amp;gt;, george &amp;lt;dbl&amp;gt;, num650 &amp;lt;dbl&amp;gt;, lab &amp;lt;dbl&amp;gt;, labs &amp;lt;dbl&amp;gt;,
## #   telnet &amp;lt;dbl&amp;gt;, num857 &amp;lt;dbl&amp;gt;, data &amp;lt;dbl&amp;gt;, num415 &amp;lt;dbl&amp;gt;, num85 &amp;lt;dbl&amp;gt;,
## #   technology &amp;lt;dbl&amp;gt;, num1999 &amp;lt;dbl&amp;gt;, parts &amp;lt;dbl&amp;gt;, pm &amp;lt;dbl&amp;gt;, direct &amp;lt;dbl&amp;gt;,
## #   cs &amp;lt;dbl&amp;gt;, meeting &amp;lt;dbl&amp;gt;, original &amp;lt;dbl&amp;gt;, project &amp;lt;dbl&amp;gt;, re &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset, created in the late 1990s at Hewlett-Packard Labs, contains 4601 emails, of which 1813 are considered spam. The remaining are not spam. (Which for simplicity, we might call, ham.) Additional details can be obtained by using &lt;code&gt;?spam&lt;/code&gt; of by visiting the &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/spambase&#34; target=&#34;_blank&#34;&gt;UCI Machine Learning Repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The response variable, &lt;code&gt;type&lt;/code&gt;, is a &lt;strong&gt;factor&lt;/strong&gt; with levels that label each email as &lt;code&gt;spam&lt;/code&gt; or &lt;code&gt;nonspam&lt;/code&gt;. When fitting models, &lt;code&gt;nonspam&lt;/code&gt; will be the reference level, &lt;span class=&#34;math inline&#34;&gt;\(Y = 0\)&lt;/span&gt;, as it comes first alphabetically.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(spam$type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(spam$type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;nonspam&amp;quot; &amp;quot;spam&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many of the predictors (often called features in machine learning) are engineered based on the emails. For example, &lt;code&gt;charDollar&lt;/code&gt; is the number of times an email contains the &lt;code&gt;$&lt;/code&gt; character. Some variables are highly specific to this dataset, for example &lt;code&gt;george&lt;/code&gt; and &lt;code&gt;num650&lt;/code&gt;. (The name and area code for one of the researchers whose emails were used.) We should keep in mind that this dataset was created based on emails send to academic type researcher in the 1990s. Any results we derive probably won’t generalize to modern emails for the general public.&lt;/p&gt;
&lt;p&gt;To get started, we’ll first test-train split the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
# spam_idx = sample(nrow(spam), round(nrow(spam) / 2))
spam_idx = sample(nrow(spam), 1000)
spam_trn = spam[spam_idx, ]
spam_tst = spam[-spam_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve used a somewhat small train set relative to the total size of the dataset. In practice it should likely be larger, but this is simply to keep training time low for illustration and rendering of this document.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_caps = glm(type ~ capitalTotal,
               data = spam_trn, family = binomial)
fit_selected = glm(type ~ edu + money + capitalTotal + charDollar,
                   data = spam_trn, family = binomial)
fit_additive = glm(type ~ .,
                   data = spam_trn, family = binomial)
fit_over = glm(type ~ capitalTotal * (.),
               data = spam_trn, family = binomial, maxit = 50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll fit four logistic regressions, each more complex than the previous. Note that we’re suppressing two warnings. The first we briefly mentioned previously.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, when we receive this warning, we should be highly suspicious of the parameter estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(fit_selected)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)           edu         money  capitalTotal    charDollar 
## -1.1199744712 -1.9837988840  0.9784675298  0.0007757011 11.5772904667&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, the model can still be used to create a classifier, and we will evaluate that classifier on its own merits.&lt;/p&gt;
&lt;p&gt;We also, “suppressed” the warning:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: glm.fit: algorithm did not converge&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In reality, we didn’t actually suppress it, but instead changed &lt;code&gt;maxit&lt;/code&gt; to &lt;code&gt;50&lt;/code&gt;, when fitting the model &lt;code&gt;fit_over&lt;/code&gt;. This was enough additional iterations to allow the iteratively reweighted least squares algorithm to converge when fitting the model.&lt;/p&gt;
&lt;div id=&#34;evaluating-classifiers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluating Classifiers&lt;/h3&gt;
&lt;p&gt;The metric we’ll be most interested in for evaluating the overall performance of a classifier is the &lt;strong&gt;misclassification rate&lt;/strong&gt;. (Sometimes, instead accuracy is reported, which is instead the proportion of correction classifications, so both metrics serve the same purpose.)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Misclass}(\hat{C}, \text{Data}) = \frac{1}{n}\sum_{i = 1}^{n}I(y_i \neq \hat{C}({\bf x_i}))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
I(y_i \neq \hat{C}({\bf x_i})) =
\begin{cases}
  0 &amp;amp; y_i = \hat{C}({\bf x_i}) \\
  1 &amp;amp; y_i \neq \hat{C}({\bf x_i}) \\
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When using this metric on the training data, it will have the same issues as RSS did for ordinary linear regression, that is, it will only go down.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# training misclassification rate
mean(ifelse(predict(fit_caps) &amp;gt; 0, &amp;quot;spam&amp;quot;, &amp;quot;nonspam&amp;quot;) != spam_trn$type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.339&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(ifelse(predict(fit_selected) &amp;gt; 0, &amp;quot;spam&amp;quot;, &amp;quot;nonspam&amp;quot;) != spam_trn$type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.224&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(ifelse(predict(fit_additive) &amp;gt; 0, &amp;quot;spam&amp;quot;, &amp;quot;nonspam&amp;quot;) != spam_trn$type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.066&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(ifelse(predict(fit_over) &amp;gt; 0, &amp;quot;spam&amp;quot;, &amp;quot;nonspam&amp;quot;) != spam_trn$type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.136&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because of this, training data isn’t useful for evaluating, as it would suggest that we should always use the largest possible model, when in reality, that model is likely overfitting. Recall, a model that is too complex will overfit. A model that is too simple will underfit. (We’re looking for something in the middle.)&lt;/p&gt;
&lt;p&gt;To overcome this, we’ll use cross-validation as we did with ordinary linear regression, but this time we’ll cross-validate the misclassification rate. To do so, we’ll use the &lt;code&gt;cv.glm()&lt;/code&gt; function from the &lt;code&gt;boot&lt;/code&gt; library. It takes arguments for the data (in this case training), a model fit via &lt;code&gt;glm()&lt;/code&gt;, and &lt;code&gt;K&lt;/code&gt;, the number of folds. See &lt;code&gt;?cv.glm&lt;/code&gt; for details.&lt;/p&gt;
&lt;p&gt;Previously, for cross-validating RMSE in ordinary linear regression, we used LOOCV. We certainly could do that here. However, with logistic regression, we no longer have the clever trick that would allow use to obtain a LOOCV metric without needing to fit the model &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; times. So instead, we’ll use 5-fold cross-validation. (5 and 10 fold are the most common in practice.) Instead of leaving a single observation out repeatedly, we’ll leave out a fifth of the data.&lt;/p&gt;
&lt;p&gt;Essentially we’ll repeat the following process 5 times:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Randomly set aside a fifth of the data (each observation will only be held-out once)&lt;/li&gt;
&lt;li&gt;Train model on remaining data&lt;/li&gt;
&lt;li&gt;Evaluate misclassification rate on held-out data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The 5-fold cross-validated misclassification rate will be the average of these misclassification rates. By only needing to refit the model 5 times, instead of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; times, we will save a lot of computation time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(boot)
set.seed(1)
cv.glm(spam_trn, fit_caps, K = 5)$delta[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2166961&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.glm(spam_trn, fit_selected, K = 5)$delta[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1587043&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.glm(spam_trn, fit_additive, K = 5)$delta[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08684467&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv.glm(spam_trn, fit_over, K = 5)$delta[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.137&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we’re suppressing warnings again here. (Now there would be a lot more, since were fitting a total of 20 models.)&lt;/p&gt;
&lt;p&gt;Based on these results, &lt;code&gt;fit_caps&lt;/code&gt; and &lt;code&gt;fit_selected&lt;/code&gt; are underfitting relative to &lt;code&gt;fit_additive&lt;/code&gt;. Similarly, &lt;code&gt;fit_over&lt;/code&gt; is overfitting relative to &lt;code&gt;fit_additive&lt;/code&gt;. Thus, based on these results, we prefer the classifier created based on the logistic regression fit and stored in &lt;code&gt;fit_additive&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Going forward, to evaluate and report on the efficacy of this classifier, we’ll use the test dataset. We’re going to take the position that the test data set should &lt;strong&gt;never&lt;/strong&gt; be used in training, which is why we used cross-validation within the training dataset to select a model. Even though cross-validation uses hold-out sets to generate metrics, at some point all of the data is used for training.&lt;/p&gt;
&lt;p&gt;To quickly summarize how well this classifier works, we’ll create a confusion matrix.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;images/confusion.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Confusion Matrix&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It further breaks down the classification errors into false positives and false negatives.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_conf_mat = function(predicted, actual) {
  table(predicted = predicted, actual = actual)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s explicitly store the predicted values of our classifier on the test dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spam_tst_pred = ifelse(predict(fit_additive, spam_tst) &amp;gt; 0,
                       &amp;quot;spam&amp;quot;,
                       &amp;quot;nonspam&amp;quot;)
spam_tst_pred = ifelse(predict(fit_additive, spam_tst, type = &amp;quot;response&amp;quot;) &amp;gt; 0.5,
                       &amp;quot;spam&amp;quot;,
                       &amp;quot;nonspam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous two lines of code produce the same output, that is the same predictions, since&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\eta({\bf x}) = 0 \iff p({\bf x}) = 0.5
\]&lt;/span&gt;
Now we’ll use these predictions to create a confusion matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(conf_mat_50 = make_conf_mat(predicted = spam_tst_pred, actual = spam_tst$type))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          actual
## predicted nonspam spam
##   nonspam    2057  157
##   spam        127 1260&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Prev} = \frac{\text{P}}{\text{Total Obs}}= \frac{\text{TP + FN}}{\text{Total Obs}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(spam_tst$type) / nrow(spam_tst)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   nonspam      spam 
## 0.6064982 0.3935018&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, note that to be a reasonable classifier, it needs to outperform the obvious classifier of simply classifying all observations to the majority class. In this case, classifying everything as non-spam for a test misclassification rate of 0.3935018&lt;/p&gt;
&lt;p&gt;Next, we can see that using the classifier create from &lt;code&gt;fit_additive&lt;/code&gt;, only a total of &lt;span class=&#34;math inline&#34;&gt;\(137 + 161 = 298\)&lt;/span&gt; from the total of 3601 email in the test set are misclassified. Overall, the accuracy in the test set it&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(spam_tst_pred == spam_tst$type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.921133&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other words, the test misclassification is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(spam_tst_pred != spam_tst$type)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.07886698&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This seems like a decent classifier…&lt;/p&gt;
&lt;p&gt;However, are all errors created equal? In this case, absolutely not. The 137 non-spam emails that were marked as spam (false positives) are a problem. We can’t allow important information, say, a job offer, miss our inbox and get sent to the spam folder. On the other hand, the 161 spam email that would make it to an inbox (false negatives) are easily dealt with, just delete them.&lt;/p&gt;
&lt;p&gt;Instead of simply evaluating a classifier based on its misclassification rate (or accuracy), we’ll define two additional metrics, sensitivity and specificity. Note that these are simply two of many more metrics that can be considered. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Sensitivity_and_specificity&#34; target=&#34;_blank&#34;&gt;Wikipedia page for sensitivity and specificity&lt;/a&gt; details a large number of metrics that can be derived form a confusion matrix.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt; is essentially the true positive rate. So when sensitivity is high, the number of false negatives is low.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Sens} = \text{True Positive Rate} = \frac{\text{TP}}{\text{P}} = \frac{\text{TP}}{\text{TP + FN}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we have an &lt;code&gt;R&lt;/code&gt; function to calculate the sensitivity based on the confusion matrix. Note that this function is good for illustrative purposes, but is easily broken. (Think about what happens if there are no “positives” predicted.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sens = function(conf_mat) {
  conf_mat[2, 2] / sum(conf_mat[, 2])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Specificity&lt;/strong&gt; is essentially the true negative rate. So when specificity is high, the number of false positives is low.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Spec} = \text{True Negative Rate} = \frac{\text{TN}}{\text{N}} = \frac{\text{TN}}{\text{TN + FP}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_spec =  function(conf_mat) {
  conf_mat[1, 1] / sum(conf_mat[, 1])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We calculate both based on the confusion matrix we had created for our classifier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sens(conf_mat_50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8892025&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_spec(conf_mat_50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9418498&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recall that we had created this classifier using a probability of &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt; as a “cutoff” for how observations should be classified. Now we’ll modify this cutoff. We’ll see that by modifying the cutoff, &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, we can improve sensitivity or specificity at the expense of the overall accuracy (misclassification rate).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(\bf x) =
\begin{cases}
      1 &amp;amp; \hat{p}({\bf x}) &amp;gt; c \\
      0 &amp;amp; \hat{p}({\bf x}) \leq c
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Additionally, if we change the cutoff to improve sensitivity, we’ll decrease specificity, and vice versa.&lt;/p&gt;
&lt;p&gt;First let’s see what happens when we lower the cutoff from &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(0.1\)&lt;/span&gt; to create a new classifier, and thus new predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spam_tst_pred_10 = ifelse(predict(fit_additive, spam_tst, type = &amp;quot;response&amp;quot;) &amp;gt; 0.1,
                          &amp;quot;spam&amp;quot;,
                          &amp;quot;nonspam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is essentially &lt;em&gt;decreasing&lt;/em&gt; the threshold for an email to be labeled as spam, so far &lt;em&gt;more&lt;/em&gt; emails will be labeled as spam. We see that in the following confusion matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(conf_mat_10 = make_conf_mat(predicted = spam_tst_pred_10, actual = spam_tst$type))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          actual
## predicted nonspam spam
##   nonspam    1583   29
##   spam        601 1388&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unfortunately, while this does greatly reduce false negatives, false positives have almost quadrupled. We see this reflected in the sensitivity and specificity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sens(conf_mat_10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9795342&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_spec(conf_mat_10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7248168&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This classifier, using &lt;span class=&#34;math inline&#34;&gt;\(0.1\)&lt;/span&gt; instead of &lt;span class=&#34;math inline&#34;&gt;\(0.5\)&lt;/span&gt; has a higher sensitivity, but a much lower specificity. Clearly, we should have moved the cutoff in the other direction. Let’s try &lt;span class=&#34;math inline&#34;&gt;\(0.9\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spam_tst_pred_90 = ifelse(predict(fit_additive, spam_tst, type = &amp;quot;response&amp;quot;) &amp;gt; 0.9,
                          &amp;quot;spam&amp;quot;,
                          &amp;quot;nonspam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is essentially &lt;em&gt;increasing&lt;/em&gt; the threshold for an email to be labeled as spam, so far &lt;em&gt;fewer&lt;/em&gt; emails will be labeled as spam. Again, we see that in the following confusion matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(conf_mat_90 = make_conf_mat(predicted = spam_tst_pred_90, actual = spam_tst$type))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          actual
## predicted nonspam spam
##   nonspam    2136  537
##   spam         48  880&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the result we’re looking for. We have far fewer false positives. While sensitivity is greatly reduced, specificity has gone up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sens(conf_mat_90)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6210303&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_spec(conf_mat_90)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.978022&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While this is far fewer false positives, is it acceptable though? Still probably not. Also, don’t forget, this would actually be a terrible spam detector today since this is based on data from a very different era of the internet, for a very specific set of people. Spam has changed a lot since 90s! (Ironically, machine learning is probably partially to blame.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applied Logistic Regression</title>
      <link>https://ssc442.netlify.app/assignment/11-assignment/</link>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/11-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#backstory-and-set-up&#34;&gt;Backstory and Set Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on &lt;strong&gt;Sunday, November 13th.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backstory-and-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Backstory and Set Up&lt;/h2&gt;
&lt;p&gt;You work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.) They’ve given you a dataset on defaults (encoded as the variable &lt;code&gt;y&lt;/code&gt;). You’re going to try to predict this.&lt;/p&gt;
&lt;p&gt;This is as in Lab 11.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bank &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/ajkirkpatrick/FS20/Spring2021/classdata/bank.csv&amp;quot;,
                 header = TRUE,
                 sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s not going to be a whole lot of wind-up here. You should be well-versed in doing these sorts of things by now (if not, look back at the previous lab for sample code).&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 1&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Split the data into an 80/20 train vs. test split. Make sure you explicitly set the seed for replicability, but do not share your seed with others in the class. (We may compare some results across people.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run a series of &lt;strong&gt;logistic regressions&lt;/strong&gt; with between 1 and 4 predictors of your choice (you can use interactions).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create eight total confusion matrices: four by applying your models to the training data, and four by applying your models to the test data. &lt;strong&gt;Briefly discuss your findings.&lt;/strong&gt; How does the error rate, sensitivity, and specificity change as the number of predictors increases?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compare your results from Lab 11 to those of Lab 10 in a few sentences.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonparametric Regression</title>
      <link>https://ssc442.netlify.app/example/10-example/</link>
      <pubDate>Thu, 04 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/10-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#our-data-and-goal&#34;&gt;Our data and goal&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#some-functions&#34;&gt;Some functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-tasks-1&#34;&gt;In-Class Tasks #1&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-task-1-discussion&#34;&gt;In-Class Task #1 Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#automating-models&#34;&gt;Automating models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#loops&#34;&gt;Loops&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-task-2&#34;&gt;In-Class Task #2&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-task-2-discussion&#34;&gt;In-Class Task #2 Discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-rmse-from-the-list&#34;&gt;Getting RMSE from the list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-task-3&#34;&gt;In-Class Task #3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;our-data-and-goal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Our data and goal&lt;/h2&gt;
&lt;p&gt;We want to use the &lt;code&gt;wooldridge::wage2&lt;/code&gt; data on wages to generate and tune a non-parametric model of wages using a regression tree.&lt;/p&gt;
&lt;p&gt;We’ve learned that our RMSE calculations have a hard time with &lt;code&gt;NA&lt;/code&gt;s in the data. So let’s use the &lt;code&gt;skim&lt;/code&gt; output to drop variables with &lt;code&gt;NA&lt;/code&gt; (see &lt;code&gt;n_missing&lt;/code&gt;). Of course, there are other things we can do (impute the &lt;code&gt;NA&lt;/code&gt;s, or make dummies for them), but for now, it’s easiest to drop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wage_clean = wage2 %&amp;gt;%
  dplyr::select(-brthord, -meduc, -feduc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once cleaned, we should be able to use &lt;code&gt;rpart(wage ~ ., data = wage_clean)&lt;/code&gt; and not have any &lt;code&gt;NA&lt;/code&gt;s in our prediction.&lt;/p&gt;
&lt;div id=&#34;some-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some functions&lt;/h3&gt;
&lt;p&gt;These functions are taken from our previous Content and Examples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;in-class-tasks-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In-Class Tasks #1&lt;/h2&gt;
&lt;p&gt;Today in class, you’ll work in groups of 3-4 on live-coding. Your output will be the weekly writing. One person in the group will need to have Rstudio up, be able to share screen, and have the correct packages loaded (&lt;code&gt;caret&lt;/code&gt;, &lt;code&gt;rpart&lt;/code&gt;, and &lt;code&gt;rpart.plot&lt;/code&gt;, plus &lt;code&gt;skimr&lt;/code&gt;). Copy the &lt;code&gt;rmse&lt;/code&gt; and &lt;code&gt;get_rmse&lt;/code&gt; code into a blank R script (you don’t have to use RMarkdown, just use a blank R script and run from there).&lt;/p&gt;
&lt;p&gt;For the first task, all I want you to do is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Estimate a default regression tree on &lt;code&gt;wage_clean&lt;/code&gt; using the default parameters.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;rpart.plot&lt;/code&gt; to vizualize your regression tree, and &lt;em&gt;talk through the interpretation&lt;/em&gt; with each other.&lt;/li&gt;
&lt;li&gt;Calculate the RMSE for your regression tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll cue you back in about 5 minutes. Remember, you can use &lt;code&gt;?wage2&lt;/code&gt; to see the variable names. Make sure you know what variables are showing up in the plot and explaining &lt;code&gt;wage&lt;/code&gt; in your model. You may find something odd at first and may need to drop more variables…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5 minutes&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;in-class-task-1-discussion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In-Class Task #1 Discussion&lt;/h3&gt;
&lt;p&gt;Let’s choose a group to share their plot and discuss the results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;automating-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automating models&lt;/h2&gt;
&lt;p&gt;Let’s talk about a little code shortcut that helps iterate through your model selection.&lt;/p&gt;
&lt;p&gt;First, before, we used &lt;code&gt;list()&lt;/code&gt; to store all of our models. This is because &lt;code&gt;list()&lt;/code&gt; can “hold” anything at all, unlike a &lt;code&gt;matrix&lt;/code&gt;, which is only numeric, or a &lt;code&gt;data.frame&lt;/code&gt; which needs all rows in a column to be the same data type. &lt;code&gt;list()&lt;/code&gt; is also recursive, so each element in a list can be a list. Of lists. Of lists!&lt;/p&gt;
&lt;p&gt;Lists are also really easy to add to iteratively. We can initiate an empty list using &lt;code&gt;myList &amp;lt;- list()&lt;/code&gt;, then we can add things to it. Note that we use the double &lt;code&gt;[&lt;/code&gt; to index:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList &amp;lt;- list()
myList[[&amp;#39;first&amp;#39;]] = &amp;#39;This is the first thing on my list&amp;#39;
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] &amp;quot;This is the first thing on my list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lists let you name the “containers” (much like you can name colums in a &lt;code&gt;data.frame&lt;/code&gt;). Our first one is called “first”. We can add more later:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList[[&amp;#39;second&amp;#39;]] = c(1,2,3)
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] &amp;quot;This is the first thing on my list&amp;quot;
## 
## $second
## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And still more:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList[[&amp;#39;third&amp;#39;]] = data.frame(a = c(1,2,3), b = c(&amp;#39;Albert&amp;#39;,&amp;#39;Alex&amp;#39;,&amp;#39;Alice&amp;#39;))
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] &amp;quot;This is the first thing on my list&amp;quot;
## 
## $second
## [1] 1 2 3
## 
## $third
##   a      b
## 1 1 Albert
## 2 2   Alex
## 3 3  Alice&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a &lt;code&gt;data.frame&lt;/code&gt; in there! We can use &lt;code&gt;lapply&lt;/code&gt; to do something to every element in the list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(myList, length) # the length() function with the first entry being the list element&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] 1
## 
## $second
## [1] 3
## 
## $third
## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get back a list of equal length but each (still-named) container is now the length of the original list’s contents.&lt;/p&gt;
&lt;div id=&#34;loops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loops&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; has a very useful looping function that takes the form:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in c(&amp;#39;first&amp;#39;,&amp;#39;second&amp;#39;,&amp;#39;third&amp;#39;)){
print(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;first&amp;quot;
## [1] &amp;quot;second&amp;quot;
## [1] &amp;quot;third&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, &lt;code&gt;R&lt;/code&gt; is repeating the thing in the loop (&lt;code&gt;print(i)&lt;/code&gt;) with a different value for &lt;code&gt;i&lt;/code&gt; each time. &lt;code&gt;R&lt;/code&gt; repeats only what is &lt;em&gt;inside the curly-brackets&lt;/em&gt;, then when it reaches the close-curly-bracket, it goes back to the top, changes &lt;code&gt;i&lt;/code&gt; to the next element, and repeats.&lt;/p&gt;
&lt;p&gt;We can use this to train our models. First, we clear our list object &lt;code&gt;myList&lt;/code&gt; by setting it equal to an empty list. Then, we loop over some regression tree tuning parameters. First, we have to figure out how to use the loop to set a &lt;em&gt;unique&lt;/em&gt; name for each list container. To do this, we’ll use &lt;code&gt;paste0(&#39;Tuning&#39;,i)&lt;/code&gt; which will result in a character string of &lt;code&gt;Tuning0&lt;/code&gt; when &lt;code&gt;i=0&lt;/code&gt;, &lt;code&gt;Tuning0.01&lt;/code&gt; when &lt;code&gt;i=0.01&lt;/code&gt;, etc.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList &amp;lt;- list()   # resets the list. Otherwise, you&amp;#39;ll just be adding to your old list!

for(i in c(0, 0.01, 0.02)){
  myList[[paste0(&amp;#39;Tuning&amp;#39;,i)]] = rpart(wage ~ ., data = wage_clean, cp = i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you use &lt;code&gt;names(myList)&lt;/code&gt; you’ll see the result of our &lt;code&gt;paste0&lt;/code&gt; naming. If you want to see the plotted results, you can use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(myList[[&amp;#39;Tuning0.01&amp;#39;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/10-example_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;in-class-task-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In-Class Task #2&lt;/h2&gt;
&lt;p&gt;Let’s send you back to your groups. Using the loop method, generate 5 regression trees to explain &lt;code&gt;wage&lt;/code&gt; in &lt;code&gt;wage_clean&lt;/code&gt;. You can iterate through values of &lt;code&gt;cp&lt;/code&gt;, the complexity parameter, or &lt;code&gt;minsplit&lt;/code&gt;, the minimum # of points that have to be in each split.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10 minutes&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;in-class-task-2-discussion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;In-Class Task #2 Discussion&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-rmse-from-the-list&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting RMSE from the list&lt;/h2&gt;
&lt;p&gt;Finally, we want to move towards getting the RMSE for each of these trees. We’ve done this before using &lt;code&gt;lapply&lt;/code&gt;. Let’s introduce a neat coding shortcut, the &lt;code&gt;anonymous function&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myRMSE &amp;lt;- lapply(myList, function(x){
                  get_rmse(x, wage_clean, &amp;#39;wage&amp;#39;)
                  } )

print(myRMSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Tuning0
## [1] 292.5856
## 
## $Tuning0.01
## [1] 359.4592
## 
## $Tuning0.02
## [1] 367.2748&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, it gets us the right answer, but whaaaaaat is going on? Curly brackets? &lt;code&gt;x&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;This is an “anonymous function”, or a function created on the fly. Here’s how it works in &lt;code&gt;lapply&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first argument is the list you want to do something to&lt;/li&gt;
&lt;li&gt;The second argument would usually be the function you want to apply, like &lt;code&gt;get_rmse&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Here, we’re going to ask R to &lt;em&gt;temporarily&lt;/em&gt; create a function that takes one argument, &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is going to be each list element in &lt;code&gt;myList&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Think of it as a loop:
&lt;ul&gt;
&lt;li&gt;Take the first element of &lt;code&gt;myList&lt;/code&gt; and refer to it as &lt;code&gt;x&lt;/code&gt;. Run the function.&lt;/li&gt;
&lt;li&gt;Then it’ll take the second element of &lt;code&gt;myList&lt;/code&gt; and refer to it as &lt;code&gt;x&lt;/code&gt; and run the function.&lt;/li&gt;
&lt;li&gt;Repeat until all elements of &lt;code&gt;x&lt;/code&gt; have been used.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Once the anonymous function has been applied to &lt;code&gt;x&lt;/code&gt;, the result is passed back and saved as the new element of the list output, always in the same position from where the &lt;code&gt;x&lt;/code&gt; was taken.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So you can think of it like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x = myList[[1]]
myList[[1]] = get_rmse(x, wage_clean, &amp;#39;wage&amp;#39;)

x = myList[[2]]
myList[[2]] = get_rmse(x, wage_clean, &amp;#39;wage&amp;#39;)

x = myList[[3]]
myList[[3]] = get_rmse(x, wage_clean, &amp;#39;wage&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can refer to &lt;code&gt;myList&lt;/code&gt; by name:
- &lt;code&gt;myList[[&#39;Tuning0.01&#39;]]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Or you can refer to &lt;code&gt;myList&lt;/code&gt; by the index:
- &lt;code&gt;myList[[2]]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Just like you can refer to &lt;code&gt;data.frame&lt;/code&gt; columns by name or by index.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-class-task-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In-Class Task #3&lt;/h2&gt;
&lt;p&gt;Let’s send you to work one more time, and use &lt;code&gt;lapply&lt;/code&gt; to get a list of your RMSE’s (one for each of your models). Note that we are not yet splitting into &lt;code&gt;test&lt;/code&gt; and &lt;code&gt;train&lt;/code&gt; (which you &lt;strong&gt;will&lt;/strong&gt; need to do on your lab assignment).&lt;/p&gt;
&lt;p&gt;Once you have your list, &lt;strong&gt;create the plot of RMSEs&lt;/strong&gt; similar to the one we looked at in Content this week. Note: you can use &lt;code&gt;unlist(myRMSE)&lt;/code&gt; to get a numeric vector of the RMSE’s (as long as all of the elements in &lt;code&gt;myRMSE&lt;/code&gt; are numeric). Then, it’s a matter of plotting either with base &lt;code&gt;plot&lt;/code&gt; or with &lt;code&gt;ggplot&lt;/code&gt; (if you use &lt;code&gt;ggplot&lt;/code&gt; you’ll have to &lt;code&gt;tidy&lt;/code&gt; the RMSE by adding the index column or naming the x-axis).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remaining time&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Model Evaluation</title>
      <link>https://ssc442.netlify.app/assignment/10-assignment/</link>
      <pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/10-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#backstory-and-set-up&#34;&gt;Backstory and Set Up&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; knitted document. Submit this to D2L by 11:59 PM Eastern Time on Sunday, November 7.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backstory-and-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Backstory and Set Up&lt;/h2&gt;
&lt;p&gt;You work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.) They’ve given you a dataset on defaults (encoded as the variable &lt;code&gt;y&lt;/code&gt;). You’re going to try to predict this.&lt;/p&gt;
&lt;p&gt;This is some new data. The snippet below loads it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bank &amp;lt;- read.table(&amp;quot;https://raw.githubusercontent.com/ajkirkpatrick/FS20/Spring2021/classdata/bank.csv&amp;quot;,
                 header = TRUE,
                 sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s not going to be a whole lot of wind-up here. You should be well-versed in doing these sorts of things by now (if not, look back at the previous lab for sample code).&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 1&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Split the data into an 80/20 train vs. test split. Make sure you explicitly set the seed for replicability, but do not share your seed with others in the class. (We may compare some results across people.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run a series of KNN models with &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; ranging from 2 to 100. (You need not do every &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; between 2 and 100, but you can easily write a short function to do this; see the Content tab).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a chart plotting the model complexity as the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis variable and RMSE as the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis variable for &lt;strong&gt;both&lt;/strong&gt; the training and test data. What do you think is the optimal &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Illustrating Bias vs. Variance</title>
      <link>https://ssc442.netlify.app/example/09-example/</link>
      <pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/09-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#review-and-clarify&#34;&gt;Review and Clarify&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#illustration-of-bias-vs.-variance&#34;&gt;Illustration of Bias vs. Variance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-quick-bit-of-r-code-to-help-with-todays-example&#34;&gt;A quick bit of R code to help with todays example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#todays-example&#34;&gt;Today’s Example&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simulation&#34;&gt;Simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#heres-the-code&#34;&gt;Here’s the code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;review-and-clarify&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Review and Clarify&lt;/h2&gt;
&lt;p&gt;Bias and Variance are tricky subjects. Hopefully the illustrations from the content are helpful, but it’s understandable if these ideas are a bit opaque. Let’s talk through a few things based on commonly asked questions related to: Content 9.&lt;/p&gt;
&lt;div id=&#34;illustration-of-bias-vs.-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Illustration of Bias vs. Variance&lt;/h3&gt;
&lt;p&gt;Bias is about how close you are on average to the correct answer. Variance is about how scattered your estimates would be if you repeated your experiment with new data.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://www.machinelearningplus.com/wp-content/uploads/2020/10/output_31_0.png&#34; alt=&#34;Image from MachineLearningPlus.com&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Image from MachineLearningPlus.com
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We care about these things because we usually only have our one dataset (when we’re not creating simulated data, that is), but need to know something about how bias and variance tend to look when we change our model complexity.&lt;/p&gt;
&lt;div id=&#34;deriving-bias-and-variance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Deriving Bias and Variance&lt;/h4&gt;
&lt;p&gt;For this section, recall our model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y = f(x) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This tells us that some of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; can be predicted by the &lt;strong&gt;true&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, and some is just noise &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. In our simulation from last week, &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x^2\)&lt;/span&gt;, so &lt;span class=&#34;math inline&#34;&gt;\(y = x^2 + \epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We want to predict &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We call our prediction &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Our best guess for &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; is our model. It might be from a linear regression with 1, 2, 9, 15, etc. predictors or interactions of predictors. It might be from a k-nearest-neighbors estimation with &lt;code&gt;k = 4&lt;/code&gt;. It might be from a regression tree with &lt;code&gt;cp = .1&lt;/code&gt; and &lt;code&gt;minsplit=2&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even when we really nail &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; (which means &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x) = f(x)\)&lt;/span&gt;), there is &lt;em&gt;still&lt;/em&gt; error in our prediction because of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y \neq \hat{y}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we think of two different measures of error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
EPE = E[(y - \hat{y})^2] =
\underbrace{\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]}_\textrm{reducible error - MSE from imperfect model} +
\underbrace{\mathbb{V}_{Y \mid X} \left[ Y \mid X = x \right]}_{\textrm{irreducible error from }\epsilon}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MSE(f(x), \hat{f}(x)) = E_{\mathcal{D}}\left[\left(f(x) - \hat{f}(x)\right)^2\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Some of you asked about this equation from last time that decomposed our MSE:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE}\left(f(x), \hat{f}(x)\right) =
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] =
\underbrace{\left(f(x) - \mathbb{E} \left[ \hat{f}(x) \right]  \right)^2}_{\text{bias}^2 \left(\hat{f}(x) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}(x) - \mathbb{E} \left[ \hat{f}(x) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}(x) \right)}
\]&lt;/span&gt;
This can be derived by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] &amp;amp;=&amp;amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)] + E[\hat{f}(x)] - \hat{f}(x)\right)^2 \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)]\right)^2 + \left(E[\hat{f}(x)] - \hat{f}(x)\right)^2 + 2\left(f(x) - E[\hat{f}(x)]\right) \left(E[\hat{f}(x)] - \hat{f}(x)\right) \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)]\right)^2\right] + \mathbb{E}_{\mathcal{D}} \left[\left(E[\hat{f}(x)] - \hat{f}(x)\right)^2\right] +  \mathbb{E}_{\mathcal{D}} \left[2\left(f(x) - E[\hat{f}(x)]\right) \left(E[\hat{f}(x)] - \hat{f}(x)\right) \right] \\
&amp;amp;=&amp;amp; \left(f(x) - E[\hat{f}(x)]\right)^2 + Var\left(\hat{f}(x)\right) + 0
\end{eqnarray*}
\]&lt;/span&gt;
Let’s talk about what’s in this equation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MSE is the Mean Squared Error between &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;It does not have the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; in it&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;It is an expectation over all the possible &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; draws of the data we could have
&lt;ul&gt;
&lt;li&gt;Because of this &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}\left[\hat{f}(x)\right]\)&lt;/span&gt; can move out of the expectation. This lets us cancel that last term with the “2” in it.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main takeaway is that, even given the error, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;, we &lt;em&gt;still&lt;/em&gt; have additional error coming from our inability to perfectly get &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x) = f(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-quick-bit-of-r-code-to-help-with-todays-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A quick bit of R code to help with todays example&lt;/h2&gt;
&lt;p&gt;We saw before the usefulness of having a list&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList = list()
myList[[&amp;#39;thisThing&amp;#39;]] = c(1,2,3)
myList[[&amp;#39;thisOtherThing&amp;#39;]] = c(&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;,&amp;#39;C&amp;#39;,&amp;#39;D&amp;#39;)
myList&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $thisThing
## [1] 1 2 3
## 
## $thisOtherThing
## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot; &amp;quot;D&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked really well for holding results from models since we could name the things in the list. But if we put it into a loop:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}

print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $I
## [1] &amp;quot;This loop is on I&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened? Every time we used the loop, it re-initiated the list, so we only get the last result!&lt;/p&gt;
&lt;p&gt;So what we want is to create the list &lt;strong&gt;if&lt;/strong&gt; it doesn’t exist, and add to it afterwards. We can do that with &lt;code&gt;exists(&#39;myList&#39;)&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  if(!exists(&amp;#39;myList&amp;#39;)) myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $I
## [1] &amp;quot;This loop is on I&amp;quot;
## 
## $G
## [1] &amp;quot;This loop is on G&amp;quot;
## 
## $H
## [1] &amp;quot;This loop is on H&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re almost there. It turns out, we have our original &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; in there left over from the previous creation of the list. That’s why its out of order. What we want to do is start with a fresh, clean list. If we run &lt;code&gt;rm(myList)&lt;/code&gt;, the old list will no longer exist, and *our code will create a fresh one when we run it again!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(myList)
for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  if(!exists(&amp;#39;myList&amp;#39;)) myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $G
## [1] &amp;quot;This loop is on G&amp;quot;
## 
## $H
## [1] &amp;quot;This loop is on H&amp;quot;
## 
## $I
## [1] &amp;quot;This loop is on I&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’re going to use this in your breakout rooms today. You’re going to be asked to run some code that stores a plot in a list. To reset the list that stores things, just use &lt;code&gt;rm(listName)&lt;/code&gt; (where &lt;code&gt;listName&lt;/code&gt; is the name of the list).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;todays-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Today’s Example&lt;/h2&gt;
&lt;p&gt;Our goal today is to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;See the code that produced this week’s Content&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why? Because it helps to illustrate the &lt;em&gt;true&lt;/em&gt; sources of noise in the data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See what larger sample sizes and higher/lower irreducible error does to our Bias vs. Variance tradeoff.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simulation&lt;/h3&gt;
&lt;p&gt;We will use the exact code from Content 9, which I have reproduced here. I have removed the in-between parts with notation so we can focus on the example. I have &lt;strong&gt;copied all of the relevant code into one chunk down at the bottom as well&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We’ll need the following libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(patchwork)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here, I’ve made a little change to Content 9’s code so we can play with sample size &lt;code&gt;NN&lt;/code&gt; and the SD of the irreducible Bayes error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NN = 100   #----&amp;gt; In class, we will change this to
#                 see how our results change in response
SD.of.Bayes.Error = .75   #-----&amp;gt; This, too, will change.

# Note that both of these are used in the next chunk(s) to generate data.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;begin-content-9-code-here&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Begin Content 9 code here:&lt;/h4&gt;
&lt;p&gt;We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f = function(x) {
  x ^ 2
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  data.frame(x, y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To completely specify the data generating process, we have made more model assumptions than simply &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[Y \mid X = x] = x^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{V}[Y \mid X = x] = \sigma ^ 2\)&lt;/span&gt;. In particular,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from a uniform distribution over &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; are independent.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from the conditional normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using this setup, we will generate datasets, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, with a sample size &lt;span class=&#34;math inline&#34;&gt;\(NN\)&lt;/span&gt; and fit four models.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\texttt{predict(fit0, x)} &amp;amp;= \hat{f}_0(x) = \hat{\beta}_0\\
\texttt{predict(fit1, x)} &amp;amp;= \hat{f}_1(x) = \hat{\beta}_0 + \hat{\beta}_1 x \\
\texttt{predict(fit2, x)} &amp;amp;= \hat{f}_2(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \\
\texttt{predict(fit9, x)} &amp;amp;= \hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
sim_data = get_sim_data(f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/09-example_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;p&gt;We will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates for &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; given by these four models at the point &lt;span class=&#34;math inline&#34;&gt;\(x = 0.90\)&lt;/span&gt;. We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile all of the results:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/09-example_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Try it!&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Set &lt;code&gt;NN&lt;/code&gt; = 100, the value we used in our Content 9 lecture. The value is set in one of the first code chunks. Step through the code to get your final plot and make sure it looks like the plot in Content 9. I changed the code to use ggplot (easier to save output), so the formatting and colors will be different - that’s OK, we want to get the same results, not copy the layout of the plot. Note that at the end of the code, a list is created that will hold all of your results. In case you need to clear this list, &lt;code&gt;rm(FinalResults)&lt;/code&gt; will do so and the code will initate a new blank list to hold subsequent results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set &lt;code&gt;NN&lt;/code&gt; to a larger number. Usually, more data means more precise predictions. Run your code again stepping through it, until you get to this plot. Note that at the end of the code provided, there is a list that aggregates your results. &lt;strong&gt;Repeat this&lt;/strong&gt; with a 3rd, even larger value for &lt;code&gt;NN&lt;/code&gt;. Don’t go much beyond 50,000 or it’ll take too long. Your &lt;code&gt;FinalResults&lt;/code&gt; list should have 3 elements in it. Use &lt;code&gt;wrap_plots(FinalResults, nrow = 1)&lt;/code&gt; to see all 3 side-by-side.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, change the &lt;code&gt;SD.of.Bayes.Error&lt;/code&gt; value to make it higher or lower. Remember, this is the &lt;em&gt;irreducible&lt;/em&gt; error. Run your code again with your first, second, and third different value for sample size &lt;code&gt;NN&lt;/code&gt;. You should have 6 plots in your &lt;code&gt;FinalResults&lt;/code&gt; list - 3 from before, and 3 more with the new SD of Bayes Error. Use &lt;code&gt;wrap_plots&lt;/code&gt; with the right number of rows to see a 2x3 grid of the results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Usually we think larger sample sizes and lower error lead to better overall prediction. Do we see any change in the bias vs. tradeoff relationship with lower/higher sample size &lt;code&gt;NN&lt;/code&gt; and lower/higher SD of Bayes Error?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;heres-the-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Here’s the code&lt;/h3&gt;
&lt;p&gt;I’ve merged all of the code together for you here. Copy this into a new .R script - you don’t need to use a full Markdown.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(patchwork)



NN = 100   #----&amp;gt; In class, we will change this to
#                 see how our results change in response
SD.of.Bayes.Error = 0.75   #-----&amp;gt; This, too, will change.

# Note that both of these are used in the next chunk(s) to generate data.







f = function(x) {
  x ^ 2
}



get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  data.frame(x, y)
}



set.seed(1)
sim_data = get_sim_data(f)



fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)





set.seed(42)
plot(y ~ x, data = sim_data, col = &amp;quot;grey&amp;quot;, pch = 20,
     main = &amp;quot;Four Polynomial Models fit to a Simulated Dataset&amp;quot;)

grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = &amp;quot;black&amp;quot;, lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = &amp;quot;dodgerblue&amp;quot;,  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &amp;quot;firebrick&amp;quot;,   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &amp;quot;springgreen&amp;quot;, lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = &amp;quot;darkorange&amp;quot;,  lwd = 2, lty = 5)

legend(&amp;quot;topleft&amp;quot;,
       c(&amp;quot;y ~ 1&amp;quot;, &amp;quot;y ~ poly(x, 1)&amp;quot;, &amp;quot;y ~ poly(x, 2)&amp;quot;,  &amp;quot;y ~ poly(x, 9)&amp;quot;, &amp;quot;truth&amp;quot;),
       col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;, &amp;quot;black&amp;quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)



set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)



for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}



predictions.proc = (predictions)
colnames(predictions.proc) = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;9&amp;quot;)
predictions.proc = as.data.frame(predictions.proc)

tall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)

## Here, you can save your ggplot output
FinalPlot &amp;lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) +
                      geom_boxplot() +
                      geom_jitter(alpha = .5) +
                      geom_hline(yintercept = f(x = .90)) +
                      labs(col = &amp;#39;Model&amp;#39;, x = &amp;#39;Model&amp;#39;, y = &amp;#39;Prediction&amp;#39;, title = paste0(&amp;#39;Bias v Var - Sample Size: &amp;#39;,NN), subtitle = paste0(&amp;#39;SD of Bayes Err: &amp;#39;,SD.of.Bayes.Error)) +
                      theme_bw()



FinalPlot


## This is going to aggregate your results for you:
if(!exists(&amp;#39;FinalResults&amp;#39;)) FinalResults = list()

FinalResults[[paste0(&amp;#39;finalPlot.NN.&amp;#39;,NN,&amp;#39;.SDBayes.&amp;#39;,SD.of.Bayes.Error)]] = FinalPlot

#
# boxplot(value ~ key, data = tall_predictions, border = &amp;quot;darkgrey&amp;quot;, xlab = &amp;quot;Polynomial Degree&amp;quot;, ylab = &amp;quot;Predictions&amp;quot;,
#         main = &amp;quot;Simulated Predictions for Polynomial Models&amp;quot;)
# grid()
# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = &amp;quot;jitter&amp;quot;, jitter = 0.15, pch = 1, col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;))
# abline(h = f(x = 0.90), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to output &lt;em&gt;whatever is in your list&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This not run automatically.
## To plot the whole list of ggplot objects in FinalResults:
wrap_plots(FinalResults, nrow = 2, guides = &amp;#39;collect&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Advanced Model Building</title>
      <link>https://ssc442.netlify.app/assignment/08-assignment/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/08-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#backstory-and-set-up&#34;&gt;Backstory and Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-models&#34;&gt;Linear Models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#assesing-model-accuracy&#34;&gt;Assesing Model Accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-complexity&#34;&gt;Model Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#test-train-split&#34;&gt;Test-Train Split&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-flexibility-to-linear-models&#34;&gt;Adding Flexibility to Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;READ THIS CAREFULLY&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The content below describes both Lab 8 and Lab 9. Lab 8 is Exercise 1; Lab 9 is Exercise 2. Also, you may find some other tasks in the text…&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, October 24th for Lab 8; turn in Lab 9 by 11:59 PM Eastern Time on Sunday, October 31st (a very spooky due date!)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backstory-and-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Backstory and Set Up&lt;/h2&gt;
&lt;p&gt;You still work for Zillow as a junior analyst (sorry). But you’re hunting a promotion. Your job is to present some more advanced predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.&lt;/p&gt;
&lt;p&gt;As always, let’s load the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Ames &amp;lt;- read.table(&amp;quot;https://ssc442.netlify.app/projects/data/ames.csv&amp;quot;,
                 header = TRUE,
                 sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-cleaning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Oh, the Ames data yet again. It’s given us lots of trouble. Many of you have found a few variables (columns) that should be avoided. The main problem is that some columns have only one value in them, or they have only &lt;code&gt;NA&lt;/code&gt; and one value, so once &lt;code&gt;lm(...)&lt;/code&gt; drops the &lt;code&gt;NA&lt;/code&gt; rows, they are left with only one value. Linear regression by OLS does not like variables that don’t vary! So, let’s be systematic about figuring out which columns in our data are to be avoided.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;skimr&lt;/code&gt; package is very helpful for seeing what our data contains. Install it, and then use &lt;code&gt;skim(Ames)&lt;/code&gt; directly in the console (we’re just looking at data at the moment). Take a look at the “complete rate” column - this tells us the fraction of observations in that column that are &lt;code&gt;NA&lt;/code&gt;. If it’s very small (see &lt;code&gt;Alley&lt;/code&gt;), then that variable will be problematic. The “n_unique” column tells us if there are few or many different values - a “1” in “n_unique” is definitely going to be a problem!&lt;/p&gt;
&lt;p&gt;You can make a note of those columns that have extremely low “complete rates” and drop them to start off. There are about 5-6 of them that will cause an error if we include them in a regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Models&lt;/h2&gt;
&lt;p&gt;When exploring linear models in other classes, we often emphasize asymptotic results under distributional assumptions. That is, we make assumptions about the model in order to derive properties of large samples. This general approach is useful for creating and performing hypothesis tests. Frequently, when developing a linear regression model, part of the goal is to &lt;strong&gt;explain&lt;/strong&gt; a relationship. However, this isn’t always the case. And it’s often not a valid approach, as we discussed in this week’s content.&lt;/p&gt;
&lt;p&gt;So, we will ignore much of what we have learned in other classes (sorry, EC420) and instead simply use regression as a tool to &lt;strong&gt;predict&lt;/strong&gt;. Instead of a model which supposedly explains relationships, we seek a model which minimizes &lt;strong&gt;errors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To discuss linear models in the context of prediction, we return to the &lt;code&gt;Ames&lt;/code&gt; data. Accordingly, you should utilize some of the early code from Lab 2 to hasten your progress in this lab.&lt;/p&gt;
&lt;div id=&#34;assesing-model-accuracy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assesing Model Accuracy&lt;/h3&gt;
&lt;p&gt;There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}(\hat{f}, \text{Data}) = \sqrt{\frac{1}{n}\displaystyle\sum_{i = 1}^{n}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For a linear model , the estimate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;, is given by the fitted regression line.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{y}({\bf{x}_i}) = \hat{f}({\bf{x}_i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can write an &lt;code&gt;R&lt;/code&gt; function that will be useful for performing this calculation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-complexity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Complexity&lt;/h3&gt;
&lt;p&gt;Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We write a simple &lt;code&gt;R&lt;/code&gt; function to extract this information from a model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_complexity = function(model) {
  length(coef(model)) - 1
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When deciding how complex of a model to use, we can utilize two techniques: &lt;em&gt;forward selection&lt;/em&gt; or &lt;em&gt;backward selection&lt;/em&gt;. Forward selection means that we start with the simplest model (with a single predictor) and then add one at a time until we decide to stop. Backward selection means that we start with the most complex model (with every available predictor) and then remove one at a time until we decide to stop. There are many criteria for “when to stop”. Below, we’ll try to give you some intuition on the model-building process.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 1&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Load the &lt;code&gt;Ames&lt;/code&gt; data. Using &lt;code&gt;skimr::skim&lt;/code&gt;, find the variables that have a complete rate of below 60% and drop them. 60% isn’t a magic number by any means, the “right” number is entirely dependent on your data. It is always standard practice to document the fields you have dropped from the data, so make sure you state which variables have been dropped.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take a look at &lt;code&gt;Utilities&lt;/code&gt;. Use the &lt;code&gt;table&lt;/code&gt; function to see a tabulation of the values of &lt;code&gt;Utilities&lt;/code&gt;. Do you see why this field is not likely to be useful to us, or even problematic?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using &lt;strong&gt;forward selection&lt;/strong&gt; (that is, select one variable, then select another) create a series of models up to complexity length 15. You may use any variable within the dataset, including categorical variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a chart plotting the model complexity as the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis variable and RMSE as the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis variable. Describe any patterns you see. Do you think you should use the full-size model? Why or why not? What criterion are you using to make this statement?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;test-train-split&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Test-Train Split&lt;/h3&gt;
&lt;p&gt;There is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts: it is essentially cheating. As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down—or, in very specific cases where a new predictor is completely uncorrelated with the target, stay the same. This might seem to suggest that in order to predict well, we should use the largest possible model. However, in reality we have fit to a specific dataset, but as soon as we see new data, a large model may (in fact) predict poorly. This is called &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The most common approach to overfitting is to take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the &lt;strong&gt;training&lt;/strong&gt; data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the &lt;strong&gt;test&lt;/strong&gt; data. Test data should &lt;em&gt;never&lt;/em&gt; be used to train a model—its pupose is to evaluate the fitted model once you’ve settled on something.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here we use the &lt;code&gt;sample()&lt;/code&gt; function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the &lt;code&gt;set.seed()&lt;/code&gt; function to allow use to reproduce the same random split each time we perform this analysis. Sometimes we don’t want to do this; if we want to run lots of independent splits, then we do not need to set the initial seed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(9)
num_obs = nrow(Ames)

train_index = sample(num_obs, size = trunc(0.50 * num_obs))
train_data = Ames[train_index, ]
test_data = Ames[-train_index, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will look at two measures that assess how well a model is predicting: &lt;strong&gt;train RMSE&lt;/strong&gt; and &lt;strong&gt;test RMSE&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_\text{Train} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Tr}}}\sum_{i \in \text{Train}}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Tr}\)&lt;/span&gt; is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_{\text{Test}} = \text{RMSE}(\hat{f}, \text{Test Data}) = \sqrt{\frac{1}{n_{\text{Te}}}\sum_{i \in \text{Test}} \left ( y_i - \hat{f}(\bf{x}_i) \right ) ^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Te}\)&lt;/span&gt; is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict &lt;strong&gt;in general&lt;/strong&gt;, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.&lt;/p&gt;
&lt;p&gt;We will start with the simplest possible linear model, that is, a model with no predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(SalePrice ~ 1, data = train_data)
get_complexity(fit_0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
sqrt(mean((train_data$SalePrice - predict(fit_0, train_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80875.98&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
sqrt(mean((test_data$SalePrice - predict(fit_0, test_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77928.62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
rmse(actual = train_data$SalePrice, predicted = predict(fit_0, train_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80875.98&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
rmse(actual = test_data$SalePrice, predicted = predict(fit_0, test_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77928.62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = train_data, response = &amp;quot;SalePrice&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80875.98&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = test_data, response = &amp;quot;SalePrice&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77928.62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Apply this basic function with different arguments. Do you understand how we’ve nested functions within functions?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Define a total of five models using the first five models you fit in Exercise 1. Define these as &lt;code&gt;fit_1&lt;/code&gt; through &lt;code&gt;fit_5&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-flexibility-to-linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding Flexibility to Linear Models&lt;/h3&gt;
&lt;p&gt;Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting. To better understand the relationship between train RMSE, test RMSE, and model complexity, we’ll explore the results from Exercise 1.&lt;/p&gt;
&lt;p&gt;Hopefully, you tried the in-line excercise above. If so, we can create a list of the models fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then obtain train RMSE, test RMSE, and model complexity for each. In doing so, we’ll introduce a handy function from &lt;code&gt;R&lt;/code&gt; called &lt;code&gt;sapply()&lt;/code&gt;. You can likely intuit what it does by looking at the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_rmse = sapply(model_list, get_rmse, data = train_data, response = &amp;quot;SalePrice&amp;quot;)
test_rmse = sapply(model_list, get_rmse, data = test_data, response = &amp;quot;SalePrice&amp;quot;)
model_complexity = sapply(model_list, get_complexity)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Run &lt;code&gt;?sapply()&lt;/code&gt; to understand what are valid arguments to the function.&lt;/p&gt;
&lt;p&gt;Once you’ve done this, you’ll notice the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is the same as the apply command above

test_rmse = c(get_rmse(fit_1, test_data, &amp;quot;SalePrice&amp;quot;),
              get_rmse(fit_2, test_data, &amp;quot;SalePrice&amp;quot;),
              get_rmse(fit_3, test_data, &amp;quot;SalePrice&amp;quot;),
              get_rmse(fit_4, test_data, &amp;quot;SalePrice&amp;quot;),
              get_rmse(fit_5, test_data, &amp;quot;SalePrice&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the results. If you execute the code below, you’ll see the train RMSE in blue, while the test RMSE is given in orange.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model_complexity, train_rmse, type = &amp;quot;b&amp;quot;,
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02,
              max(c(train_rmse, test_rmse)) + 0.02),
     col = &amp;quot;dodgerblue&amp;quot;,
     xlab = &amp;quot;Model Size&amp;quot;,
     ylab = &amp;quot;RMSE&amp;quot;)
lines(model_complexity, test_rmse, type = &amp;quot;b&amp;quot;, col = &amp;quot;darkorange&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also summarize the results as a table. &lt;code&gt;fit_1&lt;/code&gt; is the least flexible, and &lt;code&gt;fit_5&lt;/code&gt; is the most flexible. If we were to do this (see the exercise below) we would see that Train RMSE decreases as flexibility increases forever. However, this may not be the case for the Test RMSE.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;26%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Train RMSE&lt;/th&gt;
&lt;th&gt;Test RMSE&lt;/th&gt;
&lt;th&gt;Predictors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;RMSE&lt;span class=&#34;math inline&#34;&gt;\(_{\text{train}}\)&lt;/span&gt; for model 1&lt;/td&gt;
&lt;td&gt;RMSE&lt;span class=&#34;math inline&#34;&gt;\(_{\text{test}}\)&lt;/span&gt; for model 1&lt;/td&gt;
&lt;td&gt;put predictors here&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;…&lt;/td&gt;
&lt;td&gt;…&lt;/td&gt;
&lt;td&gt;….&lt;/td&gt;
&lt;td&gt;…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;RMSE&lt;span class=&#34;math inline&#34;&gt;\(_{\text{train}}\)&lt;/span&gt; for model 5&lt;/td&gt;
&lt;td&gt;RMSE&lt;span class=&#34;math inline&#34;&gt;\(_{\text{train}}\)&lt;/span&gt; for model 5&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; predictors&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To summarize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Underfitting models:&lt;/strong&gt; In general &lt;em&gt;High&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overfitting models:&lt;/strong&gt; In general &lt;em&gt;Low&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; Then a model is underfitting if there exists a more complex model with lower Test RMSE.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(AKA Lab 9)&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Make a table exactly like the table above for the 15 models you fit in Exercise 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;This question should be the most time-consuming question.&lt;/strong&gt; Using any method you choose and any number of regressors, predict &lt;code&gt;SalePrice&lt;/code&gt;. Calculate the Train and Test RMSE. Your goal is to have a lower Test RMSE than others in the class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In a short paragraph, describe the resulting model. Discuss how you arrived at this model, what interactions you’re using (if any) and how confident you are that your prediction will perform well, relative to other people in the class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Difficult; extra credit:&lt;/strong&gt; Visualize your final model in a sensible way and provide a two-paragraph interpretation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;A final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that &lt;strong&gt;predicted&lt;/strong&gt; well, and paid no attention to a model for &lt;strong&gt;explaination&lt;/strong&gt;. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Note that sometimes the terms &lt;em&gt;evaluation set&lt;/em&gt; and &lt;em&gt;test set&lt;/em&gt; are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. We often see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next section. We will discuss how we can help create this U-shape much later. Also, we might intuitively expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get (un)lucky and this will not be true.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The labels of under and overfitting are &lt;em&gt;relative&lt;/em&gt; to the best model we see. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Model Building</title>
      <link>https://ssc442.netlify.app/assignment/07-assignment/</link>
      <pubDate>Tue, 12 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/07-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#backstory-and-set-up&#34;&gt;Backstory and Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-a-model&#34;&gt;Building a Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, October 17th.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This week’s lab will extend last week’s lab. The introduction is a verbatim repeat of that lab.&lt;/p&gt;
&lt;div id=&#34;backstory-and-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Backstory and Set Up&lt;/h2&gt;
&lt;p&gt;You have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.&lt;/p&gt;
&lt;p&gt;First, let’s load the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ameslist &amp;lt;- read.table(&amp;quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv&amp;quot;,
                 header = TRUE,
                 sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building a Model&lt;/h2&gt;
&lt;p&gt;We’re now ready to start playing with a model. We will start by using the &lt;code&gt;lm()&lt;/code&gt; function to fit a simple linear regression
model, with &lt;code&gt;SalePrice&lt;/code&gt; as the response and lstat as the predictor.&lt;/p&gt;
&lt;p&gt;Recall that the basic &lt;code&gt;lm()&lt;/code&gt; syntax is &lt;code&gt;lm(y∼x,data)&lt;/code&gt;, where &lt;code&gt;y&lt;/code&gt; is the &lt;strong&gt;response&lt;/strong&gt;, &lt;code&gt;x&lt;/code&gt; is the &lt;strong&gt;predictor&lt;/strong&gt;, and &lt;code&gt;data&lt;/code&gt; is the data set in which these two variables are kept. Let’s quickly run this with two variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit = lm(SalePrice ~ GrLivArea)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This yields:
&lt;code&gt;Error in eval(expr, envir, enclos) : Object &#34;SalePrice&#34; not found&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This command causes an error because &lt;code&gt;R&lt;/code&gt; does not know where to find the variables. We can fix this by attaching the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Ames)
lm.fit = lm(SalePrice ~ GrLivArea)
# Alternatively...
lm.fit = lm(SalePrice ~ GrLivArea, data=Ames)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next line tells &lt;code&gt;R&lt;/code&gt; that the variables are in the object known as &lt;code&gt;Ames&lt;/code&gt;. If you haven’t created this object yet (as in the previous lab) you’ll get an error at this stage. But once we attach &lt;code&gt;Ames&lt;/code&gt;, the first line works fine because &lt;code&gt;R&lt;/code&gt; now recognizes the variables. Alternatively, we could specify this within the &lt;code&gt;lm()&lt;/code&gt; call using &lt;code&gt;data = Ames&lt;/code&gt;. We’ve presented this way because it may be new to you; choose whichever you find most reasonable.&lt;/p&gt;
&lt;p&gt;If we type &lt;code&gt;lm.fit&lt;/code&gt;, some basic information about the model is output. For more detailed information, we use &lt;code&gt;summary(lm.fit)&lt;/code&gt;. This gives us p-values and standard errors for the coefficients, as well as the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; statistic and &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-statistic for the entire model.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Utilizing these functions hels us see some interesting results. Note that we built (nearly) the simplest possible model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{SalePrice} = \beta_0 + \beta_1*(\text{GrLivArea}) + \epsilon.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But even on its own, this model is instructive. It suggest that an increase in overall living area of 1 ft &lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt; is correlated with an expected increase in sales price of $107. (Note that we &lt;strong&gt;cannot&lt;/strong&gt; make causal claims!)&lt;/p&gt;
&lt;p&gt;Saving the model as we did above is useful because we can explore other pieces of information it stores. Specifically, we can use the &lt;code&gt;names()&lt;/code&gt; function in order to find out what else is stored in &lt;code&gt;lm.fit&lt;/code&gt;. Although we can extract these quan- tities by name—e.g. &lt;code&gt;lm.fit$coefficients&lt;/code&gt;—it is safer to use the extractor functions like &lt;code&gt;coef()&lt;/code&gt; to access them. We can also use a handy tool like &lt;code&gt;plot()&lt;/code&gt; applied directly to &lt;code&gt;lm.fit&lt;/code&gt; to see some interesting data that is automatically stored by the model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Use &lt;code&gt;plot()&lt;/code&gt; to explore the model above. Do you suspect that some outliers have a large influence on the data? We will explore this point specifically in the future.&lt;/p&gt;
&lt;p&gt;We can now go crazy adding variables to our model. It’s as simple as appending them to the previous code—though you should be careful executing this, as it will overwrite your previous output:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit = lm(SalePrice ~ GrLivArea + LotArea)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Does controlling for &lt;code&gt;LotArea&lt;/code&gt; change the &lt;em&gt;qualitative&lt;/em&gt; conclusions from the previous regression? What about the &lt;em&gt;quantitative&lt;/em&gt; results? Does the direction of the change in the quantitative results make sense to you?&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;lm()&lt;/code&gt; function in a &lt;strong&gt;simple&lt;/strong&gt; linear regression (e.g., with only one predictor) with &lt;code&gt;SalePrice&lt;/code&gt; as the response to determine the value of a garage.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;lm()&lt;/code&gt; function to perform a multiple linear regression with &lt;code&gt;SalePrice&lt;/code&gt; as the response and all other variables from your &lt;code&gt;Ames&lt;/code&gt; data as the predictors. Use the &lt;code&gt;summary()&lt;/code&gt; function to print the results. Comment on the output. For instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is there a relationship between the predictors and the response?&lt;/li&gt;
&lt;li&gt;Which predictors appear to have a statistically significant relationship to the response? (Hint: look for stars)&lt;/li&gt;
&lt;li&gt;What does the coefficient for the year variable suggest?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;:&lt;/code&gt; symbols to fit a linear regression model with &lt;em&gt;one&lt;/em&gt; well-chosen interaction effects. Why did you do this?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try a few (e.g., two) different transformations of the variables, such as &lt;span class=&#34;math inline&#34;&gt;\(ln(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sqrt x\)&lt;/span&gt;. Do any of these make sense to include in a model of &lt;code&gt;SalePrice&lt;/code&gt;? Comment on your findings.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(Bonus; very very challenging)&lt;/strong&gt; How might we build a model to estimate the elasticity of demand from this dataset?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;When we use the simple regression model with a single input, the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-stat includes the intercept term. Otherwise, it does not. See Lecture 5 for more detail.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Correlations and Simple Models</title>
      <link>https://ssc442.netlify.app/assignment/06-assignment/</link>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/06-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#backstory-and-set-up&#34;&gt;Backstory and Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-exploration-and-processing&#34;&gt;Data Exploration and Processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, October 10th.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backstory-and-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Backstory and Set Up&lt;/h2&gt;
&lt;p&gt;You have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.&lt;/p&gt;
&lt;p&gt;First, let’s load the data. The code below ought to work, but if it doesn’t you can access the data at the link below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ameslist &amp;lt;- read.table(&amp;quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv&amp;quot;,
                 header = TRUE,
                 sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we proceed, let’s note a few things about the (simple) code above. First, we have specified &lt;code&gt;header = TRUE&lt;/code&gt; because—you guessed it—the original dataset has headers. Although simple, this is an incredibly important step because it allows &lt;code&gt;R&lt;/code&gt; to do some smart &lt;code&gt;R&lt;/code&gt; things. Specifically, once the headers are in, the variables are formatted as &lt;code&gt;int&lt;/code&gt; and &lt;code&gt;factor&lt;/code&gt; where appropriate. It is absolutely vital that we format the data correctly; otherwise, many &lt;code&gt;R&lt;/code&gt; commands will whine at us.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Run the above, but instead specifying &lt;code&gt;header = FALSE&lt;/code&gt;. What data type are the various columns? Now try ommitting the line altogether. What is the default behavior of the &lt;code&gt;read.table&lt;/code&gt; function?&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;data-exploration-and-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Exploration and Processing&lt;/h3&gt;
&lt;p&gt;We are not going to tell you anything about this data. This is intended to replicate a real-world experience that you will all encounter in the (possibly near) future: someone hands you data and you’re expected to make sense of it. Fortunately for us, this data is (somewhat) self-contained. We’ll first check the variable names to try to divine some information. Recall, we have a handy little function for that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(ameslist)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, when doing data exploration, we will sometimes choose to not save our output. This is a judgement call; here we’ve chosen to merely inspect the variables rather than diving in.&lt;/p&gt;
&lt;p&gt;Inspection yields some obvious truths. For example:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Explanation&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;ID&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Unique identifier for each row&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;LotArea&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Size of lot (&lt;strong&gt;units unknown&lt;/strong&gt;)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;SalePrice&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Sale price of house ($)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;…but we face some not-so-obvious things as well. For example:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Explanation&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;LotShape&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;? Something about the lot&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;factor&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;MSSubClass&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;? No clue at all&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;Condition1&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;? Seems like street info&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;factor&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It will be difficult to learn anything about the data that is of type &lt;code&gt;int&lt;/code&gt; without outside documentation. However, we can learn something more about the &lt;code&gt;factor&lt;/code&gt;-type variables. In order to understand these a little better, we need to review some of the values that each take on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Go through the variables in the dataset and make a note about your interpretation for each. Many will be obvious, but some require additional thought.&lt;/p&gt;
&lt;p&gt;We now turn to another central issue—and one that explains our nomenclature choice thus far: the data object is of type &lt;code&gt;list&lt;/code&gt;. To verify this for yourself, check:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;typeof(ameslist)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This isn’t ideal—for some visualization packages, for instance, we need data frames and not lists. We’ll make a mental note of this as something to potentially clean up if we desire.&lt;/p&gt;
&lt;p&gt;Although there are some variables that would be difficult to clean, there are a few that we can address with relative ease. Consider, for instance, the variable &lt;code&gt;GarageType&lt;/code&gt;. This might not be that important, but, remember, the weather in Ames, IA is pretty crummy—a detached garage might be a dealbreaker for some would-be homebuyers. Let’s inspect the values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; unique(ameslist$GarageType)
[1] Attchd  Detchd  BuiltIn CarPort &amp;lt;NA&amp;gt; Basment 2Types&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, we could make an informed decision and create a new variable. Let’s create &lt;code&gt;OutdoorGarage&lt;/code&gt; to indicate, say, homes that have any type of garage that requires the homeowner to walk outdoors after parking their car. (For those who aren’t familiar with different garage types, a car port is not insulated and is therefore considered outdoors. A detached garage presumably requires that the person walks outside after parking. The three other types are inside the main structure, and &lt;code&gt;2Types&lt;/code&gt; we can assume includes at least one attached garage of some sort). This is going to require a bit more coding and we will have to think through each step carefully.&lt;/p&gt;
&lt;p&gt;First, let’s create a new object that has indicator variables (that is, a variable whose values are either zero or one) for each of the &lt;code&gt;GarageType&lt;/code&gt; values. As with everything in &lt;code&gt;R&lt;/code&gt;, there’s a handy function to do this for us:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GarageTemp = model.matrix( ~ GarageType - 1, data=ameslist )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have two separate objects living in our computer’s memory: &lt;code&gt;ameslist&lt;/code&gt; and &lt;code&gt;GarageTemp&lt;/code&gt;—so named to indicate that it is a temporary object.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; We now need to stitch it back onto our original data; we’ll use a simple concatenation and write over our old list with the new one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ameslist &amp;lt;- cbind(ameslist, GarageTemp)
&amp;gt; Error in data.frame(..., check.names = FALSE) :
  arguments imply differing number of rows: 1460, 1379&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Huh. What’s going on?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Figure out what’s going on above. Fix this code so that you have a working version.&lt;/p&gt;
&lt;p&gt;Now that we’ve got that working (ha!) we can generate a new variable for our outdoor garage. We’ll use a somewhat gross version below because it is &lt;em&gt;verbose&lt;/em&gt;; that said, this can be easily accomplished using logical indexing for those who like that approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ameslist$GarageOutside &amp;lt;- ifelse(ameslist$GarageTypeDetchd == 1 | ameslist$GarageTypeCarPort == 1, 1, 0)
unique(ameslist$GarageOutside)
[1]  0  1 NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This seems to have worked. The command above &lt;code&gt;ifelse()&lt;/code&gt; does what it says: &lt;code&gt;if&lt;/code&gt; some condition is met (here, either of two variables equals one) then it returns a one; &lt;code&gt;else&lt;/code&gt; it returns a zero. Such functions are very handy, though as mentioned above, there are other ways of doing this. Also note, that while fixed the issue with &lt;code&gt;NA&lt;/code&gt; above, we’ve got new issues: we definitely don’t want &lt;code&gt;NA&lt;/code&gt; outputted from this operation. Accordingly, we’re going to need to deal with it somehow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Utilizing a similar approach to what you did above, fix this so that the only outputs are zero and one.&lt;/p&gt;
&lt;p&gt;Generally speaking, this is a persistent issue, and you will spend an extraordinary amount of time dealing with missing data or data that does not encode a variable exactly as you want it. This is expecially true if you deal with real-world data: you will need to learn how to handle &lt;code&gt;NA&lt;/code&gt;s. There are a number of fixes (as always, Google is your friend) and anything that works is good. But you should spend some time thinking about this and learning at least one approach.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Prune the data to all of the variables that are &lt;code&gt;type = int&lt;/code&gt; about which you have some reasonable intuition for what they mean. This &lt;strong&gt;must&lt;/strong&gt; include the variable &lt;code&gt;SalePrice&lt;/code&gt;. Save this new dataset as &lt;code&gt;Ames&lt;/code&gt;. Produce documentation for this object in the form of a .txt file. This must describe each of the preserved variables, the values it can take (e.g., can it be negative?) and your interpretation of the variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Produce a &lt;em&gt;scatterplot matrix&lt;/em&gt; which includes 12 of the variables that are &lt;code&gt;type = int&lt;/code&gt; in the data set. Choose those that you believe are likely to be correlated with &lt;code&gt;SalePrice&lt;/code&gt;.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute a matrix of correlations between these variables using the function &lt;code&gt;cor()&lt;/code&gt;. Does this match your prior beliefs? Briefly discuss the correlation between the miscellaneous variables and &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Produce a scatterplot between &lt;code&gt;SalePrice&lt;/code&gt; and &lt;code&gt;GrLivArea&lt;/code&gt;. Run a linear model using &lt;code&gt;lm()&lt;/code&gt; to explore the relationship. Finally, use the &lt;code&gt;abline()&lt;/code&gt; function to plot the relationship that you’ve found in the simple linear regression.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the largest outlier that is above the regression line? Produce the other information about this house.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(Bonus)&lt;/strong&gt; Create a visualization that shows the rise of air conditioning over time in homes in Ames.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Of course, you could find out the defaults of the function by simply using the handy &lt;code&gt;?&lt;/code&gt; command. Don’t forget about this tool!&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;It’s not exactly true that these objects are in memory. They are… sort of. But how &lt;code&gt;R&lt;/code&gt; handles memory is complicated and silly and blah blah who cares. It’s basically in memory.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;If you are not familiar with this type of visualization, consult the book (&lt;em&gt;Introduction to Statistical Learning&lt;/em&gt;), Chapters 2 and 3. Google it; it’s free.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Uncertainty</title>
      <link>https://ssc442.netlify.app/example/05-example/</link>
      <pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/05-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#part-1-statistical-inference-and-polls&#34;&gt;Part 1: Statistical Inference and Polls&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#polls&#34;&gt;Polls&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-sampling-model-for-polls&#34;&gt;The sampling model for polls&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#populations-samples-parameters-and-estimates&#34;&gt;Populations, samples, parameters, and estimates&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-sample-average&#34;&gt;The sample average&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameters&#34;&gt;Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#polling-versus-forecasting&#34;&gt;Polling versus forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-our-estimate-expected-value-and-standard-error&#34;&gt;Properties of our estimate: expected value and standard error&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clt&#34;&gt;Central Limit Theorem&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-monte-carlo-simulation&#34;&gt;A Monte Carlo simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-spread&#34;&gt;The spread&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-why-not-run-a-very-large-poll&#34;&gt;Bias: why not run a very large poll?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-2-supplemental-additional-visualization-techniques&#34;&gt;Part 2: (Supplemental) Additional Visualization Techniques&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34;&gt;Code&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-and-clean-data&#34;&gt;Load and clean data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plots&#34;&gt;Density plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#box-violin-and-rain-cloud-plots&#34;&gt;Box, violin, and rain cloud plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Probabilistic thinking is central in the human experience. How we describe that thinking is mixed, but most of the time we use (rather imprecise) language. With only a few moments of searching, one can find thousands of articles that use probabilistic words to describe events. Here are some examples:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“‘Highly unlikely’ State of the Union will happen amid shutdown” – The Hill&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Tiger Woods makes Masters 15th and most improbable major” – Fox&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Trump predicts ‘very good chance’ of China trade deal” – CNN&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet people don’t have a good sense of what these things mean. Uncertainty is key to data analytics: if we were certain of things, A study in the 1960s explored the perception of probabilistic words like these among NATO officers. A more modern replication of this found the following basic pattern:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/words.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A deep, basic fact about humans is that we struggle to understand probabilities. But visualizing things can help. The graphic above shows the uncertainty about uncertainty (very meta). We can convey all manner of uncertainty with clever graphics. Today’s practical example works through some of the myriad of ways to visualize variable data. We’ll cover some territory that we’ve already hit, in hopes of locking in some key concepts.&lt;/p&gt;
&lt;div id=&#34;part-1-statistical-inference-and-polls&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part 1: Statistical Inference and Polls&lt;/h1&gt;
&lt;p&gt;In this Example we will describe, in some detail, how poll aggregators such as FiveThirtyEight use data to predict election outcomes. To understand how they do this, we first need to learn the basics of &lt;em&gt;Statistical Inference&lt;/em&gt;, the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;div id=&#34;polls&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Polls&lt;/h2&gt;
&lt;p&gt;Opinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as &lt;em&gt;inference&lt;/em&gt; and it is the main topic of this chapter.&lt;/p&gt;
&lt;p&gt;Perhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.&lt;/p&gt;
&lt;p&gt;Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.&lt;/p&gt;
&lt;p&gt;Although typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.&lt;/p&gt;
&lt;p&gt;Real Clear Politics&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Poll
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Sample
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
MoE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Clinton
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Trump
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Spread
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Final Results
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +2.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RCP Average
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/1 - 11/7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +3.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bloomberg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/4 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
799 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IBD
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/4 - 11/7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1107 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Economist
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/4 - 11/7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3669 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
49.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
LA Times
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/1 - 11/7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2935 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
4.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
47.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Trump +3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ABC
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/3 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2220 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
49.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FOX News
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/3 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1295 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Monmouth
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/3 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
748 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NBC News
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/3 - 11/5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1282 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CBS News
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/2 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1426 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
47.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Reuters
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/2 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2196 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +5
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- (Source: [Real Clear Politics](https://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html)) --&gt;
&lt;p&gt;Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we will go into some detail on this later.&lt;/p&gt;
&lt;p&gt;Let’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different &lt;em&gt;spread&lt;/em&gt;: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled &lt;strong&gt;MoE&lt;/strong&gt; which stands for &lt;em&gt;margin of error&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this example, we will show how the probability concepts we learned in the previous content can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define &lt;em&gt;estimates&lt;/em&gt; and &lt;em&gt;margins of errors&lt;/em&gt;, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: &lt;em&gt;confidence intervals&lt;/em&gt; and &lt;em&gt;p-values&lt;/em&gt;. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.&lt;/p&gt;
&lt;p&gt;We start by connecting probability theory to the task of using polls to learn about a population.&lt;/p&gt;
&lt;div id=&#34;the-sampling-model-for-polls&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The sampling model for polls&lt;/h3&gt;
&lt;p&gt;To help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this hypothetical urn.&lt;/p&gt;
&lt;p&gt;Before making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you 10 cents per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay \$25 to collect your \$25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;dslabs&lt;/strong&gt; package includes a function that shows a random draw from this urn:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
take_poll(25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/first-simulated-poll-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Think about how you would construct your interval based on the data shown above.&lt;/p&gt;
&lt;p&gt;We have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;populations-samples-parameters-and-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Populations, samples, parameters, and estimates&lt;/h2&gt;
&lt;p&gt;We want to predict the proportion of blue beads in the urn. Let’s call this quantity &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which then tells us the proportion of red beads &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt;, and the spread &lt;span class=&#34;math inline&#34;&gt;\(p - (1-p)\)&lt;/span&gt;, which simplifies to &lt;span class=&#34;math inline&#34;&gt;\(2p - 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In statistical textbooks, the beads in the urn are called the &lt;em&gt;population&lt;/em&gt;. The proportion of blue beads in the population &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is called a &lt;em&gt;parameter&lt;/em&gt;. The 25 beads we see in the previous plot are called a &lt;em&gt;sample&lt;/em&gt;. The task of statistical inference is to predict the parameter &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; using the observed data in the sample.&lt;/p&gt;
&lt;p&gt;Can we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; &amp;gt; .9 or &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; &amp;lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?&lt;/p&gt;
&lt;p&gt;We want to construct an estimate of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample &lt;span class=&#34;math inline&#34;&gt;\(0.48\)&lt;/span&gt; must be at least related to the actual proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. But do we simply predict &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to be 0.48? First, remember that the sample proportion is a random variable. If we run the command &lt;code&gt;take_poll(25)&lt;/code&gt; four times, we get a different answer each time, since the sample proportion is a random variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/four-simulated-polls-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.&lt;/p&gt;
&lt;div id=&#34;the-sample-average&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The sample average&lt;/h3&gt;
&lt;p&gt;Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an &lt;em&gt;estimate&lt;/em&gt; of the parameter &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Once we have this estimate, we can easily report an estimate for the spread &lt;span class=&#34;math inline&#34;&gt;\(2p-1\)&lt;/span&gt;, but for simplicity we will illustrate the concepts for estimating &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We start by defining the random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as: &lt;span class=&#34;math inline&#34;&gt;\(X=1\)&lt;/span&gt; if we pick a blue bead at random and &lt;span class=&#34;math inline&#34;&gt;\(X=0\)&lt;/span&gt; if it is red. This implies that the population is a list of 0s and 1s. If we sample &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; beads, then the average of the draws &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_N\)&lt;/span&gt; is equivalent to the proportion of blue beads in our sample. This is because adding the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s is equivalent to counting the blue beads and dividing this count by the total &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is equivalent to computing a proportion. We use the symbol &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant &lt;span class=&#34;math inline&#34;&gt;\(1/N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\bar{X} = 1/N \times \sum_{i=1}^N X_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, the proportion of blue beads.&lt;/p&gt;
&lt;p&gt;Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to &lt;strong&gt;estimate&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parameters&lt;/h3&gt;
&lt;p&gt;Just like we use variables to define unknowns in systems of equations, in statistical inference we define &lt;em&gt;parameters&lt;/em&gt; to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to represent this quantity. &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we are going to &lt;em&gt;estimate this parameter&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-versus-forecasting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling versus forecasting&lt;/h3&gt;
&lt;p&gt;Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for that moment and not for election day. The &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;properties-of-our-estimate-expected-value-and-standard-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Properties of our estimate: expected value and standard error&lt;/h3&gt;
&lt;p&gt;To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt;. Remember that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is the sum of independent draws so the rules we covered in the probability chapter apply.&lt;/p&gt;
&lt;p&gt;Using what we have learned, the expected value of the sum &lt;span class=&#34;math inline&#34;&gt;\(N\bar{X}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(N \times\)&lt;/span&gt; the average of the urn, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. So dividing by the non-random constant &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; gives us that the expected value of the average &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We can write it using our mathematical notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}(\bar{X}) = p
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can also use what we learned to figure out the standard error: the standard error of the sum is &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{N} \times\)&lt;/span&gt; the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is &lt;span class=&#34;math inline&#34;&gt;\((1-0) \sqrt{p (1-p)}\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p (1-p)}\)&lt;/span&gt;. Because we are dividing the sum by &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, we arrive at the following formula for the standard error of the average:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This result reveals the power of polls. The expected value of the sample proportion &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is the parameter of interest &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and we can make the standard error as small as we want by increasing &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. The law of large numbers tells us that with a large enough poll, our estimate converges to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?&lt;/p&gt;
&lt;p&gt;One problem is that we do not know &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, so we can’t compute the standard error. However, for illustrative purposes, let’s assume that &lt;span class=&#34;math inline&#34;&gt;\(p=0.51\)&lt;/span&gt; and make a plot of the standard error versus the sample size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/standard-error-versus-sample-size-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and &lt;span class=&#34;math inline&#34;&gt;\(p=0.51\)&lt;/span&gt;, the standard error is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(p*(1-p))/sqrt(1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01580823&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or 1.5 percentage points. So even with large polls, for close elections, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;clt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Central Limit Theorem&lt;/h2&gt;
&lt;p&gt;The Central Limit Theorem (CLT) tells us that the distribution function for a sum of draws is approximately normal. You also may recall that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is approximately normal.&lt;/p&gt;
&lt;p&gt;In summary, we have that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; has an approximately normal distribution with expected value &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and standard error &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p(1-p)/N}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now how does this help us? Suppose we want to know what is the probability that we are within 1% from &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We are basically asking what is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(| \bar{X} - p| \leq .01)
\]&lt;/span&gt;
which is the same as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(\bar{X}\leq p + .01) - \mbox{Pr}(\bar{X} \leq p - .01)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Can we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, on the left. Since &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the expected value and &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}\)&lt;/span&gt; is the standard error we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq \frac{ \,.01} {\mbox{SE}(\bar{X})} \right) -
\mbox{Pr}\left(Z \leq - \frac{ \,.01} {\mbox{SE}(\bar{X})} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One problem we have is that since we don’t know &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we don’t know &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SE}(\bar{X})\)&lt;/span&gt;. But it turns out that the CLT still works if we estimate the standard error by using &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; in place of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We say that we &lt;em&gt;plug-in&lt;/em&gt; the estimate. Our estimate of the standard error is therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N}
\]&lt;/span&gt;
In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now we continue with our calculation, but dividing by &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N})\)&lt;/span&gt; instead. In our first sample we had 12 blue and 13 red so &lt;span class=&#34;math inline&#34;&gt;\(\bar{X} = 0.48\)&lt;/span&gt; and our estimate of standard error is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_hat &amp;lt;- 0.48
se &amp;lt;- sqrt(x_hat*(1-x_hat)/25)
se&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09991997&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we can answer the question of the probability of being close to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. The answer is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(0.01/se) - pnorm(-0.01/se)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.07971926&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, there is a small chance that we will be close. A poll of only &lt;span class=&#34;math inline&#34;&gt;\(N=25\)&lt;/span&gt; people is not really very useful, at least not for a close election.&lt;/p&gt;
&lt;p&gt;Earlier we mentioned the &lt;em&gt;margin of error&lt;/em&gt;. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.96*se&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1958431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq \, 1.96\,\mbox{SE}(\bar{X})  / \mbox{SE}(\bar{X}) \right) -
\mbox{Pr}\left(Z \leq - 1.96\, \mbox{SE}(\bar{X}) / \mbox{SE}(\bar{X}) \right)
\]&lt;/span&gt;
which is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq 1.96 \right) -
\mbox{Pr}\left(Z \leq - 1.96\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which we know is about 95%:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(1.96)-pnorm(-1.96)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9500042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hence, there is a 95% probability that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; will be within &lt;span class=&#34;math inline&#34;&gt;\(1.96\times \hat{SE}(\bar{X})\)&lt;/span&gt;, in our case within about 0.2, of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.&lt;/p&gt;
&lt;p&gt;In summary, the CLT tells us that our poll based on a sample size of &lt;span class=&#34;math inline&#34;&gt;\(25\)&lt;/span&gt; is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.&lt;/p&gt;
&lt;p&gt;From the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt;=0.48 with a sample size of 2,000, our standard error &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mbox{SE}}(\bar{X})\)&lt;/span&gt; would have been 0.0111714. So our result is an estimate of &lt;code&gt;48&lt;/code&gt;% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.&lt;/p&gt;
&lt;div id=&#34;a-monte-carlo-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Monte Carlo simulation&lt;/h3&gt;
&lt;p&gt;Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
N &amp;lt;- 1000
x_hat &amp;lt;- replicate(B, {
  x &amp;lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem is, of course, we don’t know &lt;code&gt;p&lt;/code&gt;. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function &lt;code&gt;take_poll(n=1000)&lt;/code&gt; instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.&lt;/p&gt;
&lt;p&gt;One thing we therefore do to corroborate theoretical results is to pick one or several values of &lt;code&gt;p&lt;/code&gt; and run the simulations. Let’s set &lt;code&gt;p=0.45&lt;/code&gt;. We can then simulate a poll:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- 0.45
N &amp;lt;- 1000

x &amp;lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat &amp;lt;- mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this particular sample, our estimate is &lt;code&gt;x_hat&lt;/code&gt;. We can use that code to do a Monte Carlo simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
x_hat &amp;lt;- replicate(B, {
  x &amp;lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To review, the theory tells us that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is approximately normally distributed, has expected value &lt;span class=&#34;math inline&#34;&gt;\(p=\)&lt;/span&gt; 0.45 and standard error &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p(1-p)/N}\)&lt;/span&gt; = 0.0157321. The simulation confirms this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4500761&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(x_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01579523&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A histogram and qq-plot confirm that the normal approximation is accurate as well:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/normal-approximation-for-polls-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course, in real life we would never be able to run such an experiment because we don’t know &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. But we could run it for various values of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;N&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-spread&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The spread&lt;/h3&gt;
&lt;p&gt;The competition is to predict the spread, not the proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. However, because we are assuming there are only two parties, we know that the spread is &lt;span class=&#34;math inline&#34;&gt;\(p - (1-p) = 2p - 1\)&lt;/span&gt;. As a result, everything we have done can easily be adapted to an estimate of &lt;span class=&#34;math inline&#34;&gt;\(2p - 1\)&lt;/span&gt;. Once we have our estimate &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mbox{SE}}(\bar{X})\)&lt;/span&gt;, we estimate the spread with &lt;span class=&#34;math inline&#34;&gt;\(2\bar{X} - 1\)&lt;/span&gt; and, since we are multiplying by 2, the standard error is &lt;span class=&#34;math inline&#34;&gt;\(2\hat{\mbox{SE}}(\bar{X})\)&lt;/span&gt;. Note that subtracting 1 does not add any variability so it does not affect the standard error.&lt;/p&gt;
&lt;p&gt;For our 25 item sample above, our estimate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is &lt;code&gt;.48&lt;/code&gt; with margin of error &lt;code&gt;.20&lt;/code&gt; and our estimate of the spread is &lt;code&gt;0.04&lt;/code&gt; with margin of error &lt;code&gt;.40&lt;/code&gt;. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we have it for the spread &lt;span class=&#34;math inline&#34;&gt;\(2p-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-why-not-run-a-very-large-poll&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bias: why not run a very large poll?&lt;/h3&gt;
&lt;p&gt;For realistic values of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/standard-error-versus-p-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this shortly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-supplemental-additional-visualization-techniques&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part 2: (Supplemental) Additional Visualization Techniques&lt;/h1&gt;
&lt;p&gt;For this second part of the example, we’re going to use historical weather data from &lt;a href=&#34;https://darksky.net/forecast/33.7546,-84.39/us12/en&#34;&gt;Dark Sky&lt;/a&gt; about wind speed and temperature trends for downtown Atlanta (&lt;a href=&#34;https://www.google.com/maps/place/33°45&amp;#39;16.4%22N+84°23&amp;#39;24.0%22W/@33.754557,-84.3921977,17z/&#34;&gt;specifically &lt;code&gt;33.754557, -84.390009&lt;/code&gt;&lt;/a&gt;) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the &lt;a href=&#34;https://github.com/hrbrmstr/darksky&#34;&gt;&lt;strong&gt;darksky&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/atl-weather-2019.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;atl-weather-2019.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;div id=&#34;load-and-clean-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load and clean data&lt;/h3&gt;
&lt;p&gt;First, we load the libraries we’ll be using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(ggridges)
library(gghalves)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we load the data with &lt;code&gt;read_csv()&lt;/code&gt;. Here we assume that the CSV file lives in a subfolder in my project named &lt;code&gt;data&lt;/code&gt;. Naturally, you’ll need to point this to wherever you stashed the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_raw &amp;lt;- read_csv(&amp;quot;data/atl-weather-2019.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll add a couple columns that we can use for faceting and filling using the &lt;code&gt;month()&lt;/code&gt; and &lt;code&gt;wday()&lt;/code&gt; functions from &lt;strong&gt;lubridate&lt;/strong&gt; for extracting parts of the date:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl &amp;lt;- weather_atl_raw %&amp;gt;%
  mutate(Month = month(time, label = TRUE, abbr = FALSE),
         Day = wday(time, label = TRUE, abbr = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to go!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;histograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histograms&lt;/h3&gt;
&lt;p&gt;We can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/basic-histogram-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the &lt;code&gt;boundary&lt;/code&gt; argument for that. We also add &lt;code&gt;scale_x_continuous()&lt;/code&gt; to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;, boundary = 1) +
  scale_x_continuous(breaks = seq(0, 12, by = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/basic-histogram-better-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can show the distribution of wind speed by month if we map the &lt;code&gt;Month&lt;/code&gt; column we made onto the fill aesthetic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;, boundary = 1) +
  scale_x_continuous(breaks = seq(0, 12, by = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/histogram-by-month-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;, boundary = 1) +
  scale_x_continuous(breaks = seq(0, 12, by = 1)) +
  guides(fill = FALSE) +
  facet_wrap(vars(Month))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/histogram-by-month-facet-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Neat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density plots&lt;/h3&gt;
&lt;p&gt;The code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the &lt;code&gt;geom&lt;/code&gt; layer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_density(color = &amp;quot;grey20&amp;quot;, fill = &amp;quot;grey50&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/basic-density-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we want, we can mess with some of the calculus options like the kernel and bandwidth:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_density(color = &amp;quot;grey20&amp;quot;, fill = &amp;quot;grey50&amp;quot;,
               bw = 0.1, kernel = &amp;quot;epanechnikov&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/density-kernel-bw-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_density(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/density-fill-by-month-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_density(alpha = 0.5) +
  guides(fill = FALSE) +
  facet_wrap(vars(Month))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/density-facet-by-month-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or we can stack the density plots behind each other with &lt;a href=&#34;https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html&#34;&gt;&lt;strong&gt;ggridges&lt;/strong&gt;&lt;/a&gt;. For that to work, we also need to map &lt;code&gt;Month&lt;/code&gt; to the y-axis. We can reverse the y-axis so that January is at the top if we use the &lt;code&gt;fct_rev()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +
  geom_density_ridges() +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/ggridges-basic-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can add some extra information to &lt;code&gt;geom_density_ridges()&lt;/code&gt; with some other arguments like &lt;code&gt;quantile_lines&lt;/code&gt;. We can use the &lt;code&gt;quantiles&lt;/code&gt; argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +
  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/ggridges-quantile-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have good working code, we can easily substitute in other variables by changing the x mapping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) +
  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/ggridges-quantile-temp-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use &lt;code&gt;geom_density_ridges_gradient()&lt;/code&gt;, and we need to change the &lt;code&gt;fill&lt;/code&gt; mapping to the strange looking &lt;code&gt;..x..&lt;/code&gt;, which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, &lt;code&gt;fill = temperatureHigh&lt;/code&gt; doesn’t work 🤷:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) +
  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;) +
  labs(x = &amp;quot;High temperature&amp;quot;, y = NULL, color = &amp;quot;Temp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/ggridges-gradient-temp-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And finally, we can get &lt;em&gt;extra&lt;/em&gt; fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: &lt;code&gt;temperatureLow&lt;/code&gt; and &lt;code&gt;temperatureHigh&lt;/code&gt;. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like &lt;code&gt;linetype&lt;/code&gt;), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using &lt;code&gt;pivot_longer()&lt;/code&gt; from &lt;strong&gt;tidyr&lt;/strong&gt;, which was already loaded with &lt;code&gt;library(tidyverse)&lt;/code&gt;. In the RStudio primers, you did this same thing with &lt;code&gt;gather()&lt;/code&gt;—&lt;code&gt;pivot_longer()&lt;/code&gt; is the newer version of &lt;code&gt;gather()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_long &amp;lt;- weather_atl %&amp;gt;%
  pivot_longer(cols = c(temperatureLow, temperatureHigh),
               names_to = &amp;quot;temp_type&amp;quot;,
               values_to = &amp;quot;temp&amp;quot;) %&amp;gt;%
  # Clean up the new temp_type column so that &amp;quot;temperatureHigh&amp;quot; becomes &amp;quot;High&amp;quot;, etc.
  mutate(temp_type = recode(temp_type,
                            temperatureHigh = &amp;quot;High&amp;quot;,
                            temperatureLow = &amp;quot;Low&amp;quot;)) %&amp;gt;%
  # This is optional—just select a handful of columns
  select(time, temp_type, temp, Month)

# Show the first few rows
head(weather_atl_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 4
##   time                temp_type  temp Month  
##   &amp;lt;dttm&amp;gt;              &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;ord&amp;gt;  
## 1 2019-01-01 05:00:00 Low        50.6 January
## 2 2019-01-01 05:00:00 High       63.9 January
## 3 2019-01-02 05:00:00 Low        49.0 January
## 4 2019-01-02 05:00:00 High       57.4 January
## 5 2019-01-03 05:00:00 Low        53.1 January
## 6 2019-01-03 05:00:00 High       55.3 January&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a column for the temperature (&lt;code&gt;temp&lt;/code&gt;) and a column indicating if it is high or low (&lt;code&gt;temp_type&lt;/code&gt;). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the &lt;code&gt;linetype&lt;/code&gt; aesthetic to show high/low in the border of the plots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month),
                             fill = ..x.., linetype = temp_type)) +
  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;) +
  labs(x = &amp;quot;High temperature&amp;quot;, y = NULL, color = &amp;quot;Temp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/ggridges-gradient-temp-high-low-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;box-violin-and-rain-cloud-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Box, violin, and rain cloud plots&lt;/h3&gt;
&lt;p&gt;Finally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the &lt;code&gt;Day&lt;/code&gt; variable we made indicating weekday:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, fill = Day)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/basic-boxplot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can switch this to a violin plot by just changing the &lt;code&gt;geom&lt;/code&gt; layer and mapping &lt;code&gt;Day&lt;/code&gt; to the x-axis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/basic-violin-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin() +
  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/violin-strip-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also add larger points for the daily averages. We’ll use a special layer for this: &lt;code&gt;stat_summary()&lt;/code&gt;. It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here &lt;code&gt;&#34;mean&#34;&lt;/code&gt;) and then plotting that result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin() +
  stat_summary(geom = &amp;quot;point&amp;quot;, fun = &amp;quot;mean&amp;quot;, size = 5, color = &amp;quot;white&amp;quot;) +
  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/violin-strip-mean-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also show the mean and confidence interval at the same time by changing the summary function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin() +
  stat_summary(geom = &amp;quot;pointrange&amp;quot;, fun.data = &amp;quot;mean_se&amp;quot;, size = 1, color = &amp;quot;white&amp;quot;) +
  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/violin-strip-mean-ci-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Overlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use &lt;a href=&#34;https://github.com/erocoar/gghalves&#34;&gt;the &lt;strong&gt;gghalves&lt;/strong&gt; package&lt;/a&gt;, we can use special halved versions of some of these geoms like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(x = fct_rev(Day), y = temperatureHigh)) +
  geom_half_point(aes(color = Day), side = &amp;quot;l&amp;quot;, size = 0.5) +
  geom_half_boxplot(aes(fill = Day), side = &amp;quot;r&amp;quot;) +
  guides(color = FALSE, fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/gghalves-point-boxplot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note the &lt;code&gt;side&lt;/code&gt; argument for specifying which half of the column the geom goes. We can also use &lt;code&gt;geom_half_violin()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(x = fct_rev(Day), y = temperatureHigh)) +
  geom_half_point(aes(color = Day), side = &amp;quot;l&amp;quot;, size = 0.5) +
  geom_half_violin(aes(fill = Day), side = &amp;quot;r&amp;quot;) +
  guides(color = FALSE, fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/gghalves-point-violon-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we flip the plot, we can make a &lt;a href=&#34;https://micahallen.org/2018/03/15/introducing-raincloud-plots/&#34;&gt;rain cloud plot&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(x = fct_rev(Day), y = temperatureHigh)) +
  geom_half_boxplot(aes(fill = Day), side = &amp;quot;l&amp;quot;, width = 0.5, nudge = 0.1) +
  geom_half_point(aes(color = Day), side = &amp;quot;l&amp;quot;, size = 0.5) +
  geom_half_violin(aes(fill = Day), side = &amp;quot;r&amp;quot;) +
  guides(color = FALSE, fill = FALSE) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/05-example_files/figure-html/gghalves-rain-cloud-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.realclearpolitics.com&#34; class=&#34;uri&#34;&gt;http://www.realclearpolitics.com&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html&#34; class=&#34;uri&#34;&gt;http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Models</title>
      <link>https://ssc442.netlify.app/assignment/05-assignment/</link>
      <pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/05-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#models&#34;&gt;Statistical models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#poll-aggregators&#34;&gt;Poll aggregators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#poll-data&#34;&gt;Poll data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pollster-bias&#34;&gt;Pollster bias&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-driven-model&#34;&gt;Data-driven models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, October 3rd.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For this exercise you will need to ensure that you’ve carefully read this week’s content and example. We will build on both. The exercises (which you will turn in as this week’s lab) are at the bottom. Note that this week’s lab is much more theoretical than any other week in this class. This is to ensure that you have the foundations necessary to build rich statistical models and apply them to real-world data.&lt;/p&gt;
&lt;div id=&#34;models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Statistical models&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;“All models are wrong, but some are useful.” –George E. P. Box&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. Four years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer. Political commentator Joe Scarborough said during his show&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Anybody that thinks that this race is anything but a toss-up right now is such an ideologue … they’re jokes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To which Nate Silver responded via Twitter:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you think it’s a toss-up, let’s bet. If Obama wins, you donate $1,000 to the American Red Cross. If Romney wins, I do. Deal?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. In contrast, most other forecasters were almost certain she would win. She lost. But 71% is still more than 50%, so was Mr. Silver wrong? And what does probability mean in this context anyway? Are dice being tossed somewhere?&lt;/p&gt;
&lt;p&gt;In this lab we will demonstrate how &lt;em&gt;poll aggregators&lt;/em&gt;, such as FiveThirtyEight, collected and combined data reported by different experts to produce improved predictions. We will introduce ideas behind the &lt;em&gt;statistical models&lt;/em&gt;, also known as &lt;em&gt;probability models&lt;/em&gt;, that were used by poll aggregators to improve election forecasts beyond the power of individual polls. First, we’ll motivate the models, building on the statistical inference concepts we learned in this week’s content and example. We start with relatively simple models, realizing that the actual data science exercise of forecasting elections involves rather complex ones. We will introduce such modeks towards the end of this section of the course.&lt;/p&gt;
&lt;div id=&#34;poll-aggregators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poll aggregators&lt;/h2&gt;
&lt;p&gt;A few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
d &amp;lt;- 0.039
Ns &amp;lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p &amp;lt;- (d + 1) / 2

polls &amp;lt;- map_df(Ns, function(N) {
  x &amp;lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
  x_hat &amp;lt;- mean(x)
  se_hat &amp;lt;- sqrt(x_hat * (1 - x_hat) / N)
  list(estimate = 2 * x_hat - 1,
    low = 2*(x_hat - 1.96*se_hat) - 1,
    high = 2*(x_hat + 1.96*se_hat) - 1,
    sample_size = N)
}) %&amp;gt;% mutate(poll = seq_along(Ns))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/simulated-polls-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.&lt;/p&gt;
&lt;p&gt;Poll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.&lt;/p&gt;
&lt;p&gt;Although as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(polls$sample_size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11269&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;participants. Basically, we construct an estimate of the spread, let’s call it &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, with a weighted average in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_hat &amp;lt;- polls %&amp;gt;%
  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %&amp;gt;%
  pull(avg)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have an estimate of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.0184545.&lt;/p&gt;
&lt;p&gt;Thus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/confidence-coverage-2008-election-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits.&lt;/p&gt;
&lt;p&gt;Since the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. The day before the election the &lt;em&gt;New York Times&lt;/em&gt; reported&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; the following probabilities for Hillary Clinton winning the presidency:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
NYT
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
538
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
HuffPost
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
PW
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
PEC
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
DK
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Cook
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Roth
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Win Prob
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
85%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
71%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
98%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
89%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&amp;gt;99%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
92%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Lean Dem
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Lean Dem
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--(Source: [New York Times](https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html))--&gt;
&lt;p&gt;For example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled &lt;em&gt;Trump Is Just A Normal Polling Error Behind Clinton&lt;/em&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.
By understanding statistical models and how these forecasters use them, we will start to understand how this happened.&lt;/p&gt;
&lt;p&gt;Although not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. FiveThirtyEight predicted a 3.6% advantage for Clinton&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her an 81.4% chance. Their prediction was summarized with a chart like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/fivethirtyeight-densities-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model.
&lt;!--(Source: [FiveThirtyEight](https://projects.fivethirtyeight.com/2016-election-forecast/))--&gt;&lt;/p&gt;
&lt;p&gt;We introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. To understand the “81.4% chance” statement we need to describe Bayesian statistics, which we do in Sections &lt;a href=&#34;#bayesian-statistics&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;#bayesian-approach&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;poll-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poll data&lt;/h3&gt;
&lt;p&gt;We use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of the &lt;strong&gt;dslabs&lt;/strong&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(polls_us_election_2016)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls &amp;lt;- polls_us_election_2016 %&amp;gt;%
  filter(state == &amp;quot;U.S.&amp;quot; &amp;amp; enddate &amp;gt;= &amp;quot;2016-10-31&amp;quot; &amp;amp;
           (grade %in% c(&amp;quot;A+&amp;quot;,&amp;quot;A&amp;quot;,&amp;quot;A-&amp;quot;,&amp;quot;B+&amp;quot;) | is.na(grade)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We add a spread estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls &amp;lt;- polls %&amp;gt;%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this example, we will assume that there are only two parties and call &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; the proportion voting for Clinton and &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt; the proportion voting for Trump. We are interested in the spread &lt;span class=&#34;math inline&#34;&gt;\(2p-1\)&lt;/span&gt;. Let’s call the spread &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; (for difference).&lt;/p&gt;
&lt;p&gt;We have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the standard error is &lt;span class=&#34;math inline&#34;&gt;\(2\sqrt{p (1 - p) / N}\)&lt;/span&gt;. Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_hat &amp;lt;- polls %&amp;gt;%
  summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %&amp;gt;%
  pull(d_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the standard error is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_hat &amp;lt;- (d_hat+1)/2
moe &amp;lt;- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))
moe&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.006623178&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?&lt;/p&gt;
&lt;p&gt;A histogram of the reported spreads shows a problem:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls %&amp;gt;%
  ggplot(aes(spread)) +
  geom_histogram(color=&amp;quot;black&amp;quot;, binwidth = .01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/polls-2016-spread-histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data does not appear to be normally distributed and the standard error appears to be larger than 0.0066232. The theory is not quite working here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pollster-bias&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pollster bias&lt;/h3&gt;
&lt;p&gt;Notice that various pollsters are involved and some are taking several polls a week:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls %&amp;gt;% group_by(pollster) %&amp;gt;% summarize(n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 × 2
##    pollster                                                   `n()`
##    &amp;lt;fct&amp;gt;                                                      &amp;lt;int&amp;gt;
##  1 ABC News/Washington Post                                       7
##  2 Angus Reid Global                                              1
##  3 CBS News/New York Times                                        2
##  4 Fox News/Anderson Robbins Research/Shaw &amp;amp; Company Research     2
##  5 IBD/TIPP                                                       8
##  6 Insights West                                                  1
##  7 Ipsos                                                          6
##  8 Marist College                                                 1
##  9 Monmouth University                                            1
## 10 Morning Consult                                                1
## 11 NBC News/Wall Street Journal                                   1
## 12 RKM Research and Communications, Inc.                          1
## 13 Selzer &amp;amp; Company                                               1
## 14 The Times-Picayune/Lucid                                       8
## 15 USC Dornsife/LA Times                                          8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s visualize the data for the pollsters that are regularly polling:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/pollster-bias-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls %&amp;gt;% group_by(pollster) %&amp;gt;%
  filter(n() &amp;gt;= 6) %&amp;gt;%
  summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 2
##   pollster                     se
##   &amp;lt;fct&amp;gt;                     &amp;lt;dbl&amp;gt;
## 1 ABC News/Washington Post 0.0265
## 2 IBD/TIPP                 0.0333
## 3 Ipsos                    0.0225
## 4 The Times-Picayune/Lucid 0.0196
## 5 USC Dornsife/LA Times    0.0183&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences &lt;em&gt;across the polls&lt;/em&gt;. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We also call them &lt;em&gt;pollster bias&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In the following section, rather than use the urn model theory, we are instead going to develop a data-driven model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-driven-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data-driven models&lt;/h2&gt;
&lt;p&gt;For each pollster, let’s collect their last reported result before the election:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one_poll_per_pollster &amp;lt;- polls %&amp;gt;% group_by(pollster) %&amp;gt;%
  filter(enddate == max(enddate)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a histogram of the data for these 15 pollsters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(spread, data = one_poll_per_pollster, binwidth = 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/pollster-bias-histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly.&lt;/p&gt;
&lt;p&gt;The new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We &lt;em&gt;assume&lt;/em&gt; that the expected value of our urn is the actual spread &lt;span class=&#34;math inline&#34;&gt;\(d=2p-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Because instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p(1-p)}\)&lt;/span&gt;. Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is used to represent this parameter.&lt;/p&gt;
&lt;p&gt;In summary, we have two unknown parameters: the expected value &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our task is to estimate &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Because we model the observed values &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots X_N\)&lt;/span&gt; as a random sample from the urn, the CLT might still work in this situation because it is an average of independent random variables. For a large enough sample size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, the probability distribution of the sample average &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is approximately normal with expected value &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and standard error &lt;span class=&#34;math inline&#34;&gt;\(\sigma/\sqrt{N}\)&lt;/span&gt;. If we are willing to consider &lt;span class=&#34;math inline&#34;&gt;\(N=15\)&lt;/span&gt; large enough, we can use this to construct confidence intervals.&lt;/p&gt;
&lt;p&gt;A problem is that we don’t know &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. But theory tells us that we can estimate the urn model &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; with the &lt;em&gt;sample standard deviation&lt;/em&gt; defined as
&lt;span class=&#34;math inline&#34;&gt;\(s = \sqrt{ \sum_{i=1}^N (X_i - \bar{X})^2 / (N-1)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Unlike for the population standard deviation definition, we now divide by &lt;span class=&#34;math inline&#34;&gt;\(N-1\)&lt;/span&gt;. This makes &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; a better estimate of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;sd&lt;/code&gt; function in R computes the sample standard deviation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(one_poll_per_pollster$spread)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02419369&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to form a new confidence interval based on our new data-driven model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- one_poll_per_pollster %&amp;gt;%
  summarize(avg = mean(spread),
            se = sd(spread) / sqrt(length(spread))) %&amp;gt;%
  mutate(start = avg - 1.96 * se,
         end = avg + 1.96 * se)
round(results * 100, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   avg  se start end
## 1 2.9 0.6   1.7 4.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Note that using dollar signs &lt;code&gt;$ $&lt;/code&gt; to enclose some text is how you make the fancy math you see below. If you installed &lt;code&gt;tinytex&lt;/code&gt; or some other Latex distribution in order to render your PDFs, you should be equipped to insert mathematics directly into your .Rmd file. It only works in the text – inside the code chunks, the dollar sign is still the accessor.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(polls_us_election_2016)
polls &amp;lt;- polls_us_election_2016 %&amp;gt;%
  filter(pollster %in% c(&amp;quot;Rasmussen Reports/Pulse Opinion Research&amp;quot;,
                         &amp;quot;The Times-Picayune/Lucid&amp;quot;) &amp;amp;
           enddate &amp;gt;= &amp;quot;2016-10-15&amp;quot; &amp;amp;
           state == &amp;quot;U.S.&amp;quot;) %&amp;gt;%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to answer the question: is there a poll bias? First, make a plot showing the spreads for each poll.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We will model the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_{i,j}\)&lt;/span&gt; in the following way:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{i,j} = d + b_i + \varepsilon_{i,j}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(i=1,2\)&lt;/span&gt; indexing the two pollsters, &lt;span class=&#34;math inline&#34;&gt;\(b_i\)&lt;/span&gt; the bias for pollster &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_ij\)&lt;/span&gt; poll to poll chance variability. We assume the &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; are independent from each other, have expected value &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; regardless of &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Which of the following best represents our question?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Is &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i,j}\)&lt;/span&gt; = 0?&lt;/li&gt;
&lt;li&gt;How close are the &lt;span class=&#34;math inline&#34;&gt;\(Y_{i,j}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;?&lt;/li&gt;
&lt;li&gt;Is &lt;span class=&#34;math inline&#34;&gt;\(b_1 \neq b_2\)&lt;/span&gt;?&lt;/li&gt;
&lt;li&gt;Are &lt;span class=&#34;math inline&#34;&gt;\(b_1 = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_2 = 0\)&lt;/span&gt; ?&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Suppose we define &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_1\)&lt;/span&gt; as the average of poll results from the first poll, &lt;span class=&#34;math inline&#34;&gt;\(Y_{1,1},\dots,Y_{1,N_1}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt; the number of polls conducted by the first pollster:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls %&amp;gt;%
  filter(pollster==&amp;quot;Rasmussen Reports/Pulse Opinion Research&amp;quot;) %&amp;gt;%
  summarize(N_1 = n())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_1\)&lt;/span&gt;?&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What is the standard error of &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_1\)&lt;/span&gt;? (It may be helpful to compute the expected value and standard error of &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2\)&lt;/span&gt; as well.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Suppose we define &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2\)&lt;/span&gt; as the average of poll results from the first poll, &lt;span class=&#34;math inline&#34;&gt;\(Y_{2,1},\dots,Y_{2,N_2}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt; the number of polls conducted by the first pollster. What is the expected value &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2\)&lt;/span&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does the CLT tell us about the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2 - \bar{Y}_1\)&lt;/span&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Nothing because this is not the average of a sample.&lt;/li&gt;
&lt;li&gt;Because the &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij}\)&lt;/span&gt; are approximately normal, so are the averages.&lt;/li&gt;
&lt;li&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_1\)&lt;/span&gt; are sample averages, so if we assume &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt; are large enough, each is approximately normal. The difference of normals is also normal.&lt;/li&gt;
&lt;li&gt;The data are not 0 or 1, so CLT does not apply.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Construct a random variable that has expected value &lt;span class=&#34;math inline&#34;&gt;\(b_2 - b_1\)&lt;/span&gt;, the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2\)&lt;/span&gt; (the variances of the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; above), but we can plug the sample standard deviations. &lt;strong&gt;Compute those now&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The statistic formed by dividing our estimate of &lt;span class=&#34;math inline&#34;&gt;\(b_2-b_1\)&lt;/span&gt; by its estimated standard error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\bar{Y}_2 - \bar{Y}_1}{\sqrt{s_2^2/N_2 + s_1^2/N_1}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is called the t-statistic. Now you should be able to answer the question: is &lt;span class=&#34;math inline&#34;&gt;\(b_2 - b_1\)&lt;/span&gt; different from 0?&lt;/p&gt;
&lt;p&gt;Notice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can actually construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect?&lt;/p&gt;
&lt;p&gt;For this exercise, create a new table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls &amp;lt;- polls_us_election_2016 %&amp;gt;%
  filter(enddate &amp;gt;= &amp;quot;2016-10-15&amp;quot; &amp;amp;
           state == &amp;quot;U.S.&amp;quot;) %&amp;gt;%
  group_by(pollster) %&amp;gt;%
  filter(n() &amp;gt;= 5) %&amp;gt;%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TbKkjm-gheY&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=TbKkjm-gheY&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html&#34; class=&#34;uri&#34;&gt;https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/&#34; class=&#34;uri&#34;&gt;https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://projects.fivethirtyeight.com/2016-election-forecast/&#34; class=&#34;uri&#34;&gt;https://projects.fivethirtyeight.com/2016-election-forecast/&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizations in Practice</title>
      <link>https://ssc442.netlify.app/example/04-example/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/04-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-viz-in-the-real-world&#34;&gt;Data Viz in the Real World&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-new-insights-on-poverty&#34;&gt;Case study: new insights on poverty&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-the-data&#34;&gt;Exploring the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slope-charts&#34;&gt;Slope charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bland-altman-plot&#34;&gt;Bland-Altman plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bump-charts&#34;&gt;Bump charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#themes&#34;&gt;Themes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#small-multiples&#34;&gt;Small multiples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sparklines&#34;&gt;Sparklines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-ecological-fallacy-and-importance-of-showing-the-data&#34;&gt;The ecological fallacy and importance of showing the data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#logit&#34;&gt;Logistic transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#show-the-data&#34;&gt;Show the data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vaccines&#34;&gt;Case study: vaccines and infectious diseases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#try-it&#34;&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-viz-in-the-real-world&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Viz in the Real World&lt;/h1&gt;
&lt;p&gt;In this section, we will demonstrate how relatively simple &lt;strong&gt;ggplot2&lt;/strong&gt; code can create insightful and aesthetically pleasing plots. As motivation we will create plots that help us better understand trends in world health and economics. We will implement what we learned in previous sections of the class and learn how to augment the code to perfect the plots. As we go through our case study, we will describe relevant general data visualization principles and learn concepts such as &lt;em&gt;faceting&lt;/em&gt;, &lt;em&gt;time series plots&lt;/em&gt;, &lt;em&gt;transformations&lt;/em&gt;, and &lt;em&gt;ridge plots&lt;/em&gt;.&lt;/p&gt;
&lt;div id=&#34;case-study-new-insights-on-poverty&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: new insights on poverty&lt;/h2&gt;
&lt;p&gt;Hans Rosling&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; was the co-founder of the Gapminder Foundation&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world. The organization uses data to show how actual trends in health and economics contradict the narratives that emanate from sensationalist media coverage of catastrophes, tragedies, and other unfortunate events. As stated in the Gapminder Foundation’s website:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Journalists and lobbyists tell dramatic stories. That’s their job. They tell stories about extraordinary events and unusual people. The piles of dramatic stories pile up in peoples’ minds into an over-dramatic worldview and strong negative stress feelings: “The world is getting worse!”, “It’s we vs. them!”, “Other people are strange!”, “The population just keeps growing!” and “Nobody cares!”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hans Rosling conveyed actual data-based trends in a dramatic way of his own, using effective data visualization. This section is based on two talks that exemplify this approach to education: [New Insights on Poverty]&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; and The Best Stats You’ve Ever Seen&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Specifically, in this section, we use data to attempt to answer the following two questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Is it a fair characterization of today’s world to say it is divided into western rich nations and the developing world in Africa, Asia, and Latin America?&lt;/li&gt;
&lt;li&gt;Has income inequality across countries worsened during the last 40 years?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To answer these questions, we will be using the &lt;code&gt;gapminder&lt;/code&gt; dataset provided in &lt;strong&gt;dslabs&lt;/strong&gt;. This dataset was created using a number of spreadsheets available from the Gapminder Foundation. You can access the table like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
library(ggrepel)
library(ggthemes)
gapminder = dslabs::gapminder %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exploring-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploring the Data&lt;/h3&gt;
&lt;p&gt;Taking an exercise from the &lt;em&gt;New Insights on Poverty&lt;/em&gt; video, we start by testing our knowledge regarding differences in child mortality across different countries. For each of the six pairs of countries below, which country do you think had the highest child mortality rates in 2015? Which pairs do you think are most similar?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sri Lanka or Turkey&lt;/li&gt;
&lt;li&gt;Poland or South Korea&lt;/li&gt;
&lt;li&gt;Malaysia or Russia&lt;/li&gt;
&lt;li&gt;Pakistan or Vietnam&lt;/li&gt;
&lt;li&gt;Thailand or South Africa&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When answering these questions without data, the non-European countries are typically picked as having higher child mortality rates: Sri Lanka over Turkey, South Korea over Poland, and Malaysia over Russia. It is also common to assume that countries considered to be part of the developing world: Pakistan, Vietnam, Thailand, and South Africa, have similarly high mortality rates.&lt;/p&gt;
&lt;p&gt;To answer these questions &lt;strong&gt;with data&lt;/strong&gt;, we can use &lt;strong&gt;dplyr&lt;/strong&gt;. For example, for the first comparison we see that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dslabs::gapminder %&amp;gt;%
  filter(year == 2015 &amp;amp; country %in% c(&amp;quot;Sri Lanka&amp;quot;,&amp;quot;Turkey&amp;quot;)) %&amp;gt;%
  select(country, infant_mortality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     country infant_mortality
## 1 Sri Lanka              8.4
## 2    Turkey             11.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turkey has the higher infant mortality rate.&lt;/p&gt;
&lt;p&gt;We can use this code on all comparisons and find the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## New names:
## * country -&amp;gt; country...1
## * infant_mortality -&amp;gt; infant_mortality...2
## * country -&amp;gt; country...3
## * infant_mortality -&amp;gt; infant_mortality...4&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
country
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
infant mortality
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
country
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
infant mortality
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sri Lanka
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Turkey
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Poland
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Korea
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Malaysia
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Russia
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pakistan
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65.8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Vietnam
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thailand
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10.5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Africa
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that the European countries on this list have higher child mortality rates: Poland has a higher rate than South Korea, and Russia has a higher rate than Malaysia. We also see that Pakistan has a much higher rate than Vietnam, and South Africa has a much higher rate than Thailand. It turns out that when Hans Rosling gave this quiz to educated groups of people, the average score was less than 2.5 out of 5, worse than what they would have obtained had they guessed randomly. This implies that more than ignorant, we are misinformed. In this chapter we see how data visualization helps inform us.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;slope-charts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slope charts&lt;/h3&gt;
&lt;p&gt;One exception where another type of plot may be more informative is when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a &lt;em&gt;slope chart&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;There is no geometry for slope charts in &lt;strong&gt;ggplot2&lt;/strong&gt;, but we can construct one using &lt;code&gt;geom_line&lt;/code&gt;. We need to do some tinkering to add labels. We’ll paste together a character stright with the country name and the starting life expectancy, then do the same with just the later life expectancy for the right side. Below is an example comparing 2010 to 2015 for large western countries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;west &amp;lt;- c(&amp;quot;Western Europe&amp;quot;,&amp;quot;Northern Europe&amp;quot;,&amp;quot;Southern Europe&amp;quot;,
          &amp;quot;Northern America&amp;quot;,&amp;quot;Australia and New Zealand&amp;quot;)

dat &amp;lt;- gapminder %&amp;gt;%
  filter(year%in% c(2010, 2015) &amp;amp; region %in% west &amp;amp;
           !is.na(life_expectancy) &amp;amp; population &amp;gt; 10^7) %&amp;gt;%
    mutate(label_first = ifelse(year == 2010, paste0(country, &amp;quot;: &amp;quot;, round(life_expectancy, 1), &amp;#39; years&amp;#39;), NA),
           label_last = ifelse(year == 2015,  paste0(round(life_expectancy, 1),&amp;#39; years&amp;#39;), NA))

dat %&amp;gt;%
  mutate(location = ifelse(year == 2010, 1, 2),
         location = ifelse(year == 2015 &amp;amp;
                             country %in% c(&amp;quot;United Kingdom&amp;quot;, &amp;quot;Portugal&amp;quot;),
                           location+0.22, location),
         hjust = ifelse(year == 2010, 1, 0)) %&amp;gt;%
  mutate(year = as.factor(year)) %&amp;gt;%
  ggplot(aes(year, life_expectancy, group = country)) +
  geom_line(aes(color = country), show.legend = FALSE) +
  geom_text_repel(aes(label = label_first, color = country), direction = &amp;#39;y&amp;#39;, nudge_x = -1, seed = 1234, show.legend = FALSE) +
  geom_text_repel(aes(label = label_last, color = country), direction = &amp;#39;y&amp;#39;, nudge_x =  1, seed = 1234, show.legend = FALSE) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;Life Expectancy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 12 rows containing missing values (geom_text_repel).

## Warning: Removed 12 rows containing missing values (geom_text_repel).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/slope-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/scatter-plot-instead-of-slope-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the scatterplot, we have followed the principle &lt;em&gt;use common axes&lt;/em&gt; since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bland-altman-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bland-Altman plot&lt;/h3&gt;
&lt;p&gt;Since we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also known as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  mutate(year = paste0(&amp;quot;life_expectancy_&amp;quot;, year)) %&amp;gt;%
  select(country, year, life_expectancy) %&amp;gt;%
  spread(year, life_expectancy) %&amp;gt;%
  mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2,
         difference = life_expectancy_2015 - life_expectancy_2010) %&amp;gt;%
  ggplot(aes(average, difference, label = country)) +
  geom_point() +
  geom_text_repel() +
  geom_abline(lty = 2) +
  xlab(&amp;quot;Average of 2010 and 2015&amp;quot;) +
  ylab(&amp;quot;Difference between 2015 and 2010&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/bland-altman-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bump-charts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bump charts&lt;/h3&gt;
&lt;p&gt;Finally, we can make a bump chart that shows changes in rankings over time. We’ll look at fertility in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the &lt;code&gt;rank()&lt;/code&gt; function to rank countries by the &lt;code&gt;fertility&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sa_fe &amp;lt;- gapminder %&amp;gt;%
  filter(region == &amp;quot;Southern Asia&amp;quot;) %&amp;gt;%
  filter(year &amp;gt;= 2004, year &amp;lt; 2015) %&amp;gt;%
  group_by(year) %&amp;gt;%
  mutate(rank = rank(fertility))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then plot this with points and lines, reversing the y-axis so 1 is at the top:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sa_fe, aes(x = year, y = rank, color = country)) +
  geom_line() +
  geom_point() +
  scale_y_reverse(breaks = 1:8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/make-bump-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Iran holds the number 1 spot, while Sri Lanka dropped from 2 to 6, and Bangladesh increased from 4 to 2.&lt;/p&gt;
&lt;p&gt;As with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use &lt;code&gt;geom_text()&lt;/code&gt; again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the &lt;code&gt;data&lt;/code&gt; argument in &lt;code&gt;geom_text()&lt;/code&gt; though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot &amp;lt;- ggplot(sa_fe, aes(x = year, y = rank, color = country)) +
  geom_line(size = 2) +
  geom_point(size = 4) +
  geom_text(data = sa_fe %&amp;gt;% dplyr::filter(year==2004) %&amp;gt;% arrange(rank),
            aes(label = country, x = 2003), fontface = &amp;quot;bold&amp;quot;, angle = 45) +
 geom_text(data = sa_fe %&amp;gt;% dplyr::filter(year==2014) %&amp;gt;% arrange(rank),
            aes(label = country, x = 2015), fontface = &amp;quot;bold&amp;quot;, angle = 45) +
  guides(color = FALSE) +
  scale_y_reverse(breaks = 1:8) +
  scale_x_continuous(breaks = 2004:2014) +
  scale_color_viridis_d(option = &amp;quot;C&amp;quot;, begin = 0.2, end = 0.9) +
  labs(x = NULL, y = &amp;quot;Rank&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; =
## &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/bump-plot-fancier-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you want to be &lt;em&gt;super&lt;/em&gt; fancy, you can use flags instead of country codes, but that’s a little more complicated (you need to install the &lt;a href=&#34;https://github.com/rensa/ggflags&#34;&gt;&lt;strong&gt;ggflags&lt;/strong&gt; package&lt;/a&gt;. &lt;a href=&#34;https://dominikkoch.github.io/Bump-Chart/&#34;&gt;See here for an example&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;themes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Themes&lt;/h3&gt;
&lt;p&gt;We can go a little further towards a clean, easy-to-read data visualization by using &lt;strong&gt;themes&lt;/strong&gt; in our plots. Themes allow us to set a particular range of plot settings in one command, and let us further tweak things like fonts, background colors, and much more. We’ve used them in passing a few times without highlighting them, but we’ll discuss them here.&lt;/p&gt;
&lt;p&gt;A pre-constructed set of instructions for making a visual theme can be had by using a theme’s &lt;code&gt;ggplot&lt;/code&gt; function. Let’s look at two of my favorites.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;theme_bw()&lt;/code&gt; uses the black-and-white theme, which is helpful in making a nice, clean plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The background shading is gone, which gives the plot a nice, crisp feel. It adds the black outline around the plot, but doesn’t mess with the colors in the plot.&lt;/p&gt;
&lt;p&gt;Here’s &lt;code&gt;theme_minimal()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Themes can alter things in the plot as well. If we really want to strip it down and remove the Y-axis (which is rarely a good idea, but in a bump chart, it makes sense):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot + theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now &lt;strong&gt;that’s&lt;/strong&gt; clean!&lt;/p&gt;
&lt;p&gt;In our opening unit, we had a plot that was styled after the plots in the magazine, &lt;em&gt;The Economist&lt;/em&gt;. That’s a theme (in the &lt;code&gt;ggthemes&lt;/code&gt; package that we loaded at the top)!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot + theme_economist()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Themes affect some of the plot elements that we haven’t gotten much into (like length of axis ticks and the color of the panel grid behind the plot). We can use a theme, then make further changes to the theme. We won’t go into a lot of detail, but here’s an example. Use the &lt;code&gt;?theme&lt;/code&gt; to learn more about what you can change. Half the challenge is finding the right term for the thing you want to tweak! Theme changes occur in code order, so you can update a pre-set theme with your own details:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bumpplot +   theme_bw() + theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;),
                   plot.title = element_text(face = &amp;quot;bold&amp;quot;),
                   axis.text.x = element_text(angle = 45, hjust = 1),
                   panel.grid.major.y = element_blank(), # turn off all of the Y grid
                   panel.grid.minor.y = element_blank(),
                   panel.grid.minor.x = element_blank()) # turn off small x grid&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/bump-plot-fancierest-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;small-multiples&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Small multiples&lt;/h3&gt;
&lt;p&gt;First we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;life_expectancy_small &amp;lt;- gapminder %&amp;gt;%
  filter(country %in% c(&amp;quot;Argentina&amp;quot;, &amp;quot;Bolivia&amp;quot;, &amp;quot;Brazil&amp;quot;,
                        &amp;quot;Belize&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Chile&amp;quot;))
ggplot(data = life_expectancy_small,
       mapping = aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_wrap(vars(country))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Small multiples! That’s all we need to do.&lt;/p&gt;
&lt;p&gt;We can do some fancier things, though. We can make this plot hyper minimalist with a &lt;strong&gt;theme&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = life_expectancy_small,
       mapping = aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_wrap(vars(country), scales = &amp;quot;free_y&amp;quot;) +
  theme_void() +
  theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/life-expectancy-small-minimalist-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can do a whole part of a continent (poor Syria 😞)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;life_expectancy_mena &amp;lt;- gapminder %&amp;gt;%
  filter(region == &amp;quot;Northern Africa&amp;quot; | region == &amp;quot;Western Asia&amp;quot;)

ggplot(data = life_expectancy_mena,
       mapping = aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_wrap(vars(country), scales = &amp;quot;free_y&amp;quot;, nrow = 3) +
  theme_void() +
  theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/life-expectancy-mena-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use the &lt;a href=&#34;https://hafen.github.io/geofacet/&#34;&gt;&lt;strong&gt;geofacet&lt;/strong&gt; package&lt;/a&gt; to arrange these facets by geography:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(geofacet)

life_expectancy_eu &amp;lt;- gapminder %&amp;gt;%
  filter(region== &amp;#39;Western Europe&amp;#39; | region==&amp;#39;Northern Europe&amp;#39; | region==&amp;#39;Southern Europe&amp;#39;)

ggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_geo(vars(country), grid = &amp;quot;europe_countries_grid1&amp;quot;, scales = &amp;quot;free_y&amp;quot;) +
  labs(x = NULL, y = NULL, title = &amp;quot;Life expectancy from 1960–2015&amp;quot;,
       caption = &amp;quot;Source: Gapminder&amp;quot;) +
  theme_minimal() +
  theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;),
        plot.title = element_text(face = &amp;quot;bold&amp;quot;),
        axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/life-expectancy-eu-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Neat!&lt;/p&gt;
&lt;p&gt;Anybody see any problems here?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sparklines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sparklines&lt;/h3&gt;
&lt;p&gt;Sparklines are just line charts (or bar charts) that are really really small.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;india_fert &amp;lt;- gapminder %&amp;gt;%
  filter(country == &amp;quot;India&amp;quot;)
plot_india &amp;lt;- ggplot(india_fert, aes(x = year, y = fertility)) +
  geom_line() +
  theme_void()
plot_india&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/india-spark-1.png&#34; width=&#34;96&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;india_co2.pdf&amp;quot;, plot_india, width = 1, height = 0.15, units = &amp;quot;in&amp;quot;)
ggsave(&amp;quot;india_co2.png&amp;quot;, plot_india, width = 1, height = 0.15, units = &amp;quot;in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;china_fert &amp;lt;- gapminder %&amp;gt;%
  filter(country == &amp;quot;China&amp;quot;)
plot_china &amp;lt;- ggplot(china_fert, aes(x = year, y = fertility)) +
  geom_line() +
  theme_void()
plot_china&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/china-spark-1.png&#34; width=&#34;96&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;china_co2.pdf&amp;quot;, plot_china, width = 1, heighlt = 0.15, units = &amp;quot;in&amp;quot;)
ggsave(&amp;quot;china_co2.png&amp;quot;, plot_china, width = 1, height = 0.15, units = &amp;quot;in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then use those saved tiny plots in your text (with a little html extra in there).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Both India &amp;lt;img class=&amp;quot;img-inline&amp;quot; src=&amp;quot;/your/path/to/india_co2.png&amp;quot; width = &amp;quot;100&amp;quot;/&amp;gt; and
China &amp;lt;img class=&amp;quot;img-inline&amp;quot; src=&amp;quot;/your/path/to/china-spark-1.png&amp;quot; width = &amp;quot;100&amp;quot;/&amp;gt; have
seen increased CO~2~ emissions over the past 20 years.&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Both India &lt;img class=&#34;img-inline&#34; src=&#34;https://ssc442.netlify.app/example/08-example_files/figure-html/india-spark-1.png&#34; width = &#34;100&#34;/&gt; and China &lt;img class=&#34;img-inline&#34; src=&#34;https://ssc442.netlify.app/example/08-example_files/figure-html/china-spark-1.png&#34; width = &#34;100&#34;/&gt; have seen increased CO&lt;sub&gt;2&lt;/sub&gt; emissions over the past 20 years.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-ecological-fallacy-and-importance-of-showing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The ecological fallacy and importance of showing the data&lt;/h2&gt;
&lt;p&gt;Throughout this section, we have been comparing regions of the world. We have seen that, on average, some regions do better than others. In this section, we focus on describing the importance of variability within the groups when examining the relationship between a country’s infant mortality rates and average income.&lt;/p&gt;
&lt;p&gt;We define a few more regions and compare the averages across regions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/ecological-fallacy-averages-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The relationship between these two variables is almost perfectly linear and the graph shows a dramatic difference. While in the West less than 0.5% of infants die, in Sub-Saharan Africa the rate is higher than 6%!&lt;/p&gt;
&lt;p&gt;Note that the plot uses a new transformation, the logistic transformation.&lt;/p&gt;
&lt;div id=&#34;logit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logistic transformation&lt;/h3&gt;
&lt;p&gt;The logistic or logit transformation for a proportion or rate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(p) = \log \left( \frac{p}{1-p} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is a proportion or probability, the quantity that is being logged, &lt;span class=&#34;math inline&#34;&gt;\(p/(1-p)\)&lt;/span&gt;, is called the &lt;em&gt;odds&lt;/em&gt;. In this case &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the proportion of infants that survived. The odds tell us how many more infants are expected to survive than to die. The log transformation makes this symmetric. If the rates are the same, then the log odds is 0. Fold increases or decreases turn into positive and negative increments, respectively.&lt;/p&gt;
&lt;p&gt;This scale is useful when we want to highlight differences near 0 or 1. For survival rates this is important because a survival rate of 90% is unacceptable, while a survival of 99% is relatively good. We would much prefer a survival rate closer to 99.9%. We want our scale to highlight these difference and the logit does this. Note that 99.9/0.1 is about 10 times bigger than 99/1 which is about 10 times larger than 90/10. By using the log, these fold changes turn into constant increases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;show-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Show the data&lt;/h3&gt;
&lt;p&gt;Now, back to our plot. Based on the plot above, do we conclude that a country with a low income is destined to have low survival rate? Do we conclude that survival rates in Sub-Saharan Africa are all lower than in Southern Asia, which in turn are lower than in the Pacific Islands, and so on?&lt;/p&gt;
&lt;p&gt;Jumping to this conclusion based on a plot showing averages is referred to as the &lt;em&gt;ecological fallacy&lt;/em&gt;. The almost perfect relationship between survival rates and income is only observed for the averages at the region level. Once we show all the data, we see a somewhat more complicated story:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: ggrepel: 1 unlabeled data points (too many overlaps). Consider
## increasing max.overlaps&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/ecological-fallacy-all-data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Specifically, we see that there is a large amount of variability. We see that countries from the same regions can be quite different and that countries with the same income can have different survival rates. For example, while on average Sub-Saharan Africa had the worse health and economic outcomes, there is wide variability within that group. Mauritius and Botswana are doing better than Angola and Sierra Leone, with Mauritius comparable to Western countries.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vaccines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: vaccines and infectious diseases&lt;/h2&gt;
&lt;p&gt;Vaccines have helped save millions of lives. In the 19th century, before herd immunization was achieved through vaccination programs, deaths from infectious diseases, such as smallpox and polio, were common. However, today vaccination programs have become somewhat controversial despite all the scientific evidence for their importance.&lt;/p&gt;
&lt;p&gt;The controversy started with a paper&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; published in 1988 and led by Andrew Wakefield claiming
there was a link between the administration of the measles, mumps, and rubella (MMR) vaccine and the appearance of autism and bowel disease.
Despite much scientific evidence contradicting this finding, sensationalist media reports and fear-mongering from conspiracy theorists led parts of the public into believing that vaccines were harmful. As a result, many parents ceased to vaccinate their children. This dangerous practice can be potentially disastrous given that the Centers for Disease Control (CDC) estimates that vaccinations will prevent more than 21 million hospitalizations and 732,000 deaths among children born in the last 20 years (see Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;).
The 1988 paper has since been retracted and Andrew Wakefield was eventually “struck off the UK medical register, with a statement identifying deliberate falsification in the research published in The Lancet, and was thereby barred from practicing medicine in the UK.” (source: Wikipedia&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;). Yet misconceptions persist, in part due to self-proclaimed activists who continue to disseminate misinformation about vaccines.&lt;/p&gt;
&lt;p&gt;Effective communication of data is a strong antidote to misinformation and fear-mongering. Earlier we used an example provided by a Wall Street Journal article&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; showing data related to the impact of vaccines on battling infectious diseases. Here we reconstruct that example.&lt;/p&gt;
&lt;p&gt;The data used for these plots were collected, organized, and distributed by the Tycho Project&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. They include weekly reported counts for seven diseases from 1928 to 2011, from all fifty states. The yearly totals are helpfully included in the &lt;strong&gt;dslabs&lt;/strong&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RColorBrewer)
data(us_contagious_diseases)
names(us_contagious_diseases)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;disease&amp;quot;         &amp;quot;state&amp;quot;           &amp;quot;year&amp;quot;            &amp;quot;weeks_reporting&amp;quot;
## [5] &amp;quot;count&amp;quot;           &amp;quot;population&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a temporary object &lt;code&gt;dat&lt;/code&gt; that stores only the measles data, includes a per 100,000 rate, orders states by average value of disease and removes Alaska and Hawaii since they only became states in the late 1950s. Note that there is a &lt;code&gt;weeks_reporting&lt;/code&gt; column that tells us for how many weeks of the year data was reported. We have to adjust for that value when computing the rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_disease &amp;lt;- &amp;quot;Measles&amp;quot;
dat &amp;lt;- us_contagious_diseases %&amp;gt;%
  filter(!state%in%c(&amp;quot;Hawaii&amp;quot;,&amp;quot;Alaska&amp;quot;) &amp;amp; disease == the_disease) %&amp;gt;%
  mutate(rate = count / population * 10000 * 52 / weeks_reporting) %&amp;gt;%
  mutate(state = reorder(state, rate))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now easily plot disease rates per year. Here are the measles data from California:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% filter(state == &amp;quot;California&amp;quot; &amp;amp; !is.na(rate)) %&amp;gt;%
  ggplot(aes(year, rate)) +
  geom_line() +
  ylab(&amp;quot;Cases per 10,000&amp;quot;)  +
  geom_vline(xintercept=1963, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/california-measles-time-series-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We add a vertical line at 1963 since this is when the vaccine was introduced [Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 (the yellow book). p. 250. ISBN 9780199948505].&lt;/p&gt;
&lt;p&gt;Now can we show data for all states in one plot? We have three variables to show: year, state, and rate. In the WSJ figure, they use the x-axis for year, the y-axis for state, and color hue to represent rates. However, the color scale they use, which goes from yellow to blue to green to orange to red, can be improved.&lt;/p&gt;
&lt;p&gt;In our example, we want to use a sequential palette since there is no meaningful center, just low and high rates.&lt;/p&gt;
&lt;p&gt;We use the geometry &lt;code&gt;geom_tile&lt;/code&gt; to tile the region with colors representing disease rates. We use a square root transformation to avoid having the really high counts dominate the plot. Notice that missing values are shown in grey. Note that once a disease was pretty much eradicated, some states stopped reporting cases all together. This is why we see so much grey after 1980.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% ggplot(aes(year, state, fill = rate)) +
  geom_tile(color = &amp;quot;grey50&amp;quot;) +
  scale_x_continuous(expand=c(0,0)) +
  scale_fill_gradientn(colors = brewer.pal(9, &amp;quot;Reds&amp;quot;), trans = &amp;quot;sqrt&amp;quot;) +
  geom_vline(xintercept=1963, col = &amp;quot;blue&amp;quot;) +
  theme_minimal() +
  theme(panel.grid = element_blank(),
        legend.position=&amp;quot;bottom&amp;quot;,
        text = element_text(size = 8)) +
  ggtitle(the_disease) +
  ylab(&amp;quot;&amp;quot;) + xlab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/vaccines-plot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot makes a very striking argument for the contribution of vaccines. However, one limitation of this plot is that it uses color to represent quantity, which we earlier explained makes it harder to know exactly how high values are going. Position and lengths are better cues. If we are willing to lose state information, we can make a version of the plot that shows the values with position. We can also show the average for the US, which we compute like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg &amp;lt;- us_contagious_diseases %&amp;gt;%
  filter(disease==the_disease) %&amp;gt;% group_by(year) %&amp;gt;%
  summarize(us_rate = sum(count, na.rm = TRUE) /
              sum(population, na.rm = TRUE) * 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to make the plot we simply use the &lt;code&gt;geom_line&lt;/code&gt; geometry:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  filter(!is.na(rate)) %&amp;gt;%
    ggplot() +
  geom_line(aes(year, rate, group = state),  color = &amp;quot;grey50&amp;quot;,
            show.legend = FALSE, alpha = 0.2, size = 1) +
  geom_line(mapping = aes(year, us_rate),  data = avg, size = 1) +
  scale_y_continuous(trans = &amp;quot;sqrt&amp;quot;, breaks = c(5, 25, 125, 300)) +
  ggtitle(&amp;quot;Cases per 10,000 by state&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  geom_text(data = data.frame(x = 1955, y = 50),
            mapping = aes(x, y, label=&amp;quot;US average&amp;quot;),
            color=&amp;quot;black&amp;quot;) +
  geom_vline(xintercept=1963, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/04-example_files/figure-html/time-series-vaccines-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In theory, we could use color to represent the categorical value state, but it is hard to pick 50 distinct colors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;try-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/h2&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Reproduce the image plot we previously made but for smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now reproduce the time series plot we previously made, but this time following the instructions of the previous question for smallpox.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the state of California, make a time series plot showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now do the same for the rates for the US. Hint: compute the US rate by using summarize: the total divided by total population.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Hans_Rosling&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Hans_Rosling&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.gapminder.org/&#34; class=&#34;uri&#34;&gt;http://www.gapminder.org/&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract&#34; class=&#34;uri&#34;&gt;http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm&#34; class=&#34;uri&#34;&gt;https://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Andrew_Wakefield&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Andrew_Wakefield&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://graphics.wsj.com/infectious-diseases-and-vaccines/&#34; class=&#34;uri&#34;&gt;http://graphics.wsj.com/infectious-diseases-and-vaccines/&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.tycho.pitt.edu/&#34; class=&#34;uri&#34;&gt;http://www.tycho.pitt.edu/&lt;/a&gt;&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Large(ish) Data</title>
      <link>https://ssc442.netlify.app/assignment/04-assignment/</link>
      <pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/04-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started&#34;&gt;Getting started&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bonus-exercise&#34;&gt;Bonus Exercise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#turning-everything-in&#34;&gt;Turning everything in&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#postscript-how-we-got-this-unemployment-data&#34;&gt;Postscript: how we got this unemployment data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, September 26th&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Getting started&lt;/h1&gt;
&lt;p&gt;For this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, &lt;a href=&#34;#postscript-how-we-got-this-unemployment-data&#34;&gt;we describe how we built this dataset down below&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/03-lab/data/unemployment.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;unemployment.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;To help you&lt;/strong&gt;, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/03-lab/03-lab.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; &lt;code&gt;03-lab.Rmd&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the end, to help you master file organization, we suggest that the structure of your project directory should look something like this:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;your-project-name\
  03-lab.Rmd
  your-project-name.Rproj
  data\
    unemployment.csv&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/03-example/&#34;&gt;The example for this week&lt;/a&gt; will be &lt;strong&gt;&lt;em&gt;incredibly&lt;/em&gt;&lt;/strong&gt; helpful for this exercise. Reference it.&lt;/p&gt;
&lt;p&gt;For this week, you need to start making your plots look nice. For full credit, you will have to label axes, label the plot, and experiment with themes. Experiment with adding a &lt;code&gt;labs()&lt;/code&gt; layer or changing colors. Or, if you’re super brave, try modifying a theme and its elements. Default plots will not receive full credit.&lt;/p&gt;
&lt;p&gt;You’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type &lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;alt&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; on Windows, or &lt;kbd&gt;⌘&lt;/kbd&gt; + &lt;kbd&gt;⌥&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; on macOS.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use data from the US Bureau of Labor Statistics (BLS) to show the trends in employment rate for all 50 states between 2006 and 2016. What stories does this plot tell? Which states struggled to recover from the 2008–09 recession?&lt;/p&gt;
&lt;p&gt;Some hints/tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You won’t need to filter out any missing rows because the data here is complete—there are no state-year combinations with missing unemployment data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll be plotting 51 facets. You can filter out DC if you want to have a better grid (like 5 × 10), or you can try using &lt;code&gt;facet_geo()&lt;/code&gt; from the &lt;strong&gt;geofacet&lt;/strong&gt; package to lay out the plots like a map of the US (try this!).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot the &lt;code&gt;date&lt;/code&gt; column along the x-axis, &lt;em&gt;not&lt;/em&gt; the &lt;code&gt;year&lt;/code&gt; column. If you plot by year, you’ll get weird looking lines (try it for fun?), since these observations are monthly. If you really want to plot by year only, you’ll need to create a different data frame where you group by year and state and calculate the average unemployment rate for each year/state combination (i.e. &lt;code&gt;group_by(year, state) %&amp;gt;% summarize(avg_unemployment = mean(unemployment))&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try mapping other aesthetics onto the graph too. You’ll notice there are columns for region and division—play with those as colors, for instance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This plot might be big, so make sure you adjust &lt;code&gt;fig.width&lt;/code&gt; and &lt;code&gt;fig.height&lt;/code&gt; in the chunk options so that it’s visible when you knit it. You might also want to used &lt;code&gt;ggsave()&lt;/code&gt; to save it with extra large dimensions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use data from the BLS to create a slopegraph that compares the unemployment rate in January 2006 with the unemployment rate in January 2009, either for all 50 states at once (good luck with that!) or for a specific region or division. Make sure the plot doesn’t look too busy or crowded in the end.&lt;/p&gt;
&lt;p&gt;What story does this plot tell? Which states in the US (or in the specific region you selected) were the most/least affected the Great Recession?&lt;/p&gt;
&lt;p&gt;Some hints/tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should use &lt;code&gt;filter()&lt;/code&gt; to only select rows where the year is 2006 or 2009 (i.e. &lt;code&gt;filter(year %in% c(2006, 2009)&lt;/code&gt;) and to select rows where the month is January (&lt;code&gt;filter(month == 1)&lt;/code&gt; or &lt;code&gt;filter(month_name == &#34;January&#34;)&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order for the year to be plotted as separate categories on the x-axis, it needs to be a factor, so use &lt;code&gt;mutate(year = factor(year))&lt;/code&gt; to convert it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To make ggplot draw lines between the 2006 and 2009 categories, you need to include &lt;code&gt;group = state&lt;/code&gt; in the aesthetics.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bonus-exercise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bonus Exercise&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This is entirely optional but might be fun.&lt;/strong&gt; Then again, it might not be fun. I don’t know.&lt;/p&gt;
&lt;p&gt;For extra fun times, if you feel like it, create a bump chart showing something from the unemployment data (perhaps the top 10 states or bottom 10 states in unemployment?) Adapt the code in the &lt;a href=&#34;https://ssc442.netlify.app/example/03-example/&#34;&gt;example for today’s session&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you do this, plotting 51 lines is going to be a huge mess. But filtering the data is also a bad idea, because states could drop in and out of the top/bottom 10 over time, and we don’t want to get rid of them. Instead, you can zoom in on a specific range of data in your plot with &lt;code&gt;coord_cartesian(ylim = c(1, 10))&lt;/code&gt;, for instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;turning-everything-in&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Turning everything in&lt;/h2&gt;
&lt;p&gt;When you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. Upload the PDF file to D2L.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;postscript-how-we-got-this-unemployment-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Postscript: how we got this unemployment data&lt;/h2&gt;
&lt;p&gt;For the curious, &lt;a href=&#34;https://ssc442.netlify.app/projects/get_bls_data.R&#34;&gt;here’s the code we used&lt;/a&gt; to download the unemployment data from the BLS.&lt;/p&gt;
&lt;p&gt;And to pull the curtain back and show how much googling is involved in data visualization (and data analysis and programming in general), here was my process for getting this data:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We thought “We want to have students show variation in something domestic over time” and then we googled “us data by state”. Nothing really came up (since it was an exceedingly vague search in the first place), but some results mentioned unemployment rates, so we figured that could be cool.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We googled “unemployment statistics by state over time” and found that the BLS keeps statistics on this. We clicked on the &lt;a href=&#34;https://www.bls.gov/data/&#34;&gt;“Data Tools” link in their main navigation bar&lt;/a&gt;, clicked on “Unemployment”, and then clicked on the “Multi-screen data search” button for the Local Area Unemployment Statistics (LAUS).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We walked through the multiple screens and got excited that we’d be able to download all unemployment stats for all states for a ton of years, &lt;em&gt;but then&lt;/em&gt; the final page had links to 51 individual Excel files, which was dumb.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;So we went back to Google and searched for “download bls data r” and found a few different packages people have written to do this. The first one we clicked on was &lt;a href=&#34;https://github.com/keberwein/blscrapeR&#34;&gt;&lt;code&gt;blscrapeR&lt;/code&gt; at GitHub&lt;/a&gt;, and it looked like it had been updated recently, so we went with it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We followed the examples in the &lt;code&gt;blscrapeR&lt;/code&gt; package and downloaded data for every state.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another day in the life of doing modern data science. This is an example of something you will be able to do by the end of this class. we had no idea people had written &lt;code&gt;R&lt;/code&gt; packages to access BLS data, but there are (at least) 3 packages out there. After a few minutes of tinkering, we got it working and it is relatively straightforward.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ggplot2: Everything you ever wanted to know</title>
      <link>https://ssc442.netlify.app/example/03-example/</link>
      <pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/03-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#distributions&#34;&gt;Visualizing data distributions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#variable-types&#34;&gt;Variable types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-describing-student-heights&#34;&gt;Case study: describing student heights&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#distribution-function&#34;&gt;Distribution function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cdf-intro&#34;&gt;Cumulative distribution functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geometries-for-describing-distributions&#34;&gt;Geometries for describing distributions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#smoothed-density&#34;&gt;Smoothed density&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpreting-the-y-axis&#34;&gt;Interpreting the y-axis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#densities-permit-stratification&#34;&gt;Densities permit stratification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normal-distribution&#34;&gt;The normal distribution&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#standard-units&#34;&gt;Standard units&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantile-quantile-plots&#34;&gt;Quantile-quantile plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#percentiles&#34;&gt;Percentiles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-geometries&#34;&gt;ggplot2 geometries&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#barplots&#34;&gt;Barplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histograms-1&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plots&#34;&gt;Density plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boxplots&#34;&gt;Boxplots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#try-it&#34;&gt;Try it!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;distributions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualizing data distributions&lt;/h1&gt;
&lt;p&gt;Throughout your education, you may have noticed that numerical data is often summarized with the &lt;em&gt;average&lt;/em&gt; value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the &lt;em&gt;standard deviation&lt;/em&gt;. For example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list?&lt;/p&gt;
&lt;p&gt;Our first data visualization building block is learning to summarize lists of factors or numeric vectors—the two primary data types that we encounter in data analytics. More often than not, the best way to share or explore this summary is through data visualization. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as a distribution, there are several data visualization techniques to effectively relay this information.&lt;/p&gt;
&lt;p&gt;In this section, we first discuss properties of a variety of distributions and how to visualize distributions using a motivating example of student heights. We then discuss some principles of data visualizations more broadly, and introduce new ggplot geometries to help us along the way.&lt;/p&gt;
&lt;div id=&#34;variable-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variable types&lt;/h2&gt;
&lt;p&gt;We will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.&lt;/p&gt;
&lt;p&gt;When each entry in a vector comes from one of a small number of groups, we refer to the data as &lt;em&gt;categorical data&lt;/em&gt;. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as &lt;em&gt;ordinal&lt;/em&gt; data. In psychology, a number of different terms are used for this same idea.&lt;/p&gt;
&lt;p&gt;Examples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as population sizes, are discrete because they have to be integers—that’s how we count.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-describing-student-heights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: describing student heights&lt;/h2&gt;
&lt;p&gt;Here we consider an artificial problem to help us illustrate the underlying concepts.&lt;/p&gt;
&lt;p&gt;Pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the &lt;code&gt;heights&lt;/code&gt; data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
data(heights)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights.&lt;/p&gt;
&lt;div id=&#34;distribution-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Distribution function&lt;/h3&gt;
&lt;p&gt;It turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.&lt;/p&gt;
&lt;p&gt;The most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##    Female      Male 
## 0.2266667 0.7733333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This two-category &lt;em&gt;frequency table&lt;/em&gt; is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/state-region-distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cdf-intro&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cumulative distribution functions&lt;/h3&gt;
&lt;p&gt;Numerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of &lt;code&gt;68.503937007874&lt;/code&gt; inches and only one student reported a height &lt;code&gt;68.8976377952756&lt;/code&gt; inches. We assume that they converted from 174 and 175 centimeters, respectively.&lt;/p&gt;
&lt;p&gt;Statistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; for all possible values of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;. This function is called the cumulative distribution function (CDF). In statistics, the following notation is used:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ F(a) = \mbox{Pr}(x \leq a) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here is a plot of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; for the male height data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/ecdf-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar to what the frequency table does for categorical data, the CDF
defines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since &lt;span class=&#34;math inline&#34;&gt;\(F(66)=\)&lt;/span&gt; 0.1637931, or that 84% of the values are below 72, since &lt;span class=&#34;math inline&#34;&gt;\(F(72)=\)&lt;/span&gt; 0.841133,
and so on. In fact, we can report the proportion of values between any two heights, say &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, by computing &lt;span class=&#34;math inline&#34;&gt;\(F(b) - F(a)\)&lt;/span&gt;. This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.&lt;/p&gt;
&lt;p&gt;A final note: because CDFs can be defined mathematically—and absent any data—the word &lt;em&gt;empirical&lt;/em&gt; is added to make the distinction when data is used. We therefore use the term empirical CDF (eCDF).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;geometries-for-describing-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geometries for describing distributions&lt;/h2&gt;
&lt;p&gt;Now, we’ll introduce ggplot geometries useful for describing distributions (or for many other things).&lt;/p&gt;
&lt;div id=&#34;histograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histograms&lt;/h3&gt;
&lt;p&gt;Although the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? I doubt you can figure these out from glancing at the plot above. Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret.&lt;/p&gt;
&lt;p&gt;The simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: &lt;span class=&#34;math inline&#34;&gt;\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_histogram(binwidth = 1, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/height-histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we send this &lt;strong&gt;histogram&lt;/strong&gt; plot to some uninformed reader, she will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, this reader could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.&lt;/p&gt;
&lt;p&gt;What information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;geom_histogram&lt;/code&gt; layer only requires one aesthetic mapping - the x-axis. This is because the y-axis is computed from counts of the x-axis. Giving an aesthetic mapping to an additional variable for y will result in an error. Using an aesthetic mapping like &lt;code&gt;fill&lt;/code&gt; will work - it’ll give you two histograms on top of each other. Try it! Try setting the &lt;code&gt;alpha&lt;/code&gt; aesthetic to .5 (not an aesthetic mapping) so you can see both layers when they overlap.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;smoothed-density&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Smoothed density&lt;/h3&gt;
&lt;p&gt;Smooth density plots are aesthetically more appealing than histograms. &lt;code&gt;geom_density&lt;/code&gt; is the geometry that gives a smoothed density. Here is what a smooth density plot looks like for our heights data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_density(alpha = .2, fill= &amp;quot;#00BFC4&amp;quot;, color = &amp;#39;gray50&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/example-of-smoothed-density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to &lt;em&gt;density&lt;/em&gt;. That is, the area under the curve will add up to 1, so we can read it like a probability density.&lt;/p&gt;
&lt;p&gt;To understand the smooth densities, we have to understand &lt;em&gt;estimates&lt;/em&gt;, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool.&lt;/p&gt;
&lt;p&gt;The main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.&lt;/p&gt;
&lt;p&gt;However, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/simulated-data-histogram-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/simulated-data-histogram-2-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts. We do this by using the double-dot object &lt;code&gt;..density..&lt;/code&gt;. Objects surrounded by &lt;code&gt;..&lt;/code&gt; are objects that are &lt;em&gt;calculated by ggplot&lt;/em&gt;. If we look at &lt;code&gt;?geom_histogram&lt;/code&gt;, and go down to “Computed variables”, we see that we could use &lt;code&gt;..count..&lt;/code&gt; to get “number of points in a bin”; &lt;code&gt;..ncount..&lt;/code&gt; for the count scaled to a max of 1; or &lt;code&gt;..ndensity..&lt;/code&gt; which scales the density to a max of 1 (which is a strange one). We can manually set the y aesthetic mapping, which defaults to &lt;code&gt;..count..&lt;/code&gt;, to &lt;code&gt;..density..&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x %&amp;gt;% ggplot(aes(x = height)) +
  geom_histogram(aes(y=..density..), binwidth = 0.1, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/simulated-density-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, back to reality. We don’t have millions of measurements. In this concrete example, we have 812 and we can’t make a histogram with very small bins.&lt;/p&gt;
&lt;p&gt;We therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots (loosely) demonstrate the steps that the computer goes through to ultimately create a smooth density:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/smooth-density-2-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpreting-the-y-axis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpreting the y-axis&lt;/h3&gt;
&lt;p&gt;Note that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/area-under-curve-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The proportion of this area is about
0.3,
meaning that about
30%
of male heights are between 65 and 68 inches.&lt;/p&gt;
&lt;p&gt;By understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_density(alpha=.2, fill= &amp;quot;#00BFC4&amp;quot;, color = &amp;#39;black&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/example-of-smoothed-density-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the only aesthetic mapping is &lt;code&gt;x = height&lt;/code&gt;, while the fill and color are set as un-mapped aesthetics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;densities-permit-stratification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Densities permit stratification&lt;/h3&gt;
&lt;p&gt;As a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities make it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(height, fill=sex)) +
  geom_density(alpha = 0.2, color = &amp;#39;black&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/two-densities-one-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the right argument, &lt;code&gt;ggplot&lt;/code&gt; automatically shades the intersecting region with a different color.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The normal distribution&lt;/h2&gt;
&lt;p&gt;Histograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.&lt;/p&gt;
&lt;p&gt;The normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.&lt;/p&gt;
&lt;p&gt;Rather than using data, the normal distribution is defined with a mathematical formula. For any interval &lt;span class=&#34;math inline&#34;&gt;\((a,b)\)&lt;/span&gt;, the proportion of values in that interval can be computed using this formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{Pr}(a &amp;lt; x &amp;lt; b) = \int_a^b \frac{1}{\sqrt{2\pi}s} e^{-\frac{1}{2}\left( \frac{x-m}{s} \right)^2} \, dx\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. The rest of the symbols in the formula represent the interval ends that we determine, &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and known mathematical constants &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. These two parameters, &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, are referred to as the &lt;em&gt;average&lt;/em&gt; (also called the &lt;em&gt;mean&lt;/em&gt;) and the &lt;em&gt;standard deviation&lt;/em&gt; (SD) of the distribution, respectively.&lt;/p&gt;
&lt;p&gt;The distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/normal-distribution-density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.&lt;/p&gt;
&lt;p&gt;For a list of numbers contained in a vector &lt;code&gt;x&lt;/code&gt;, the average is defined as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- sum(x) / length(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the SD is defined as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- sqrt(sum((x-mu)^2) / length(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which can be interpreted as the average distance between values and their average.&lt;/p&gt;
&lt;p&gt;Let’s compute the values for the height for males which we will store in the object &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- heights$sex == &amp;quot;Male&amp;quot;
x &amp;lt;- heights$height[index]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pre-built functions &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sd&lt;/code&gt; (note that for reasons explained later, &lt;code&gt;sd&lt;/code&gt; divides by &lt;code&gt;length(x)-1&lt;/code&gt; rather than &lt;code&gt;length(x)&lt;/code&gt;) can be used here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- mean(x)
s &amp;lt;- sd(x)
c(average = m, sd = s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   average        sd 
## 69.314755  3.611024&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/data-and-normal-densities-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, we can ask the question “is our height data approximately normally distributed?”. The normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.&lt;/p&gt;
&lt;div id=&#34;standard-units&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Standard units&lt;/h3&gt;
&lt;p&gt;For data that is approximately normally distributed, it is convenient to think in terms of &lt;em&gt;standard units&lt;/em&gt;. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value &lt;code&gt;x&lt;/code&gt; from a vector &lt;code&gt;X&lt;/code&gt;, we define the value of &lt;code&gt;x&lt;/code&gt; in standard units as &lt;code&gt;z = (x - m)/s&lt;/code&gt; with &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;s&lt;/code&gt; the average and standard deviation of &lt;code&gt;X&lt;/code&gt;, respectively. Why is this convenient?&lt;/p&gt;
&lt;p&gt;First look back at the formula for the normal distribution and note that what is being exponentiated is &lt;span class=&#34;math inline&#34;&gt;\(-z^2/2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; equivalent to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in standard units. Because the maximum of &lt;span class=&#34;math inline&#34;&gt;\(e^{-z^2/2}\)&lt;/span&gt; is when &lt;span class=&#34;math inline&#34;&gt;\(z=0\)&lt;/span&gt;, this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since &lt;span class=&#34;math inline&#34;&gt;\(- z^2/2\)&lt;/span&gt; is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (&lt;span class=&#34;math inline&#34;&gt;\(z=0\)&lt;/span&gt;), one of the largest (&lt;span class=&#34;math inline&#34;&gt;\(z \approx 2\)&lt;/span&gt;), one of the smallest (&lt;span class=&#34;math inline&#34;&gt;\(z \approx -2\)&lt;/span&gt;), or an extremely rare occurrence (&lt;span class=&#34;math inline&#34;&gt;\(z &amp;gt; 3\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(z &amp;lt; -3\)&lt;/span&gt;). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.&lt;/p&gt;
&lt;p&gt;In R, we can obtain standard units using the function &lt;code&gt;scale&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- scale(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to see how many men are within 2 SDs from the average, we simply type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(abs(z) &amp;lt; 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9495074&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-quantile-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quantile-quantile plots&lt;/h3&gt;
&lt;p&gt;A systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot). If our heights distribution is really normal, then the 10th percentile of our heights data should be the same as the 10th percentile of a theoretical normal, as should the 20th, 30th, 33rd, 37.5th, etc. percentiles.&lt;/p&gt;
&lt;p&gt;First let’s define the theoretical quantiles (percentiles) for the normal distribution. In statistics books we use the symbol &lt;span class=&#34;math inline&#34;&gt;\(\Phi(x)\)&lt;/span&gt; to define the function that gives us the probability of a standard normal distribution being smaller than &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. So, for example, &lt;span class=&#34;math inline&#34;&gt;\(\Phi(-1.96) = 0.025\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Phi(1.96) = 0.975\)&lt;/span&gt;. In R, we can evaluate &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; using the &lt;code&gt;pnorm&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(-1.96)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0249979&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The inverse function &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1}(x)\)&lt;/span&gt; gives us the &lt;em&gt;theoretical quantiles&lt;/em&gt; for the normal distribution. So, for example, &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1}(0.975) = 1.96\)&lt;/span&gt;. In R, we can evaluate the inverse of &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; using the &lt;code&gt;qnorm&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qnorm(0.975)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.959964&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution. We can do this using the &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sd&lt;/code&gt; arguments in the &lt;code&gt;pnorm&lt;/code&gt; and &lt;code&gt;qnorm&lt;/code&gt; function. For example, we can use &lt;code&gt;qnorm&lt;/code&gt; to determine quantiles of a distribution with a specific average and standard deviation&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qnorm(0.975, mean = 5, sd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8.919928&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the normal distribution, all the calculations related to quantiles are done without data, thus the name &lt;em&gt;theoretical quantiles&lt;/em&gt;. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we can define the quantile associated with any proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; as the &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; for which the proportion of values below &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Using R code, we can define &lt;code&gt;q&lt;/code&gt; as the value for which &lt;code&gt;mean(x &amp;lt;= q) = p&lt;/code&gt;. Notice that not all &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; have a &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; for which the proportion is exactly &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. There are several ways of defining the best &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; as discussed in the help for the &lt;code&gt;quantile&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;To give a quick example, for the male heights data, we have that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 69.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5147783&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So about 50% are shorter or equal to 69 inches. This implies that if &lt;span class=&#34;math inline&#34;&gt;\(p=0.50\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(q=69.5\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Define a vector of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; proportions &lt;span class=&#34;math inline&#34;&gt;\(p_1, p_2, \dots, p_m\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Define a vector of quantiles &lt;span class=&#34;math inline&#34;&gt;\(q_1, \dots, q_m\)&lt;/span&gt; for your data for the proportions &lt;span class=&#34;math inline&#34;&gt;\(p_1, \dots, p_m\)&lt;/span&gt;. We refer to these as the &lt;em&gt;sample quantiles&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Define a vector of theoretical quantiles for the proportions &lt;span class=&#34;math inline&#34;&gt;\(p_1, \dots, p_m\)&lt;/span&gt; for a normal distribution with the same average and standard deviation as the data.&lt;/li&gt;
&lt;li&gt;Plot the sample quantiles versus the theoretical quantiles.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s construct a QQ-plot using R code. Start by defining the vector of proportions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- seq(0.005, 0.995, 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain the quantiles from the data, we can use the &lt;code&gt;quantile&lt;/code&gt; function like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_quantiles &amp;lt;- quantile(x, p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the &lt;code&gt;qnorm&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theoretical_quantiles &amp;lt;- qnorm(p, mean = mean(x), sd = sd(x))
df = data.frame(sample_quantiles, theoretical_quantiles)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see if they match or not, we plot them against each other and draw the identity line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df, aes(x = theoretical_quantiles, y = sample_quantiles)) +
  geom_point() +
  geom_abline() # a 45-degree line&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/qqplot-original-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that this code becomes much cleaner if we use standard units:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_quantiles &amp;lt;- quantile(z, p)
theoretical_quantiles &amp;lt;- qnorm(p)
df2 =  data.frame(sample_quantiles, theoretical_quantiles)
ggplot(data = df2, aes(x = theoretical_quantiles, y = sample_quantiles)) +
  geom_point() +
  geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/qqplot-standardized-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above code is included to help describe QQ-plots. However, in practice it is easier to use the &lt;code&gt;ggplot&lt;/code&gt; geometry &lt;code&gt;geom_qq&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% filter(sex == &amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(sample = scale(height))) +
  geom_qq() +
  geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While for the illustration above we used 100 quantiles, the default from the &lt;code&gt;geom_qq&lt;/code&gt; function is to use as many quantiles as data points.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;percentiles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Percentiles&lt;/h3&gt;
&lt;p&gt;Before we move on, let’s define some terms that are commonly used in exploratory data analysis.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Percentiles&lt;/em&gt; are special cases of &lt;em&gt;quantiles&lt;/em&gt; that are commonly used. The percentiles are the quantiles you obtain when setting the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(0.01, 0.02, ..., 0.99\)&lt;/span&gt;. We call, for example, the case of &lt;span class=&#34;math inline&#34;&gt;\(p=0.25\)&lt;/span&gt; the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the &lt;em&gt;median&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For the normal distribution the &lt;em&gt;median&lt;/em&gt; and average are the same, but this is generally not the case.&lt;/p&gt;
&lt;p&gt;Another special case that receives a name are the &lt;em&gt;quartiles&lt;/em&gt;, which are obtained when setting &lt;span class=&#34;math inline&#34;&gt;\(p=0.25,0.50\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(0.75\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;other-geometries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ggplot2 geometries&lt;/h2&gt;
&lt;p&gt;Alhough we haven’t gone into detain about the &lt;strong&gt;ggplot2&lt;/strong&gt; package for data visualization, we now will briefly discuss some of the geometries involved in the plots above. We will discuss &lt;strong&gt;ggplot2&lt;/strong&gt; in (excruciating) detail later this week. For now, we will briefly demonstrate how to generate plots related to distributions.&lt;/p&gt;
&lt;div id=&#34;barplots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Barplots&lt;/h3&gt;
&lt;p&gt;To generate a barplot we can use the &lt;code&gt;geom_bar&lt;/code&gt; geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot(aes(region)) + geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/barplot-geom-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(murders)
tab &amp;lt;- murders %&amp;gt;%
  count(region) %&amp;gt;%
  mutate(proportion = n/sum(n))
tab&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          region  n proportion
## 1     Northeast  9  0.1764706
## 2         South 17  0.3333333
## 3 North Central 12  0.2352941
## 4          West 13  0.2549020&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We no longer want &lt;code&gt;geom_bar&lt;/code&gt; to count, but rather just plot a bar to the height provided by the &lt;code&gt;proportion&lt;/code&gt; variable. For this we need to provide &lt;code&gt;x&lt;/code&gt; (the categories) and &lt;code&gt;y&lt;/code&gt; (the values) and use the &lt;code&gt;stat=&#34;identity&#34;&lt;/code&gt; option. This tells R to just use the actual value in &lt;code&gt;proportion&lt;/code&gt; for the y aesthetic. This is only necessary when you’re telling R that you have your own field (&lt;code&gt;proportion&lt;/code&gt;) that you want to use instead of just the count.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab %&amp;gt;% ggplot(aes(x = region, y = proportion)) + geom_bar(stat = &amp;quot;identity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/region-freq-barplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;histograms-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histograms&lt;/h3&gt;
&lt;p&gt;To generate histograms we use &lt;code&gt;geom_histogram&lt;/code&gt;. By looking at the help file for this function, we learn that the only required argument is &lt;code&gt;x&lt;/code&gt;, the variable for which we will construct a histogram. We dropped the &lt;code&gt;x&lt;/code&gt; because we know it is the first argument.
The code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we run the code above, it gives us a message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;stat_bin()&lt;/code&gt; using &lt;code&gt;bins = 30&lt;/code&gt;. Pick better value with
&lt;code&gt;binwidth&lt;/code&gt;.
We previously used a bin size of 1 inch (of observed height), so the code looks like this:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1, fill = &amp;quot;blue&amp;quot;, col = &amp;quot;black&amp;quot;) +
  labs(x = &amp;quot;Male heights in inches&amp;quot;, title = &amp;quot;Histogram&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/height-histogram-geom-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density plots&lt;/h3&gt;
&lt;p&gt;To create a smooth density, we use the &lt;code&gt;geom_density&lt;/code&gt;. To make a smooth density plot with the data previously shown as a histogram we can use this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To fill in with color, we can use the &lt;code&gt;fill&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_density(fill=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/ggplot-density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To change the smoothness of the density, we use the &lt;code&gt;adjust&lt;/code&gt; argument to multiply the default value by that &lt;code&gt;adjust&lt;/code&gt;. For example, if we want the bandwidth to be twice as big we use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_density(fill=&amp;quot;blue&amp;quot;, adjust = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Boxplots&lt;/h3&gt;
&lt;p&gt;The geometry for boxplot is &lt;code&gt;geom_boxplot&lt;/code&gt;. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments &lt;code&gt;x&lt;/code&gt; as the categories, and &lt;code&gt;y&lt;/code&gt; as the values.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/female-male-boxplots-geom-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that our x-axis is a categorical variable. The order is determined by either the factor variable levels in &lt;code&gt;heights&lt;/code&gt; or, if no levels are set, in the order in which the &lt;code&gt;sex&lt;/code&gt; variable first encounters them. Later on, we’ll learn how to change the ordering.&lt;/p&gt;
&lt;p&gt;We can do much more with boxplots when we have more data. Right now, our &lt;code&gt;heights&lt;/code&gt; data has only two variables - &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;height&lt;/code&gt;. Let’s say we took the measurements over two different years - 2010 and 2020. That’s not in our data, so &lt;strong&gt;purely for exposition&lt;/strong&gt;, we’ll add it by randomly drawing a year for each observation. We’ll do this with &lt;code&gt;sample&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights = heights %&amp;gt;%
  dplyr::mutate(year = sample(x = c(2010, 2020), size = n(), replace = TRUE, prob = c(.5, .5)))
head(heights)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      sex height year
## 1   Male     75 2020
## 2   Male     70 2010
## 3   Male     68 2020
## 4   Male     74 2010
## 5   Male     61 2020
## 6 Female     65 2010&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s look at the boxplot of heights by sex, but broken out by year. We can do this by adding &lt;code&gt;year&lt;/code&gt; as an aesthetic mapping. Because our &lt;code&gt;year&lt;/code&gt; variable is an integer, R will start by thinking it’s a continuous numeric, but we want to treat it as a discrete variable. So, we wrap it in &lt;code&gt;as.factor()&lt;/code&gt; to force R to recognize it as a discrete variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% ggplot(aes(x = sex, y = height, fill = as.factor(year))) +
  geom_boxplot() +
  labs(fill = &amp;#39;Year&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/addYear-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we have each sex broken out by year! Since we randomly assigned year to our data (and didn’t actually take samples in two different decades), the distribution between years and within sex is nearly identical.&lt;/p&gt;
&lt;p&gt;What if we wanted to have &lt;code&gt;year&lt;/code&gt; on the x-axis, but then put the sex boxplots next to each other. This would let us compare the difference in heights by sex over the two sample years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% ggplot(aes(x = year, y = height, fill = sex)) +
  geom_boxplot() +
  labs(fill = &amp;#39;Sex&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/addYear2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Woah. Wait. What? Remember, in our data, &lt;code&gt;class(heights$year)&lt;/code&gt; is numeric, so when we ask R to put &lt;code&gt;year&lt;/code&gt; on the x-axis, it thinks it’s plotting a number. It gives us a nonsense x-axis. How do we fix this? We force &lt;code&gt;as.factor(year)&lt;/code&gt; to tell R that yes, &lt;code&gt;year&lt;/code&gt; is a categorical variable. Note that we &lt;strong&gt;didn’t&lt;/strong&gt; have to use &lt;code&gt;as.factor(sex)&lt;/code&gt; - that’s because sex is already a categorical variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% ggplot(aes(x = as.factor(year), y = height, fill = sex)) +
  geom_boxplot() +
  labs(fill = &amp;#39;Sex&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/03-example_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can see the height difference by sex, by year.&lt;/p&gt;
&lt;p&gt;We will explore more with boxplots and colors in our next lecture.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;try-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Try it!&lt;/h2&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Try it!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start by loading the &lt;strong&gt;dplyr&lt;/strong&gt; and &lt;strong&gt;ggplot2&lt;/strong&gt; library as well as the &lt;code&gt;murders&lt;/code&gt; and &lt;code&gt;heights&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(dslabs)
data(heights)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First, create a new variable in &lt;code&gt;murders&lt;/code&gt; that has &lt;code&gt;murders_per_capita&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders = murders %&amp;gt;%
  mutate(........)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Make a histogram of murders per capita. Use the default values for color and fill, but make sure you label the x-axis with a meaningful label.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make the same histogram, but set the fill aesthetic to MSU Green and the color to black.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do the same, but make it a smooth density plot&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, plot the smooth density but use a &lt;code&gt;fill&lt;/code&gt; aesthetic mapping so that each &lt;code&gt;region&lt;/code&gt;’s density is shown. Set a meaningful title on the legend, and make sure you make the density transparent so we can see all of the region’s densities (see &lt;code&gt;alpha&lt;/code&gt; aesthetic).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now, try making a boxplot to show the same data - the distribution across states of murders per capita by region. What is the average Northeastern state’s murder rate? What about the average Southern state?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Keep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applying ggplot2 to Real Data</title>
      <link>https://ssc442.netlify.app/assignment/03-assignment/</link>
      <pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/03-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-markdown&#34;&gt;R Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-prep&#34;&gt;Data Prep&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-help&#34;&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#turning-everything-in&#34;&gt;Turning everything in&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, September 19th.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;As always, we will first have to load &lt;code&gt;ggplot2&lt;/code&gt;. To do this, we will load the tidyverse by running this code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;The New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided &lt;a href=&#34;https://www1.nyc.gov/assets/buildings/html/essential-active-construction.html&#34;&gt;an interactive map here&lt;/a&gt; where you can see the different projects. There’s also a link there to download the complete dataset.&lt;/p&gt;
&lt;p&gt;For this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).&lt;/p&gt;
&lt;p&gt;As you hopefully figured out by now, you’ll be doing all your &lt;code&gt;R&lt;/code&gt; work in &lt;code&gt;R Markdown&lt;/code&gt;. You can use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud), but this is optional. If you decide to do so, either create a new project for this exercise only, or make a project for all your work in this class.&lt;/p&gt;
&lt;p&gt;You’ll need to download one CSV file and put it somewhere on your computer or upload it to RStudio.cloud—preferably in a folder named &lt;code&gt;data&lt;/code&gt; in your project folder. You can download the data from &lt;a href=&#34;https://www1.nyc.gov/assets/buildings/html/essential-active-construction.html&#34;&gt;the DOB’s map&lt;/a&gt;, or use this link to get it directly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/EssentialConstruction.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;EssentialConstruction.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;r-markdown&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Markdown&lt;/h3&gt;
&lt;p&gt;Writing regular text with &lt;code&gt;R Markdown&lt;/code&gt; follows the rules of Markdown. You can make lists; different-size headers, etc. This should be relatively straightfoward. We talked about a few Markdown features like &lt;strong&gt;bold&lt;/strong&gt; and &lt;em&gt;italics&lt;/em&gt; in class. See &lt;a href=&#34;https://ssc442kirkpatrick.netlify.app/resource/&#34;&gt;this resource for more formatting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You’ll also need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type &lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;alt&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; on Windows, or &lt;kbd&gt;⌘&lt;/kbd&gt; + &lt;kbd&gt;⌥&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; on macOS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/assignments/insert-chunk-button.png&#34; width=&#34;19%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-prep&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Prep&lt;/h3&gt;
&lt;p&gt;Once you download the &lt;code&gt;EssentialConstruction.csv&lt;/code&gt; file and save it in your project folder, you can open it and start cleaning. I’ll help with that. I’ll give you a .Rmd that will get you started on the Exercises below. Download this and use it in place of your lab assignment template.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/02-lab/02-lab.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; &lt;code&gt;02-lab.Rmd&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NOTE: You must change the title to Lab Assignment 02&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise 1 of 1: Essential pandemic construction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the Lab 02 template above, do the following:&lt;/p&gt;
&lt;p&gt;A. Show the count or proportion of approved projects by borough using a bar chart.&lt;/p&gt;
&lt;p&gt;B. Show the count or proportion of approved projects by category using a lollipop chart. Not sure of what a lollipop chart is? Google &lt;code&gt;R ggplot lollipop&lt;/code&gt;. A huge portion of knowing how to code is knowing how to google, find examples, and figure out where to put your variables from your data!&lt;/p&gt;
&lt;p&gt;C. Show the proportion of approved projects by borough and category &lt;em&gt;simultaneously&lt;/em&gt; using a heatmap.&lt;/p&gt;
&lt;p&gt;You don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a &lt;code&gt;labs()&lt;/code&gt; layer or changing fill colors with &lt;code&gt;scale_fill_manual()&lt;/code&gt; or with palettes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Overlay the data from Part 1 above onto a map of NYC. For double bonus, color the boroughs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-help&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting help&lt;/h2&gt;
&lt;p&gt;Use the SSC442 Slack if you get stuck (click the Slack logo at the top right of this website header).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;turning-everything-in&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Turning everything in&lt;/h2&gt;
&lt;p&gt;When you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. Upload the PDF file to D2L.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Visualization</title>
      <link>https://ssc442.netlify.app/example/02-example/</link>
      <pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/02-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-data-visualization&#34;&gt;Introduction to data visualization&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction-to-data-visualization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to data visualization&lt;/h1&gt;
&lt;p&gt;Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total
## 1    Alabama  AL  South    4779736   135
## 2     Alaska  AK   West     710231    19
## 3    Arizona  AZ   West    6392017   232
## 4   Arkansas  AR  South    2915918    93
## 5 California  CA   West   37253956  1257
## 6   Colorado  CO   West    5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t &lt;strong&gt;learn&lt;/strong&gt; anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggthemes)
library(ggrepel)

r &amp;lt;- murders %&amp;gt;%
  summarize(pop=sum(population), tot=sum(total)) %&amp;gt;%
  mutate(rate = tot/pop*10^6) %&amp;gt;% pull(rate)

murders %&amp;gt;% ggplot(aes(x = population/10^6, y = total, label = abb)) +
  geom_abline(intercept = log10(r), lty=2, col=&amp;quot;darkgrey&amp;quot;) +
  geom_point(aes(color=region), size = 3) +
  geom_text_repel() +
  scale_x_log10() +
  scale_y_log10() +
  xlab(&amp;quot;Populations in millions (log scale)&amp;quot;) +
  ylab(&amp;quot;Total number of murders (log scale)&amp;quot;) +
  ggtitle(&amp;quot;US Gun Murders in 2010&amp;quot;) +
  scale_color_discrete(name=&amp;quot;Region&amp;quot;) +
  theme_economist_white()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/ggplot-example-plot-0-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.&lt;/p&gt;
&lt;p&gt;The growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing &lt;em&gt;data journalism&lt;/em&gt; and including effective &lt;em&gt;infographics&lt;/em&gt; as part of their reporting.&lt;/p&gt;
&lt;p&gt;A particularly salient example—given the current state of the world—is a Wall Street Journal article&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/wsj-vaccines-example-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: &lt;a href=&#34;http://graphics.wsj.com/infectious-diseases-and-vaccines/&#34;&gt;Wall Street Journal&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Another striking example comes from a New York Times chart&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, which summarizes scores from the NYC Regents Exams. As described in
the article&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/regents-exams-example-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: &lt;a href=&#34;http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif&#34;&gt;New York Times&lt;/a&gt; via Amanda Cox)&lt;/p&gt;
&lt;p&gt;The most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.&lt;/p&gt;
&lt;p&gt;This is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call &lt;em&gt;exploratory data analysis&lt;/em&gt; (EDA). John W. Tukey&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, considered the father of EDA, once said,&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“The greatest value of a picture is when it forces us to notice what we never expected to see.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Many widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.&lt;/p&gt;
&lt;p&gt;Data visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; and The Best Stats You’ve Ever Seen&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/figure-html/gampnider-example-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.&lt;/p&gt;
&lt;p&gt;Today, we will discuss the basics of data visualization and exploratory data analysis. We will use the &lt;strong&gt;ggplot2&lt;/strong&gt; package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.&lt;/p&gt;
&lt;p&gt;Of course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ER Tufte (1983) The visual display of quantitative information.
Graphics Press.&lt;/li&gt;
&lt;li&gt;ER Tufte (1990) Envisioning information. Graphics Press.&lt;/li&gt;
&lt;li&gt;ER Tufte (1997) Visual explanations. Graphics Press.&lt;/li&gt;
&lt;li&gt;WS Cleveland (1993) Visualizing data. Hobart Press.&lt;/li&gt;
&lt;li&gt;WS Cleveland (1994) The elements of graphing data. CRC Press.&lt;/li&gt;
&lt;li&gt;A Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach:
Turning tables into graphs. The American Statistician 56:121-130.&lt;/li&gt;
&lt;li&gt;NB Robbins (2004) Creating more effective graphs. Wiley.&lt;/li&gt;
&lt;li&gt;A Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders.&lt;/li&gt;
&lt;li&gt;N Yau (2013) Data points: Visualization that means something. Wiley.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;https://shiny.rstudio.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://d3js.org/&#34;&gt;https://d3js.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;Some of the code from today’s class will be available below &lt;em&gt;after&lt;/em&gt; the class.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e&#34; class=&#34;uri&#34;&gt;http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif&#34; class=&#34;uri&#34;&gt;http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.nytimes.com/2011/02/19/nyregion/19schools.html&#34; class=&#34;uri&#34;&gt;https://www.nytimes.com/2011/02/19/nyregion/19schools.html&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/John_Tukey&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/John_Tukey&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Programming Basics in R</title>
      <link>https://ssc442.netlify.app/assignment/01-assignment/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/01-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#programming-basics&#34;&gt;Programming basics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#conditionals&#34;&gt;Conditional expressions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#defining-functions&#34;&gt;Defining functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#namespaces&#34;&gt;Namespaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#for-loops&#34;&gt;For-loops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectorization&#34;&gt;Vectorization and functionals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM on Sunday, September 12th.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As you read through this assignment, practice with each of the examples (copy-paste them into an empty &lt;code&gt;R&lt;/code&gt; script and run them). At the bottom of this page you will find the questions that comprise the assignment. These questions apply and expand on the topics and &lt;code&gt;R&lt;/code&gt; functions in the assignment. Many assignments will have this same structure: some instruction preceeding specific exercises.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ajkirkpatrick/FS20/Spring2021/Rmarkdown_templates/SSC442_Lab_Assignment_Template.Rmd&#34;&gt;Right-click to download the .Rmd template for labs &lt;i class=&#34;fas fa-file-download&#34;&gt;&lt;/i&gt;&lt;/a&gt;. Please save the template into the labs folder in the SSC442 folder on your local hard drive. If you don’t have a nice file structure setup for the course, please make one now. &lt;em&gt;It will save you from headaches in the future&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;programming-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Programming basics&lt;/h1&gt;
&lt;p&gt;We teach &lt;code&gt;R&lt;/code&gt; because it greatly facilitates data analysis. By coding in &lt;code&gt;R&lt;/code&gt;, we can efficiently perform exploratory data analysis, build data analysis pipelines, and prepare data visualization to communicate results. However, &lt;code&gt;R&lt;/code&gt; is not just a data analysis environment but a programming language. Advanced &lt;code&gt;R&lt;/code&gt; programmers can develop complex packages and even improve &lt;code&gt;R&lt;/code&gt; itself. But we do not cover advanced programming in this course. Nonetheless, in this section, we introduce three key programming concepts: conditional expressions, for-loops, and functions. These are not just key building blocks for advanced programming, but are sometimes useful during data analysis. We also note that there are several functions that are widely used to program in &lt;code&gt;R&lt;/code&gt; but that we will not cover directly in this course. These include &lt;code&gt;split&lt;/code&gt;, &lt;code&gt;cut&lt;/code&gt;, &lt;code&gt;do.call&lt;/code&gt;, and &lt;code&gt;Reduce&lt;/code&gt;, as well as the &lt;strong&gt;data.table&lt;/strong&gt; package. These are worth learning if you plan to become an expert &lt;code&gt;R&lt;/code&gt; programmer.&lt;/p&gt;
&lt;div id=&#34;conditionals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional expressions&lt;/h2&gt;
&lt;p&gt;Conditional expressions are one of the basic features of programming. They are used for what is called &lt;em&gt;flow control&lt;/em&gt;. The most common conditional expression is the if-else statement. In &lt;code&gt;R&lt;/code&gt;, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally, and you will need them once you start writing your own functions and packages.&lt;/p&gt;
&lt;p&gt;Here is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of &lt;code&gt;a&lt;/code&gt; unless &lt;code&gt;a&lt;/code&gt; is 0:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 0

if(a!=0){
  print(1/a)
} else{
  print(&amp;quot;No reciprocal for 0.&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;No reciprocal for 0.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at one more example using the US murders data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
murder_rate &amp;lt;- murders$total / murders$population*100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The &lt;code&gt;if&lt;/code&gt; statement protects us from the case in which no state satisfies the condition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- which.min(murder_rate)

if(murder_rate[ind] &amp;lt; 0.5){
  print(murders$state[ind])
} else{
  print(&amp;quot;No state has murder rate that low&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Vermont&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we try it again with a rate of 0.25, we get a different answer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(murder_rate[ind] &amp;lt; 0.25){
  print(murders$state[ind])
} else{
  print(&amp;quot;No state has a murder rate that low.&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;No state has a murder rate that low.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A related function that is very useful is &lt;code&gt;ifelse&lt;/code&gt;. This function takes three arguments: a logical and two possible answers. If the logical is &lt;code&gt;TRUE&lt;/code&gt;, the value in the second argument is returned and if &lt;code&gt;FALSE&lt;/code&gt;, the value in the third argument is returned. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 0
ifelse(a &amp;gt; 0, 1/a, NA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is &lt;code&gt;TRUE&lt;/code&gt;, or elements from the vector provided in the third argument, if the entry is &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- c(0, 1, 2, -4, 5)
result &amp;lt;- ifelse(a &amp;gt; 0, 1/a, NA)&lt;/code&gt;&lt;/pre&gt;
This table helps us see what happened:
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
a
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
is_a_positive
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
answer1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
answer2
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
result
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
Inf
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.50
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here is an example of how this function can be readily used to replace all the missing values in a vector with zeros:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(na_example)
no_nas &amp;lt;- ifelse(is.na(na_example), 0, na_example)
sum(is.na(no_nas))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two other useful functions are &lt;code&gt;any&lt;/code&gt; and &lt;code&gt;all&lt;/code&gt;. The &lt;code&gt;any&lt;/code&gt; function takes a vector of logicals and returns &lt;code&gt;TRUE&lt;/code&gt; if any of the entries is &lt;code&gt;TRUE&lt;/code&gt;. The &lt;code&gt;all&lt;/code&gt; function takes a vector of logicals and returns &lt;code&gt;TRUE&lt;/code&gt; if all of the entries are &lt;code&gt;TRUE&lt;/code&gt;. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- c(TRUE, TRUE, FALSE)
any(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;defining-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Defining functions&lt;/h2&gt;
&lt;p&gt;As you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector &lt;code&gt;x&lt;/code&gt; using the &lt;code&gt;sum&lt;/code&gt; and &lt;code&gt;length&lt;/code&gt; functions: &lt;code&gt;sum(x)/length(x)&lt;/code&gt;. Because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the &lt;code&gt;mean&lt;/code&gt; function and it is included in base &lt;code&gt;R&lt;/code&gt;. However, you will encounter situations in which the function does not already exist, so &lt;code&gt;R&lt;/code&gt; permits you to write your own. A simple version of a function that computes the average can be defined like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg &amp;lt;- function(x){
  s &amp;lt;- sum(x)
  n &amp;lt;- length(x)
  s/n
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now &lt;code&gt;avg&lt;/code&gt; is a function that computes the mean:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:100
identical(mean(x), avg(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that variables defined inside a function are not saved in the workspace. So while we use &lt;code&gt;s&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt; when we call &lt;code&gt;avg&lt;/code&gt;, the values are created and changed only during the call. Here is an illustrative example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- 3
avg(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how &lt;code&gt;s&lt;/code&gt; is still 3 after we call &lt;code&gt;avg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In general, functions are objects, so we assign them to variable names with &lt;code&gt;&amp;lt;-&lt;/code&gt;. The function &lt;code&gt;function&lt;/code&gt; tells &lt;code&gt;R&lt;/code&gt; you are about to define a function. The general form of a function definition looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_function &amp;lt;- function(VARIABLE_NAME){
  perform operations on VARIABLE_NAME and calculate VALUE
  VALUE
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg &amp;lt;- function(x, arithmetic = TRUE){
  n &amp;lt;- length(x)
  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will learn more about how to create functions through experience as we face more complex tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;namespaces&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Namespaces&lt;/h2&gt;
&lt;p&gt;Once you start becoming more of an &lt;code&gt;R&lt;/code&gt; expert user, you will likely need to load several add-on packages for some of your analysis. Once you start doing this, it is likely that two packages use the same name for two different functions. And often these functions do completely different things. In fact, you have already encountered this because both &lt;strong&gt;dplyr&lt;/strong&gt; and the R-base &lt;strong&gt;stats&lt;/strong&gt; package define a &lt;code&gt;filter&lt;/code&gt; function. There are five other examples in &lt;strong&gt;dplyr&lt;/strong&gt;. We know this because when we first load &lt;strong&gt;dplyr&lt;/strong&gt; we see the following message:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what does &lt;code&gt;R&lt;/code&gt; do when we type &lt;code&gt;filter&lt;/code&gt;? Does it use the &lt;strong&gt;dplyr&lt;/strong&gt; function or the &lt;strong&gt;stats&lt;/strong&gt; function? From our previous work we know it uses the &lt;strong&gt;dplyr&lt;/strong&gt; one. But what if we want to use the &lt;strong&gt;stats&lt;/strong&gt; version?&lt;/p&gt;
&lt;p&gt;These functions live in different &lt;em&gt;namespaces&lt;/em&gt;. &lt;code&gt;R&lt;/code&gt; will follow a certain order when searching for a function in these &lt;em&gt;namespaces&lt;/em&gt;. You can see the order by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;search()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first entry in this list is the global environment which includes all the objects you define.&lt;/p&gt;
&lt;p&gt;So what if we want to use the &lt;strong&gt;stats&lt;/strong&gt; &lt;code&gt;filter&lt;/code&gt; instead of the &lt;strong&gt;dplyr&lt;/strong&gt; filter but &lt;strong&gt;dplyr&lt;/strong&gt; appears first in the search list? You can force the use of a specific namespace by using double colons (&lt;code&gt;::&lt;/code&gt;) like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats::filter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to be absolutely sure that we use the &lt;strong&gt;dplyr&lt;/strong&gt; &lt;code&gt;filter&lt;/code&gt;, we can use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dplyr::filter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also note that if we want to use a function in a package without loading the entire package, we can use the double colon as well.&lt;/p&gt;
&lt;p&gt;For more on this more advanced topic we recommend the &lt;code&gt;R&lt;/code&gt; packages book&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;for-loops&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;For-loops&lt;/h2&gt;
&lt;p&gt;If we had to write this section in a single sentence, it would be: Don’t use for-loops. Looping is intuitive, but &lt;code&gt;R&lt;/code&gt; is designed to provide more computationally efficient solutions. For-loops should be considered a quick-and-dirty way to get an answer. But, hey, you live your own life. Below we provide a brief overview to for-looping.&lt;/p&gt;
&lt;p&gt;The formula for the sum of the series &lt;span class=&#34;math inline&#34;&gt;\(1+2+\dots+n\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n(n+1)/2\)&lt;/span&gt;. What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_s_n &amp;lt;- function(n){
  x &amp;lt;- 1:n
  sum(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How can we compute &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; for various values of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(n=1,\dots,25\)&lt;/span&gt;? Do we write 25 lines of code calling &lt;code&gt;compute_s_n&lt;/code&gt;? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over, and the only thing that is changing is the value of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. For-loops let us define the range that our variable takes (in our example &lt;span class=&#34;math inline&#34;&gt;\(n=1,\dots,10\)&lt;/span&gt;), then change the value and evaluate expression as you &lt;em&gt;loop&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Perhaps the simplest example of a for-loop is this useless piece of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:5){
  print(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1
## [1] 2
## [1] 3
## [1] 4
## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the for-loop we would write for our &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- 25
s_n &amp;lt;- vector(length = m) # create an empty vector
for(n in 1:m){
  s_n[n] &amp;lt;- compute_s_n(n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In each iteration &lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n=2\)&lt;/span&gt;, etc…, we compute &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; and store it in the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;th entry of &lt;code&gt;s_n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now we can create a plot to search for a pattern:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1:m
plot(n, s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/01-assignment_files/figure-html/sum-of-consecutive-squares-1.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you noticed that it appears to be a quadratic, you are on the right track because the formula is &lt;span class=&#34;math inline&#34;&gt;\(n(n+1)/2\)&lt;/span&gt;.
&lt;!--
which we can confirm with a table:


```r
head(data.frame(s_n = s_n, formula = n*(n+1)/2))
```

```
##   s_n formula
## 1   1       1
## 2   3       3
## 3   6       6
## 4  10      10
## 5  15      15
## 6  21      21
```

We can also overlay the two results by using the function `lines` to draw a line over the previously plotted points:


```r
plot(n, s_n)
lines(n, n*(n+1)/2)
```

&lt;img src=&#34;https://ssc442.netlify.app/assignment/01-assignment_files/figure-html/s_n-v-n-1.png&#34; width=&#34;672&#34; /&gt;

--&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vectorization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vectorization and functionals&lt;/h2&gt;
&lt;p&gt;Although for-loops are an important concept to understand, in &lt;code&gt;R&lt;/code&gt; we rarely use them. As you learn more &lt;code&gt;R&lt;/code&gt;, you will realize that &lt;em&gt;vectorization&lt;/em&gt; is preferred over for-loops since it results in shorter and clearer code. (It’s also vastly more efficient computationally, which can matter as your data grows.) A &lt;em&gt;vectorized&lt;/em&gt; function is a function that will apply the same operation on each of the vectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:10
sqrt(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- 1:10
x*y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]   1   4   9  16  25  36  49  64  81 100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make this calculation, there is no need for for-loops. However, not all functions work this way. For instance, the function we just wrote, &lt;code&gt;compute_s_n&lt;/code&gt;, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of &lt;code&gt;n&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1:25
compute_s_n(n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Functionals&lt;/em&gt; are functions that help us apply the same function to each entry in a vector, matrix, data frame, or list. Here we cover the functional that operates on numeric, logical, and character vectors: &lt;code&gt;sapply&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;sapply&lt;/code&gt; permits us to perform element-wise operations on any function. Here is how it works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:10
sapply(x, sqrt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each element of &lt;code&gt;x&lt;/code&gt; is passed on to the function &lt;code&gt;sqrt&lt;/code&gt; and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original &lt;code&gt;x&lt;/code&gt;. This implies that the for-loop above can be written as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1:25
s_n &amp;lt;- sapply(n, compute_s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other functionals are &lt;code&gt;apply&lt;/code&gt;, &lt;code&gt;lapply&lt;/code&gt;, &lt;code&gt;tapply&lt;/code&gt;, &lt;code&gt;mapply&lt;/code&gt;, &lt;code&gt;vapply&lt;/code&gt;, and &lt;code&gt;replicate&lt;/code&gt;. We mostly use &lt;code&gt;sapply&lt;/code&gt;, &lt;code&gt;apply&lt;/code&gt;, and &lt;code&gt;replicate&lt;/code&gt; in this book, but we recommend familiarizing yourselves with the others as they can be very useful.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;This is your first weekly lab assignment. Each lab assignment will need to be done in Rmarkdown using &lt;a href=&#34;https://raw.githubusercontent.com/ajkirkpatrick/FS20/Spring2021/Rmarkdown_templates/SSC442_Lab_Assignment_Template.Rmd&#34;&gt;the lab template&lt;/a&gt;, just right-click and Save As…&lt;strong&gt;Start a new folder on your drive for this course, and inside that a new folder for lab assignments, and inside that a new folder for Lab No. 1&lt;/strong&gt;. Rmarkdown will place some intermediate files in that folder, so leaving .Rmd files on your desktop will make things messy, fast.&lt;/p&gt;
&lt;p&gt;Once you’ve saved the file, open it up in Rstudio.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Change the title to “Lab 1”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Put your name on it&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Leave the date alone. That little &lt;code&gt;`r Sys.time(...)`&lt;/code&gt; will ask R to return the date (with M-D-Y formatting), which Rmarkdown will put in as if you had typed in the actual date.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you type &lt;code&gt;## 1. Text of...&lt;/code&gt;, Markdown will recognize “1. Text of” as a header and will &lt;em&gt;automatically&lt;/em&gt; make it big.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;So please copy the number and text of the question you are answering here.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next will be the &lt;code&gt;```{r q1}&lt;/code&gt; text that will be in gray. &lt;strong&gt;R will recognize this as code and will treat it as such&lt;/strong&gt;. Anything run in that block will have an output.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you want to see what the code will do, copy the code and paste it into the gray area. Then, click the green right arrow in the top-right corner &lt;em&gt;of the gray code chunk&lt;/em&gt;. It should show you the results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the results (plus your understanding of the code) to answer the question&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With each completed question, clidk the “Knit” button up above the script window. Rmarkdown will create a .pdf for you of your work (as long as it doesn’t hit any R errors). Knit often to make sure you haven’t hit an error!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;\newpage&lt;/code&gt; line is a Latex command (the program that makes the typesetting look nice). It will start a new pdf page.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the next page, copy question #2 to a new header using &lt;code&gt;##&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once done, render one last .pdf and turn it in on D2L!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;0&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In your first code chunk, load the package library &lt;code&gt;tidyverse&lt;/code&gt;, which you will need for Question 8. Always load all your package libraries at the top, in the first code chunk!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What will this conditional expression return and why?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(1,2,-3,4)

if(all(x&amp;gt;0)){
  print(&amp;quot;All Postives&amp;quot;)
} else{
  print(&amp;quot;Not all positives&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which of the following expressions is always &lt;code&gt;FALSE&lt;/code&gt; when at least one entry of a logical vector &lt;code&gt;x&lt;/code&gt; is TRUE?&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;all(x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;any(x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;any(!x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;all(!x)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The function &lt;code&gt;nchar&lt;/code&gt; tells you how many characters long a character vector is. Write a line of code that assigns to the object &lt;code&gt;new_names&lt;/code&gt; the state abbreviation when the state name is longer than 8 characters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a function &lt;code&gt;sum_n&lt;/code&gt; that for any given value, say &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, computes the sum of the integers from 1 to n (inclusive). Use the function to determine the sum of integers from 1 to 5,000.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a function &lt;code&gt;altman_plot&lt;/code&gt; that takes two arguments, &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, and plots the difference against the sum. Use it to make an altman plot of &lt;code&gt;x &amp;lt;- c(5,7,9)&lt;/code&gt; and &lt;code&gt;y &amp;lt;- c(10,11,12)&lt;/code&gt;. When your function creates the plot, it will output automatically in your Rmarkdown knitted .pdf.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After running the code below, what is the value of &lt;code&gt;x&lt;/code&gt; and why?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 3
my_func &amp;lt;- function(y){
  x &amp;lt;- 5
  y+5
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write a function &lt;code&gt;compute_s_n&lt;/code&gt; that for any given &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; computes the sum &lt;span class=&#34;math inline&#34;&gt;\(S_n = 1^2 + 2^2 + 3^2 + \dots n^2\)&lt;/span&gt;. Report the value of the sum when &lt;span class=&#34;math inline&#34;&gt;\(n=10\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define an empty numerical vector &lt;code&gt;s_n&lt;/code&gt; of size 25 using &lt;code&gt;s_n &amp;lt;- vector(&#34;numeric&#34;, 25)&lt;/code&gt; and store in the results of &lt;span class=&#34;math inline&#34;&gt;\(S_1, S_2, \dots S_{25}\)&lt;/span&gt; using a for-loop.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat exercise 8, but this time use &lt;code&gt;sapply&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat exercise 8, but this time use &lt;code&gt;map_dbl&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; versus &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Use points defined by &lt;span class=&#34;math inline&#34;&gt;\(n=1,\dots,25\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Confirm that the formula for this sum is &lt;span class=&#34;math inline&#34;&gt;\(S_n= n(n+1)(2n+1)/6\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://r-pkgs.had.co.nz/namespace.html&#34; class=&#34;uri&#34;&gt;http://r-pkgs.had.co.nz/namespace.html&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Basics of ggplot</title>
      <link>https://ssc442.netlify.app/assignment/02-assignment/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/02-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#using-ggplot2&#34;&gt;Using ggplot2&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-use-ggplot2-the-too-fast-and-wholly-unclear-recipe&#34;&gt;How to use &lt;code&gt;ggplot2&lt;/code&gt; – the too-fast and wholly unclear recipe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mappings-link-data-to-things-you-see&#34;&gt;Mappings Link Data to Things You See&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-recipe&#34;&gt;The Recipe&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mapping-aesthetics-vs-setting-them&#34;&gt;Mapping Aesthetics vs Setting them&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM on Tuesday, September 14th.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Our primary tool for data visualization in the course will be &lt;code&gt;ggplot&lt;/code&gt;. Technically, we’re using &lt;code&gt;ggplot2&lt;/code&gt;; the o.g. version lacked some of the modern features of its big brother. &lt;code&gt;ggplot2&lt;/code&gt; implements the grammar of graphics, a coherent and relatively straightforward system for describing and building graphs. With &lt;code&gt;ggplot2&lt;/code&gt;, you can do more faster by learning one system and applying it in many places. Other languages provide more specific tools, but require you to learn a different tool for each application. In this class, we’ll dig into a single package for our visuals.&lt;/p&gt;
&lt;div id=&#34;using-ggplot2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using ggplot2&lt;/h2&gt;
&lt;p&gt;In order to get our hands dirty, we will first have to load &lt;code&gt;ggplot2&lt;/code&gt;. To do this, and to access the datasets, help pages, and functions that we will use in this assignment, we will load the so-called tidyverse by running this code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you run this code and get an error message “there is no package called ‘tidyverse’”, you’ll need to first install it, then run library() once again. To install packages in &lt;code&gt;R&lt;/code&gt;, we utilize the simple function install.packages(). In this case, we would write:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we’re up and running, we’re ready to dive into some basic exercises. &lt;code&gt;ggplot2&lt;/code&gt; works by specifying the connections between the variables in the data and the colors, points, and shapes you see on the screen. These logical connections are called &lt;em&gt;aesthetic mappings&lt;/em&gt; or simply &lt;em&gt;aesthetics&lt;/em&gt;.&lt;/p&gt;
&lt;div id=&#34;how-to-use-ggplot2-the-too-fast-and-wholly-unclear-recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to use &lt;code&gt;ggplot2&lt;/code&gt; – the too-fast and wholly unclear recipe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;data =&lt;/code&gt;: Define what your data is. For instance, below we’ll use the mpg data frame found in ggplot2 (by using &lt;code&gt;ggplot2::mpg&lt;/code&gt;). As a reminder, a data frame is a rectangular collection of variables (in the columns) and observations (in the rows). This structure of data is often called a “table” but we’ll try to use terms slightly more precisely. The &lt;code&gt;mpg&lt;/code&gt; data frame contains observations collected by the US Environmental Protection Agency on 38 different models of car.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;mapping = aes(...)&lt;/code&gt;: How to map the variables in the data to aesthetics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Axes, size of points, intensities of colors, which colors, shape of points, lines/points&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then say what type of plot you want:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;boxplot, scatterplot, histogram, …&lt;/li&gt;
&lt;li&gt;these are called ‘geoms’ in ggplot’s grammar, such as &lt;code&gt;geom_point()&lt;/code&gt; giving scatter plots&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;library(ggplot2)
... + geom_point() # Produces scatterplots
... + geom_bar() # Bar plots
.... + geom_boxplot() # boxplots
... #&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You link these steps by &lt;em&gt;literally&lt;/em&gt; adding them together with &lt;code&gt;+&lt;/code&gt; as we’ll see.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; What other types of plots are there? Try to find several more &lt;code&gt;geom_&lt;/code&gt; functions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mappings-link-data-to-things-you-see&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mappings Link Data to Things You See&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gapminder)
library(ggplot2)
gapminder::gapminder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,704 × 6
##    country     continent  year lifeExp      pop gdpPercap
##    &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 Afghanistan Asia       1952    28.8  8425333      779.
##  2 Afghanistan Asia       1957    30.3  9240934      821.
##  3 Afghanistan Asia       1962    32.0 10267083      853.
##  4 Afghanistan Asia       1967    34.0 11537966      836.
##  5 Afghanistan Asia       1972    36.1 13079460      740.
##  6 Afghanistan Asia       1977    38.4 14880372      786.
##  7 Afghanistan Asia       1982    39.9 12881816      978.
##  8 Afghanistan Asia       1987    40.8 13867957      852.
##  9 Afghanistan Asia       1992    41.7 16317921      649.
## 10 Afghanistan Asia       1997    41.8 22227415      635.
## # … with 1,694 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/02-assignment_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above we’ve loaded a different dataset and have started to explore a particular relationship. Before putting in this code yourself, try to intuit what &lt;em&gt;might&lt;/em&gt; be going on.&lt;/p&gt;
&lt;p&gt;Any ideas?&lt;/p&gt;
&lt;p&gt;Here’s a breakdown of everything that happens after the &lt;code&gt;p&amp;lt;- ggplot()&lt;/code&gt; call:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data = gapminder&lt;/code&gt; tells ggplot to use gapminder dataset, so if variable names are mentioned, they should be looked up in gapminder&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mapping = aes(...)&lt;/code&gt; shows that the mapping is a function call. There is a deeper logic to this that I will disucss below, but it’s easiest to simply accept that this is how you write it. Put another way, the &lt;code&gt;mapping = aes(...)&lt;/code&gt; argument &lt;em&gt;links variables&lt;/em&gt; to &lt;em&gt;things you will see&lt;/em&gt; on the plot.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;aes(x = gdpPercap, y = lifeExp)&lt;/code&gt; maps the GDP data onto &lt;code&gt;x&lt;/code&gt;, which is a known aesthetic (the x-coordinate) and life expectancy data onto &lt;code&gt;y&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are predefined names that are used by &lt;code&gt;ggplot&lt;/code&gt; and friends&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise 1:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let’s return to the &lt;code&gt;mpg&lt;/code&gt; data. Among the variables in &lt;code&gt;mpg&lt;/code&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;displ&lt;/code&gt;, a car’s engine size, in litres.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;hwy&lt;/code&gt;, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generate a scatterplot between these two variables. Does it capture the intuitive relationship you expected? What happens if you make a scatterplot of &lt;code&gt;class&lt;/code&gt; vs &lt;code&gt;drv&lt;/code&gt;? Why is the plot not useful?&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It turns out there’s a reason for doing all of this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The greatest value of a picture is when it forces us to notice what we never expected to see.”” — John Tukey&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the plot you made above, one group of points seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars?&lt;/p&gt;
&lt;p&gt;Let’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The &lt;code&gt;class&lt;/code&gt; variable of the &lt;code&gt;mpg&lt;/code&gt; dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular).&lt;/p&gt;
&lt;p&gt;You can add a third variable, like &lt;code&gt;class&lt;/code&gt;, to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its aesthetic properties. Since we already use the word “&lt;strong&gt;value&lt;/strong&gt;” to describe data, let’s use the word “&lt;strong&gt;level&lt;/strong&gt;” to describe aesthetic properties. Thus, we are interested in exploring &lt;code&gt;class&lt;/code&gt; as a level.&lt;/p&gt;
&lt;p&gt;You can convey information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. To map an aesthetic to a variable, associate the name of the aesthetic to the name of the variable inside &lt;code&gt;aes()&lt;/code&gt;. &lt;code&gt;ggplot2&lt;/code&gt; will automatically assign a unique level of the aesthetic (here a unique color) to each unique value of the variable, a process known as scaling. &lt;code&gt;ggplot2&lt;/code&gt; will also add a legend that explains which levels correspond to which values.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise 2:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using your previous scatterplot of &lt;code&gt;displ&lt;/code&gt; and &lt;code&gt;hwy&lt;/code&gt;, map the colors of your points to the class variable to reveal the class of each car. What conclusions can we make?&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let’s explore our previously saved &lt;code&gt;p&lt;/code&gt; in greater detail. As with Exercise 1, we’ll add a &lt;em&gt;layer&lt;/em&gt;. This says how some data gets turned into concrete visual aspects.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p + geom_point()
p + geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Both of the above geom’s use the same mapping, where the x-axis represents &lt;code&gt;gdpPercap&lt;/code&gt; and the y-axis represents &lt;code&gt;lifeExp&lt;/code&gt;. You can find this yourself with some ease. But the first one maps the data to individual points, the other one maps it to a smooth line with error ranges.&lt;/p&gt;
&lt;p&gt;We get a message that tells us that &lt;code&gt;geom_smooth()&lt;/code&gt; is using the method = ‘gam’, so presumably we can use other methods. Let’s see if we can figure out which other methods there are.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;?geom_smooth
p + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ...)
p + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ..., color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may start to see why &lt;code&gt;ggplot2&lt;/code&gt;’s way of breaking up tasks is quite powerful: the geometric objects can all reuse the &lt;em&gt;same&lt;/em&gt; mapping of data to aesthetics, yet the results are quite different. And if we want later geoms to use different mappings, then we can override them – but it isn’t necessary.&lt;/p&gt;
&lt;p&gt;Consider the output we’ve explored thus far. One potential issue lurking in the data is that most of it is bunched to the left. If we instead used a logarithmic scale, we should be able to spread the data out better.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p + geom_point() + geom_smooth(method = &amp;quot;lm&amp;quot;) + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Describe what the &lt;code&gt;scale_x_log10()&lt;/code&gt; does. Why is it a more evenly distributed cloud of points now? (2-3 sentences.)&lt;/p&gt;
&lt;p&gt;Nice. We’re starting to get somewhere. But, you might notice that the x-axis now has scientific notation. Let’s change that.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(scales)
p + geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  scale_x_log10(labels = scales::dollar)
p + geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  scale_x_log10(labels = scales::...)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; What does the &lt;code&gt;dollar()&lt;/code&gt; call do? How can you find other ways of relabeling the scales when using &lt;code&gt;scale_x_log10()&lt;/code&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;?dollar()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-recipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Recipe&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Tell the &lt;code&gt;ggplot()&lt;/code&gt; function what our data is.&lt;/li&gt;
&lt;li&gt;Tell &lt;code&gt;ggplot()&lt;/code&gt; &lt;em&gt;what&lt;/em&gt; relationships we want to see. For convenience we will put the results of the first two steps in an object called &lt;code&gt;p&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Tell &lt;code&gt;ggplot&lt;/code&gt; &lt;em&gt;how&lt;/em&gt; we want to see the relationships in our data.&lt;/li&gt;
&lt;li&gt;Layer on geoms as needed, by adding them on the &lt;code&gt;p&lt;/code&gt; object one at a time.&lt;/li&gt;
&lt;li&gt;Use some additional functions to adjust scales, labels, tickmarks, titles.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;e.g. &lt;code&gt;scale_&lt;/code&gt;, &lt;code&gt;labs()&lt;/code&gt;, and &lt;code&gt;guides()&lt;/code&gt; functions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you start to run more &lt;code&gt;R&lt;/code&gt; code, you’re likely to run into problems. Don’t worry — it happens to everyone. I have been writing code in numerous languages for years, and every day I still write code that doesn’t work. Sadly, &lt;code&gt;R&lt;/code&gt; is particularly persnickity, and its error messages are often opaque.&lt;/p&gt;
&lt;p&gt;Start by carefully comparing the code that you’re running to the code in these notes. &lt;code&gt;R&lt;/code&gt; is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every ” is paired with another “. Sometimes you’ll run the code and nothing happens. Check the left-hand of your console: if it’s a +, it means that R doesn’t think you’ve typed a complete expression and it’s waiting for you to finish it. In this case, it’s usually easy to start from scratch again by pressing ESCAPE to abort processing the current command.&lt;/p&gt;
&lt;p&gt;One common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start.&lt;/p&gt;
&lt;div id=&#34;mapping-aesthetics-vs-setting-them&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mapping Aesthetics vs Setting them&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp, color = &amp;#39;yellow&amp;#39;))
p + geom_point() + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is interesting (or annoying): the points are not yellow. How can we tell ggplot to draw yellow points?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp, ...))
p + geom_point(...) + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; describe in your words what is going on.
One way to avoid such mistakes is to read arguments inside &lt;code&gt;aes(&amp;lt;property&amp;gt; = &amp;lt;variable&amp;gt;)&lt;/code&gt;as &lt;em&gt;the property &lt;property&gt; in the graph is determined by the data in &lt;variable&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Write the above sentence for the original call &lt;code&gt;aes(x = gdpPercap, y = lifeExp, color = &#39;yellow&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Aesthetics convey information about a variable in the dataset, whereas setting the color of all points to yellow conveys no information about the dataset - it changes the appearance of the plot in a way that is independent of the underlying data.&lt;/p&gt;
&lt;p&gt;Remember: &lt;code&gt;color = &#39;yellow&#39;&lt;/code&gt; and &lt;code&gt;aes(color = &#39;yellow&#39;)&lt;/code&gt; are very different, and the second makes usually no sense, as &lt;code&gt;&#39;yellow&#39;&lt;/code&gt; is treated as &lt;em&gt;data&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point() + geom_smooth(color = &amp;quot;orange&amp;quot;, se = FALSE, size = 8, method = &amp;quot;lm&amp;quot;) + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Write down what all those arguments in &lt;code&gt;geom_smooth(...)&lt;/code&gt; do.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p + geom_point(alpha = 0.3) +
  geom_smooth(method = &amp;quot;gam&amp;quot;) +
  scale_x_log10(labels = scales::dollar) +
  labs(x = &amp;quot;GDP Per Capita&amp;quot;, y = &amp;quot;Life Expectancy in Years&amp;quot;,
       title = &amp;quot;Economic Growth and Life Expectancy&amp;quot;,
       subtitle = &amp;quot;Data Points are country-years&amp;quot;,
       caption = &amp;quot;Source: Gapminder&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coloring by continent:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(scales)
p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp, color = continent, fill = continent))
p + geom_point()
p + geom_point() + scale_x_log10(labels = dollar)
p + geom_point() + scale_x_log10(labels = dollar) + geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; What does &lt;code&gt;fill = continent&lt;/code&gt; do? What do you think about the match of colors between lines and error bands?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(mapping = aes(color = continent)) + geom_smooth() + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Notice how the above code leads to a single smooth line, not one per continent. Why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; What is bad about the following example, assuming the graph is the one we want? Think about why you should set aesthetics at the top level rather than at the individual geometry level if that’s your intent.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(mapping = aes(color = continent)) +
  geom_smooth(mapping = aes(color = continent, fill = continent)) +
  scale_x_log10() +
  geom_smooth(mapping = aes(color = continent), method = &amp;quot;gam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise 3:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Generate two new plots with &lt;code&gt;data = gapminder&lt;/code&gt; (note: you’ll need to install the package by the same name if you have not already). Label the axes and the header with clear, easy to understand language. In a few sentences, describe what you’ve visualized and why.&lt;/p&gt;
&lt;p&gt;Note that this is your first foray into &lt;code&gt;ggplot2&lt;/code&gt;; accordingly, you should ry to make sure that you do not bite off more than you can chew. We will improve and refine our abilities as we progress through the semester.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Installing R, RStudio, tidyverse, and tinytex</title>
      <link>https://ssc442.netlify.app/resource/install/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/resource/install/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#install-r&#34;&gt;Install &lt;code&gt;R&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#install-rstudio&#34;&gt;Install RStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#install-tidyverse&#34;&gt;Install &lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#install-tinytex&#34;&gt;Install &lt;code&gt;tinytex&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;As mentioned in the syllabus, you will do all of your work in this class with the open source programming language &lt;a href=&#34;https://cran.r-project.org/&#34;&gt;&lt;code&gt;R&lt;/code&gt;&lt;/a&gt;. You will use &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt; as the main program to access &lt;code&gt;R&lt;/code&gt;. Think of &lt;code&gt;R&lt;/code&gt; as an engine and RStudio as a car dashboard—–&lt;code&gt;R&lt;/code&gt; handles all the calculations and the actual statistics, while RStudio provides a nice interface for running &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;p&gt;Hopefully you’re well-versed in dealing with these things, but if you’re lost, here’s how you install the required software for the course.&lt;/p&gt;
&lt;div id=&#34;install-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Install &lt;code&gt;R&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;First you need to install &lt;code&gt;R&lt;/code&gt; itself (the engine).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go to the CRAN (Collective &lt;code&gt;R&lt;/code&gt; Archive Network)&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; website: &lt;a href=&#34;https://cran.r-project.org/&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on “Download &lt;code&gt;R&lt;/code&gt; for &lt;code&gt;XXX&lt;/code&gt;”, where &lt;code&gt;XXX&lt;/code&gt; is either Mac or Windows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/install/install-r-links.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you use macOS, scroll down to the first &lt;code&gt;.pkg&lt;/code&gt; file in the list of files (in this picture, it’s &lt;code&gt;R-4.0.0.pkg&lt;/code&gt;; as of right now, the current version is also 4.0.0) and download it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/install/install-r-mac.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/install/install-r-windows.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Double click on the downloaded file (check your &lt;code&gt;Downloads&lt;/code&gt; folder). Click yes through all the prompts to install like any other program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you use macOS, &lt;a href=&#34;https://www.xquartz.org/&#34;&gt;download and install XQuartz&lt;/a&gt;. You do not need to do this on Windows.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;install-rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Install RStudio&lt;/h3&gt;
&lt;p&gt;Next, you need to install RStudio, the nicer graphical user interface (GUI) for &lt;code&gt;R&lt;/code&gt; (the dashboard). Once &lt;code&gt;R&lt;/code&gt; and RStudio are both installed, you can ignore &lt;code&gt;R&lt;/code&gt; and only use RStudio. RStudio will use &lt;code&gt;R&lt;/code&gt; automatically and you won’t ever have to interact with it directly.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go to the free download location on RStudio’s website: &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/#download&#34; class=&#34;uri&#34;&gt;https://www.rstudio.com/products/rstudio/download/#download&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/install/install-r-rstudio1.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/install/install-r-rstudio2.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Double click on the downloaded file (again, check your &lt;code&gt;Downloads&lt;/code&gt; folder). Click yes through all the prompts to install like any other program.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Double click on RStudio to run it (check your applications folder or start menu).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;install-tidyverse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Install &lt;code&gt;tidyverse&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/install/install-r-package-panel.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This can sometimes be tedious when you’re installing lots of packages, though. &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;The tidyverse&lt;/a&gt;, for instance, consists of dozens of packages (including the ever-present &lt;strong&gt;ggplot2&lt;/strong&gt;) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.&lt;/p&gt;
&lt;p&gt;Go to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/install/install-r-tidyverse.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice also that RStudio will generate a line of code for you and run it: &lt;code&gt;install.packages(&#34;tidyverse&#34;)&lt;/code&gt;. You can also just paste and run this instead of using the packages panel. Hopefully you’ve experienced installing packages before now; if not, consider this a crash course!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;install-tinytex&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Install &lt;code&gt;tinytex&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;When you knit to PDF, &lt;code&gt;R&lt;/code&gt; uses a special scientific typesetting program named LaTeX.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LaTeX is neat and makes pretty documents, but it’s a huge program—&lt;a href=&#34;https://tug.org/mactex/mactex-download.html&#34;&gt;the macOS version, for instance, is nearly 4 GB&lt;/a&gt;. To make life easier, there’s &lt;a href=&#34;https://yihui.org/tinytex/&#34;&gt;an &lt;code&gt;R&lt;/code&gt; package named &lt;strong&gt;tinytex&lt;/strong&gt;&lt;/a&gt; that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.&lt;/p&gt;
&lt;p&gt;Here’s how to install &lt;strong&gt;tinytex&lt;/strong&gt; so you can knit to pretty PDFs:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use the Packages in panel in RStudio to install &lt;strong&gt;tinytex&lt;/strong&gt; like you did above with &lt;strong&gt;tidyverse&lt;/strong&gt;. Alternatively, run &lt;code&gt;install.packages(&#34;tinytex&#34;)&lt;/code&gt; in the console.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;tinytex::install_tinytex()&lt;/code&gt; in the console.&lt;/li&gt;
&lt;li&gt;Wait for a bit while &lt;code&gt;R&lt;/code&gt; downloads and installs everything you need.&lt;/li&gt;
&lt;li&gt;The end! You should now be able to knit to PDF.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;It’s a goofy name, but CRAN is where most &lt;code&gt;R&lt;/code&gt; packages—and R itself—lives.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Pronounced “lay-tek” for those who are correct; or “lah-tex” to those who love goofy nerdy pronunciation. Technically speaking, the x is the “ch” sound in “Bach”, but most people just say it as “k”. While either saying “lay” or “lah” is correct, “layteks” is frowned upon because it clearly shows you’re not cool.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Working with R and RStudio</title>
      <link>https://ssc442.netlify.app/example/01-example/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/01-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-examples&#34;&gt;Introduction to Examples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started&#34;&gt;Getting started with &lt;code&gt;R&lt;/code&gt; and RStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-r-console&#34;&gt;The &lt;code&gt;R&lt;/code&gt; console&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scripts&#34;&gt;Scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rstudio&#34;&gt;RStudio&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-panes&#34;&gt;The panes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#key-bindings&#34;&gt;Key bindings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#running-commands-while-editing-scripts&#34;&gt;Running commands while editing scripts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#installing-r-packages&#34;&gt;Installing &lt;code&gt;R&lt;/code&gt; packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rmarkdown&#34;&gt;Rmarkdown&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction-to-examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to Examples&lt;/h1&gt;
&lt;p&gt;Examples in this class are designed to be presented in-class. Accordingly, the notes here are &lt;em&gt;not&lt;/em&gt; comprehensive. Instead, they are intended to guide students through&lt;/p&gt;
&lt;p&gt;I’m also aware that my writing is dry and lifeless. If you’re reading this online without the advantage of seeing it in person, don’t worry—I’ll be “funnier” in class.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting started with &lt;code&gt;R&lt;/code&gt; and RStudio&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is not a programming language like &lt;code&gt;C&lt;/code&gt; or &lt;code&gt;Java&lt;/code&gt;. It was not created by software engineers for software development. Instead, it was developed by statisticians as an interactive environment for data analysis. You can read the full history in the paper A Brief History of S&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. The interactivity is an indispensable feature in data science because, as you will soon learn, the ability to quickly explore data is a necessity for success in this field. However, like in other programming languages, you can save your work as scripts that can be easily executed at any moment. These scripts serve as a record of the analysis you performed, a key feature that facilitates reproducible work. If you are an expert programmer, you should not expect &lt;code&gt;R&lt;/code&gt; to follow the conventions you are used—assuming this will leave you disappointed. If you are patient, you will come to appreciate the unequal power of &lt;code&gt;R&lt;/code&gt; when it comes to data analysis and data visualization.&lt;/p&gt;
&lt;p&gt;Other attractive features of &lt;code&gt;R&lt;/code&gt; are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;R&lt;/code&gt; is free and open source&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;It runs on all major platforms: Windows, Mac OS, UNIX/Linux.&lt;/li&gt;
&lt;li&gt;Scripts and data objects can be shared seamlessly across platforms.&lt;/li&gt;
&lt;li&gt;There is a large, growing, and active community of &lt;code&gt;R&lt;/code&gt; users and, as a result, there are numerous resources for learning and asking questions&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;It is easy for others to contribute add-ons which enables developers to share software implementations of new data science methodologies. The latest methods and tools are developed in &lt;code&gt;R&lt;/code&gt; for a wide variety of disciplines and since social science is so broad, &lt;code&gt;R&lt;/code&gt; is one of the few tools that spans the varied social sciences.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-r-console&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;R&lt;/code&gt; console&lt;/h2&gt;
&lt;p&gt;Interactive data analysis usually occurs on the &lt;em&gt;R console&lt;/em&gt; that executes commands as you type them. There are several ways to gain access to an &lt;code&gt;R&lt;/code&gt; console. One way is to simply start &lt;code&gt;R&lt;/code&gt; on your computer. The console looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/R_console2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As a quick example, try using the console to calculate a 15% tip on a meal that cost $19.71:&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.15 * 19.71&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.9565&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note that in this course (at least, on most browsers), grey boxes are used to show &lt;code&gt;R&lt;/code&gt; code typed into the &lt;code&gt;R&lt;/code&gt; console. The symbol &lt;code&gt;##&lt;/code&gt; is used to denote what the &lt;code&gt;R&lt;/code&gt; console outputs.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scripts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scripts&lt;/h2&gt;
&lt;p&gt;One of the great advantages of &lt;code&gt;R&lt;/code&gt; over point-and-click analysis software is that you can save your work as scripts. You can edit and save these scripts using a text editor. The material in this course was developed using the interactive &lt;em&gt;integrated development environment&lt;/em&gt; (IDE) RStudio&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. RStudio includes an editor with many &lt;code&gt;R&lt;/code&gt; specific features, a console to execute your code, and other useful panes, including one to show figures.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/RStudio.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most web-based &lt;code&gt;R&lt;/code&gt; consoles also provide a pane to edit scripts, but not all permit you to save the scripts for later use. On the upper-right part of this webpage you’ll see a little button with the &lt;code&gt;R&lt;/code&gt; logo. You can access a web-based console there.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rstudio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;RStudio&lt;/h2&gt;
&lt;p&gt;RStudio will be our launching pad for data science projects. It not only provides an editor for us to create and edit our scripts but also provides many other useful tools. In this section, we go over some of the basics.&lt;/p&gt;
&lt;div id=&#34;the-panes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The panes&lt;/h3&gt;
&lt;p&gt;When you start RStudio for the first time, you will see three panes. The left pane shows the &lt;code&gt;R&lt;/code&gt; console. On the right, the top pane includes tabs such as &lt;em&gt;Environment&lt;/em&gt; and &lt;em&gt;History&lt;/em&gt;, while the bottom pane shows five tabs: &lt;em&gt;File&lt;/em&gt;, &lt;em&gt;Plots&lt;/em&gt;, &lt;em&gt;Packages&lt;/em&gt;, &lt;em&gt;Help&lt;/em&gt;, and &lt;em&gt;Viewer&lt;/em&gt; (these tabs may change in new versions). You can click on each tab to move across the different features.&lt;/p&gt;
&lt;p&gt;To start a new script, you can click on File, then New File, then &lt;code&gt;R&lt;/code&gt; Script.&lt;/p&gt;
&lt;p&gt;This starts a new pane on the left and it is here where you can start writing your script.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;key-bindings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Key bindings&lt;/h3&gt;
&lt;p&gt;Many tasks we perform with the mouse can be achieved with a combination of key strokes instead. These keyboard versions for performing tasks are referred to as &lt;em&gt;key bindings&lt;/em&gt;. For example, we just showed how to use the mouse to start a new script, but you can also use a key binding: Ctrl+Shift+N on Windows and command+shift+N on the Mac.&lt;/p&gt;
&lt;p&gt;Although in this tutorial we often show how to use the mouse, &lt;strong&gt;we highly recommend that you memorize key bindings for the operations you use most&lt;/strong&gt;. RStudio provides a useful cheat sheet with the most widely used commands. You might want to keep this handy so you can look up key-bindings when you find yourself performing repetitive point-and-clicking.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-commands-while-editing-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Running commands while editing scripts&lt;/h3&gt;
&lt;p&gt;There are many editors specifically made for coding. These are useful because color and indentation are automatically added to make code more readable. RStudio is one of these editors, and it was specifically developed for R. One of the main advantages provided by RStudio over other editors is that we can test our code easily as we edit our scripts. Below we show an example.&lt;/p&gt;
&lt;p&gt;Let’s start by opening a new script as we did before. A next step is to give the script a name. We can do this through the editor by saving the current new unnamed script. To do this, click on the save icon or use the key binding Ctrl+S on Windows and command+S on the Mac.&lt;/p&gt;
&lt;p&gt;When you ask for the document to be saved for the first time, RStudio will prompt you for a name. A good convention is to use a descriptive name, with lower case letters, no spaces, only hyphens to separate words, and then followed by the suffix &lt;em&gt;.R&lt;/em&gt;. We will call this script &lt;em&gt;my-first-script.R&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now we are ready to start editing our first script. The first lines of code in an &lt;code&gt;R&lt;/code&gt; script are dedicated to loading the libraries we will use. Another useful RStudio feature is that once we type &lt;code&gt;library()&lt;/code&gt; it starts auto-completing with libraries that we have installed. Note what happens when we type &lt;code&gt;library(ti)&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;Another feature you may have noticed is that when you type &lt;code&gt;library(&lt;/code&gt; the second parenthesis is automatically added. This will help you avoid one of the most common errors in coding: forgetting to close a parenthesis.&lt;/p&gt;
&lt;p&gt;Now we can continue to write code. As an example, we will make a graph showing murder totals versus population totals by state. Once you are done writing the code needed to make this plot, you can try it out by &lt;em&gt;executing&lt;/em&gt; the code. To do this, click on the &lt;em&gt;Run&lt;/em&gt; button on the upper right side of the editing pane. You can also use the key binding: Ctrl+Shift+Enter on Windows or command+shift+return on the Mac.&lt;/p&gt;
&lt;p&gt;Once you run the code, you will see it appear in the &lt;code&gt;R&lt;/code&gt; console and, in this case, the generated plot appears in the plots console. Note that the plot console has a useful interface that permits you to click back and forward across different plots, zoom in to the plot, or save the plots as files.&lt;/p&gt;
&lt;p&gt;To run one line at a time instead of the entire script, you can use Control-Enter on Windows and command-return on the Mac.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;SETUP TIP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Change the option &lt;em&gt;Save workspace to .RData on exit&lt;/em&gt; to &lt;em&gt;Never&lt;/em&gt; and uncheck the &lt;em&gt;Restore .RData into workspace at start&lt;/em&gt;. By default, when you exit &lt;code&gt;R&lt;/code&gt; saves all the objects you have created into a file called .RData. This is done so that when you restart the session in the same folder, it will load these objects. I find that this causes confusion especially when sharing code with colleagues or peers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-r-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing &lt;code&gt;R&lt;/code&gt; packages&lt;/h2&gt;
&lt;p&gt;The functionality provided by a fresh install of &lt;code&gt;R&lt;/code&gt; is only a small fraction of what is possible. In fact, we refer to what you get after your first install as &lt;em&gt;base R&lt;/em&gt;. The extra functionality comes from add-ons available from developers. There are currently hundreds of these available from CRAN and many others shared via other repositories such as GitHub. However, because not everybody needs all available functionality, &lt;code&gt;R&lt;/code&gt; instead makes different components available via &lt;em&gt;packages&lt;/em&gt;. &lt;code&gt;R&lt;/code&gt; makes it very easy to install packages from within R. For example, to install the &lt;strong&gt;dslabs&lt;/strong&gt; package, which we use to share datasets and code related to this book, you would type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;dslabs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In RStudio, you can navigate to the &lt;em&gt;Tools&lt;/em&gt; tab and select install packages. We can then load the package into our &lt;code&gt;R&lt;/code&gt; sessions using the &lt;code&gt;library&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dslabs&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:gapminder&amp;#39;:
## 
##     gapminder&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you go through this book, you will see that we load packages without installing them. This is because once you install a package, it remains installed and only needs to be loaded with &lt;code&gt;library&lt;/code&gt;. The package remains loaded until we quit the &lt;code&gt;R&lt;/code&gt; session. If you try to load a package and get an error, it probably means you need to
install it first.&lt;/p&gt;
&lt;p&gt;We can install more than one package at once by feeding a character vector to this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;tidyverse&amp;quot;, &amp;quot;dslabs&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One advantage of using RStudio is that it auto-completes package names once you start typing, which is helpful when you do not remember the exact spelling of the package. Once you select your package, we recommend selecting all the defaults. Note that installing &lt;strong&gt;tidyverse&lt;/strong&gt; actually installs several packages. This commonly occurs when a package has &lt;em&gt;dependencies&lt;/em&gt;, or uses functions from other packages. When you load a package using &lt;code&gt;library&lt;/code&gt;, you also load its dependencies.&lt;/p&gt;
&lt;p&gt;Once packages are installed, you can load them into &lt;code&gt;R&lt;/code&gt; and you do not need to install them again, unless you install a fresh version of R. Remember packages are installed in &lt;code&gt;R&lt;/code&gt; not RStudio.&lt;/p&gt;
&lt;p&gt;It is helpful to keep a list of all the packages you need for your work in a script because if you need to perform a fresh install of R, you can re-install all your packages by simply running a script.&lt;/p&gt;
&lt;p&gt;You can see all the packages you have installed using the following function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;installed.packages()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we move through this course, we will constantly be adding to our toolbox of packages. Accordingly, you will need to keep track to ensure you have the requisite package for any given lecture.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rmarkdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rmarkdown&lt;/h2&gt;
&lt;p&gt;Markdown is a general-purpose syntax for laying out documents. Rmarkdown is a combination of R and markdown, as the name implies. When using markdown, one can define headers and tables using specific notation, and depending on the rendering engine, the headers and tables (and a whole lot more) are customized. In fact, this whole website is built in R using Rmarkdown (and a lot of add-ons like Hugo and blogdown). In other contexts, the rendering engine may recognize that your headers are likely to be entries in a table of contents, and does so for you. The table of contents at the top of this document is built from the markdown headers.&lt;/p&gt;
&lt;p&gt;The power of Rmarkdown is that it lets us mix formatted text with R code. That is, you can have a section of the document that understands R code, and a separate section right after that discusses the results from the R code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ajkirkpatrick/FS20/Spring2021/Rmarkdown_templates/SSC442_Weekly_Writing_Template.Rmd&#34;&gt;Try it out using the Weekly Writing Template&lt;/a&gt;. If it opens in your web browser, just right-click the link and select Save As…. &lt;strong&gt;Make sure you save the file to its own folder on your hard drive&lt;/strong&gt;. In converting your Rmarkdown .Rmd file to a .pdf, your system will make multiple interim files&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. It also creates folders to store the output of any plots or graphics you create with your R code.&lt;/p&gt;
&lt;p&gt;If we have time today, let’s open the template linked above and see what happens when we select “knit to pdf”.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Comments from previous classes indicate that I am not, in fact, funny.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf&#34; class=&#34;uri&#34;&gt;https://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://opensource.org/history&#34; class=&#34;uri&#34;&gt;https://opensource.org/history&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/138/free-resources-for-learning-r&#34; class=&#34;uri&#34;&gt;https://stats.stackexchange.com/questions/138/free-resources-for-learning-r&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.r-project.org/help.html&#34; class=&#34;uri&#34;&gt;https://www.r-project.org/help.html&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/documentation/r/topics&#34; class=&#34;uri&#34;&gt;https://stackoverflow.com/documentation/r/topics&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;But probably tip more than 15%. Times are tough, man.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.rstudio.com/&#34; class=&#34;uri&#34;&gt;https://www.rstudio.com/&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;Specifically, knitr will create an intermediate .md file which is then processed with Pandoc using Latex to create a pdf. Whew!&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>https://ssc442.netlify.app/example/12-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/12-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-to-be-wrangled&#34;&gt;Data to be wrangled&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-to-be-wrangled&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data to be wrangled&lt;/h1&gt;
&lt;p&gt;You work for a travel booking website as a data analyst. A hotel has asked your company for data on corporate bookings at the hotel via your site. Specifically, they have five corporations that are frequent customers of the hotel, and they want to know who spends the most with them. They’ve asked you to help out. Most of the corporate spending is in the form of room reservations, but there are also parking fees that the hotel wants included in the analysis. Your goal: total up spending by corporation and report the biggest and smallest spenders inclusive of rooms and parking.&lt;/p&gt;
&lt;p&gt;Unfortunately, you only have the following data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Example11_booking.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;booking.csv&lt;/code&gt;&lt;/a&gt; - Contains the corporation name, the room type, and the dates someone from the corporation stayed at the hoted.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Example11_roomrates.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;roomrates.csv&lt;/code&gt;&lt;/a&gt; - Contains the price of each room on each day&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Example11_parking.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;parking.csv&lt;/code&gt;&lt;/a&gt; - Contains the corporations who negotiated free parking for employees&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Parking at the hotel is $60 if you don’t have free parking. This hotel is in California, so everyone drives and parks when they stay.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Some tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Right-click on each of the links, copy the address, and read the URL in using &lt;code&gt;read.csv&lt;/code&gt; or &lt;code&gt;read_csv&lt;/code&gt; or whatever you prefer to read .csv’s&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll find you need to use most of the tools we covered on Tuesday including &lt;code&gt;gather&lt;/code&gt;, &lt;code&gt;separate&lt;/code&gt; and more.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Regression</title>
      <link>https://ssc442.netlify.app/example/06-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/06-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34;&gt;Code&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-and-clean-data&#34;&gt;Load and clean data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#legal-dual-y-axes&#34;&gt;Legal dual y-axes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#combining-plots&#34;&gt;Combining plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scatterplot-matrices&#34;&gt;Scatterplot matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlograms&#34;&gt;Correlograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simple-regression&#34;&gt;Simple regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-plots&#34;&gt;Coefficient plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#marginal-effects-plots&#34;&gt;Marginal effects plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will alternate between the content from this week (since we did not finish a complete discussion) and the example below.&lt;/p&gt;
&lt;p&gt;We will also use the Ames data again (in addition to the Atlanta weather data). The Ames data is linked below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;For this example, we’re again going to use historical weather data from &lt;a href=&#34;https://darksky.net/forecast/33.7546,-84.39/us12/en&#34;&gt;Dark Sky&lt;/a&gt; about wind speed and temperature trends for downtown Atlanta (&lt;a href=&#34;https://www.google.com/maps/place/33°45&amp;#39;16.4%22N+84°23&amp;#39;24.0%22W/@33.754557,-84.3921977,17z/&#34;&gt;specifically &lt;code&gt;33.754557, -84.390009&lt;/code&gt;&lt;/a&gt;) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the &lt;a href=&#34;https://github.com/hrbrmstr/darksky&#34;&gt;&lt;strong&gt;darksky&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/atl-weather-2019.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;atl-weather-2019.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;div id=&#34;load-and-clean-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load and clean data&lt;/h3&gt;
&lt;p&gt;First, we load the libraries we’ll be using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)  # For ggplot, dplyr, and friends
library(patchwork)  # For combining ggplot plots
library(GGally)     # For scatterplot matrices
library(broom)      # For converting model objects to data frames&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we load the data with &lt;code&gt;read_csv()&lt;/code&gt;. Here we assume that the CSV file lives in a subfolder named &lt;code&gt;data&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl &amp;lt;- read_csv(&amp;quot;data/atl-weather-2019.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;legal-dual-y-axes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Legal dual y-axes&lt;/h3&gt;
&lt;p&gt;It is fine (and often helpful) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.&lt;/p&gt;
&lt;p&gt;To do this, you need to add an argument (&lt;code&gt;sec.axis&lt;/code&gt;) to &lt;code&gt;scale_y_continuous()&lt;/code&gt; to tell it to use a second axis. This &lt;code&gt;sec.axis&lt;/code&gt; argument takes a &lt;code&gt;sec_axis()&lt;/code&gt; function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a &lt;code&gt;~&lt;/code&gt;, which indicates that it’s a function, and it needs to use &lt;code&gt;.&lt;/code&gt; to stand in for the original value in the original axis.&lt;/p&gt;
&lt;p&gt;Since the equation for converting Fahrenheit to Celsius is this…&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{C} = (32 - \text{F}) \times -\frac{5}{9}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;…we can specify this with code like so (where &lt;code&gt;.&lt;/code&gt; stands for the Fahrenheit value):&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;~ (32 - .) * -5 / 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +
  geom_line() +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,
                                         name = &amp;quot;Celsius&amp;quot;)) +
  labs(x = NULL, y = &amp;quot;Fahrenheit&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/atl-weather-dual-axes-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For fun, we could also convert it to Kelvin, which uses this formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{K} = (\text{F} - 32) \times \frac{5}{9} + 273.15
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +
  geom_line() +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15,
                                         name = &amp;quot;Kelvin&amp;quot;)) +
  labs(x = NULL, y = &amp;quot;Fahrenheit&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/atl-weather-dual-axes-kelvin-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;combining-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Combining plots&lt;/h3&gt;
&lt;p&gt;A good alternative to using two y-axes is to use two plots instead. The &lt;a href=&#34;https://github.com/thomasp85/patchwork&#34;&gt;&lt;strong&gt;patchwork&lt;/strong&gt; package&lt;/a&gt; makes this &lt;em&gt;really&lt;/em&gt; easy to do with R. There are other similar packages that do this, like &lt;strong&gt;cowplot&lt;/strong&gt; and &lt;strong&gt;gridExtra&lt;/strong&gt;, but I’ve found that &lt;strong&gt;patchwork&lt;/strong&gt; is the easiest to use &lt;em&gt;and&lt;/em&gt; it actually aligns the different plot elements like axis lines and legends (yay alignment in CRAP!). The &lt;a href=&#34;https://patchwork.data-imaginist.com/articles/guides/assembly.html&#34;&gt;documentation for &lt;strong&gt;patchwork&lt;/strong&gt;&lt;/a&gt; is really great and full of examples—you should check it out to see all the things you can do with it!&lt;/p&gt;
&lt;p&gt;To use &lt;strong&gt;patchwork&lt;/strong&gt;, we need to (1) save our plots as objects and (2) add them together with &lt;code&gt;+&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Temperature in Atlanta
temp_plot &amp;lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +
  geom_line() +
  geom_smooth() +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,
                                         name = &amp;quot;Celsius&amp;quot;)) +
  labs(x = NULL, y = &amp;quot;Fahrenheit&amp;quot;) +
  theme_minimal()
temp_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/create-temp-humid-plots-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Humidity in Atlanta
humidity_plot &amp;lt;- ggplot(weather_atl, aes(x = time, y = humidity)) +
  geom_line() +
  geom_smooth() +
  labs(x = NULL, y = &amp;quot;Humidity&amp;quot;) +
  theme_minimal()
humidity_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/create-temp-humid-plots-2.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Right now, these are two separate plots, but we can combine them with &lt;code&gt;+&lt;/code&gt; if we load &lt;strong&gt;patchwork&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)

temp_plot + humidity_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/patchwork-first-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By default, &lt;strong&gt;patchwork&lt;/strong&gt; will put these side-by-side, but we can change that with the &lt;code&gt;plot_layout()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp_plot + humidity_plot +
  plot_layout(ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/patchwork-vertical-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also play with other arguments in &lt;code&gt;plot_layout()&lt;/code&gt;. If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp_plot + humidity_plot +
  plot_layout(ncol = 1, heights = c(0.7, 0.3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/patchwork-vertical-resized-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplot-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scatterplot matrices&lt;/h3&gt;
&lt;p&gt;We can visualize the correlations between pairs of variables with the &lt;code&gt;ggpairs()&lt;/code&gt; function in the &lt;strong&gt;GGally&lt;/strong&gt; package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into &lt;code&gt;ggpairs()&lt;/code&gt; to see all the correlation information:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GGally)

weather_correlations &amp;lt;- weather_atl %&amp;gt;%
  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability)

ggpairs(weather_correlations)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/ggpairs-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind spped and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).&lt;/p&gt;
&lt;p&gt;Even though &lt;code&gt;ggpairs()&lt;/code&gt; doesn’t use the standard &lt;code&gt;ggplot(...) + geom_whatever()&lt;/code&gt; syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpairs(weather_correlations) +
  labs(title = &amp;quot;Correlations!&amp;quot;) +
  theme_dark()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Make a ggpairs plot for some of the Ames data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;correlograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlograms&lt;/h3&gt;
&lt;p&gt;Scatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In the readings for today, Claus Wilke showed a type of plot called a &lt;a href=&#34;https://serialmentor.com/dataviz/visualizing-associations.html#associations-correlograms&#34;&gt;&lt;em&gt;correlogram&lt;/em&gt;&lt;/a&gt; which &lt;em&gt;is&lt;/em&gt; more appropriate for publication.&lt;/p&gt;
&lt;p&gt;These are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.&lt;/p&gt;
&lt;p&gt;First we need to build a correlation matrix of the main variables we care about. Ordinarily the &lt;code&gt;cor()&lt;/code&gt; function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into &lt;code&gt;cor()&lt;/code&gt; though, it’ll calculate the correlation between each pair of columns&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a correlation matrix
things_to_correlate &amp;lt;- weather_atl %&amp;gt;%
  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %&amp;gt;%
  cor()

things_to_correlate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   temperatureHigh temperatureLow humidity windSpeed precipProbability
## temperatureHigh              1.00          0.920   -0.030    -0.377            -0.124
## temperatureLow               0.92          1.000    0.112    -0.450            -0.026
## humidity                    -0.03          0.112    1.000     0.011             0.722
## windSpeed                   -0.38         -0.450    0.011     1.000             0.196
## precipProbability           -0.12         -0.026    0.722     0.196             1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to &lt;code&gt;NA&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get rid of the lower triangle
things_to_correlate[lower.tri(things_to_correlate)] &amp;lt;- NA
things_to_correlate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   temperatureHigh temperatureLow humidity windSpeed precipProbability
## temperatureHigh                 1           0.92    -0.03    -0.377            -0.124
## temperatureLow                 NA           1.00     0.11    -0.450            -0.026
## humidity                       NA             NA     1.00     0.011             0.722
## windSpeed                      NA             NA       NA     1.000             0.196
## precipProbability              NA             NA       NA        NA             1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the &lt;code&gt;things_to_correlate&lt;/code&gt; matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named &lt;code&gt;measure1&lt;/code&gt;, and take all the correlation numbers and put them in a column named &lt;code&gt;cor&lt;/code&gt; In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;things_to_correlate_long &amp;lt;- things_to_correlate %&amp;gt;%
  # Convert from a matrix to a data frame
  as.data.frame() %&amp;gt;%
  # Matrixes have column names that don&amp;#39;t get converted to columns when using
  # as.data.frame(), so this adds those names as a column
  rownames_to_column(&amp;quot;measure2&amp;quot;) %&amp;gt;%
  # Make this long. Take all the columns except measure2 and put their names in
  # a column named measure1 and their values in a column named cor
  pivot_longer(cols = -measure2,
               names_to = &amp;quot;measure1&amp;quot;,
               values_to = &amp;quot;cor&amp;quot;) %&amp;gt;%
  # Make a new column with the rounded version of the correlation value
  mutate(nice_cor = round(cor, 2)) %&amp;gt;%
  # Remove rows where the two measures are the same (like the correlation
  # between humidity and humidity)
  filter(measure2 != measure1) %&amp;gt;%
  # Get rid of the empty triangle
  filter(!is.na(cor)) %&amp;gt;%
  # Put these categories in order
  mutate(measure1 = fct_inorder(measure1),
         measure2 = fct_inorder(measure2))

things_to_correlate_long&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 4
##    measure2        measure1              cor nice_cor
##    &amp;lt;fct&amp;gt;           &amp;lt;fct&amp;gt;               &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 temperatureHigh temperatureLow     0.920      0.92
##  2 temperatureHigh humidity          -0.0301    -0.03
##  3 temperatureHigh windSpeed         -0.377     -0.38
##  4 temperatureHigh precipProbability -0.124     -0.12
##  5 temperatureLow  humidity           0.112      0.11
##  6 temperatureLow  windSpeed         -0.450     -0.45
##  7 temperatureLow  precipProbability -0.0255    -0.03
##  8 humidity        windSpeed          0.0108     0.01
##  9 humidity        precipProbability  0.722      0.72
## 10 windSpeed       precipProbability  0.196      0.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Phew. With the data all tidied like that, we can make a correlogram with a heatmap. We have manipulated the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(things_to_correlate_long,
       aes(x = measure2, y = measure1, fill = cor)) +
  geom_tile() +
  geom_text(aes(label = nice_cor)) +
  scale_fill_gradient2(low = &amp;quot;#E16462&amp;quot;, mid = &amp;quot;white&amp;quot;, high = &amp;quot;#0D0887&amp;quot;,
                       limits = c(-1, 1)) +
  labs(x = NULL, y = NULL) +
  coord_equal() +
  theme_minimal() +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/cor-heatmap-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instead of using a heatmap, we can also use points, which encode the correlation information both as color &lt;em&gt;and&lt;/em&gt; as size. To do that, we just need to switch &lt;code&gt;geom_tile()&lt;/code&gt; to &lt;code&gt;geom_point()&lt;/code&gt; and set the &lt;code&gt;size = cor&lt;/code&gt; mapping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(things_to_correlate_long,
       aes(x = measure2, y = measure1, color = cor)) +
  # Size by the absolute value so that -0.7 and 0.7 are the same size
  geom_point(aes(size = abs(cor))) +
  scale_color_gradient2(low = &amp;quot;#E16462&amp;quot;, mid = &amp;quot;white&amp;quot;, high = &amp;quot;#0D0887&amp;quot;,
                        limits = c(-1, 1)) +
  scale_size_area(max_size = 15, limits = c(-1, 1), guide = FALSE) +
  labs(x = NULL, y = NULL) +
  coord_equal() +
  theme_minimal() +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please use `guide = &amp;quot;none&amp;quot;` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/cor-points-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simple regression&lt;/h3&gt;
&lt;p&gt;We finally get to this week’s content. Although correlation is nice, we eventually may want to visualize regression. The lecture has shown us some very intuitive, straightforward ways to think about regression (aka, a line). Simple regression is easy to visualize, since you’re only working with an &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and a &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. For instance, what’s the relationship between humidity and high temperatures during the summer?&lt;/p&gt;
&lt;p&gt;First, let’s filter the data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_summer &amp;lt;- weather_atl %&amp;gt;%
  filter(time &amp;gt;= &amp;quot;2019-05-01&amp;quot;, time &amp;lt;= &amp;quot;2019-09-30&amp;quot;) %&amp;gt;%
  mutate(humidity_scaled = humidity * 100,
         moonPhase_scaled = moonPhase * 100,
         precipProbability_scaled = precipProbability * 100,
         cloudCover_scaled = cloudCover * 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can build a simple regression model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_simple &amp;lt;- lm(temperatureHigh ~ humidity_scaled,
                   data = weather_atl_summer)

tidy(model_simple, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 7
##   term            estimate std.error statistic  p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)      104.       2.35       44.3  1.88e-88   99.5     109.   
## 2 humidity_scaled   -0.241    0.0358     -6.74 3.21e-10   -0.312    -0.170&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can interpret these coefficients like so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The intercept shows that the average temperature when there’s 0% humidity is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line.&lt;/li&gt;
&lt;li&gt;The coefficient for &lt;code&gt;humidity_scaled&lt;/code&gt; shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Visualizing this model is simple, since there are only two variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl_summer,
       aes(x = humidity_scaled, y = temperatureHigh)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/plot-simple-model-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And indeed, as humidity increases, temperatures decrease.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Coefficient plots&lt;/h3&gt;
&lt;p&gt;But if we use multiple variables in the model (and we will do this a lot going forward), it gets really hard to visualize the results since we’re working with multiple dimensions. Instead, we can use coefficient plots to see the individual coefficients in the model.&lt;/p&gt;
&lt;p&gt;First, let’s build a more complex model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_complex &amp;lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled +
                      precipProbability_scaled + windSpeed + pressure + cloudCover_scaled,
                    data = weather_atl_summer)
tidy(model_complex, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 × 7
##   term                     estimate std.error statistic   p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;                       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)              262.      125.         2.09  0.0380    14.8      510.    
## 2 humidity_scaled           -0.111     0.0757    -1.47  0.143     -0.261      0.0381
## 3 moonPhase_scaled           0.0116    0.0126     0.917 0.360     -0.0134     0.0366
## 4 precipProbability_scaled   0.0356    0.0203     1.75  0.0820    -0.00458    0.0758
## 5 windSpeed                 -1.78      0.414     -4.29  0.0000326 -2.59      -0.958 
## 6 pressure                  -0.157     0.122     -1.28  0.203     -0.398      0.0854
## 7 cloudCover_scaled         -0.0952    0.0304    -3.14  0.00207   -0.155     -0.0352&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can interpret these coefficients like so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Holding everything else constant, a 1% increase in humidity is associated with a 0.11° decrease in the high temperature, on average, but the effect is not statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1% increase in moon visibility is associated with a 0.01° increase in the high temperature, on average, and the effect is not statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1% increase in the probability of precipitation is associated with a 0.04° increase in the high temperature, on average, and the effect is not statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1 mph increase in the wind speed is associated with a 1.8° decrease in the high temperature, on average, and the effect &lt;em&gt;is&lt;/em&gt; statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1 unit increase in barometric pressure is associated with a 0.15° decrease in the high temperature, on average, and the effect is not statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1% increase in cloud cover is associated with a 0.01° decrease in the high temperature, on average, and the effect &lt;em&gt;is&lt;/em&gt; statistically significant&lt;/li&gt;
&lt;li&gt;The intercept is pretty useless. It shows that the predicted temperature will be 262° when humidity is 0%, the moon is invisible, there’s no chance of precipitation, no wind, no barometric pressure, and no cloud cover. Yikes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To plot all these things at once, we’ll store the results of &lt;code&gt;tidy(model_complex)&lt;/code&gt; as a data frame, remove the useless intercept, and plot it using &lt;code&gt;geom_pointrange()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_tidied &amp;lt;- tidy(model_complex, conf.int = TRUE) %&amp;gt;%
  filter(term != &amp;quot;(Intercept)&amp;quot;)

ggplot(model_tidied,
       aes(x = estimate, y = term)) +
  geom_vline(xintercept = 0, color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dotted&amp;quot;) +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +
  labs(x = &amp;quot;Coefficient estimate&amp;quot;, y = NULL) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/coef-plot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Neat! Now we can see how big these different coefficients are and how close they are to zero. Wind speed has a big significant effect on temperature. The others are all very close to zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-effects-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Marginal effects plots&lt;/h3&gt;
&lt;p&gt;Instead of just looking at the coefficients, we can also see the effect of moving different variables up and down like sliders and switches. Remember that regression coefficients allow us to build actual mathematical formulas that predict the value of Y. The coefficients from &lt;code&gt;model_compex&lt;/code&gt; yield the following big hairy ugly equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat{\text{High temperature}} =&amp;amp; 262 - 0.11 \times \text{humidity_scaled } \\
&amp;amp; + 0.01 \times \text{moonPhase_scaled } + 0.04 \times \text{precipProbability_scaled } \\
&amp;amp; - 1.78 \times \text{windSpeed} - 0.16 \times \text{pressure} - 0.095 \times \text{cloudCover_scaled}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we plug in values for each of the explanatory variables, we can get the predicted value of high temperature, or &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;augment()&lt;/code&gt; function in the &lt;strong&gt;broom&lt;/strong&gt; library allows us to take a data frame of explanatory variable values, plug them into the model equation, and get predictions out. For example, let’s set each of the variables to some arbitrary values (50% for humidity, moon phase, chance of rain, and cloud cover; 1000 for pressure, and 1 MPH for wind speed).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata_example &amp;lt;- tibble(humidity_scaled = 50, moonPhase_scaled = 50,
                          precipProbability_scaled = 50, windSpeed = 1,
                          pressure = 1000, cloudCover_scaled = 50)
newdata_example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 6
##   humidity_scaled moonPhase_scaled precipProbability_scaled windSpeed pressure cloudCover_scaled
##             &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;                    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
## 1              50               50                       50         1     1000                50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plug these values into the model with &lt;code&gt;augment()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# I use select() here because augment() returns columns for all the explanatory
# variables, and the .fitted column with the predicted value is on the far right
# and gets cut off
augment(model_complex, newdata = newdata_example, se_fit=TRUE) %&amp;gt;%
  select(.fitted, .se.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given these weather conditions, the predicted high temperature is 96.2°. Now you’re an armchair meteorologist!&lt;/p&gt;
&lt;p&gt;We can follow the same pattern to show how the predicted temperature changes as specific variables change across a whole range. Here, we create a data frame of possible wind speeds and keep all the other explanatory variables at their means:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata &amp;lt;- tibble(windSpeed = seq(0, 8, 0.5),
                  pressure = mean(weather_atl_summer$pressure),
                  precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),
                  moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),
                  humidity_scaled = mean(weather_atl_summer$humidity_scaled),
                  cloudCover_scaled = mean(weather_atl_summer$cloudCover_scaled))
newdata&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17 × 6
##    windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled
##        &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;                    &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
##  1       0      1016.                     40.2             50.7            64.8              29.5
##  2       0.5    1016.                     40.2             50.7            64.8              29.5
##  3       1      1016.                     40.2             50.7            64.8              29.5
##  4       1.5    1016.                     40.2             50.7            64.8              29.5
##  5       2      1016.                     40.2             50.7            64.8              29.5
##  6       2.5    1016.                     40.2             50.7            64.8              29.5
##  7       3      1016.                     40.2             50.7            64.8              29.5
##  8       3.5    1016.                     40.2             50.7            64.8              29.5
##  9       4      1016.                     40.2             50.7            64.8              29.5
## 10       4.5    1016.                     40.2             50.7            64.8              29.5
## 11       5      1016.                     40.2             50.7            64.8              29.5
## 12       5.5    1016.                     40.2             50.7            64.8              29.5
## 13       6      1016.                     40.2             50.7            64.8              29.5
## 14       6.5    1016.                     40.2             50.7            64.8              29.5
## 15       7      1016.                     40.2             50.7            64.8              29.5
## 16       7.5    1016.                     40.2             50.7            64.8              29.5
## 17       8      1016.                     40.2             50.7            64.8              29.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we feed this big data frame into &lt;code&gt;augment()&lt;/code&gt;, we can get the predicted high temperature for each row. We can also use the &lt;code&gt;.se.fit&lt;/code&gt; column to calculate the 95% confidence interval for each predicted value. We take the standard error, multiply it by -1.96 and 1.96 (or the quantile of the normal distribution at 2.5% and 97.5%), and add that value to the estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted_values &amp;lt;- augment(model_complex, newdata = newdata, se_fit=TRUE) %&amp;gt;%
  mutate(conf.low = .fitted + (-1.96 * .se.fit),
         conf.high = .fitted + (1.96 * .se.fit))

predicted_values %&amp;gt;%
  select(windSpeed, .fitted, .se.fit, conf.low, conf.high) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 5
##   windSpeed .fitted .se.fit conf.low conf.high
##       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1       0      95.3   1.63      92.2      98.5
## 2       0.5    94.5   1.42      91.7      97.2
## 3       1      93.6   1.22      91.2      96.0
## 4       1.5    92.7   1.03      90.7      94.7
## 5       2      91.8   0.836     90.1      93.4
## 6       2.5    90.9   0.653     89.6      92.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool! Just looking at the data in the table, we can see that predicted temperature drops as windspeed increases. We can plot this to visualize the effect:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(predicted_values, aes(x = windSpeed, y = .fitted)) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high),
              fill = &amp;quot;#BF3984&amp;quot;, alpha = 0.5) +
  geom_line(size = 1, color = &amp;quot;#BF3984&amp;quot;) +
  labs(x = &amp;quot;Wind speed (MPH)&amp;quot;, y = &amp;quot;Predicted high temperature (F)&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/mfx-plot-simple-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We just manipulated one of the model coefficients and held everything else at its mean. We can manipulate multiple variables too and encode them all on the graph. For instance, what is the effect of windspeed &lt;em&gt;and&lt;/em&gt; cloud cover on the temperature?&lt;/p&gt;
&lt;p&gt;We’ll follow the same process, but vary both &lt;code&gt;windSpeed&lt;/code&gt; and &lt;code&gt;cloudCover_scaled&lt;/code&gt;. Instead of using &lt;code&gt;tibble()&lt;/code&gt;, we use &lt;code&gt;exapnd_grid()&lt;/code&gt;, which creates every combination of the variables we specify. Instead of varying cloud cover by every possible value (like from 0 to 100), we’ll choose four possible cloud cover types: 0%, 33%, 66%, and 100%. Everything else will be at its mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata_fancy &amp;lt;- expand_grid(windSpeed = seq(0, 8, 0.5),
                             pressure = mean(weather_atl_summer$pressure),
                             precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),
                             moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),
                             humidity_scaled = mean(weather_atl_summer$humidity_scaled),
                             cloudCover_scaled = c(0, 33, 66, 100))
newdata_fancy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 68 × 6
##    windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled
##        &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;                    &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
##  1       0      1016.                     40.2             50.7            64.8                 0
##  2       0      1016.                     40.2             50.7            64.8                33
##  3       0      1016.                     40.2             50.7            64.8                66
##  4       0      1016.                     40.2             50.7            64.8               100
##  5       0.5    1016.                     40.2             50.7            64.8                 0
##  6       0.5    1016.                     40.2             50.7            64.8                33
##  7       0.5    1016.                     40.2             50.7            64.8                66
##  8       0.5    1016.                     40.2             50.7            64.8               100
##  9       1      1016.                     40.2             50.7            64.8                 0
## 10       1      1016.                     40.2             50.7            64.8                33
## # … with 58 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice now that &lt;code&gt;windSpeed&lt;/code&gt; repeats four times (0, 0, 0, 0, 0.5, 0.5, etc.), since there are four possible &lt;code&gt;cloudCover_scaled&lt;/code&gt; values (0, 33, 66, 100).&lt;/p&gt;
&lt;p&gt;We can plot this now, just like before, with wind speed on the x-axis, the predicted temperature on the y-axis, and colored and faceted by cloud cover:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted_values_fancy &amp;lt;- augment(model_complex, newdata = newdata_fancy, se_fit=TRUE) %&amp;gt;%
  mutate(conf.low = .fitted + (-1.96 * .se.fit),
         conf.high = .fitted + (1.96 * .se.fit)) %&amp;gt;%
  # Make cloud cover a categorical variable
  mutate(cloudCover_scaled = factor(cloudCover_scaled))

ggplot(predicted_values_fancy, aes(x = windSpeed, y = .fitted)) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled),
              alpha = 0.5) +
  geom_line(aes(color = cloudCover_scaled), size = 1) +
  labs(x = &amp;quot;Wind speed (MPH)&amp;quot;, y = &amp;quot;Predicted high temperature (F)&amp;quot;) +
  theme_minimal() +
  guides(fill = FALSE, color = FALSE) +
  facet_wrap(vars(cloudCover_scaled), nrow = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `guides(&amp;lt;scale&amp;gt; = FALSE)` is deprecated. Please use `guides(&amp;lt;scale&amp;gt; = &amp;quot;none&amp;quot;)` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/06-example_files/figure-html/mfx-complex-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nice. Temperatures go down slightly as cloud cover increases. If we wanted to improve the model, we’d add an interaction term between cloud cover and windspeed so that each line would have a different slope in addition to a different intercept, but that’s beyond the scope of this class.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression: Interpreting Coefficients</title>
      <link>https://ssc442.netlify.app/example/07-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/07-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dummy-variables&#34;&gt;Dummy Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interactions&#34;&gt;Interactions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#factor-variables&#34;&gt;Factor Variables&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#factors-with-more-than-two-levels&#34;&gt;Factors with More Than Two Levels&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameterization&#34;&gt;Parameterization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-larger-models&#34;&gt;Building Larger Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will pivot between the content from this week and the example below.&lt;/p&gt;
&lt;p&gt;We will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;So far in each of our analyses, we have only used numeric variables as predictors. We have also only used &lt;em&gt;additive models&lt;/em&gt;, meaning the effect any predictor had on the response was not dependent on the other predictors. In this chapter, we will remove both of these restrictions. We will fit models with categorical predictors, and use models that allow predictors to &lt;em&gt;interact&lt;/em&gt;. The mathematics of multiple regression will remain largely unchanging, however, we will pay close attention to interpretation, as well as some difference in &lt;code&gt;R&lt;/code&gt; usage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dummy-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dummy Variables&lt;/h2&gt;
&lt;p&gt;For this example and discussion, we will briefly use the built in dataset &lt;code&gt;mtcars&lt;/code&gt; before returning to our favorite &lt;code&gt;autompg&lt;/code&gt; dataset. During the in-class lecture / example, I will also use much more interesting datasets. The reason to use these easy, straightforward datasets is that they make visualization of the &lt;strong&gt;entire dataset&lt;/strong&gt; trivially easy. Accordingly, the &lt;code&gt;mtcars&lt;/code&gt; dataset is small, so we’ll quickly take a look at the entire dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be interested in three of the variables: &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;hp&lt;/code&gt;, and &lt;code&gt;am&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mpg&lt;/code&gt;: fuel efficiency, in miles per gallon.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hp&lt;/code&gt;: horsepower, in foot-pounds per second.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;am&lt;/code&gt;: transmission. Automatic or manual.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we often do, we will start by plotting the data. We are interested in &lt;code&gt;mpg&lt;/code&gt; as the response variable, and &lt;code&gt;hp&lt;/code&gt; as a predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, cex = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since we are also interested in the transmission type, we could also label the points accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Automatic&amp;quot;, &amp;quot;Manual&amp;quot;), col = c(1, 2), pch = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now fit the SLR model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;hp&lt;/code&gt;. For notational brevity, we drop the index &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; for observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp_slr = lm(mpg ~ hp, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then re-plot the data and add the fitted line to the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
abline(mpg_hp_slr, lwd = 3, col = &amp;quot;grey&amp;quot;)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Automatic&amp;quot;, &amp;quot;Manual&amp;quot;), col = c(1, 2), pch = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We should notice a pattern here. The red, manual observations largely fall above the line, while the black, automatic observations are mostly below the line. This means our model underestimates the fuel efficiency of manual transmissions, and overestimates the fuel efficiency of automatic transmissions. To correct for this, we will add a predictor to our model, namely, &lt;code&gt;am&lt;/code&gt; as &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our new model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; remain the same, but now&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_2 =
  \begin{cases}
   1 &amp;amp; \text{manual transmission} \\
   0       &amp;amp; \text{automatic transmission}
  \end{cases}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, we call &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; a &lt;strong&gt;dummy variable&lt;/strong&gt;. A dummy variable (also called an “indicator” variable) is somewhat unfortunately named, as it is in no way “dumb”. In fact, it is actually somewhat clever. A dummy variable is a numerical variable that is used in a regression analysis to “code” for a binary categorical variable. Let’s see how this works.&lt;/p&gt;
&lt;p&gt;First, note that &lt;code&gt;am&lt;/code&gt; is already a dummy variable, since it uses the values &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; to represent automatic and manual transmissions. Often, a variable like &lt;code&gt;am&lt;/code&gt; would store the character values &lt;code&gt;auto&lt;/code&gt; and &lt;code&gt;man&lt;/code&gt; and we would either have to convert these to &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, or, as we will see later, &lt;code&gt;R&lt;/code&gt; will take care of creating dummy variables for us.&lt;/p&gt;
&lt;p&gt;So, to fit the above model, we do so like any other multiple regression model we have seen before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp_add = lm(mpg ~ hp + am, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Briefly checking the output, we see that &lt;code&gt;R&lt;/code&gt; has estimated the three &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp_add&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ hp + am, data = mtcars)
## 
## Coefficients:
## (Intercept)           hp           am  
##    26.58491     -0.05889      5.27709&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; can only take values &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, we can effectively write two different models, one for manual and one for automatic transmissions.&lt;/p&gt;
&lt;p&gt;For automatic transmissions, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 0\)&lt;/span&gt;, we have,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then for manual transmissions, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 1\)&lt;/span&gt;, we have,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = (\beta_0 + \beta_2) + \beta_1 x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice that these models share the same slope, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, but have different intercepts, differing by &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt;. So the change in &lt;code&gt;mpg&lt;/code&gt; is the same for both models, but on average &lt;code&gt;mpg&lt;/code&gt; differs by &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; between the two transmission types.&lt;/p&gt;
&lt;p&gt;We’ll now calculate the estimated slope and intercept of these two models so that we can add them to a plot. Note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0\)&lt;/span&gt; = &lt;code&gt;coef(mpg_hp_add)[1]&lt;/code&gt; = 26.5849137&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt; = &lt;code&gt;coef(mpg_hp_add)[2]&lt;/code&gt; = -0.0588878&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_2\)&lt;/span&gt; = &lt;code&gt;coef(mpg_hp_add)[3]&lt;/code&gt; = 5.2770853&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can then combine these to calculate the estimated slope and intercepts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_auto = coef(mpg_hp_add)[1]
int_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3]

slope_auto = coef(mpg_hp_add)[2]
slope_manu = coef(mpg_hp_add)[2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Re-plotting the data, we use these slopes and intercepts to add the “two” fitted models to the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
abline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # add line for auto
abline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # add line for manual
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Automatic&amp;quot;, &amp;quot;Manual&amp;quot;), col = c(1, 2), pch = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We notice right away that the points are no longer systematically incorrect. The red, manual observations vary about the red line in no particular pattern without underestimating the observations as before. The black, automatic points vary about the black line, also without an obvious pattern.&lt;/p&gt;
&lt;p&gt;They say a picture is worth a thousand words, but as a statistician, sometimes a picture is worth an entire analysis. The above picture makes it plainly obvious that &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; is significant, but let’s verify mathematically. Essentially we would like to test:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_2 = 0 \quad \text{vs} \quad H_1: \beta_2 \neq 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is nothing new. Again, the math is the same as the multiple regression analyses we have seen before. We could perform either a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test here. The only difference is a slight change in interpretation. We could think of this as testing a model with a single line (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) against a model that allows two lines (&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;To obtain the test statistic and p-value for the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test, we would use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mpg_hp_add)$coefficients[&amp;quot;am&amp;quot;,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate   Std. Error      t value     Pr(&amp;gt;|t|) 
## 5.277085e+00 1.079541e+00 4.888270e+00 3.460318e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To do the same for the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test, we would use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_hp_slr, mpg_hp_add)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ hp
## Model 2: mpg ~ hp + am
##   Res.Df    RSS Df Sum of Sq      F   Pr(&amp;gt;F)    
## 1     30 447.67                                 
## 2     29 245.44  1    202.24 23.895 3.46e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that these are indeed testing the same thing, as the p-values are exactly equal. (And the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test statistic is the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; test statistic squared.)&lt;/p&gt;
&lt;p&gt;Recapping some interpretations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 = 26.5849137\)&lt;/span&gt; is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a car with an automatic transmission and &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;hp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 + \hat{\beta}_2 = 31.8619991\)&lt;/span&gt; is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a car with a manual transmission and &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;hp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_2 = 5.2770853\)&lt;/span&gt; is the estimated &lt;strong&gt;difference&lt;/strong&gt; in average &lt;code&gt;mpg&lt;/code&gt; for cars with manual transmissions as compared to those with automatic transmission, for &lt;strong&gt;any&lt;/strong&gt; &lt;code&gt;hp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 = -0.0588878\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in one &lt;code&gt;hp&lt;/code&gt;, for &lt;strong&gt;either&lt;/strong&gt; transmission types.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We should take special notice of those last two. In the model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we see &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the average change in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for an increase in &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;, &lt;em&gt;no matter&lt;/em&gt; the value of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. Also, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; is always the difference in the average of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for &lt;em&gt;any&lt;/em&gt; value of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;. These are two restrictions we won’t always want, so we need a way to specify a more flexible model.&lt;/p&gt;
&lt;p&gt;Here we restricted ourselves to a single numerical predictor &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and one dummy variable &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. However, the concept of a dummy variable can be used with larger multiple regression models. We only use a single numerical predictor here for ease of visualization since we can think of the “two lines” interpretation. But in general, we can think of a dummy variable as creating “two models,” one for each category of a binary categorical variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interactions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactions&lt;/h2&gt;
&lt;p&gt;To remove the “same slope” restriction, we will now discuss &lt;strong&gt;interaction&lt;/strong&gt;. To illustrate this concept, we will return to the &lt;code&gt;autompg&lt;/code&gt; dataset we created in the last chapter, with a few more modifications.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read data frame from the web
autompg = read.table(
  &amp;quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&amp;quot;,
  quote = &amp;quot;\&amp;quot;&amp;quot;,
  comment.char = &amp;quot;&amp;quot;,
  stringsAsFactors = FALSE)
# give the dataframe headers
colnames(autompg) = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;acc&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;origin&amp;quot;, &amp;quot;name&amp;quot;)
# remove missing data, which is stored as &amp;quot;?&amp;quot;
autompg = subset(autompg, autompg$hp != &amp;quot;?&amp;quot;)
# remove the plymouth reliant, as it causes some issues
autompg = subset(autompg, autompg$name != &amp;quot;plymouth reliant&amp;quot;)
# give the dataset row names, based on the engine, year and name
rownames(autompg) = paste(autompg$cyl, &amp;quot;cylinder&amp;quot;, autompg$year, autompg$name)
# remove the variable for name
autompg = subset(autompg, select = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;acc&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;origin&amp;quot;))
# change horsepower from character to numeric
autompg$hp = as.numeric(autompg$hp)
# create a dummary variable for foreign vs domestic cars. domestic = 1.
autompg$domestic = as.numeric(autompg$origin == 1)
# remove 3 and 5 cylinder cars (which are very rare.)
autompg = autompg[autompg$cyl != 5,]
autompg = autompg[autompg$cyl != 3,]
# the following line would verify the remaining cylinder possibilities are 4, 6, 8
#unique(autompg$cyl)
# change cyl to a factor variable
autompg$cyl = as.factor(autompg$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    383 obs. of  9 variables:
##  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...
##  $ cyl     : Factor w/ 3 levels &amp;quot;4&amp;quot;,&amp;quot;6&amp;quot;,&amp;quot;8&amp;quot;: 3 3 3 3 3 3 3 3 3 3 ...
##  $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...
##  $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...
##  $ wt      : num  3504 3693 3436 3433 3449 ...
##  $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
##  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...
##  $ origin  : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve removed cars with &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; cylinders , as well as created a new variable &lt;code&gt;domestic&lt;/code&gt; which indicates whether or not a car was built in the United States. Removing the &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; cylinders is simply for ease of demonstration later in the chapter and would not be done in practice. The new variable &lt;code&gt;domestic&lt;/code&gt; takes the value &lt;code&gt;1&lt;/code&gt; if the car was built in the United States, and &lt;code&gt;0&lt;/code&gt; otherwise, which we will refer to as “foreign.” (We are arbitrarily using the United States as the reference point here.) We have also made &lt;code&gt;cyl&lt;/code&gt; and &lt;code&gt;origin&lt;/code&gt; into factor variables, which we will discuss later.&lt;/p&gt;
&lt;p&gt;We’ll now be concerned with three variables: &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;disp&lt;/code&gt;, and &lt;code&gt;domestic&lt;/code&gt;. We will use &lt;code&gt;mpg&lt;/code&gt; as the response. We can fit a model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;domestic&lt;/code&gt; as described above, which is a dummy variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_2 =
  \begin{cases}
   1 &amp;amp; \text{Domestic} \\
   0 &amp;amp; \text{Foreign}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will fit this model, extract the slope and intercept for the “two lines,” plot the data and add the lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_add = lm(mpg ~ disp + domestic, data = autompg)

int_for = coef(mpg_disp_add)[1]
int_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3]

slope_for = coef(mpg_disp_add)[2]
slope_dom = coef(mpg_disp_add)[2]

plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)
abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # add line for foreign cars
abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # add line for domestic cars
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Foreign&amp;quot;, &amp;quot;Domestic&amp;quot;), pch = c(1, 2), col = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a model that allows for two &lt;em&gt;parallel&lt;/em&gt; lines, meaning the &lt;code&gt;mpg&lt;/code&gt; can be different on average between foreign and domestic cars of the same engine displacement, but the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in displacement is the same for both. We can see this model isn’t doing very well here. The red line fits the red points fairly well, but the black line isn’t doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes.&lt;/p&gt;
&lt;p&gt;Consider the following model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are the same as before, but we have added a new &lt;strong&gt;interaction&lt;/strong&gt; term &lt;span class=&#34;math inline&#34;&gt;\(x_1 x_2\)&lt;/span&gt; which multiplies &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, so we also have an additional &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This model essentially creates two slopes and two intercepts, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; being the difference in intercepts and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; being the difference in slopes. To see this, we will break down the model into the two “sub-models” for foreign and domestic cars.&lt;/p&gt;
&lt;p&gt;For foreign cars, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 0\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For domestic cars, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 1\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These two models have both different slopes and intercepts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a foreign car with &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt;, for &lt;strong&gt;foreign&lt;/strong&gt; cars.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_2\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a domestic car with &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1 + \beta_3\)&lt;/span&gt; is the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt;, for &lt;strong&gt;domestic&lt;/strong&gt; cars.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do we fit this model in &lt;code&gt;R&lt;/code&gt;? There are a number of ways.&lt;/p&gt;
&lt;p&gt;One method would be to simply create a new variable, then fit a model like any other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autompg$x3 = autompg$disp * autompg$domestic # THIS CODE NOT RUN!
do_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # THIS CODE NOT RUN!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should only do this as a last resort. We greatly prefer not to have to modify our data simply to fit a model. Instead, we can tell &lt;code&gt;R&lt;/code&gt; we would like to use the existing data with an interaction term, which it will create automatically when we use the &lt;code&gt;:&lt;/code&gt; operator.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative method, which will fit the exact same model as above would be to use the &lt;code&gt;*&lt;/code&gt; operator. This method automatically creates the interaction term, as well as any “lower order terms,” which in this case are the first order terms for &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;domestic&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can quickly verify that these are doing the same thing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(mpg_disp_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)          disp      domestic disp:domestic 
##    46.0548423    -0.1569239   -12.5754714     0.1025184&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(mpg_disp_int2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)          disp      domestic disp:domestic 
##    46.0548423    -0.1569239   -12.5754714     0.1025184&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that both the variables, and their coefficient estimates are indeed the same for both models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mpg_disp_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp + domestic + disp:domestic, data = autompg)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.8332  -2.8956  -0.8332   2.2828  18.7749 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)    46.05484    1.80582  25.504  &amp;lt; 2e-16 ***
## disp           -0.15692    0.01668  -9.407  &amp;lt; 2e-16 ***
## domestic      -12.57547    1.95644  -6.428 3.90e-10 ***
## disp:domestic   0.10252    0.01692   6.060 3.29e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 4.308 on 379 degrees of freedom
## Multiple R-squared:  0.7011, Adjusted R-squared:  0.6987 
## F-statistic: 296.3 on 3 and 379 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that using &lt;code&gt;summary()&lt;/code&gt; gives the usual output for a multiple regression model. We pay close attention to the row for &lt;code&gt;disp:domestic&lt;/code&gt; which tests,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_3 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, testing for &lt;span class=&#34;math inline&#34;&gt;\(\beta_3 = 0\)&lt;/span&gt; is testing for two lines with parallel slopes versus two lines with possibly different slopes. The &lt;code&gt;disp:domestic&lt;/code&gt; line in the &lt;code&gt;summary()&lt;/code&gt; output uses a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test to perform the test.&lt;/p&gt;
&lt;p&gt;We could also use an ANOVA &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test. The additive model, without interaction is our null model, and the interaction model is the alternative.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_disp_add, mpg_disp_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + domestic
## Model 2: mpg ~ disp + domestic + disp:domestic
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    380 7714.0                                  
## 2    379 7032.6  1    681.36 36.719 3.294e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again we see this test has the same p-value as the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test. Also the p-value is extremely low, so between the two, we choose the interaction model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_for = coef(mpg_disp_int)[1]
int_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3]

slope_for = coef(mpg_disp_int)[2]
slope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we again calculate the slope and intercepts for the two lines for use in plotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)
abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # line for foreign cars
abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # line for domestic cars
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Foreign&amp;quot;, &amp;quot;Domestic&amp;quot;), pch = c(1, 2), col = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that these lines fit the data much better, which matches the result of our tests.&lt;/p&gt;
&lt;p&gt;So far we have only seen interaction between a categorical variable (&lt;code&gt;domestic&lt;/code&gt;) and a numerical variable (&lt;code&gt;disp&lt;/code&gt;). While this is easy to visualize, since it allows for different slopes for two lines, it is not the only type of interaction we can use in a model. We can also consider interactions between two numerical variables.&lt;/p&gt;
&lt;p&gt;Consider the model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;hp&lt;/code&gt;, the horsepower, in foot-pounds per second.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How does &lt;code&gt;mpg&lt;/code&gt; change based on &lt;code&gt;disp&lt;/code&gt; in this model? We can rearrange some terms to see how.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + (\beta_1 + \beta_3 x_2) x_1 + \beta_2 x_2 + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, for a one unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;), the mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;) increases &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 + \beta_3 x_2\)&lt;/span&gt;, which is a different value depending on the value of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; (&lt;code&gt;hp&lt;/code&gt;)!&lt;/p&gt;
&lt;p&gt;Since we’re now working in three dimensions, this model can’t be easily justified via visualizations like the previous example. Instead, we will have to rely on a test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg)
mpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg)
summary(mpg_disp_int_hp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * hp, data = autompg)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.7849  -2.3104  -0.5699   2.1453  17.9211 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  5.241e+01  1.523e+00   34.42   &amp;lt;2e-16 ***
## disp        -1.002e-01  6.638e-03  -15.09   &amp;lt;2e-16 ***
## hp          -2.198e-01  1.987e-02  -11.06   &amp;lt;2e-16 ***
## disp:hp      5.658e-04  5.165e-05   10.96   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.896 on 379 degrees of freedom
## Multiple R-squared:  0.7554, Adjusted R-squared:  0.7535 
## F-statistic: 390.2 on 3 and 379 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;summary()&lt;/code&gt; we focus on the row for &lt;code&gt;disp:hp&lt;/code&gt; which tests,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_3 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, we see a very low p-value so we reject the null (additive model) in favor of the interaction model. Again, there is an equivalent &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_disp_add_hp, mpg_disp_int_hp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + hp
## Model 2: mpg ~ disp * hp
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    380 7576.6                                  
## 2    379 5754.2  1    1822.3 120.03 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a closer look at the coefficients of our fitted interaction model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(mpg_disp_int_hp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)          disp            hp       disp:hp 
## 52.4081997848 -0.1001737655 -0.2198199720  0.0005658269&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 = 52.4081998\)&lt;/span&gt; is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a car with 0 &lt;code&gt;disp&lt;/code&gt; and 0 &lt;code&gt;hp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 = -0.1001738\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in 1 &lt;code&gt;disp&lt;/code&gt;, &lt;strong&gt;for a car with 0 &lt;code&gt;hp&lt;/code&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_2 = -0.21982\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in 1 &lt;code&gt;hp&lt;/code&gt;, &lt;strong&gt;for a car with 0 &lt;code&gt;disp&lt;/code&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_3 = 5.658269\times 10^{-4}\)&lt;/span&gt; is an estimate of the modification to the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in &lt;code&gt;disp&lt;/code&gt;, for a car of a certain &lt;code&gt;hp&lt;/code&gt; (or vice versa).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That last coefficient needs further explanation. Recall the rearrangement we made earlier&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + (\beta_1 + \beta_3 x_2) x_1 + \beta_2 x_2 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, our estimate for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 + \beta_3 x_2\)&lt;/span&gt;, is &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 + \hat{\beta}_3 x_2\)&lt;/span&gt;, which in this case is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-0.1001738 + 5.658269\times 10^{-4} x_2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This says that, for an increase of one &lt;code&gt;disp&lt;/code&gt; we see an estimated change in average &lt;code&gt;mpg&lt;/code&gt; of &lt;span class=&#34;math inline&#34;&gt;\(-0.1001738 + 5.658269\times 10^{-4} x_2\)&lt;/span&gt;. So how &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;mpg&lt;/code&gt; are related, depends on the &lt;code&gt;hp&lt;/code&gt; of the car.&lt;/p&gt;
&lt;p&gt;So for a car with 50 &lt;code&gt;hp&lt;/code&gt;, the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-0.1001738 + 5.658269\times 10^{-4} \cdot 50 = -0.0718824
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And for a car with 350 &lt;code&gt;hp&lt;/code&gt;, the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-0.1001738 + 5.658269\times 10^{-4} \cdot 350 = 0.0978657
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice the sign changed!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factor-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Factor Variables&lt;/h2&gt;
&lt;p&gt;So far in this chapter, we have limited our use of categorical variables to binary categorical variables. Specifically, we have limited ourselves to dummy variables which take a value of &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt; and represent a categorical variable numerically.&lt;/p&gt;
&lt;p&gt;We will now discuss &lt;strong&gt;factor&lt;/strong&gt; variables, which is a special way that &lt;code&gt;R&lt;/code&gt; deals with categorical variables. With factor variables, a human user can simply think about the categories of a variable, and &lt;code&gt;R&lt;/code&gt; will take care of the necessary dummy variables without any 0/1 assignment being done by the user.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(autompg$domestic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Earlier when we used the &lt;code&gt;domestic&lt;/code&gt; variable, it was &lt;strong&gt;not&lt;/strong&gt; a factor variable. It was simply a numerical variable that only took two possible values, &lt;code&gt;1&lt;/code&gt; for domestic, and &lt;code&gt;0&lt;/code&gt; for foreign. Let’s create a new variable &lt;code&gt;origin&lt;/code&gt; that stores the same information, but in a different way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autompg$origin[autompg$domestic == 1] = &amp;quot;domestic&amp;quot;
autompg$origin[autompg$domestic == 0] = &amp;quot;foreign&amp;quot;
head(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the &lt;code&gt;origin&lt;/code&gt; variable stores &lt;code&gt;&#34;domestic&#34;&lt;/code&gt; for domestic cars and &lt;code&gt;&#34;foreign&#34;&lt;/code&gt; for foreign cars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this is simply a vector of character values. A vector of car models is a character variable in &lt;code&gt;R&lt;/code&gt;. A vector of Vehicle Identification Numbers (VINs) is a character variable as well. But those don’t represent a short list of levels that might influence a response variable. We will want to &lt;strong&gt;coerce&lt;/strong&gt; this origin variable to be something more: a factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autompg$origin = as.factor(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now when we check the structure of the &lt;code&gt;autompg&lt;/code&gt; dataset, we see that &lt;code&gt;origin&lt;/code&gt; is a factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    383 obs. of  9 variables:
##  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...
##  $ cyl     : Factor w/ 3 levels &amp;quot;4&amp;quot;,&amp;quot;6&amp;quot;,&amp;quot;8&amp;quot;: 3 3 3 3 3 3 3 3 3 3 ...
##  $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...
##  $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...
##  $ wt      : num  3504 3693 3436 3433 3449 ...
##  $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
##  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...
##  $ origin  : Factor w/ 2 levels &amp;quot;domestic&amp;quot;,&amp;quot;foreign&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Factor variables have &lt;strong&gt;levels&lt;/strong&gt; which are the possible values (categories) that the variable may take, in this case foreign or domestic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;domestic&amp;quot; &amp;quot;foreign&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recall that previously we have fit the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;domestic&lt;/code&gt; a dummy variable where &lt;code&gt;1&lt;/code&gt; indicates a domestic car.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mod_dummy = lm(mpg ~ disp * domestic, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * domestic, data = autompg)
## 
## Coefficients:
##   (Intercept)           disp       domestic  disp:domestic  
##       46.0548        -0.1569       -12.5755         0.1025&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here we see that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 + \hat{\beta}_2 = 46.0548423 + -12.5754714 = 33.4793709
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a &lt;strong&gt;domestic&lt;/strong&gt; car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now let’s try to do the same, but using our new factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mod_factor = lm(mpg ~ disp * origin, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * origin, data = autompg)
## 
## Coefficients:
##        (Intercept)                disp       originforeign  disp:originforeign  
##           33.47937            -0.05441            12.57547            -0.10252&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that it doesn’t produce the same results. Right away we notice that the intercept is different, as is the the coefficient in front of &lt;code&gt;disp&lt;/code&gt;. We also notice that the remaining two coefficients are of the same magnitude as their respective counterparts using the domestic variable, but with a different sign. Why is this happening?&lt;/p&gt;
&lt;p&gt;It turns out, that by using a factor variable, &lt;code&gt;R&lt;/code&gt; is automatically creating a dummy variable for us. However, it is not the dummy variable that we had originally used ourselves.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is fitting the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; &lt;strong&gt;is a dummy variable created by &lt;code&gt;R&lt;/code&gt;.&lt;/strong&gt; It uses &lt;code&gt;1&lt;/code&gt; to represent a &lt;strong&gt;foreign car&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So now,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 = 33.4793709
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a &lt;strong&gt;domestic&lt;/strong&gt; car with 0 &lt;code&gt;disp&lt;/code&gt;, which is indeed the same as before.&lt;/p&gt;
&lt;p&gt;When &lt;code&gt;R&lt;/code&gt; created &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, the dummy variable, it used domestic cars as the &lt;strong&gt;reference&lt;/strong&gt; level, that is the default value of the factor variable. So when the dummy variable is &lt;code&gt;0&lt;/code&gt;, the model represents this reference level, which is domestic. (&lt;code&gt;R&lt;/code&gt; makes this choice because domestic comes before foreign alphabetically.)&lt;/p&gt;
&lt;p&gt;So the two models have different estimated coefficients, but due to the different model representations, they are actually the same model.&lt;/p&gt;
&lt;div id=&#34;factors-with-more-than-two-levels&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factors with More Than Two Levels&lt;/h3&gt;
&lt;p&gt;Let’s now consider a factor variable with more than two levels. In this dataset, &lt;code&gt;cyl&lt;/code&gt; is an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(autompg$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(autompg$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;4&amp;quot; &amp;quot;6&amp;quot; &amp;quot;8&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the &lt;code&gt;cyl&lt;/code&gt; variable has three possible levels: &lt;code&gt;4&lt;/code&gt;, &lt;code&gt;6&lt;/code&gt;, and &lt;code&gt;8&lt;/code&gt;. You may wonder, why not simply use &lt;code&gt;cyl&lt;/code&gt; as a numerical variable? You certainly could.&lt;/p&gt;
&lt;p&gt;However, that would force the difference in average &lt;code&gt;mpg&lt;/code&gt; between &lt;code&gt;4&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; cylinders to be the same as the difference in average mpg between &lt;code&gt;6&lt;/code&gt; and &lt;code&gt;8&lt;/code&gt; cylinders. That usually make senses for a continuous variable, but not for a discrete variable with so few possible values. In the case of this variable, there is no such thing as a 7-cylinder engine or a 6.23-cylinder engine in personal vehicles. For these reasons, we will simply consider &lt;code&gt;cyl&lt;/code&gt; to be categorical. This is a decision that will commonly need to be made with ordinal variables. Often, with a large number of categories, the decision to treat them as numerical variables is appropriate because, otherwise, a large number of dummy variables are then needed to represent these variables.&lt;/p&gt;
&lt;p&gt;Let’s define three dummy variables related to the &lt;code&gt;cyl&lt;/code&gt; factor variable.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v_1 =
  \begin{cases}
   1 &amp;amp; \text{4 cylinder} \\
   0       &amp;amp; \text{not 4 cylinder}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v_2 =
  \begin{cases}
   1 &amp;amp; \text{6 cylinder} \\
   0       &amp;amp; \text{not 6 cylinder}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v_3 =
  \begin{cases}
   1 &amp;amp; \text{8 cylinder} \\
   0       &amp;amp; \text{not 8 cylinder}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s fit an additive model in &lt;code&gt;R&lt;/code&gt;, using &lt;code&gt;mpg&lt;/code&gt; as the response, and &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;cyl&lt;/code&gt; as predictors. This should be a model that uses “three regression lines” to model &lt;code&gt;mpg&lt;/code&gt;, one for each of the possible &lt;code&gt;cyl&lt;/code&gt; levels. They will all have the same slope (since it is an additive model), but each will have its own intercept.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp + cyl, data = autompg)
## 
## Coefficients:
## (Intercept)         disp         cyl6         cyl8  
##    34.99929     -0.05217     -3.63325     -2.03603&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The question is, what is the model that &lt;code&gt;R&lt;/code&gt; has fit here? It has chosen to use the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_3\)&lt;/span&gt; are the dummy variables define above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Why doesn’t &lt;code&gt;R&lt;/code&gt; use &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;? Essentially because it doesn’t need to. To create three lines, it only needs two dummy variables since it is using a reference level, which in this case is a 4 cylinder car. The three “sub models” are then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_2) + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_3) + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that they all have the same slope. However, using the two dummy variables, we achieve the three intercepts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a 4 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_2\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a 6 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_3\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a 8 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So because 4 cylinder is the reference level, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is specific to 4 cylinders, but &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; are used to represent quantities relative to 4 cylinders.&lt;/p&gt;
&lt;p&gt;As we have done before, we can extract these intercepts and slopes for the three lines, and plot them accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_4cyl = coef(mpg_disp_add_cyl)[1]
int_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3]
int_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4]

slope_all_cyl = coef(mpg_disp_add_cyl)[2]

plot_colors = c(&amp;quot;Darkorange&amp;quot;, &amp;quot;Darkgrey&amp;quot;, &amp;quot;Dodgerblue&amp;quot;)
plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))
abline(int_4cyl, slope_all_cyl, col = plot_colors[1], lty = 1, lwd = 2)
abline(int_6cyl, slope_all_cyl, col = plot_colors[2], lty = 2, lwd = 2)
abline(int_8cyl, slope_all_cyl, col = plot_colors[3], lty = 3, lwd = 2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;4 Cylinder&amp;quot;, &amp;quot;6 Cylinder&amp;quot;, &amp;quot;8 Cylinder&amp;quot;),
       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On this plot, we have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: orange dots, solid orange line.&lt;/li&gt;
&lt;li&gt;6 Cylinder: grey dots, dashed grey line.&lt;/li&gt;
&lt;li&gt;8 Cylinder: blue dots, dotted blue line.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The odd result here is that we’re estimating that 8 cylinder cars have better fuel efficiency than 6 cylinder cars at &lt;strong&gt;any&lt;/strong&gt; displacement! The dotted blue line is always above the dashed grey line. That doesn’t seem right. Maybe for very large displacement engines that could be true, but that seems wrong for medium to low displacement.&lt;/p&gt;
&lt;p&gt;To attempt to fix this, we will try using an interaction model, that is, instead of simply three intercepts and one slope, we will allow for three slopes. Again, we’ll let &lt;code&gt;R&lt;/code&gt; take the wheel, (no pun intended) then figure out what model it has applied.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * cyl, data = autompg)
## 
## Coefficients:
## (Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  
##    43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# could also use mpg ~ disp + cyl + disp:cyl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; has again chosen to use 4 cylinder cars as the reference level, but this also now has an effect on the interaction terms. &lt;code&gt;R&lt;/code&gt; has fit the model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \gamma_2 x v_2 + \gamma_3 x v_3 + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’re using &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; like a &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter for simplicity, so that, for example &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_2\)&lt;/span&gt; are both associated with &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, the three “sub models” are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \epsilon\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_2) + (\beta_1 + \gamma_2) x + \epsilon\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_3) + (\beta_1 + \gamma_3) x + \epsilon\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interpreting some parameters and coefficients then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\beta_0 + \beta_2)\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; of a 6 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\hat{\beta}_1 + \hat{\gamma}_3) = -0.1306935 + 0.1081714 = -0.0225221\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt;, for an 8 cylinder car.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, as we have seen before &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; change the intercepts for 6 and 8 cylinder cars relative to the reference level of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; for 4 cylinder cars.&lt;/p&gt;
&lt;p&gt;Now, similarly &lt;span class=&#34;math inline&#34;&gt;\(\gamma_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_3\)&lt;/span&gt; change the slopes for 6 and 8 cylinder cars relative to the reference level of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; for 4 cylinder cars.&lt;/p&gt;
&lt;p&gt;Once again, we extract the coefficients and plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_4cyl = coef(mpg_disp_int_cyl)[1]
int_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3]
int_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4]

slope_4cyl = coef(mpg_disp_int_cyl)[2]
slope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5]
slope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6]

plot_colors = c(&amp;quot;Darkorange&amp;quot;, &amp;quot;Darkgrey&amp;quot;, &amp;quot;Dodgerblue&amp;quot;)
plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))
abline(int_4cyl, slope_4cyl, col = plot_colors[1], lty = 1, lwd = 2)
abline(int_6cyl, slope_6cyl, col = plot_colors[2], lty = 2, lwd = 2)
abline(int_8cyl, slope_8cyl, col = plot_colors[3], lty = 3, lwd = 2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;4 Cylinder&amp;quot;, &amp;quot;6 Cylinder&amp;quot;, &amp;quot;8 Cylinder&amp;quot;),
       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks much better! We can see that for medium displacement cars, 6 cylinder cars now perform better than 8 cylinder cars, which seems much more reasonable than before.&lt;/p&gt;
&lt;p&gt;To completely justify the interaction model (i.e., a unique slope for each &lt;code&gt;cyl&lt;/code&gt; level) compared to the additive model (single slope), we can perform an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test. Notice first, that there is no &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test that will be able to do this since the difference between the two models is not a single parameter.&lt;/p&gt;
&lt;p&gt;We will test,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \gamma_2 = \gamma_3 = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which represents the parallel regression lines we saw before,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, this is a difference of two parameters, thus no &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test will be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_disp_add_cyl, mpg_disp_int_cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + cyl
## Model 2: mpg ~ disp * cyl
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    379 7299.5                                  
## 2    377 6551.7  2    747.79 21.515 1.419e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, we see a very low p-value, and thus reject the null. We prefer the interaction model over the additive model.&lt;/p&gt;
&lt;p&gt;Recapping a bit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Null Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Number of parameters: &lt;span class=&#34;math inline&#34;&gt;\(q = 4\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Full Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \gamma_2 x v_2 + \gamma_3 x v_3 + \epsilon\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Number of parameters: &lt;span class=&#34;math inline&#34;&gt;\(p = 6\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(coef(mpg_disp_int_cyl)) - length(coef(mpg_disp_add_cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see there is a difference of two parameters, which is also displayed in the resulting ANOVA table from &lt;code&gt;R&lt;/code&gt;. Notice that the following two values also appear on the ANOVA table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(autompg) - length(coef(mpg_disp_int_cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 377&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(autompg) - length(coef(mpg_disp_add_cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 379&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;parameterization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameterization&lt;/h2&gt;
&lt;p&gt;So far we have been simply letting &lt;code&gt;R&lt;/code&gt; decide how to create the dummy variables, and thus &lt;code&gt;R&lt;/code&gt; has been deciding the parameterization of the models. To illustrate the ability to use alternative parameterizations, we will recreate the data, but directly creating the dummy variables ourselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_param_data = data.frame(
  y = autompg$mpg,
  x = autompg$disp,
  v1 = 1 * as.numeric(autompg$cyl == 4),
  v2 = 1 * as.numeric(autompg$cyl == 6),
  v3 = 1 * as.numeric(autompg$cyl == 8))

head(new_param_data, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     y   x v1 v2 v3
## 1  18 307  0  0  1
## 2  15 350  0  0  1
## 3  18 318  0  0  1
## 4  16 304  0  0  1
## 5  17 302  0  0  1
## 6  15 429  0  0  1
## 7  14 454  0  0  1
## 8  14 440  0  0  1
## 9  14 455  0  0  1
## 10 15 390  0  0  1
## 11 15 383  0  0  1
## 12 14 340  0  0  1
## 13 15 400  0  0  1
## 14 14 455  0  0  1
## 15 24 113  1  0  0
## 16 22 198  0  1  0
## 17 18 199  0  1  0
## 18 21 200  0  1  0
## 19 27  97  1  0  0
## 20 26  97  1  0  0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; is &lt;code&gt;mpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;v1&lt;/code&gt;, &lt;code&gt;v2&lt;/code&gt;, and &lt;code&gt;v3&lt;/code&gt; are dummy variables as defined above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First let’s try to fit an additive model using &lt;code&gt;x&lt;/code&gt; as well as the three dummy variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ x + v1 + v2 + v3, data = new_param_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + v1 + v2 + v3, data = new_param_data)
## 
## Coefficients:
## (Intercept)            x           v1           v2           v3  
##    32.96326     -0.05217      2.03603     -1.59722           NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is happening here? Notice that &lt;code&gt;R&lt;/code&gt; is essentially ignoring &lt;code&gt;v3&lt;/code&gt;, but why? Well, because &lt;code&gt;R&lt;/code&gt; uses an intercept, it cannot also use &lt;code&gt;v3&lt;/code&gt;. This is because&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{1} = v_1 + v_2 + v_3
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which means that &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{1}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(v_3\)&lt;/span&gt; are linearly dependent. This would make the &lt;span class=&#34;math inline&#34;&gt;\(X^\top X\)&lt;/span&gt; matrix singular, but we need to be able to invert it to solve the normal equations and obtain &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}.\)&lt;/span&gt; With the intercept, &lt;code&gt;v1&lt;/code&gt;, and &lt;code&gt;v2&lt;/code&gt;, &lt;code&gt;R&lt;/code&gt; can make the necessary “three intercepts”. So, in this case &lt;code&gt;v3&lt;/code&gt; is the reference level.&lt;/p&gt;
&lt;p&gt;If we remove the intercept, then we can directly obtain all “three intercepts” without a reference level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ 0 + x + v1 + v2 + v3, data = new_param_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ 0 + x + v1 + v2 + v3, data = new_param_data)
## 
## Coefficients:
##        x        v1        v2        v3  
## -0.05217  34.99929  31.36604  32.96326&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we are fitting the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \mu_1 v_1 + \mu_2 v_2 + \mu_3 v_3 + \beta x +\epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_1 + \beta x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_2 + \beta x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_3 + \beta x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We could also do something similar with the interaction model, and give each line an intercept and slope, without the need for a reference level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)
## 
## Coefficients:
##       v1        v2        v3      v1:x      v2:x      v3:x  
## 43.59052  30.39026  22.73346  -0.13069  -0.04770  -0.02252&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \mu_1 v_1 + \mu_2 v_2 + \mu_3 v_3 + \beta_1 x v_1 + \beta_2 x v_2 + \beta_3 x v_3 +\epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_1 + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_2 + \beta_2 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_3 + \beta_3 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the original data, we have (at least) three equivalent ways to specify the interaction model with &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(mpg ~ disp * cyl, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * cyl, data = autompg)
## 
## Coefficients:
## (Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  
##    43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ 0 + cyl + disp:cyl, data = autompg)
## 
## Coefficients:
##      cyl4       cyl6       cyl8  cyl4:disp  cyl6:disp  cyl8:disp  
##  43.59052   30.39026   22.73346   -0.13069   -0.04770   -0.02252&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ 0 + disp + cyl + disp:cyl, data = autompg)
## 
## Coefficients:
##      disp       cyl4       cyl6       cyl8  disp:cyl6  disp:cyl8  
##  -0.13069   43.59052   30.39026   22.73346    0.08299    0.10817&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They all fit the same model, importantly each using six parameters, but the coefficients mean slightly different things in each. However, once they are interpreted as slopes and intercepts for the “three lines” they will have the same result.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;?all.equal&lt;/code&gt; to learn about the &lt;code&gt;all.equal()&lt;/code&gt; function, and think about how the following code verifies that the residuals of the two models are the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)),
          fitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-larger-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building Larger Models&lt;/h2&gt;
&lt;p&gt;Now that we have seen how to incorporate categorical predictors as well as interaction terms, we can start to build much larger, much more flexible models which can potentially fit data better.&lt;/p&gt;
&lt;p&gt;Let’s define a “big” model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;hp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt; is &lt;code&gt;domestic&lt;/code&gt;, which is a dummy variable we defined, where &lt;code&gt;1&lt;/code&gt; is a domestic vehicle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First thing to note here, we have included a new term &lt;span class=&#34;math inline&#34;&gt;\(x_1 x_2 x_3\)&lt;/span&gt; which is a three-way interaction. Interaction terms can be larger and larger, up to the number of predictors in the model.&lt;/p&gt;
&lt;p&gt;Since we are using the three-way interaction term, we also use all possible two-way interactions, as well as each of the first order (&lt;strong&gt;main effect&lt;/strong&gt;) terms. This is the concept of a &lt;strong&gt;hierarchy&lt;/strong&gt;. Any time a “higher-order” term is in a model, the related “lower-order” terms should also be included. Mathematically their inclusion or exclusion is sometimes irrelevant, but from an interpretation standpoint, it is best to follow the hierarchy rules.&lt;/p&gt;
&lt;p&gt;Let’s do some rearrangement to obtain a “coefficient” in front of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_2 x_2 + \beta_3 x_3 + \beta_6 x_2 x_3 + (\beta_1 + \beta_4 x_2 + \beta_5 x_3 + \beta_7 x_2 x_3)x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Specifically, the “coefficient” in front of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + \beta_5 x_3 + \beta_7 x_2 x_3).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s discuss this “coefficient” to help us understand the idea of the &lt;em&gt;flexibility&lt;/em&gt; of a model. Recall that,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the coefficient for a first order term,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_4\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_5\)&lt;/span&gt; are coefficients for two-way interactions,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_7\)&lt;/span&gt; is the coefficient for the three-way interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the two and three way interactions were not in the model, the whole “coefficient” would simply be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\beta_1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, no matter the values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; would determine the relationship between &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;With the addition of the two-way interactions, now the “coefficient” would be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + \beta_5 x_3).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, changing &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;) has a different effect on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;), depending on the values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Lastly, adding the three-way interaction gives the whole “coefficient”&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + \beta_5 x_3 + \beta_7 x_2 x_3)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is even more flexible. Now changing &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;) has a different effect on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;), depending on the values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;, but in a more flexible way which we can see with some more rearrangement. Now the “coefficient” in front of &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt; in this “coefficient” is dependent on &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + (\beta_5 + \beta_7 x_2) x_3)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is so flexible, it is becoming hard to interpret!&lt;/p&gt;
&lt;p&gt;Let’s fit this three-way interaction model in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;big_model = lm(mpg ~ disp * hp * domestic, data = autompg)
summary(big_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * hp * domestic, data = autompg)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.9410  -2.2147  -0.4008   1.9430  18.4094 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)       6.065e+01  6.600e+00   9.189  &amp;lt; 2e-16 ***
## disp             -1.416e-01  6.344e-02  -2.232   0.0262 *  
## hp               -3.545e-01  8.123e-02  -4.364 1.65e-05 ***
## domestic         -1.257e+01  7.064e+00  -1.780   0.0759 .  
## disp:hp           1.369e-03  6.727e-04   2.035   0.0426 *  
## disp:domestic     4.933e-02  6.400e-02   0.771   0.4414    
## hp:domestic       1.852e-01  8.709e-02   2.126   0.0342 *  
## disp:hp:domestic -9.163e-04  6.768e-04  -1.354   0.1766    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.88 on 375 degrees of freedom
## Multiple R-squared:   0.76,  Adjusted R-squared:  0.7556 
## F-statistic: 169.7 on 7 and 375 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do we actually need this large of a model? Let’s first test for the necessity of the three-way interaction term. That is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_7 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Null Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We fit the null model in &lt;code&gt;R&lt;/code&gt; as &lt;code&gt;two_way_int_mod&lt;/code&gt;, then use &lt;code&gt;anova()&lt;/code&gt; to perform an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test as usual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;two_way_int_mod = lm(mpg ~ disp * hp + disp * domestic + hp * domestic, data = autompg)
#two_way_int_mod = lm(mpg ~ (disp + hp + domestic) ^ 2, data = autompg)
anova(two_way_int_mod, big_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp * hp + disp * domestic + hp * domestic
## Model 2: mpg ~ disp * hp * domestic
##   Res.Df    RSS Df Sum of Sq      F Pr(&amp;gt;F)
## 1    376 5673.2                           
## 2    375 5645.6  1    27.599 1.8332 0.1766&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see the p-value is somewhat large, so we would fail to reject. We prefer the smaller, less flexible, null model, without the three-way interaction.&lt;/p&gt;
&lt;p&gt;A quick note here: the full model does still “fit better.” Notice that it has a smaller RMSE than the null model, which means the full model makes smaller (squared) errors on average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(resid(big_model) ^ 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14.74053&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(resid(two_way_int_mod) ^ 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14.81259&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, it is not much smaller. We could even say that, the difference is insignificant. This is an idea we will return to later in greater detail.&lt;/p&gt;
&lt;p&gt;Now that we have chosen the model without the three-way interaction, can we go further? Do we need the two-way interactions? Let’s test&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_4 = \beta_5 = \beta_6 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Remember we already chose &lt;span class=&#34;math inline&#34;&gt;\(\beta_7 = 0\)&lt;/span&gt;, so,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Null Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We fit the null model in &lt;code&gt;R&lt;/code&gt; as &lt;code&gt;additive_mod&lt;/code&gt;, then use &lt;code&gt;anova()&lt;/code&gt; to perform an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test as usual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;additive_mod = lm(mpg ~ disp + hp + domestic, data = autompg)
anova(additive_mod, two_way_int_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + hp + domestic
## Model 2: mpg ~ disp * hp + disp * domestic + hp * domestic
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    379 7369.7                                  
## 2    376 5673.2  3    1696.5 37.478 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the p-value is small, so we reject the null, and we prefer the full (alternative) model. Of the models we have considered, our final preference is for&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression: Model Selection</title>
      <link>https://ssc442.netlify.app/example/08-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/example/08-example/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-selection&#34;&gt;Model Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assesing-model-accuracy&#34;&gt;Assesing Model Accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-complexity&#34;&gt;Model Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#test-train-split&#34;&gt;Test-Train Split&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-flexibility-to-linear-models&#34;&gt;Adding Flexibility to Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-a-model&#34;&gt;Choosing a Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will pivot between the content from this week and the example below.&lt;/p&gt;
&lt;p&gt;We will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Selection&lt;/h2&gt;
&lt;p&gt;Often when we are developing a linear regression model, part of our goal is to &lt;strong&gt;explain&lt;/strong&gt; a relationship. Now, we will ignore much of what we have learned and instead simply use regression as a tool to &lt;strong&gt;predict&lt;/strong&gt;. Instead of a model which explains relationships, we seek a model which minimizes errors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/07-example_files/regression.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First, note that a linear model is one of many methods used in regression.&lt;/p&gt;
&lt;p&gt;To discuss linear models in the context of prediction, we introduce the (very boring) &lt;code&gt;Advertising&lt;/code&gt; data that is discussed in the ISL text (see supplemental readings).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Advertising&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 200 × 4
##       TV Radio Newspaper Sales
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 230.   37.8      69.2  22.1
##  2  44.5  39.3      45.1  10.4
##  3  17.2  45.9      69.3   9.3
##  4 152.   41.3      58.5  18.5
##  5 181.   10.8      58.4  12.9
##  6   8.7  48.9      75     7.2
##  7  57.5  32.8      23.5  11.8
##  8 120.   19.6      11.6  13.2
##  9   8.6   2.1       1     4.8
## 10 200.    2.6      21.2  10.6
## # … with 190 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
featurePlot(x = Advertising[ , c(&amp;quot;TV&amp;quot;, &amp;quot;Radio&amp;quot;, &amp;quot;Newspaper&amp;quot;)], y = Advertising$Sales)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/08-example_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assesing-model-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assesing Model Accuracy&lt;/h2&gt;
&lt;p&gt;There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}(\hat{f}, \text{Data}) = \sqrt{\frac{1}{n}\displaystyle\sum_{i = 1}^{n}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For a linear model , the estimate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;, is given by the fitted regression line.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{y}({\bf{x}_i}) = \hat{f}({\bf{x}_i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can write an &lt;code&gt;R&lt;/code&gt; function that will be useful for performing this calculation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-complexity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Complexity&lt;/h2&gt;
&lt;p&gt;Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We write a simple &lt;code&gt;R&lt;/code&gt; function to extract this information from a model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_complexity = function(model) {
  length(coef(model)) - 1
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;test-train-split&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Test-Train Split&lt;/h2&gt;
&lt;p&gt;There is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts. It is essentially cheating! As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down, or in very specific cases, stay the same.&lt;/p&gt;
&lt;p&gt;This would suggest that to predict well, we should use the largest possible model! However, in reality we have hard fit to a specific dataset, but as soon as we see new data, a large model may in fact predict poorly. This is called &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Frequently we will take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the &lt;strong&gt;training&lt;/strong&gt; data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the &lt;strong&gt;test&lt;/strong&gt; data. Test data should &lt;em&gt;never&lt;/em&gt; be used to train a model.&lt;/p&gt;
&lt;p&gt;Note that sometimes the terms &lt;em&gt;evaluation set&lt;/em&gt; and &lt;em&gt;test set&lt;/em&gt; are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.&lt;/p&gt;
&lt;p&gt;Here we use the &lt;code&gt;sample()&lt;/code&gt; function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the &lt;code&gt;set.seed()&lt;/code&gt; function to allow use to reproduce the same random split each time we perform this analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(9)
num_obs = nrow(Advertising)

train_index = sample(num_obs, size = trunc(0.50 * num_obs))
train_data = Advertising[train_index, ]
test_data = Advertising[-train_index, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will look at two measures that assess how well a model is predicting, the &lt;strong&gt;train RMSE&lt;/strong&gt; and the &lt;strong&gt;test RMSE&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_{\text{Train}} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Tr}}}\displaystyle\sum_{i \in \text{Train}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Tr}\)&lt;/span&gt; is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_{\text{Test}} = \text{RMSE}(\hat{f}, \text{Test Data}) = \sqrt{\frac{1}{n_{\text{Te}}}\displaystyle\sum_{i \in \text{Test}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Te}\)&lt;/span&gt; is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict &lt;strong&gt;in general&lt;/strong&gt;, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.&lt;/p&gt;
&lt;p&gt;We will start with the simplest possible linear model, that is, a model with no predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(Sales ~ 1, data = train_data)
get_complexity(fit_0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
sqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.529258&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
sqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.914163&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
rmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.529258&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
rmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.914163&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.529258&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.914163&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-flexibility-to-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding Flexibility to Linear Models&lt;/h2&gt;
&lt;p&gt;Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_1 = lm(Sales ~ ., data = train_data)
get_complexity(fit_1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_1, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.888488&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_1, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.461661&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)
get_complexity(fit_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_2, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.016822&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_2, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9117228&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)
get_complexity(fit_3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_3, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6553091&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_3, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6633375&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_4 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)
get_complexity(fit_4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_4, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6421909&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_4, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7465957&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_5 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)
get_complexity(fit_5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_5, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6120887&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_5, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7864181&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing a Model&lt;/h2&gt;
&lt;p&gt;To better understand the relationship between train RMSE, test RMSE, and model complexity, we summarize our results, as the above is somewhat cluttered.&lt;/p&gt;
&lt;p&gt;First, we recap the models that we have fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_1 = lm(Sales ~ ., data = train_data)
fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)
fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)
fit_4 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)
fit_5 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we create a list of the models fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then obtain train RMSE, test RMSE, and model complexity for each.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_rmse = sapply(model_list, get_rmse, data = train_data, response = &amp;quot;Sales&amp;quot;)
test_rmse = sapply(model_list, get_rmse, data = test_data, response = &amp;quot;Sales&amp;quot;)
model_complexity = sapply(model_list, get_complexity)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model_complexity, train_rmse, type = &amp;quot;b&amp;quot;,
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02,
              max(c(train_rmse, test_rmse)) + 0.02),
     col = &amp;quot;dodgerblue&amp;quot;,
     xlab = &amp;quot;Model Size&amp;quot;,
     ylab = &amp;quot;RMSE&amp;quot;)
lines(model_complexity, test_rmse, type = &amp;quot;b&amp;quot;, col = &amp;quot;darkorange&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/08-example_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We also summarize the results as a table. &lt;code&gt;fit_1&lt;/code&gt; is the least flexible, and &lt;code&gt;fit_5&lt;/code&gt; is the most flexible. We see the Train RMSE decrease as flexibility increases. We see that the Test RMSE is smallest for &lt;code&gt;fit_3&lt;/code&gt;, thus is the model we believe will perform the best on future data not used to train the model. Note this may not be the best model, but it is the best model of the models we have seen in this example.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Train RMSE&lt;/th&gt;
&lt;th&gt;Test RMSE&lt;/th&gt;
&lt;th&gt;Predictors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;1.8884884&lt;/td&gt;
&lt;td&gt;1.4616608&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;1.0168223&lt;/td&gt;
&lt;td&gt;0.9117228&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.6553091&lt;/td&gt;
&lt;td&gt;0.6633375&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.6421909&lt;/td&gt;
&lt;td&gt;0.7465957&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.6120887&lt;/td&gt;
&lt;td&gt;0.7864181&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To summarize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Underfitting models:&lt;/strong&gt; In general &lt;em&gt;High&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE. Seen in &lt;code&gt;fit_1&lt;/code&gt; and &lt;code&gt;fit_2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overfitting models:&lt;/strong&gt; In general &lt;em&gt;Low&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE. Seen in &lt;code&gt;fit_4&lt;/code&gt; and &lt;code&gt;fit_5&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE. Then a model is underfitting if there exists a more complex model with lower Test RMSE.&lt;/p&gt;
&lt;p&gt;A number of notes on these results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The labels of under and overfitting are &lt;em&gt;relative&lt;/em&gt; to the best model we see, &lt;code&gt;fit_3&lt;/code&gt;. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.&lt;/li&gt;
&lt;li&gt;The train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. Here we see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later.&lt;/li&gt;
&lt;li&gt;Often we expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get lucky and this will not be true.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that &lt;strong&gt;predicted&lt;/strong&gt; well, and paid no attention to a model for &lt;strong&gt;explaination&lt;/strong&gt;. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Design</title>
      <link>https://ssc442.netlify.app/resource/design/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/resource/design/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#accessibility&#34;&gt;Accessibility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#colors&#34;&gt;Colors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fonts&#34;&gt;Fonts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#graphic-assets&#34;&gt;Graphic assets&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#images&#34;&gt;Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors&#34;&gt;Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors-photos-videos-and-other-assets&#34;&gt;Vectors, photos, videos, and other assets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;accessibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Accessibility&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.vischeck.com/vischeck/vischeckImage.php&#34;&gt;&lt;strong&gt;Vischeck&lt;/strong&gt;&lt;/a&gt;: Simulate how your images look for people with different forms of colorblindness (web-based)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://colororacle.org/index.html&#34;&gt;&lt;strong&gt;Color Oracle&lt;/strong&gt;&lt;/a&gt;: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;colors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Colors&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://color.adobe.com&#34;&gt;&lt;strong&gt;Adobe Color&lt;/strong&gt;&lt;/a&gt;: Create, share, and explore rule-based and custom color palettes.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.colourlovers.com/&#34;&gt;&lt;strong&gt;ColourLovers&lt;/strong&gt;&lt;/a&gt;: Like Facebook for color palettes.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34;&gt;&lt;strong&gt;viridis&lt;/strong&gt;&lt;/a&gt;: Percetually uniform color scales.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.fabiocrameri.ch/colourmaps.php&#34;&gt;&lt;strong&gt;Scientific Colour-Maps&lt;/strong&gt;&lt;/a&gt;: Perceptually uniform color scales like viridis. Use them in R with &lt;a href=&#34;https://github.com/thomasp85/scico&#34;&gt;&lt;strong&gt;scico&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://colorbrewer2.org/&#34;&gt;&lt;strong&gt;ColorBrewer&lt;/strong&gt;&lt;/a&gt;: Sequential, diverging, and qualitative color palettes that take accessibility into account.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vrl.cs.brown.edu/color&#34;&gt;&lt;strong&gt;Colorgorical&lt;/strong&gt;&lt;/a&gt;: Create color palettes based on fancy mathematical rules for perceptual distance.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://tristen.ca/hcl-picker/&#34;&gt;&lt;strong&gt;Colorpicker for data&lt;/strong&gt;&lt;/a&gt;: More fancy mathematical rules for color palettes (&lt;a href=&#34;https://www.vis4.net/blog/posts/avoid-equidistant-hsv-colors/&#34;&gt;explanation&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://tools.medialab.sciences-po.fr/iwanthue/&#34;&gt;&lt;strong&gt;iWantHue&lt;/strong&gt;&lt;/a&gt;: Yet another perceptual distance-based color palette builder.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://photochrome.io/&#34;&gt;&lt;strong&gt;Photochrome&lt;/strong&gt;&lt;/a&gt;: Word-based color pallettes.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://policyviz.com/better-presentations/design-resources/design-color-tools/&#34;&gt;&lt;strong&gt;PolicyViz Design Color Tools&lt;/strong&gt;&lt;/a&gt;: Large collection of useful color resources&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;fonts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fonts&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://fonts.google.com/&#34;&gt;&lt;strong&gt;Google Fonts&lt;/strong&gt;&lt;/a&gt;: Huge collection of free, well-made fonts.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.reliablepsd.com/ultimate-google-font-pairings/&#34;&gt;&lt;strong&gt;The Ultimate Collection of Google Font Pairings&lt;/strong&gt;&lt;/a&gt;: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;graphic-assets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Graphic assets&lt;/h2&gt;
&lt;div id=&#34;images&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Images&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use the Creative Commons filters on Google Images or Flickr&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://unsplash.com/&#34;&gt;&lt;strong&gt;Unsplash&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pexels.com/&#34;&gt;&lt;strong&gt;Pexels&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pixabay.com/&#34;&gt;&lt;strong&gt;Pixabay&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stocksnap.io/&#34;&gt;&lt;strong&gt;StockSnap.io&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://burst.shopify.com/&#34;&gt;&lt;strong&gt;Burst&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://freephotos.cc/&#34;&gt;&lt;strong&gt;freephotos.cc&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vectors&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://thenounproject.com/&#34;&gt;&lt;strong&gt;Noun Project&lt;/strong&gt;&lt;/a&gt;: Thousands of free simple vector images&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://aiconica.net/&#34;&gt;&lt;strong&gt;aiconica&lt;/strong&gt;&lt;/a&gt;: 1,000+ vector icons&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.vecteezy.com/&#34;&gt;&lt;strong&gt;Vecteezy&lt;/strong&gt;&lt;/a&gt;: Thousands of free vector images&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors-photos-videos-and-other-assets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vectors, photos, videos, and other assets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stockio.com/&#34;&gt;&lt;strong&gt;Stockio&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using Markdown</title>
      <link>https://ssc442.netlify.app/resource/markdown/</link>
      <pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/resource/markdown/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#basic-markdown-formatting&#34;&gt;Basic Markdown formatting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#math&#34;&gt;Math&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tables&#34;&gt;Tables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#footnotes&#34;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#front-matter&#34;&gt;Front matter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#citations&#34;&gt;Citations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-references&#34;&gt;Other references&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://daringfireball.net/projects/markdown/&#34;&gt;Markdown&lt;/a&gt; is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like &lt;a href=&#34;https://pandoc.org/&#34;&gt;pandoc&lt;/a&gt; to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (&lt;a href=&#34;https://pandoc.org/MANUAL.html#option--to&#34;&gt;see the full list of output types here&lt;/a&gt;)&lt;/p&gt;
&lt;div id=&#34;basic-markdown-formatting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Basic Markdown formatting&lt;/h2&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;40%&#34; /&gt;
&lt;col width=&#34;21%&#34; /&gt;
&lt;col width=&#34;38%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Type…&lt;/th&gt;
&lt;th&gt;…or…&lt;/th&gt;
&lt;th&gt;…to get&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;pre&gt;Some text in a paragraph.

More text in the next paragraph. Always
use empty lines between paragraphs.&lt;/pre&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Some text in a paragraph.&lt;/p&gt;
&lt;p&gt;More text in the next paragraph. Always
use empty lines between paragraphs.&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;*Italic*&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;_Italic_&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Italic&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;**Bold**&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;__Bold__&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Bold&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;# Heading 1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;h1 class=&#34;smaller-h1&#34;&gt;
Heading 1
&lt;/h1&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;## Heading 2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;h2 class=&#34;smaller-h2&#34;&gt;
Heading 2
&lt;/h2&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;### Heading 3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;h3 class=&#34;smaller-h3&#34;&gt;
Heading 3
&lt;/h3&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;(Go up to heading level 6 with &lt;code&gt;######&lt;/code&gt;)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;[Link text](http://www.example.com)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://www.example.com&#34;&gt;Link text&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;![Image caption](/path/to/image.png)&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/favicon-32x32.png&#34; title=&#34;fig:&#34; alt=&#34;Class logo&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;`&lt;code&gt;Inline code` with backticks&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Inline code&lt;/code&gt; with backticks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;&amp;gt; Blockquote&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;blockquote&gt;
&lt;p&gt;Blockquote&lt;/p&gt;
&lt;/blockquote&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;pre&gt;- Things in
- an unordered
- list&lt;/pre&gt;&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;* Things in
* an unordered
* list&lt;/pre&gt;&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;
&lt;li&gt;Things in&lt;/li&gt;
&lt;li&gt;an unordered&lt;/li&gt;
&lt;li&gt;list&lt;/li&gt;
&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;pre&gt;1. Things in
2. an ordered
3. list&lt;/pre&gt;&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;1) Things in
2) an ordered
3) list&lt;/pre&gt;&lt;/td&gt;
&lt;td&gt;&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Things in&lt;/li&gt;
&lt;li&gt;an ordered&lt;/li&gt;
&lt;li&gt;list&lt;/li&gt;
&lt;/ol&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;pre&gt;Horizontal line

---&lt;/pre&gt;&lt;/td&gt;
&lt;td&gt;&lt;pre&gt;Horizontal line

***&lt;/pre&gt;&lt;/td&gt;
&lt;td&gt;&lt;p&gt;Horizontal line&lt;/p&gt;
&lt;hr /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;math&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Math&lt;/h2&gt;
&lt;p&gt;Markdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find &lt;a href=&#34;http://www.malinc.se/math/latex/basiccodeen.php&#34;&gt;helpful examples of the the most common basic commands here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like &lt;code&gt;$y = mx + b$&lt;/code&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;52%&#34; /&gt;
&lt;col width=&#34;47%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Type…&lt;/th&gt;
&lt;th&gt;…to get&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;pre&gt;Based on the DAG, the regression model for
estimating the effect of education on wages
is $\hat{y} = \beta_0 + \beta_1 x_1 + \epsilon$, or
$\text{Wages} = \beta_0 + \beta_1 \text{Education} + \epsilon$.&lt;/pre&gt;&lt;/td&gt;
&lt;td&gt;Based on the DAG, the regression model for
estimating the effect of education on wages
is &lt;span class=&#34;math inline&#34;&gt;\(\hat{y} = \beta_0 + \beta_1 x_1 + \epsilon\)&lt;/span&gt;, or
&lt;span class=&#34;math inline&#34;&gt;\(\text{Wages} = \beta_0 + \beta_1 \text{Education} + \epsilon\)&lt;/span&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To put an equation on its own line in a display block, wrap it in double dollar signs, like this:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Type…&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;The quadratic equation was an important part of high school math:

$$
x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
$$

But now we just use computers to solve for $x$.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;…to get…&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The quadratic equation was an important part of high school math:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But now we just use computers to solve for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;p&gt;Because dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write &lt;code&gt;This book costs $5.75 and this other costs $40&lt;/code&gt;, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.&lt;/p&gt;
&lt;p&gt;To get around that, put a backslash (&lt;code&gt;\&lt;/code&gt;) in front of the dollar signs, so that &lt;code&gt;This book costs \$5.75 and this other costs \$40&lt;/code&gt; becomes “This book costs &lt;span&gt;$5.75&lt;/span&gt; and this other costs &lt;span&gt;$40&lt;/span&gt;”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tables&lt;/h2&gt;
&lt;p&gt;There are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like &lt;a href=&#34;https://rapporter.github.io/pander/&#34;&gt;&lt;strong&gt;pander&lt;/strong&gt;&lt;/a&gt; (use &lt;code&gt;pandoc.table()&lt;/code&gt;) or &lt;strong&gt;knitr&lt;/strong&gt; (use &lt;a href=&#34;https://bookdown.org/yihui/rmarkdown-cookbook/kable.html&#34;&gt;&lt;code&gt;kable()&lt;/code&gt;&lt;/a&gt;). The two most common are simple tables and pipe tables. &lt;a href=&#34;https://pandoc.org/MANUAL.html#tables&#34;&gt;You should look at the full documentation here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For simple tables, type…&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;  Right     Left     Center     Default
-------     ------ ----------   -------
     12     12        12            12
    123     123       123          123
      1     1          1             1

Table: Caption goes here&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;…to get…&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;Caption goes here&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Right&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Left&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Center&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;123&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;123&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;123&lt;/td&gt;
&lt;td&gt;123&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;For pipe tables, type…&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;| Right | Left | Default | Center |
|------:|:-----|---------|:------:|
|   12  |  12  |    12   |    12  |
|  123  |  123 |   123   |   123  |
|    1  |    1 |     1   |     1  |

Table: Caption goes here&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;…to get…&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;Caption goes here&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Right&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Left&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Center&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;123&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;123&lt;/td&gt;
&lt;td&gt;123&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;123&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;footnotes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;There are two different ways to add footnotes (&lt;a href=&#34;https://pandoc.org/MANUAL.html#footnotes&#34;&gt;see here for complete documentation&lt;/a&gt;): regular and inline.&lt;/p&gt;
&lt;p&gt;Regular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like &lt;code&gt;[^1]&lt;/code&gt;, but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Type…&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;Here is a footnote reference[^1] and here is another [^note-on-dags].

[^1]: This is a note.

[^note-on-dags]: DAGs are neat.

And here&amp;#39;s more of the document.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;…to get…&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here is a footnote reference&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and here is another.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And here’s more of the document.&lt;/p&gt;
&lt;hr /&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;
&lt;p&gt;
This is a note.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;
&lt;p&gt;
DAGs are neat.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;p&gt;You can also use inline footnotes with &lt;code&gt;^[Text of the note goes here]&lt;/code&gt;, which are often easier because you don’t need to worry about identifiers:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Type…&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;Causal inference is neat.^[But it can be hard too!]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;…to get…&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Causal inference is neat.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;ol&gt;
&lt;li id=&#34;fn3&#34;&gt;
&lt;p&gt;
But it can be hard too!&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;front-matter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Front matter&lt;/h2&gt;
&lt;p&gt;You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named &lt;a href=&#34;https://learn.getgrav.org/16/advanced/yaml&#34;&gt;YAML&lt;/a&gt; (or “YAML Ain’t Markup Language”) that follows this basic outline: &lt;code&gt;setting: value for setting&lt;/code&gt;. Here’s an example YAML metadata section. Note that it must start and end with three dashes (&lt;code&gt;---&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;---
title: Title of your document
date: &amp;quot;January 13, 2020&amp;quot;
author: &amp;quot;Your name&amp;quot;
---&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (&lt;code&gt;:&lt;/code&gt;) in it, it’ll confuse Markdown since it’ll be something like &lt;code&gt;title: My cool title: a subtitle&lt;/code&gt;, which has two colons. It’s better to do this:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;---
title: &amp;quot;My cool title: a subtitle&amp;quot;
---&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to use quotes inside one of the values (e.g. your document is &lt;code&gt;An evaluation of &#34;scare quotes&#34;&lt;/code&gt;), you can use single quotes instead:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;---
title: &amp;#39;An evaluation of &amp;quot;scare quotes&amp;quot;&amp;#39;
---&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;citations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Citations&lt;/h2&gt;
&lt;p&gt;One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a &lt;a href=&#34;http://www.bibtex.org/&#34;&gt;BibTeX file&lt;/a&gt; (ends in &lt;code&gt;.bib&lt;/code&gt;) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like &lt;a href=&#34;https://bibdesk.sourceforge.io/&#34;&gt;BibDesk&lt;/a&gt; on macOS), or you can use &lt;a href=&#34;https://www.zotero.org/&#34;&gt;Zotero&lt;/a&gt; (macOS and Windows) to export a &lt;code&gt;.bib&lt;/code&gt; file. You can &lt;a href=&#34;https://ssc442.netlify.app/reference/&#34;&gt;download an example &lt;code&gt;.bib&lt;/code&gt; file of all the readings from this class here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html&#34;&gt;Complete details for using citations can be found here&lt;/a&gt;. In brief, you need to do three things:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Add a &lt;code&gt;bibliography:&lt;/code&gt; entry to the YAML metadata:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;---
title: Title of your document
date: &amp;quot;January 13, 2020&amp;quot;
author: &amp;quot;Your name&amp;quot;
bibliography: name_of_file.bib
---&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ &lt;a href=&#34;https://github.com/citation-style-language/styles&#34;&gt;at this repository&lt;/a&gt;. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;---
title: Title of your document
date: &amp;quot;January 13, 2020&amp;quot;
author: &amp;quot;Your name&amp;quot;
bibliography: name_of_file.bib
csl: &amp;quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl&amp;quot;
---&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some of the most common CSLs are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/citation-style-language/styles/master/chicago-author-date.csl&#34;&gt;Chicago author-date&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/citation-style-language/styles/master/chicago-note-bibliography.csl&#34;&gt;Chicago note-bibliography&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/citation-style-language/styles/master/chicago-fullnote-bibliography.csl&#34;&gt;Chicago full note-bibliography&lt;/a&gt; (no shortened notes or ibids)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl&#34;&gt;APA 7th edition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/citation-style-language/styles/master/modern-language-association.csl&#34;&gt;MLA 8th edition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cite things in your document. &lt;a href=&#34;https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html&#34;&gt;Check the documentation for full details of how to do this&lt;/a&gt;. Essentially, you use &lt;code&gt;@citationkey&lt;/code&gt; inside square brackets (&lt;code&gt;[]&lt;/code&gt;):&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;51%&#34; /&gt;
&lt;col width=&#34;48%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Type…&lt;/th&gt;
&lt;th&gt;…to get…&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;Causal inference is neat [@Rohrer:2018; @AngristPischke:2015].&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1].&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018].&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;@AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees.&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;After compiling, you should have a perfectly formatted bibliography added to the end of your document too:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Angrist, Joshua D., and Jörn-Steffen Pischke. 2015. &lt;em&gt;Mastering ’Metrics: The Path from Cause to Effect.&lt;/em&gt; Princeton, NJ: Princeton University Press.&lt;/p&gt;
&lt;p&gt;Rohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” &lt;em&gt;Advances in Methods and Practices in Psychological Science&lt;/em&gt; 1 (1): 27–42. &lt;a href=&#34;https://doi.org/10.1177/2515245917745629&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1177/2515245917745629&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;other-references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other references&lt;/h2&gt;
&lt;p&gt;These websites have additional details and examples and practice tools:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://commonmark.org/help/tutorial/&#34;&gt;&lt;strong&gt;CommonMark’s Markdown tutorial&lt;/strong&gt;&lt;/a&gt;: A quick interactive Markdown tutorial.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.markdowntutorial.com/&#34;&gt;&lt;strong&gt;Markdown tutorial&lt;/strong&gt;&lt;/a&gt;: Another interactive tutorial to practice using Markdown.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://packetlife.net/media/library/16/Markdown.pdf&#34;&gt;&lt;strong&gt;Markdown cheatsheet&lt;/strong&gt;&lt;/a&gt;: Useful one-page reminder of Markdown syntax.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://plain-text.co/&#34;&gt;&lt;strong&gt;The Plain Person’s Guide to Plain Text Social Science&lt;/strong&gt;&lt;/a&gt;: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualization</title>
      <link>https://ssc442.netlify.app/resource/visualization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/resource/visualization/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interesting-and-excellent-real-world-examples&#34;&gt;Interesting and excellent real world examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-select-the-appropriate-chart-type&#34;&gt;How to select the appropriate chart type&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#general-resources&#34;&gt;General resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualization-in-excel&#34;&gt;Visualization in Excel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualization-in-tableau&#34;&gt;Visualization in Tableau&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;interesting-and-excellent-real-world-examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interesting and excellent real world examples&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.storiesbehindaline.com/&#34;&gt;The Stories Behind a Line&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://flowingdata.com/2017/06/28/australia-as-100-people/&#34;&gt;Australia as 100 people&lt;/a&gt;: You can make something like this with &lt;a href=&#34;https://d3js.org/&#34;&gt;d3&lt;/a&gt; and the &lt;a href=&#34;https://github.com/civisanalytics/potato&#34;&gt;potato project&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://flowingdata.com/2017/07/17/marrying-age-over-the-past-century/&#34;&gt;Marrying Later, Staying Single Longer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-select-the-appropriate-chart-type&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to select the appropriate chart type&lt;/h2&gt;
&lt;p&gt;Many people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.datavizcatalogue.com/&#34;&gt;&lt;strong&gt;The Data Visualisation Catalogue&lt;/strong&gt;&lt;/a&gt;: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://datavizproject.com/&#34;&gt;&lt;strong&gt;The Data Viz Project&lt;/strong&gt;&lt;/a&gt;: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.data-to-viz.com/&#34;&gt;&lt;strong&gt;From Data to Viz&lt;/strong&gt;&lt;/a&gt;: A decision tree for dozens of chart types with links to R and Python code.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://chartmaker.visualisingdata.com/&#34;&gt;&lt;strong&gt;The Chartmaker Directory&lt;/strong&gt;&lt;/a&gt;: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://shinyapps.stat.ubc.ca/r-graph-catalog/&#34;&gt;&lt;strong&gt;R Graph Catalog&lt;/strong&gt;&lt;/a&gt;: R code for 124 ggplot graphs.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://annkemery.com/essentials/&#34;&gt;&lt;strong&gt;Emery’s Essentials&lt;/strong&gt;&lt;/a&gt;: Descriptions and examples of 26 different chart types.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;general-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;General resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.storytellingwithdata.com/&#34;&gt;&lt;strong&gt;Storytelling with Data&lt;/strong&gt;&lt;/a&gt;: Blog and site full of resources by Cole Nussbaumer Knaflic.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://annkemery.com/blog/&#34;&gt;&lt;strong&gt;Ann K. Emery’s blog&lt;/strong&gt;&lt;/a&gt;: Blog and tutorials by &lt;a href=&#34;https://twitter.com/AnnKEmery&#34;&gt;Ann Emery&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stephanieevergreen.com/&#34;&gt;&lt;strong&gt;Evergreen Data&lt;/strong&gt;&lt;/a&gt;: Helful resources by &lt;a href=&#34;https://twitter.com/evergreendata&#34;&gt;Stephanie Evergreen&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://policyviz.com/&#34;&gt;&lt;strong&gt;PolicyViz&lt;/strong&gt;&lt;/a&gt;: Regular podcast and site full of helpful resources by &lt;a href=&#34;https://twitter.com/jschwabish&#34;&gt;Jon Schwabisch&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.visualisingdata.com/&#34;&gt;&lt;strong&gt;Visualising Data&lt;/strong&gt;&lt;/a&gt;: Fantastic collection of visualization resources, articles, and tutorials by &lt;a href=&#34;https://twitter.com/visualisingdata&#34;&gt;Andy Kirk&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://infowetrust.com/&#34;&gt;&lt;strong&gt;Info We Trust&lt;/strong&gt;&lt;/a&gt;: Detailed explorations of visualizations by &lt;a href=&#34;https://twitter.com/infowetrust&#34;&gt;RJ Andrews&lt;/a&gt;, including a &lt;a href=&#34;http://infowetrust.com/history/&#34;&gt;beautiful visual history of the field&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://flowingdata.com/&#34;&gt;&lt;strong&gt;FlowingData&lt;/strong&gt;&lt;/a&gt;: Blog by &lt;a href=&#34;https://twitter.com/flowingdata&#34;&gt;Nathan Yau&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.informationisbeautiful.net/&#34;&gt;&lt;strong&gt;Information is Beautiful&lt;/strong&gt;&lt;/a&gt;: Blog by &lt;a href=&#34;https://twitter.com/mccandelish&#34;&gt;David McCandless&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://junkcharts.typepad.com/&#34;&gt;&lt;strong&gt;Junk Charts&lt;/strong&gt;&lt;/a&gt;: Blog by &lt;a href=&#34;https://twitter.com/junkcharts&#34;&gt;Kaiser Fung&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://viz.wtf/&#34;&gt;&lt;strong&gt;WTF Visualizations&lt;/strong&gt;&lt;/a&gt;: Visualizations that make you ask “wtf?”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://annkemery.com/checklist/&#34;&gt;&lt;strong&gt;The Data Visualization Checklist&lt;/strong&gt;&lt;/a&gt;: A helpful set of criteria for grading the effectiveness of a graphic.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1kKRadOiF0LruItsvGA40fSDZkAQfCqC_Ela0gBdo8A4/edit&#34;&gt;&lt;strong&gt;Data Literacy Starter Kit&lt;/strong&gt;&lt;/a&gt;: Compilation of resources to become data literate by &lt;a href=&#34;http://lauracalloway.com/&#34;&gt;Laura Calloway&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://seeingdata.org/&#34;&gt;&lt;strong&gt;Seeing Data&lt;/strong&gt;&lt;/a&gt;: A series of research projects about perceptions and visualizations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-in-excel&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualization in Excel&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://stephanieevergreen.com/how-to/&#34;&gt;&lt;strong&gt;How to Build Data Visualizations in Excel&lt;/strong&gt;&lt;/a&gt;: Detailed tutorials for creating 14 different visualizations in Excel.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://annkemery.com/category/visualizing-data/tutorials/&#34;&gt;&lt;strong&gt;Ann Emery’s tutorials&lt;/strong&gt;&lt;/a&gt;: Fantastic series of tutorials for creating charts in Excel.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-in-tableau&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualization in Tableau&lt;/h2&gt;
&lt;p&gt;Because it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a &lt;a href=&#34;https://www.tableau.com/learn/training&#34;&gt;phenomenal library of tutorials and training videos&lt;/a&gt;. There’s &lt;a href=&#34;https://www.quora.com/What-are-some-good-video-tutorials-for-learning-Tableau&#34;&gt;a helpful collections of videos here&lt;/a&gt;, as well.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using R Markdown</title>
      <link>https://ssc442.netlify.app/resource/rmarkdown/</link>
      <pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/resource/rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#key-terms&#34;&gt;Key terms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#add-chunks&#34;&gt;Add chunks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chunk-names&#34;&gt;Chunk names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#chunk-options&#34;&gt;Chunk options&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inline-chunks&#34;&gt;Inline chunks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#output-formats&#34;&gt;Output formats&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;R Markdown&lt;/a&gt; is &lt;a href=&#34;https://ssc442.netlify.app/reference/markdown/&#34;&gt;regular Markdown&lt;/a&gt; with R code and output sprinkled in. You can do everything you can with &lt;a href=&#34;https://ssc442.netlify.app/reference/markdown/&#34;&gt;regular Markdown&lt;/a&gt;, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/index.html&#34;&gt;interactive dashboards&lt;/a&gt; with R Markdown. This whole course website is created with R Markdown (and &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;a package named &lt;strong&gt;blogdown&lt;/strong&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://rmarkdown.rstudio.com/&#34;&gt;documentation for R Markdown&lt;/a&gt; is extremely comprehensive, and their &lt;a href=&#34;https://rmarkdown.rstudio.com/lesson-1.html&#34;&gt;tutorials&lt;/a&gt; and &lt;a href=&#34;https://rmarkdown.rstudio.com/lesson-15.html&#34;&gt;cheatsheets&lt;/a&gt; are excellent—rely on those.&lt;/p&gt;
&lt;p&gt;Here are the most important things you’ll need to know about R Markdown in this class:&lt;/p&gt;
&lt;div id=&#34;key-terms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Key terms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Document&lt;/strong&gt;: A Markdown file where you type stuff&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Chunk&lt;/strong&gt;: A piece of R code that is included in your document. It looks like this:&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;```{r}
# Code goes here
```&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Knit&lt;/strong&gt;: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through &lt;a href=&#34;https://pandoc.org/&#34;&gt;pandoc&lt;/a&gt; to convert it to HTML or PDF or Word (or whatever output you’ve selected).&lt;/p&gt;
&lt;p&gt;You can knit by clicking on the “Knit” button at the top of the editor window, or by pressing &lt;code&gt;⌘⇧K&lt;/code&gt; on macOS or &lt;code&gt;control + shift + K&lt;/code&gt; on Windows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/assignments/knit-button.png&#34; width=&#34;30%&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;add-chunks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Add chunks&lt;/h2&gt;
&lt;p&gt;There are three ways to insert chunks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Press &lt;code&gt;⌘⌥I&lt;/code&gt; on macOS or &lt;code&gt;control + alt + I&lt;/code&gt; on Windows&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Click on the “Insert” button at the top of the editor window&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/reference/insert-chunk.png&#34; width=&#34;30%&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Manually type all the backticks and curly braces (don’t do this)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;chunk-names&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chunk names&lt;/h2&gt;
&lt;p&gt;You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/reference/chunk-toc.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To add a name, include it immediately after the &lt;code&gt;{r&lt;/code&gt; in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. &lt;strong&gt;All chunk names in your document must be unique.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;```{r name-of-this-chunk}
# Code goes here
```&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;chunk-options&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chunk options&lt;/h2&gt;
&lt;p&gt;There are a bunch of different options you can set for each chunk. You can see a complete list in the &lt;a href=&#34;https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf&#34;&gt;RMarkdown Reference Guide&lt;/a&gt; or at &lt;a href=&#34;https://yihui.org/knitr/options/&#34;&gt;&lt;strong&gt;knitr&lt;/strong&gt;’s website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Options go inside the &lt;code&gt;{r}&lt;/code&gt; section of the chunk:&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;```{r name-of-this-chunk, warning=FALSE, message=FALSE}
# Code goes here
```&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most common chunk options are these:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fig.width=5&lt;/code&gt; and &lt;code&gt;fig.height=3&lt;/code&gt; (&lt;em&gt;or whatever number you want&lt;/em&gt;): Set the dimensions for figures&lt;/li&gt;
&lt;li&gt;&lt;code&gt;echo=FALSE&lt;/code&gt;: The code is not shown in the final document, but the results are&lt;/li&gt;
&lt;li&gt;&lt;code&gt;message=FALSE&lt;/code&gt;: Any messages that R generates (like all the notes that appear after you load a package) are omitted&lt;/li&gt;
&lt;li&gt;&lt;code&gt;warning=FALSE&lt;/code&gt;: Any warnings that R generates are omitted&lt;/li&gt;
&lt;li&gt;&lt;code&gt;include=FALSE&lt;/code&gt;: The chunk still runs, but the code and results are not included in the final document&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/reference/chunk-options.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inline-chunks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inline chunks&lt;/h2&gt;
&lt;p&gt;You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use &lt;code&gt;`r r_code_here`&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…&lt;/p&gt;
&lt;pre class=&#34;markdown&#34;&gt;&lt;code&gt;```{r find-avg-mpg, echo=FALSE}
avg_mpg &amp;lt;- mean(mtcars$mpg)
```

The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… would knit into this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;output-formats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Output formats&lt;/h2&gt;
&lt;p&gt;You can specify what kind of document you create when you knit in the &lt;a href=&#34;https://ssc442.netlify.app/reference/markdown/#front-matter&#34;&gt;YAML front matter&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;title: &amp;quot;My document&amp;quot;
output:
  html_document: default
  pdf_document: default
  word_document: default&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also click on the down arrow on the “Knit” button to choose the output &lt;em&gt;and&lt;/em&gt; generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/reference/output-options.png&#34; width=&#34;35%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first output type listed under &lt;code&gt;output:&lt;/code&gt; will be what is generated when you click on the “Knit” button or press the keyboard shortcut (&lt;code&gt;⌘⇧K&lt;/code&gt; on macOS; &lt;code&gt;control + shift + K&lt;/code&gt; on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the &lt;code&gt;output&lt;/code&gt; section.&lt;/p&gt;
&lt;p&gt;The indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical &lt;code&gt;output&lt;/code&gt; section might look like:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;---
title: &amp;quot;My document&amp;quot;
author: &amp;quot;My name&amp;quot;
date: &amp;quot;January 13, 2020&amp;quot;
output: 
  html_document: 
    toc: yes
    fig_caption: yes
    fig_height: 8
    fig_width: 10
  pdf_document: 
    latex_engine: xelatex  # More modern PDF typesetting engine
    toc: yes
  word_document: 
    toc: yes
    fig_caption: yes
    fig_height: 4
    fig_width: 5
---&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R style suggestions</title>
      <link>https://ssc442.netlify.app/resource/style/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/resource/style/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-style-conventions&#34;&gt;R style conventions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#main-style-things-to-pay-attention-to-for-this-class&#34;&gt;Main style things to pay attention to for this class&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#spacing&#34;&gt;Spacing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#long-lines&#34;&gt;Long lines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pipes-and-ggplot-layers&#34;&gt;Pipes (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) and ggplot layers (&lt;code&gt;+&lt;/code&gt;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comments&#34;&gt;Comments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;r-style-conventions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R style conventions&lt;/h2&gt;
&lt;p&gt;R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg %&amp;gt;% 
  filter(cty &amp;gt; 10, class == &amp;quot;compact&amp;quot;)

mpg %&amp;gt;% filter(cty &amp;gt; 10, class == &amp;quot;compact&amp;quot;)

mpg %&amp;gt;% 
  filter(cty &amp;gt; 10, 
         class == &amp;quot;compact&amp;quot;)

mpg %&amp;gt;% filter(cty&amp;gt;10, class==&amp;quot;compact&amp;quot;)

filter(mpg,cty&amp;gt;10,class==&amp;quot;compact&amp;quot;)

mpg %&amp;gt;% 
filter(cty &amp;gt; 10, 
                        class == &amp;quot;compact&amp;quot;)

filter ( mpg,cty&amp;gt;10,     class==&amp;quot;compact&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But you’ll notice that only a few of those iterations (the first three) are easily readable.&lt;/p&gt;
&lt;p&gt;To help improve readability and make it easier to share code with others, there’s &lt;a href=&#34;https://style.tidyverse.org/index.html&#34;&gt;an unofficial style guide for writing R code&lt;/a&gt;. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.&lt;/p&gt;
&lt;p&gt;RStudio has a built-in way of cleaning up your code. Select some code, press &lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; (on Windows) or &lt;kbd&gt;⌘&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit &lt;kbd&gt;space&lt;/kbd&gt; a billion times.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;main-style-things-to-pay-attention-to-for-this-class&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Main style things to pay attention to for this class&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Important note&lt;/em&gt;: I won’t ever grade you on any of this! If you submit something like &lt;code&gt;filter(mpg,cty&amp;gt;10,class==&#34;compact&#34;)&lt;/code&gt;, I might recommend adding spaces, but it won’t affect your grade or points or anything.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;spacing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Spacing&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;See the &lt;a href=&#34;https://style.tidyverse.org/syntax.html#spacing&#34;&gt;“Spacing” section&lt;/a&gt; in the tidyverse style guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Put spaces after commas (like in regular English):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Good
filter(mpg, cty &amp;gt; 10)

# Bad
filter(mpg , cty &amp;gt; 10)
filter(mpg ,cty &amp;gt; 10)
filter(mpg,cty &amp;gt; 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Put spaces around operators like &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;=&lt;/code&gt;, etc.:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Good
filter(mpg, cty &amp;gt; 10)

# Bad
filter(mpg, cty&amp;gt;10)
filter(mpg, cty&amp;gt; 10)
filter(mpg, cty &amp;gt;10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Don’t put spaces around parentheses that are parts of functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Good
filter(mpg, cty &amp;gt; 10)

# Bad
filter (mpg, cty &amp;gt; 10)
filter ( mpg, cty &amp;gt; 10)
filter( mpg, cty &amp;gt; 10 )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;long-lines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Long lines&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;See the &lt;a href=&#34;https://style.tidyverse.org/syntax.html#long-lines&#34;&gt;“Long lines” section&lt;/a&gt; in the tidyverse style guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” &amp;gt; “Global Options” &amp;gt; “Code” &amp;gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.&lt;/p&gt;
&lt;p&gt;You can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Good
filter(mpg, cty &amp;gt; 10, class == &amp;quot;compact&amp;quot;)

# Good
filter(mpg, cty &amp;gt; 10, 
       class == &amp;quot;compact&amp;quot;)

# Good
filter(mpg,
       cty &amp;gt; 10,
       class == &amp;quot;compact&amp;quot;)

# Bad
filter(mpg, cty &amp;gt; 10, class %in% c(&amp;quot;compact&amp;quot;, &amp;quot;pickup&amp;quot;, &amp;quot;midsize&amp;quot;, &amp;quot;subcompact&amp;quot;, &amp;quot;suv&amp;quot;, &amp;quot;2seater&amp;quot;, &amp;quot;minivan&amp;quot;))

# Good
filter(mpg, 
       cty &amp;gt; 10, 
       class %in% c(&amp;quot;compact&amp;quot;, &amp;quot;pickup&amp;quot;, &amp;quot;midsize&amp;quot;, &amp;quot;subcompact&amp;quot;, 
                    &amp;quot;suv&amp;quot;, &amp;quot;2seater&amp;quot;, &amp;quot;minivan&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pipes-and-ggplot-layers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pipes (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) and ggplot layers (&lt;code&gt;+&lt;/code&gt;)&lt;/h3&gt;
&lt;p&gt;Put each layer of a ggplot plot on separate lines, with the &lt;code&gt;+&lt;/code&gt; at the end of the line, indented with two spaces:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Good
ggplot(mpg, aes(x = cty, y = hwy, color = class)) +
  geom_point() +
  geom_smooth() +
  theme_bw()

# Bad
ggplot(mpg, aes(x = cty, y = hwy, color = class)) +
  geom_point() + geom_smooth() +
  theme_bw()

# Super bad
ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw()

# Super bad and won&amp;#39;t even work
ggplot(mpg, aes(x = cty, y = hwy, color = class))
  + geom_point()
  + geom_smooth() 
  + theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Put each step in a dplyr pipeline on separate lines, with the &lt;code&gt;%&amp;gt;%&lt;/code&gt; at the end of the line, indented with two spaces:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Good
mpg %&amp;gt;% 
  filter(cty &amp;gt; 10) %&amp;gt;% 
  group_by(class) %&amp;gt;% 
  summarize(avg_hwy = mean(hwy))

# Bad
mpg %&amp;gt;% filter(cty &amp;gt; 10) %&amp;gt;% group_by(class) %&amp;gt;% 
  summarize(avg_hwy = mean(hwy))

# Super bad
mpg %&amp;gt;% filter(cty &amp;gt; 10) %&amp;gt;% group_by(class) %&amp;gt;% summarize(avg_hwy = mean(hwy))

# Super bad and won&amp;#39;t even work
mpg %&amp;gt;% 
  filter(cty &amp;gt; 10)
  %&amp;gt;% group_by(class)
  %&amp;gt;% summarize(avg_hwy = mean(hwy))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;comments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comments&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;See the &lt;a href=&#34;https://style.tidyverse.org/syntax.html#comments&#34;&gt;“Comments” section&lt;/a&gt; in the tidyverse style guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Comments should start with a comment symbol and a single space: &lt;code&gt;#&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Good

#Bad

    #Bad&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg %&amp;gt;% 
  filter(cty &amp;gt; 10) %&amp;gt;%  # Only rows where cty is 10 +
  group_by(class) %&amp;gt;%  # Divide into class groups
  summarize(avg_hwy = mean(hwy))  # Find the average hwy in each group&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can add extra spaces to get inline comments to align, if you want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg %&amp;gt;% 
  filter(cty &amp;gt; 10) %&amp;gt;%            # Only rows where cty is 10 +
  group_by(class) %&amp;gt;%             # Divide into class groups
  summarize(avg_hwy = mean(hwy))  # Find the average hwy in each group&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” &amp;gt; “Reflow comment”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Good
# Happy families are all alike; every unhappy family is unhappy in its own way.
# Everything was in confusion in the Oblonskys’ house. The wife had discovered
# that the husband was carrying on an intrigue with a French girl, who had been
# a governess in their family, and she had announced to her husband that she
# could not go on living in the same house with him. This position of affairs
# had now lasted three days, and not only the husband and wife themselves, but
# all the members of their family and household, were painfully conscious of it.

# Bad
# Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Though, if you’re dealing with comments that are &lt;em&gt;that&lt;/em&gt; long, consider putting the text in R Markdown instead and having it be actual prose.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Unzipping files</title>
      <link>https://ssc442.netlify.app/resource/unzipping/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/resource/unzipping/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a &lt;strong&gt;zip file&lt;/strong&gt;. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.&lt;/p&gt;
&lt;p&gt;Unzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.&lt;/p&gt;
&lt;div id=&#34;unzipping-files-on-macos&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Unzipping files on macOS&lt;/h2&gt;
&lt;p&gt;Double click on the downloaded &lt;code&gt;.zip&lt;/code&gt; file. macOS will automatically create a new folder with the same name as the &lt;code&gt;.zip&lt;/code&gt; file, and all the file’s contents will be inside. Double click on the RStudio Project file (&lt;code&gt;.Rproj&lt;/code&gt;) to get started.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/unzipping/unzip-mac.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unzipping-files-on-windows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Unzipping files on Windows&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: Right click on the &lt;code&gt;.zip&lt;/code&gt; file, select “Extract All…”, and work with the resulting unzipped folder.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Unlike macOS, Windows does &lt;em&gt;not&lt;/em&gt; automatically unzip things for you. If you double click on the &lt;code&gt;.zip&lt;/code&gt; file, Windows will show you what’s inside, but it will do so without actually extracting anything. This &lt;del&gt;can be&lt;/del&gt; is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a &lt;code&gt;.zip&lt;/code&gt; file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/unzipping/inside-zip-windows.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/unzipping/temp-wd-windows.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You most likely won’t be able to open any data files or save anything, which will be frustrating.&lt;/p&gt;
&lt;p&gt;Instead, you need to right click on the &lt;code&gt;.zip&lt;/code&gt; file and select “Extract All…”:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/unzipping/extract-windows-1.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then choose where you want to unzip all the files and click on “Extract”&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/unzipping/extract-windows-2.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/unzipping/extract-windows-3.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data</title>
      <link>https://ssc442.netlify.app/resource/data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/resource/data/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://tinyletter.com/data-is-plural&#34;&gt;&lt;strong&gt;Data is Plural newsletter&lt;/strong&gt;&lt;/a&gt;: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0&#34;&gt;He also has an archive of all the datasets he’s highlighted.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://toolbox.google.com/datasetsearch&#34;&gt;&lt;strong&gt;Google Dataset Search&lt;/strong&gt;&lt;/a&gt;: Google indexes thousands of public datasets; search for them here.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/datasets&#34;&gt;&lt;strong&gt;Kaggle&lt;/strong&gt;&lt;/a&gt;: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, &lt;a href=&#34;https://www.kaggle.com/hugomathien/soccer&#34;&gt;the European Soccer Database&lt;/a&gt;, the &lt;a href=&#34;https://www.kaggle.com/rtatman/salem-witchcraft-dataset&#34;&gt;Salem Witchcraft Dataset&lt;/a&gt; or results from an &lt;a href=&#34;https://www.kaggle.com/rtatman/oreo-flavors-tastetest-ratings&#34;&gt;Oreo flavors taste test&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.threesixtygiving.org/data/data-registry/&#34;&gt;&lt;strong&gt;360Giving&lt;/strong&gt;&lt;/a&gt;: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://us-cities.survey.okfn.org/&#34;&gt;&lt;strong&gt;US City Open Data Census&lt;/strong&gt;&lt;/a&gt;: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the &lt;a href=&#34;http://sunlightfoundation.com/&#34;&gt;Sunlight Foundation&lt;/a&gt; and &lt;a href=&#34;http://www.codeforamerica.org//&#34;&gt;Code for America&lt;/a&gt; collects this data and rates cities by how well they’re doing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Political science and economics datasets&lt;/strong&gt;: There’s a wealth of data available for political science- and economics-related topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/briatte/srqm/wiki/Data&#34;&gt;&lt;strong&gt;François Briatte’s extensive curated lists&lt;/strong&gt;&lt;/a&gt;: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/leeper/poliscitoys/issues/1&#34;&gt;&lt;strong&gt;Thomas Leeper’s list of political science datasets&lt;/strong&gt;&lt;/a&gt;: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/erikgahner/PolData&#34;&gt;&lt;strong&gt;Erik Gahner’s list of political science datasets&lt;/strong&gt;&lt;/a&gt;: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Text as Data</title>
      <link>https://ssc442.netlify.app/content/14-content/</link>
      <pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/14-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#string-processing&#34;&gt;String processing&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#stringr&#34;&gt;The stringr package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-1-us-murders-data&#34;&gt;Case study 1: US murders data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-2-self-reported-heights&#34;&gt;Case study 2: self-reported heights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-escape-when-defining-strings&#34;&gt;How to &lt;em&gt;escape&lt;/em&gt; when defining strings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regular-expressions&#34;&gt;Regular expressions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#strings-are-a-regexp&#34;&gt;Strings are a regexp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#special-characters&#34;&gt;Special characters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#character-classes&#34;&gt;Character classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#anchors&#34;&gt;Anchors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantifiers&#34;&gt;Quantifiers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#white-space-s&#34;&gt;White space &lt;code&gt;\s&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantifiers-1&#34;&gt;Quantifiers: &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;?&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#not&#34;&gt;Not&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#groups&#34;&gt;Groups&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#search-and-replace-with-regex&#34;&gt;Search and replace with regex&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#search-and-replace-using-groups&#34;&gt;Search and replace using groups&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing-and-improving&#34;&gt;Testing and improving&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trimming&#34;&gt;Trimming&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#changing-lettercase&#34;&gt;Changing lettercase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-2-self-reported-heights-continued&#34;&gt;Case study 2: self-reported heights (continued)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-extract-function&#34;&gt;The &lt;code&gt;extract&lt;/code&gt; function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#putting-it-all-together&#34;&gt;Putting it all together&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#string-splitting&#34;&gt;String splitting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-3-extracting-tables-from-a-pdf&#34;&gt;Case study 3: extracting tables from a PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#recode&#34;&gt;Recoding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#web-scraping&#34;&gt;Web scraping&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#html&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-rvest-package&#34;&gt;The rvest package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#css-selectors&#34;&gt;CSS selectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#json&#34;&gt;JSON&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What are some common issues with string data?&lt;/li&gt;
&lt;li&gt;What are the key ways to wrangle strings?&lt;/li&gt;
&lt;li&gt;What are &lt;em&gt;regular expressions&lt;/em&gt; and why are they magic&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;string-processing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;String processing&lt;/h1&gt;
&lt;p&gt;One of the most common data wrangling challenges involves extracting numeric data contained in character strings and converting them into the numeric representations required to make plots, compute summaries, or fit models in R. Also common is processing unorganized text into meaningful variable names or categorical variables. Many of the string processing challenges a data scientist faces are unique and often unexpected. It is therefore quite ambitious to write a comprehensive section on this topic. Here we use a series of case studies that help us demonstrate how string processing is a necessary step for many data wrangling challenges. Specifically, we describe the process of converting the not yet shown original &lt;em&gt;raw&lt;/em&gt; data from which we extracted the &lt;code&gt;murders&lt;/code&gt;, &lt;code&gt;heights&lt;/code&gt;, and &lt;code&gt;research_funding_rates&lt;/code&gt; example into the data frames we have studied in this book.&lt;/p&gt;
&lt;p&gt;By going over these case studies, we will cover some of the most common tasks in string processing including
extracting numbers from strings,
removing unwanted characters from text,
finding and replacing characters,
extracting specific parts of strings,
converting free form text to more uniform formats, and
splitting strings into multiple values.&lt;/p&gt;
&lt;p&gt;Base R includes functions to perform all these tasks. However, they don’t follow a unifying convention, which makes them a bit hard to memorize and use. The &lt;strong&gt;stringr&lt;/strong&gt; package basically repackages this functionality, but uses a more consistent approach of naming functions and ordering their arguments. For example, in &lt;strong&gt;stringr&lt;/strong&gt;, all the string processing functions start with &lt;code&gt;str_&lt;/code&gt;. This means that if you type &lt;code&gt;str_&lt;/code&gt; and hit tab, R will auto-complete and show all the available functions. As a result, we don’t necessarily have to memorize all the function names. Another advantage is that in the functions in this package the string being processed is always the first argument, which means we can more easily use the pipe. Therefore, we will start by describing how to use the functions in the &lt;strong&gt;stringr&lt;/strong&gt; package.&lt;/p&gt;
&lt;p&gt;Most of the examples will come from the second case study which deals with self-reported heights by students and most of the chapter is dedicated to learning regular expressions (regex), and functions in the &lt;strong&gt;stringr&lt;/strong&gt; package.&lt;/p&gt;
&lt;div id=&#34;stringr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The stringr package&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(stringr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, string processing tasks can be divided into &lt;strong&gt;detecting&lt;/strong&gt;, &lt;strong&gt;locating&lt;/strong&gt;, &lt;strong&gt;extracting&lt;/strong&gt;, or &lt;strong&gt;replacing&lt;/strong&gt; patterns in strings. We will see several examples. The table below includes the functions available to you in the &lt;strong&gt;stringr&lt;/strong&gt; package. We split them by task. We also include the R-base equivalent when available.&lt;/p&gt;
&lt;p&gt;All these functions take a character vector as first argument. Also, for each function, operations are vectorized: the operation gets applied to each string in the vector.&lt;/p&gt;
&lt;p&gt;Finally, note that in this table we mention &lt;em&gt;groups&lt;/em&gt;. These will be explained in Section &lt;a href=&#34;#groups&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;13%&#34; /&gt;
&lt;col width=&#34;50%&#34; /&gt;
&lt;col width=&#34;17%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;stringr&lt;/th&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;R-base&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_detect&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Detect&lt;/td&gt;
&lt;td&gt;Is the pattern in the string?&lt;/td&gt;
&lt;td&gt;&lt;code&gt;grepl&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_which&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Detect&lt;/td&gt;
&lt;td&gt;Returns the index of entries that contain the pattern.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;grep&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_subset&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Detect&lt;/td&gt;
&lt;td&gt;Returns the subset of strings that contain the pattern.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;grep&lt;/code&gt; with &lt;code&gt;value = TRUE&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_locate&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Locate&lt;/td&gt;
&lt;td&gt;Returns positions of first occurrence of pattern in a string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;regexpr&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_locate_all&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Locate&lt;/td&gt;
&lt;td&gt;Returns position of all occurrences of pattern in a string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;gregexpr&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_view&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Locate&lt;/td&gt;
&lt;td&gt;Show the first part of the string that matches pattern.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_view_all&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Locate&lt;/td&gt;
&lt;td&gt;Show me all the parts of the string that match the pattern.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_extract&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extract&lt;/td&gt;
&lt;td&gt;Extract the first part of the string that matches the pattern.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_extract_all&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extract&lt;/td&gt;
&lt;td&gt;Extract all parts of the string that match the pattern.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_match&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extract&lt;/td&gt;
&lt;td&gt;Extract first part of the string that matches the groups and the patterns defined by the groups.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_match_all&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extract&lt;/td&gt;
&lt;td&gt;Extract all parts of the string that matches the groups and the patterns defined by the groups.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_sub&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extract&lt;/td&gt;
&lt;td&gt;Extract a substring.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;substring&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_split&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extract&lt;/td&gt;
&lt;td&gt;Split a string into a list with parts separated by pattern.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;strsplit&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_split_fixed&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Extract&lt;/td&gt;
&lt;td&gt;Split a string into a matrix with parts separated by pattern.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;strsplit&lt;/code&gt; with &lt;code&gt;fixed = TRUE&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_count&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Describe&lt;/td&gt;
&lt;td&gt;Count number of times a pattern appears in a string.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_length&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Describe&lt;/td&gt;
&lt;td&gt;Number of character in string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nchar&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_replace&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replace&lt;/td&gt;
&lt;td&gt;Replace first part of a string matching a pattern with another.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_replace_all&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replace&lt;/td&gt;
&lt;td&gt;Replace all parts of a string matching a pattern with another.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;gsub&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_to_upper&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replace&lt;/td&gt;
&lt;td&gt;Change all characters to upper case.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;toupper&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_to_lower&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replace&lt;/td&gt;
&lt;td&gt;Change all characters to lower case.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;tolower&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_to_title&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replace&lt;/td&gt;
&lt;td&gt;Change first character to upper and rest to lower.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_replace_na&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replace&lt;/td&gt;
&lt;td&gt;Replace all &lt;code&gt;NA&lt;/code&gt;s to a new value.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_trim&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Replace&lt;/td&gt;
&lt;td&gt;Remove white space from start and end of string.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_c&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manipulate&lt;/td&gt;
&lt;td&gt;Join multiple strings.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;paste0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_conv&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manipulate&lt;/td&gt;
&lt;td&gt;Change the encoding of the string.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_sort&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manipulate&lt;/td&gt;
&lt;td&gt;Sort the vector in alphabetical order.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;sort&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_order&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manipulate&lt;/td&gt;
&lt;td&gt;Index needed to order the vector in alphabetical order.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;order&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_trunc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manipulate&lt;/td&gt;
&lt;td&gt;Truncate a string to a fixed size.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_pad&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manipulate&lt;/td&gt;
&lt;td&gt;Add white space to string to make it a fixed size.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_dup&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manipulate&lt;/td&gt;
&lt;td&gt;Repeat a string.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;rep&lt;/code&gt; then &lt;code&gt;paste&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;str_wrap&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manipulate&lt;/td&gt;
&lt;td&gt;Wrap things into formatted paragraphs.&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;str_interp&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Manipulate&lt;/td&gt;
&lt;td&gt;String interpolation.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;sprintf&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-1-us-murders-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study 1: US murders data&lt;/h2&gt;
&lt;p&gt;In this section we introduce some of the more simple string processing challenges with the following datasets as an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
url &amp;lt;- paste0(&amp;quot;https://en.wikipedia.org/w/index.php?title=&amp;quot;,
              &amp;quot;Gun_violence_in_the_United_States_by_state&amp;quot;,
              &amp;quot;&amp;amp;direction=prev&amp;amp;oldid=810166167&amp;quot;)
murders_raw &amp;lt;- read_html(url) %&amp;gt;%
  html_node(&amp;quot;table&amp;quot;) %&amp;gt;%
  html_table() %&amp;gt;%
  setNames(c(&amp;quot;state&amp;quot;, &amp;quot;population&amp;quot;, &amp;quot;total&amp;quot;, &amp;quot;murder_rate&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above shows the first step in constructing the dataset&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;from the raw data, which was extracted from a Wikipedia page.&lt;/p&gt;
&lt;p&gt;In general, string processing involves a string and a pattern. In R, we usually store strings in a character vector such as &lt;code&gt;murders$population&lt;/code&gt;. The first three strings in this vector defined by the population variable are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders_raw$population[1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;4,853,875&amp;quot; &amp;quot;737,709&amp;quot;   &amp;quot;6,817,565&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The usual coercion does not work here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(murders_raw$population[1:3])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA NA NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is because of the commas &lt;code&gt;,&lt;/code&gt;. The string processing we want to do here is remove the &lt;strong&gt;pattern&lt;/strong&gt;, &lt;code&gt;,&lt;/code&gt;, from the &lt;strong&gt;strings&lt;/strong&gt; in &lt;code&gt;murders_raw$population&lt;/code&gt; and then coerce to numbers.
We can use the &lt;code&gt;str_detect&lt;/code&gt; function to see that two of the three columns have commas in the entries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;commas &amp;lt;- function(x) any(str_detect(x, &amp;quot;,&amp;quot;))
murders_raw %&amp;gt;% summarize_all(commas)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 4
##   state population total murder_rate
##   &amp;lt;lgl&amp;gt; &amp;lt;lgl&amp;gt;      &amp;lt;lgl&amp;gt; &amp;lt;lgl&amp;gt;      
## 1 FALSE TRUE       TRUE  FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then use the &lt;code&gt;str_replace_all&lt;/code&gt; function to remove them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_1 &amp;lt;- str_replace_all(murders_raw$population, &amp;quot;,&amp;quot;, &amp;quot;&amp;quot;)
test_1 &amp;lt;- as.numeric(test_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then use &lt;code&gt;mutate_all&lt;/code&gt; to apply this operation to each column, since it won’t affect the columns without commas.&lt;/p&gt;
&lt;p&gt;It turns out that this operation is so common that &lt;code&gt;readr&lt;/code&gt; includes the function &lt;code&gt;parse_number&lt;/code&gt; specifically meant to remove non-numeric characters before coercing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_2 &amp;lt;- parse_number(murders_raw$population)
identical(test_1, test_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we can obtain our desired table using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders_new &amp;lt;- murders_raw %&amp;gt;% mutate_at(2:3, parse_number)
head(murders_new)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 4
##   state      population total murder_rate
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Alabama       4853875   348         7.2
## 2 Alaska         737709    59         8  
## 3 Arizona       6817565   309         4.5
## 4 Arkansas      2977853   181         6.1
## 5 California   38993940  1861         4.8
## 6 Colorado      5448819   176         3.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This case is relatively simple compared to the string processing challenges that we typically face in data science. The next example is a rather complex one and it provides several challenges that will permit us to learn many string processing techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-2-self-reported-heights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study 2: self-reported heights&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;dslabs&lt;/strong&gt; package includes the raw data from which the heights dataset was obtained. You can load it like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(reported_heights)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These heights were obtained using a web form in which students were asked to enter their heights. They could enter anything, but the instructions asked for &lt;em&gt;height in inches&lt;/em&gt;, a number. We compiled 1,095 submissions, but unfortunately the column vector with the reported heights had several non-numeric entries and as a result became a character vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(reported_heights$height)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we try to parse it into numbers, we get a warning:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- as.numeric(reported_heights$height)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although most values appear to be height in inches as requested:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 75 70 68 74 61 65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we do end up with many &lt;code&gt;NA&lt;/code&gt;s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(is.na(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 81&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see some of the entries that are not successfully converted by using &lt;code&gt;filter&lt;/code&gt; to keep only the entries resulting in &lt;code&gt;NA&lt;/code&gt;s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reported_heights %&amp;gt;%
  dplyr::mutate(new_height = as.numeric(height)) %&amp;gt;%
  dplyr::filter(is.na(new_height)) %&amp;gt;%
  head(n=10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             time_stamp    sex                 height new_height
## 1  2014-09-02 15:16:28   Male                  5&amp;#39; 4&amp;quot;         NA
## 2  2014-09-02 15:16:37 Female                  165cm         NA
## 3  2014-09-02 15:16:52   Male                    5&amp;#39;7         NA
## 4  2014-09-02 15:16:56   Male                  &amp;gt;9000         NA
## 5  2014-09-02 15:16:56   Male                   5&amp;#39;7&amp;quot;         NA
## 6  2014-09-02 15:17:09 Female                   5&amp;#39;3&amp;quot;         NA
## 7  2014-09-02 15:18:00   Male 5 feet and 8.11 inches         NA
## 8  2014-09-02 15:19:48   Male                   5&amp;#39;11         NA
## 9  2014-09-04 00:46:45   Male                  5&amp;#39;9&amp;#39;&amp;#39;         NA
## 10 2014-09-04 10:29:44   Male                 5&amp;#39;10&amp;#39;&amp;#39;         NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We immediately see what is happening. Some of the students did not report their heights in inches as requested. We could discard these data and continue. However, many of the entries follow patterns that, in principle, we can easily convert to inches. For example, in the output above, we see various cases that use the format &lt;code&gt;x&#39;y&#39;&#39;&lt;/code&gt; with &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; representing feet and inches, respectively. Each one of these cases can be read and converted to inches by a human, for example &lt;code&gt;5&#39;4&#39;&#39;&lt;/code&gt; is &lt;code&gt;5*12 + 4 = 64&lt;/code&gt;. So we could fix all the problematic entries &lt;em&gt;by hand&lt;/em&gt;. However, humans are prone to making mistakes, so an automated approach is preferable. Also, because we plan on continuing to collect data, it will be convenient to write code that automatically does this.&lt;/p&gt;
&lt;p&gt;A first step in this type of task is to survey the problematic entries and try to define specific patterns followed by a large groups of entries. The larger these groups, the more entries we can fix with a single programmatic approach. We want to find patterns that can be accurately described with a rule, such as “a digit, followed by a feet symbol, followed by one or two digits, followed by an inches symbol”.&lt;/p&gt;
&lt;p&gt;To look for such patterns, it helps to remove the entries that are consistent with being in inches and to view only the problematic entries. We thus write a function to automatically do this. We keep entries that either result in &lt;code&gt;NA&lt;/code&gt;s when applying &lt;code&gt;as.numeric&lt;/code&gt; or are outside a range of plausible heights. We permit a range that covers about 99.9999% of the adult population. We also use &lt;code&gt;suppressWarnings&lt;/code&gt; to avoid the warning message we know &lt;code&gt;as.numeric&lt;/code&gt; will gives us.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;not_inches &amp;lt;- function(x, smallest = 50, tallest = 84){
  inches &amp;lt;- suppressWarnings(as.numeric(x))
  ind &amp;lt;- is.na(inches) | inches &amp;lt; smallest | inches &amp;gt; tallest
  ind
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We apply this function and find the number of problematic entries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;problems &amp;lt;- reported_heights %&amp;gt;%
  dplyr::filter(not_inches(height)) %&amp;gt;%
  pull(height)
length(problems)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 292&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now view all the cases by simply printing them. We don’t do that here because there are &lt;code&gt;length(problems)&lt;/code&gt;, but after surveying them carefully, we see that three patterns can be used to define three large groups within these exceptions.&lt;/p&gt;
&lt;p&gt;1. A pattern of the form &lt;code&gt;x&#39;y&lt;/code&gt; or &lt;code&gt;x&#39; y&#39;&#39;&lt;/code&gt; or &lt;code&gt;x&#39;y&#34;&lt;/code&gt; with &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; representing feet and inches, respectively. Here are ten examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 5&amp;#39; 4&amp;quot; 5&amp;#39;7 5&amp;#39;7&amp;quot; 5&amp;#39;3&amp;quot; 5&amp;#39;11 5&amp;#39;9&amp;#39;&amp;#39; 5&amp;#39;10&amp;#39;&amp;#39; 5&amp;#39; 10 5&amp;#39;5&amp;quot; 5&amp;#39;2&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2. A pattern of the form &lt;code&gt;x.y&lt;/code&gt; or &lt;code&gt;x,y&lt;/code&gt; with &lt;code&gt;x&lt;/code&gt; feet and &lt;code&gt;y&lt;/code&gt; inches. Here are ten examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 5.3 5.5 6.5 5.8 5.6 5,3 5.9 6,8 5.5 6.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3. Entries that were reported in centimeters rather than inches. Here are ten examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 150 175 177 178 163 175 178 165 165 180&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we see these large groups following specific patterns, we can develop a plan of attack. Remember that there is rarely just one way to perform these tasks. Here we pick one that helps us teach several useful techniques. But surely there is a more efficient way of performing the task.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Plan of attack&lt;/strong&gt;: we will convert entries fitting the first two patterns into a standardized one. We will then leverage the standardization to extract the feet and inches and convert to inches. We will then define a procedure for identifying entries that are in centimeters and convert them to inches. After applying these steps, we will then check again to see what entries were not fixed and see if we can tweak our approach to be more comprehensive.&lt;/p&gt;
&lt;p&gt;At the end, we hope to have a script that makes web-based data collection methods robust to the most common user mistakes.&lt;/p&gt;
&lt;p&gt;To achieve our goal, we will use a technique that enables us to accurately detect patterns and extract the parts we want: &lt;em&gt;regular expressions&lt;/em&gt; (regex). But first, we quickly describe how to &lt;em&gt;escape&lt;/em&gt; the function of certain characters so that they can be included in strings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-escape-when-defining-strings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to &lt;em&gt;escape&lt;/em&gt; when defining strings&lt;/h2&gt;
&lt;p&gt;To define strings in R, we can use either double quotes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;quot;Hello!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or single quotes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;#39;Hello!&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure you choose the correct single quote since using the back quote will give you an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- `Hello`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error: object &amp;#39;Hello&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, what happens if the string we want to define includes double quotes? For example, if we want to write 10 inches like this &lt;code&gt;10&#34;&lt;/code&gt;?
In this case you can’t use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;quot;10&amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;because this is just the string &lt;code&gt;10&lt;/code&gt; followed by a double quote. If you type this into R, you get an error because you have an &lt;em&gt;unclosed&lt;/em&gt; double quote. To avoid this, we can use the single quotes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;#39;10&amp;quot;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we print out &lt;code&gt;s&lt;/code&gt; we see that the double quotes are &lt;em&gt;escaped&lt;/em&gt; with the backslash &lt;code&gt;\&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;10\&amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In fact, escaping with the backslash provides a way to define the string while still using the double quotes to define strings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;quot;10\&amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In R, the function &lt;code&gt;cat&lt;/code&gt; lets us see what the string actually looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, what if we want our string to be 5 feet written like this &lt;code&gt;5&#39;&lt;/code&gt;? In this case, we can use the double quotes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;quot;5&amp;#39;&amp;quot;
cat(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 5&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we’ve learned how to write 5 feet and 10 inches separately, but what if we want to write them together to represent &lt;em&gt;5 feet and 10 inches&lt;/em&gt; like this &lt;code&gt;5&#39;10&#34;&lt;/code&gt;? In this case, neither the single nor double quotes will work. This:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;#39;5&amp;#39;10&amp;quot;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;closes the string after 5 and this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;quot;5&amp;#39;10&amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;closes the string after 10. Keep in mind that if we type one of the above code snippets into R, it will get stuck waiting for you to close the open quote and you will have to exit the execution with the &lt;em&gt;esc&lt;/em&gt; button.&lt;/p&gt;
&lt;p&gt;In this situation, we need to escape the function of the quotes with the backslash &lt;code&gt;\&lt;/code&gt;. You can escape either character like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;#39;5\&amp;#39;10&amp;quot;&amp;#39;
cat(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 5&amp;#39;10&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;quot;5&amp;#39;10\&amp;quot;&amp;quot;
cat(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 5&amp;#39;10&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Escaping characters is something we often have to use when processing strings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regular-expressions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regular expressions&lt;/h2&gt;
&lt;p&gt;A regular expression (regex) is a way to describe specific patterns of characters of text. They can be used to determine if a given string matches the pattern. A set of rules has been defined to do this efficiently and precisely and here we show some examples. We can learn more about these rules by reading a detailed tutorials&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. This RStudio cheat sheet&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; is also very useful.&lt;/p&gt;
&lt;p&gt;The patterns supplied to the &lt;strong&gt;stringr&lt;/strong&gt; functions can be a regex rather than a standard string. We will learn how this works through a series of examples.&lt;/p&gt;
&lt;p&gt;Throughout this section you will see that we create strings to test out our regex. To do this, we define patterns that we know should match and also patterns that we know should not. We will call them &lt;code&gt;yes&lt;/code&gt; and &lt;code&gt;no&lt;/code&gt;, respectively. This permits us to check for the two types of errors: failing to match and incorrectly matching.&lt;/p&gt;
&lt;div id=&#34;strings-are-a-regexp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Strings are a regexp&lt;/h3&gt;
&lt;p&gt;Technically any string is a regex, perhaps the simplest example is a single character. So the comma &lt;code&gt;,&lt;/code&gt; used in the next code example is a simple example of searching with regex.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;,&amp;quot;
str_detect(murders_raw$total, pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We suppress the output which is logical vector telling us which entries have commas.&lt;/p&gt;
&lt;p&gt;Above, we noted that an entry included a &lt;code&gt;cm&lt;/code&gt;. This is also a simple example of a regex. We can show all the entries that used &lt;code&gt;cm&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_subset(reported_heights$height, &amp;quot;cm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;165cm&amp;quot;  &amp;quot;170 cm&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;special-characters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Special characters&lt;/h3&gt;
&lt;p&gt;Now let’s consider a slightly more complicated example. Which of the following strings contain the pattern &lt;code&gt;cm&lt;/code&gt; or &lt;code&gt;inches&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yes &amp;lt;- c(&amp;quot;180 cm&amp;quot;, &amp;quot;70 inches&amp;quot;)
no &amp;lt;- c(&amp;quot;180&amp;quot;, &amp;quot;70&amp;#39;&amp;#39;&amp;quot;)
s &amp;lt;- c(yes, no)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_detect(s, &amp;quot;cm&amp;quot;) | str_detect(s, &amp;quot;inches&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  TRUE  TRUE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, we don’t need to do this. The main feature that distinguishes the regex &lt;em&gt;language&lt;/em&gt; from plain strings is that we can use special characters. These are characters with a meaning. We start by introducing &lt;code&gt;|&lt;/code&gt; which means &lt;em&gt;or&lt;/em&gt;. So if we want to know if either &lt;code&gt;cm&lt;/code&gt; or &lt;code&gt;inches&lt;/code&gt; appears in the strings, we can use the regex &lt;code&gt;cm|inches&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_detect(s, &amp;quot;cm|inches&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  TRUE  TRUE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and obtain the correct answer.&lt;/p&gt;
&lt;p&gt;Another special character that will be useful for identifying feet and inches values is &lt;code&gt;\d&lt;/code&gt; which means any digit: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. The backslash is used to distinguish it from the character &lt;code&gt;d&lt;/code&gt;. In R, we have to &lt;em&gt;escape&lt;/em&gt; the backslash &lt;code&gt;\&lt;/code&gt; so we actually have to use &lt;code&gt;\\d&lt;/code&gt; to represent digits. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yes &amp;lt;- c(&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;5&amp;#39;10&amp;quot;, &amp;quot;5 feet&amp;quot;, &amp;quot;4&amp;#39;11&amp;quot;)
no &amp;lt;- c(&amp;quot;&amp;quot;, &amp;quot;.&amp;quot;, &amp;quot;Five&amp;quot;, &amp;quot;six&amp;quot;)
s &amp;lt;- c(yes, no)
pattern &amp;lt;- &amp;quot;\\d&amp;quot;
str_detect(s, pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We take this opportunity to introduce the &lt;code&gt;str_view&lt;/code&gt; function, which is helpful for troubleshooting as it shows us the first match for each string:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_view(s, pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./12-content_files/str_view-1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and &lt;code&gt;str_view_all&lt;/code&gt; shows us all the matches, so &lt;code&gt;3&#39;2&lt;/code&gt; has two matches and &lt;code&gt;5&#39;10&lt;/code&gt; has three.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_view_all(s, pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./12-content_files/str_view-2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are many other special characters. We will learn some others below, but you can see most or all of them in the cheat sheet&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; mentioned earlier.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;character-classes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Character classes&lt;/h3&gt;
&lt;p&gt;Character classes are used to define a series of characters that can be matched. We define character classes with square brackets &lt;code&gt;[]&lt;/code&gt;. So, for example, if we want the pattern to match only if we have a &lt;code&gt;5&lt;/code&gt; or a &lt;code&gt;6&lt;/code&gt;, we use the regex &lt;code&gt;[56]&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_view(s, &amp;quot;[56]&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./12-content_files/str_view-3.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Suppose we want to match values between 4 and 7. A common way to define character classes is with ranges. So, for example, &lt;code&gt;[0-9]&lt;/code&gt; is equivalent to &lt;code&gt;\\d&lt;/code&gt;. The pattern we want is therefore &lt;code&gt;[4-7]&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yes &amp;lt;- as.character(4:7)
no &amp;lt;- as.character(1:3)
s &amp;lt;- c(yes, no)
str_detect(s, &amp;quot;[4-7]&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, it is important to know that in regex everything is a character; there are no numbers. So &lt;code&gt;4&lt;/code&gt; is the character &lt;code&gt;4&lt;/code&gt; not the number four. Notice, for example, that &lt;code&gt;[1-20]&lt;/code&gt; does &lt;strong&gt;not&lt;/strong&gt; mean 1 through 20, it means the characters 1 through 2 or the character 0. So &lt;code&gt;[1-20]&lt;/code&gt; simply means the character class composed of 0, 1, and 2.&lt;/p&gt;
&lt;p&gt;Keep in mind that characters do have an order and the digits do follow the numeric order. So &lt;code&gt;0&lt;/code&gt; comes before &lt;code&gt;1&lt;/code&gt; which comes before &lt;code&gt;2&lt;/code&gt; and so on. For the same reason, we can define lower case letters as &lt;code&gt;[a-z]&lt;/code&gt;, upper case letters as &lt;code&gt;[A-Z]&lt;/code&gt;, and &lt;code&gt;[a-zA-z]&lt;/code&gt; as both.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;anchors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Anchors&lt;/h3&gt;
&lt;p&gt;What if we want a match when we have exactly 1 digit? This will be useful in our case study since feet are never more than 1 digit so a restriction will help us. One way to do this with regex is by using &lt;em&gt;anchors&lt;/em&gt;, which let us define patterns that must start or end at a specific place. The two most common anchors are
&lt;code&gt;^&lt;/code&gt; and &lt;code&gt;$&lt;/code&gt; which represent the beginning and end of a string, respectively. So the pattern &lt;code&gt;^\\d$&lt;/code&gt; is read as “start of the string followed by one digit followed by end of string”.&lt;/p&gt;
&lt;p&gt;This pattern now only detects the strings with exactly one digit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;^\\d$&amp;quot;
yes &amp;lt;- c(&amp;quot;1&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;9&amp;quot;)
no &amp;lt;- c(&amp;quot;12&amp;quot;, &amp;quot;123&amp;quot;, &amp;quot; 1&amp;quot;, &amp;quot;a4&amp;quot;, &amp;quot;b&amp;quot;)
s &amp;lt;- c(yes, no)
str_view_all(s, pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./12-content_files/str_view-4.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;1&lt;/code&gt; does not match because it does not start with the digit but rather with a space, which is actually not easy to see.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantifiers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quantifiers&lt;/h3&gt;
&lt;p&gt;For the inches part, we can have one or two digits. This can be specified in regex with &lt;em&gt;quantifiers&lt;/em&gt;. This is done by following the pattern with curly brackets containing the number of times the previous entry can be repeated. We use an example to illustrate. The pattern for one or two digits is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;^\\d{1,2}$&amp;quot;
yes &amp;lt;- c(&amp;quot;1&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;12&amp;quot;)
no &amp;lt;- c(&amp;quot;123&amp;quot;, &amp;quot;a4&amp;quot;, &amp;quot;b&amp;quot;)
str_view(c(yes, no), pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./12-content_files/str_view-5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, &lt;code&gt;123&lt;/code&gt; does &lt;strong&gt;not&lt;/strong&gt; match, but &lt;code&gt;12&lt;/code&gt; does. So to look for our feet and inches pattern, we can add the symbols for feet &lt;code&gt;&#39;&lt;/code&gt; and inches &lt;code&gt;&#34;&lt;/code&gt; after the digits.&lt;/p&gt;
&lt;p&gt;With what we have learned, we can now construct an example for the pattern &lt;code&gt;x&#39;y\&#34;&lt;/code&gt; with &lt;code&gt;x&lt;/code&gt; feet and &lt;code&gt;y&lt;/code&gt; inches.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;^[4-7]&amp;#39;\\d{1,2}\&amp;quot;$&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pattern is now getting complex, but you can look at it carefully and break it down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;^&lt;/code&gt; = start of the string&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[4-7]&lt;/code&gt; = one digit, either 4,5,6 or 7&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&#39;&lt;/code&gt; = feet symbol&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\\d{1,2}&lt;/code&gt; = one or two digits&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\&#34;&lt;/code&gt; = inches symbol&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$&lt;/code&gt; = end of the string&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s test it out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yes &amp;lt;- c(&amp;quot;5&amp;#39;7\&amp;quot;&amp;quot;, &amp;quot;6&amp;#39;2\&amp;quot;&amp;quot;,  &amp;quot;5&amp;#39;12\&amp;quot;&amp;quot;)
no &amp;lt;- c(&amp;quot;6,2\&amp;quot;&amp;quot;, &amp;quot;6.2\&amp;quot;&amp;quot;,&amp;quot;I am 5&amp;#39;11\&amp;quot;&amp;quot;, &amp;quot;3&amp;#39;2\&amp;quot;&amp;quot;, &amp;quot;64&amp;quot;)
str_detect(yes, pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE TRUE TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_detect(no, pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE FALSE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For now, we are permitting the inches to be 12 or larger. We will add a restriction later as the regex for this is a bit more complex than we are ready to show.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;white-space-s&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;White space &lt;code&gt;\s&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Another problem we have are spaces. For example, our pattern does not match &lt;code&gt;5&#39; 4&#34;&lt;/code&gt; because there is a space between &lt;code&gt;&#39;&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt; which our pattern does not permit. Spaces are characters and R does not ignore them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;identical(&amp;quot;Hi&amp;quot;, &amp;quot;Hi &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In regex, &lt;code&gt;\s&lt;/code&gt; represents white space. To find patterns like &lt;code&gt;5&#39; 4&lt;/code&gt;, we can change our pattern to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern_2 &amp;lt;- &amp;quot;^[4-7]&amp;#39;\\s\\d{1,2}\&amp;quot;$&amp;quot;
str_subset(problems, pattern_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;5&amp;#39; 4\&amp;quot;&amp;quot;  &amp;quot;5&amp;#39; 11\&amp;quot;&amp;quot; &amp;quot;5&amp;#39; 7\&amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this will not match the patterns with no space. So do we need more than one regex pattern? It turns out we can use a quantifier for this as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantifiers-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quantifiers: &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;?&lt;/code&gt;, &lt;code&gt;+&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;We want the pattern to permit spaces but not require them. Even if there are several spaces, like in this example &lt;code&gt;5&#39;   4&lt;/code&gt;, we still want it to match. There is a quantifier for exactly this purpose. In regex, the character &lt;code&gt;*&lt;/code&gt; means zero or more instances of the previous character. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yes &amp;lt;- c(&amp;quot;AB&amp;quot;, &amp;quot;A1B&amp;quot;, &amp;quot;A11B&amp;quot;, &amp;quot;A111B&amp;quot;, &amp;quot;A1111B&amp;quot;)
no &amp;lt;- c(&amp;quot;A2B&amp;quot;, &amp;quot;A21B&amp;quot;)
str_detect(yes, &amp;quot;A1*B&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE TRUE TRUE TRUE TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_detect(no, &amp;quot;A1*B&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above matches the first string which has zero 1s and all the strings with one or more 1. We can then improve our pattern by adding the &lt;code&gt;*&lt;/code&gt; after the space character &lt;code&gt;\s&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are two other similar quantifiers. For none or once, we can use &lt;code&gt;?&lt;/code&gt;, and for one or more, we can use &lt;code&gt;+&lt;/code&gt;. You can see how they differ with this example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(string = c(&amp;quot;AB&amp;quot;, &amp;quot;A1B&amp;quot;, &amp;quot;A11B&amp;quot;, &amp;quot;A111B&amp;quot;, &amp;quot;A1111B&amp;quot;),
           none_or_more = str_detect(yes, &amp;quot;A1*B&amp;quot;),
           nore_or_once = str_detect(yes, &amp;quot;A1?B&amp;quot;),
           once_or_more = str_detect(yes, &amp;quot;A1+B&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   string none_or_more nore_or_once once_or_more
## 1     AB         TRUE         TRUE        FALSE
## 2    A1B         TRUE         TRUE         TRUE
## 3   A11B         TRUE        FALSE         TRUE
## 4  A111B         TRUE        FALSE         TRUE
## 5 A1111B         TRUE        FALSE         TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will actually use all three in our reported heights example, but we will see these in a later section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;not&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Not&lt;/h3&gt;
&lt;p&gt;To specify patterns that we do &lt;strong&gt;not&lt;/strong&gt; want to detect, we can use the &lt;code&gt;^&lt;/code&gt; symbol but only &lt;strong&gt;inside&lt;/strong&gt; square brackets. Remember that outside the square bracket &lt;code&gt;^&lt;/code&gt; means the start of the string. So, for example, if we want to detect digits that are preceded by anything except a letter we can do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;[^a-zA-Z]\\d&amp;quot;
yes &amp;lt;- c(&amp;quot;.3&amp;quot;, &amp;quot;+2&amp;quot;, &amp;quot;-0&amp;quot;,&amp;quot;*4&amp;quot;)
no &amp;lt;- c(&amp;quot;A3&amp;quot;, &amp;quot;B2&amp;quot;, &amp;quot;C0&amp;quot;, &amp;quot;E4&amp;quot;)
str_detect(yes, pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE TRUE TRUE TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_detect(no, pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another way to generate a pattern that searches for &lt;em&gt;everything except&lt;/em&gt; is to use the upper case of the special character. For example &lt;code&gt;\\D&lt;/code&gt; means anything other than a digit, &lt;code&gt;\\S&lt;/code&gt; means anything except a space, and so on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Groups&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Groups&lt;/em&gt; are a powerful aspect of regex that permits the extraction of values. Groups are defined using parentheses. They don’t affect the pattern matching per se. Instead, it permits tools to identify specific parts of the pattern so we can extract them.&lt;/p&gt;
&lt;p&gt;We want to change heights written like &lt;code&gt;5.6&lt;/code&gt; to &lt;code&gt;5&#39;6&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To avoid changing patterns such as &lt;code&gt;70.2&lt;/code&gt;, we will require that the first digit be between 4 and 7 &lt;code&gt;[4-7]&lt;/code&gt; and that the second be none or more digits &lt;code&gt;\\d*&lt;/code&gt;.
Let’s start by defining a simple pattern that matches this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern_without_groups &amp;lt;- &amp;quot;^[4-7],\\d*$&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to extract the digits so we can then form the new version using a period. These are our two groups, so we encapsulate them with parentheses:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern_with_groups &amp;lt;-  &amp;quot;^([4-7]),(\\d*)$&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We encapsulate the part of the pattern that matches the parts we want to keep for later use. Adding groups does not affect the detection, since it only signals that we want to save what is captured by the groups. Note that both patterns return the same result when using &lt;code&gt;str_detect&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yes &amp;lt;- c(&amp;quot;5,9&amp;quot;, &amp;quot;5,11&amp;quot;, &amp;quot;6,&amp;quot;, &amp;quot;6,1&amp;quot;)
no &amp;lt;- c(&amp;quot;5&amp;#39;9&amp;quot;, &amp;quot;,&amp;quot;, &amp;quot;2,8&amp;quot;, &amp;quot;6.1.1&amp;quot;)
s &amp;lt;- c(yes, no)
str_detect(s, pattern_without_groups)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_detect(s, pattern_with_groups)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we define groups, we can use the function &lt;code&gt;str_match&lt;/code&gt; to extract the values these groups define:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_match(s, pattern_with_groups)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]   [,2] [,3]
## [1,] &amp;quot;5,9&amp;quot;  &amp;quot;5&amp;quot;  &amp;quot;9&amp;quot; 
## [2,] &amp;quot;5,11&amp;quot; &amp;quot;5&amp;quot;  &amp;quot;11&amp;quot;
## [3,] &amp;quot;6,&amp;quot;   &amp;quot;6&amp;quot;  &amp;quot;&amp;quot;  
## [4,] &amp;quot;6,1&amp;quot;  &amp;quot;6&amp;quot;  &amp;quot;1&amp;quot; 
## [5,] NA     NA   NA  
## [6,] NA     NA   NA  
## [7,] NA     NA   NA  
## [8,] NA     NA   NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the second and third columns contain feet and inches, respectively. The first column is the part of the string matching the pattern. If no match occurred, we see an &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now we can understand the difference between the functions &lt;code&gt;str_extract&lt;/code&gt; and &lt;code&gt;str_match&lt;/code&gt;: &lt;code&gt;str_extract&lt;/code&gt; extracts only strings that match a pattern, not the values defined by groups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_extract(s, pattern_with_groups)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;5,9&amp;quot;  &amp;quot;5,11&amp;quot; &amp;quot;6,&amp;quot;   &amp;quot;6,1&amp;quot;  NA     NA     NA     NA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;search-and-replace-with-regex&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Search and replace with regex&lt;/h2&gt;
&lt;p&gt;Earlier we defined the object &lt;code&gt;problems&lt;/code&gt; containing the strings that do not appear to be in inches. We can see that not too many of our problematic strings match the pattern:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;^[4-7]&amp;#39;\\d{1,2}\&amp;quot;$&amp;quot;
sum(str_detect(problems, pattern))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see why this is, we show some examples that expose why we don’t have more matches:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;problems[c(2, 10, 11, 12, 15)] %&amp;gt;% str_view(pattern)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./12-content_files/str_view-6.png&#34; /&gt;
An initial problem we see immediately is that some students wrote out the words “feet” and “inches”. We can see the entries that did this with the &lt;code&gt;str_subset&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_subset(problems, &amp;quot;inches&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;5 feet and 8.11 inches&amp;quot; &amp;quot;Five foot eight inches&amp;quot; &amp;quot;5 feet 7inches&amp;quot;        
## [4] &amp;quot;5ft 9 inches&amp;quot;           &amp;quot;5 ft 9 inches&amp;quot;          &amp;quot;5 feet 6 inches&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also see that some entries used two single quotes &lt;code&gt;&#39;&#39;&lt;/code&gt; instead of a double quote &lt;code&gt;&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_subset(problems, &amp;quot;&amp;#39;&amp;#39;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;5&amp;#39;9&amp;#39;&amp;#39;&amp;quot;   &amp;quot;5&amp;#39;10&amp;#39;&amp;#39;&amp;quot;  &amp;quot;5&amp;#39;10&amp;#39;&amp;#39;&amp;quot;  &amp;quot;5&amp;#39;3&amp;#39;&amp;#39;&amp;quot;   &amp;quot;5&amp;#39;7&amp;#39;&amp;#39;&amp;quot;   &amp;quot;5&amp;#39;6&amp;#39;&amp;#39;&amp;quot;   &amp;quot;5&amp;#39;7.5&amp;#39;&amp;#39;&amp;quot;
##  [8] &amp;quot;5&amp;#39;7.5&amp;#39;&amp;#39;&amp;quot; &amp;quot;5&amp;#39;10&amp;#39;&amp;#39;&amp;quot;  &amp;quot;5&amp;#39;11&amp;#39;&amp;#39;&amp;quot;  &amp;quot;5&amp;#39;10&amp;#39;&amp;#39;&amp;quot;  &amp;quot;5&amp;#39;5&amp;#39;&amp;#39;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To correct this, we can replace the different ways of representing inches and feet with a uniform symbol. We will use &lt;code&gt;&#39;&lt;/code&gt; for feet, whereas for inches we will simply not use a symbol since some entries were of the form &lt;code&gt;x&#39;y&lt;/code&gt;. Now, if we no longer use the inches symbol, we have to change our pattern accordingly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;^[4-7]&amp;#39;\\d{1,2}$&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we do this replacement before the matching, we get many more matches:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;problems %&amp;gt;%
  str_replace(&amp;quot;feet|ft|foot&amp;quot;, &amp;quot;&amp;#39;&amp;quot;) %&amp;gt;% # replace feet, ft, foot with &amp;#39;
  str_replace(&amp;quot;inches|in|&amp;#39;&amp;#39;|\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;% # remove all inches symbols
  str_detect(pattern) %&amp;gt;%
  sum()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 48&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, we still have many cases to go.&lt;/p&gt;
&lt;p&gt;Note that in the code above, we leveraged the &lt;strong&gt;stringr&lt;/strong&gt; consistency and used the pipe.&lt;/p&gt;
&lt;p&gt;For now, we improve our pattern by adding &lt;code&gt;\\s*&lt;/code&gt; in front of and after the feet symbol &lt;code&gt;&#39;&lt;/code&gt; to permit space between the feet symbol and the numbers. Now we match a few more entries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;^[4-7]\\s*&amp;#39;\\s*\\d{1,2}$&amp;quot;
problems %&amp;gt;%
  str_replace(&amp;quot;feet|ft|foot&amp;quot;, &amp;quot;&amp;#39;&amp;quot;) %&amp;gt;% # replace feet, ft, foot with &amp;#39;
  str_replace(&amp;quot;inches|in|&amp;#39;&amp;#39;|\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;% # remove all inches symbols
  str_detect(pattern) %&amp;gt;%
  sum&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 53&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We might be tempted to avoid doing this by removing all the spaces with &lt;code&gt;str_replace_all&lt;/code&gt;. However, when doing such an operation we need to make sure that it does not have unintended effects. In our reported heights examples, this will be a problem because some entries are of the form &lt;code&gt;x y&lt;/code&gt; with space separating the feet from the inches. If we remove all spaces, we will incorrectly turn &lt;code&gt;x y&lt;/code&gt; into &lt;code&gt;xy&lt;/code&gt; which implies that a &lt;code&gt;6 1&lt;/code&gt; would become &lt;code&gt;61&lt;/code&gt; inches instead of &lt;code&gt;73&lt;/code&gt; inches.&lt;/p&gt;
&lt;p&gt;The second large type of problematic entries were of the form &lt;code&gt;x.y&lt;/code&gt;, &lt;code&gt;x,y&lt;/code&gt; and &lt;code&gt;x y&lt;/code&gt;. We want to change all these to our common format &lt;code&gt;x&#39;y&lt;/code&gt;. But we can’t just do a search and replace because we would change values such as &lt;code&gt;70.5&lt;/code&gt; into &lt;code&gt;70&#39;5&lt;/code&gt;.
Our strategy will therefore be to search for a very specific pattern that assures us feet and inches are being provided and then, for those that match, replace appropriately.&lt;/p&gt;
&lt;div id=&#34;search-and-replace-using-groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Search and replace using groups&lt;/h3&gt;
&lt;p&gt;Another powerful aspect of groups is that you can refer to the extracted values in a regex when searching and replacing.&lt;/p&gt;
&lt;p&gt;The regex special character for the &lt;code&gt;i&lt;/code&gt;-th group is &lt;code&gt;\\i&lt;/code&gt;. So &lt;code&gt;\\1&lt;/code&gt; is the value extracted from the first group, &lt;code&gt;\\2&lt;/code&gt; the value from the second and so on. As a simple example, note that the following code will replace a comma with period, but only if it is between two digits:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern_with_groups &amp;lt;-  &amp;quot;^([4-7]),(\\d*)$&amp;quot;
yes &amp;lt;- c(&amp;quot;5,9&amp;quot;, &amp;quot;5,11&amp;quot;, &amp;quot;6,&amp;quot;, &amp;quot;6,1&amp;quot;)
no &amp;lt;- c(&amp;quot;5&amp;#39;9&amp;quot;, &amp;quot;,&amp;quot;, &amp;quot;2,8&amp;quot;, &amp;quot;6.1.1&amp;quot;)
s &amp;lt;- c(yes, no)
str_replace(s, pattern_with_groups, &amp;quot;\\1&amp;#39;\\2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;5&amp;#39;9&amp;quot;   &amp;quot;5&amp;#39;11&amp;quot;  &amp;quot;6&amp;#39;&amp;quot;    &amp;quot;6&amp;#39;1&amp;quot;   &amp;quot;5&amp;#39;9&amp;quot;   &amp;quot;,&amp;quot;     &amp;quot;2,8&amp;quot;   &amp;quot;6.1.1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use this to convert cases in our reported heights.&lt;/p&gt;
&lt;p&gt;We are now ready to define a pattern that helps us convert all the &lt;code&gt;x.y&lt;/code&gt;, &lt;code&gt;x,y&lt;/code&gt; and &lt;code&gt;x y&lt;/code&gt; to our preferred format. We need to adapt &lt;code&gt;pattern_with_groups&lt;/code&gt; to be a bit more flexible and capture all the cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern_with_groups &amp;lt;-&amp;quot;^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s break this one down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;^&lt;/code&gt; = start of the string&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[4-7]&lt;/code&gt; = one digit, either 4, 5, 6, or 7&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\\s*&lt;/code&gt; = none or more white space&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[,\\.\\s+]&lt;/code&gt; = feet symbol is either &lt;code&gt;,&lt;/code&gt;, &lt;code&gt;.&lt;/code&gt; or at least one space&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\\s*&lt;/code&gt; = none or more white space&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\\d*&lt;/code&gt; = none or more digits&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$&lt;/code&gt; = end of the string&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can see that it appears to be working:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_subset(problems, pattern_with_groups) %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;5.3&amp;quot;  &amp;quot;5.25&amp;quot; &amp;quot;5.5&amp;quot;  &amp;quot;6.5&amp;quot;  &amp;quot;5.8&amp;quot;  &amp;quot;5.6&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and will be able to perform the search and replace:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_subset(problems, pattern_with_groups) %&amp;gt;%
  str_replace(pattern_with_groups, &amp;quot;\\1&amp;#39;\\2&amp;quot;) %&amp;gt;% head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;5&amp;#39;3&amp;quot;  &amp;quot;5&amp;#39;25&amp;quot; &amp;quot;5&amp;#39;5&amp;quot;  &amp;quot;6&amp;#39;5&amp;quot;  &amp;quot;5&amp;#39;8&amp;quot;  &amp;quot;5&amp;#39;6&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we will deal with the inches-larger-than-twelve challenge later.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-and-improving&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing and improving&lt;/h2&gt;
&lt;p&gt;Developing the right regex on the first try is often difficult. Trial and error is a common approach to finding the regex pattern that satisfies all desired conditions. In the previous sections, we have developed a powerful string processing technique that can help us catch many of the problematic entries. Here we will test our approach, search for further problems, and tweak our approach for possible improvements. Let’s write a function that captures all the entries that can’t be converted into numbers remembering that some are in centimeters (we will deal with those later):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;not_inches_or_cm &amp;lt;- function(x, smallest = 50, tallest = 84){
  inches &amp;lt;- suppressWarnings(as.numeric(x))
  ind &amp;lt;- !is.na(inches) &amp;amp;
    ((inches &amp;gt;= smallest &amp;amp; inches &amp;lt;= tallest) |
       (inches/2.54 &amp;gt;= smallest &amp;amp; inches/2.54 &amp;lt;= tallest))
  !ind
}

problems &amp;lt;- reported_heights %&amp;gt;%
  dplyr::filter(not_inches_or_cm(height)) %&amp;gt;%
  pull(height)
length(problems)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what proportion of these fit our pattern after the processing steps we developed above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;converted &amp;lt;- problems %&amp;gt;%
  str_replace(&amp;quot;feet|foot|ft&amp;quot;, &amp;quot;&amp;#39;&amp;quot;) %&amp;gt;% # convert feet symbols to &amp;#39;
  str_replace(&amp;quot;inches|in|&amp;#39;&amp;#39;|\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%  # remove inches symbols
  str_replace(&amp;quot;^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$&amp;quot;, &amp;quot;\\1&amp;#39;\\2&amp;quot;)# change format

pattern &amp;lt;- &amp;quot;^[4-7]\\s*&amp;#39;\\s*\\d{1,2}$&amp;quot;
index &amp;lt;- str_detect(converted, pattern)
mean(index)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.615&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how we leveraged the pipe, one of the advantages of using &lt;strong&gt;stringr&lt;/strong&gt;. This last piece of code shows that we have matched well over half of the strings. Let’s examine the remaining cases:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;converted[!index]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;6&amp;quot;             &amp;quot;165cm&amp;quot;         &amp;quot;511&amp;quot;           &amp;quot;6&amp;quot;            
##  [5] &amp;quot;2&amp;quot;             &amp;quot;&amp;gt;9000&amp;quot;         &amp;quot;5 &amp;#39; and 8.11 &amp;quot; &amp;quot;11111&amp;quot;        
##  [9] &amp;quot;6&amp;quot;             &amp;quot;103.2&amp;quot;         &amp;quot;19&amp;quot;            &amp;quot;5&amp;quot;            
## [13] &amp;quot;300&amp;quot;           &amp;quot;6&amp;#39;&amp;quot;            &amp;quot;6&amp;quot;             &amp;quot;Five &amp;#39; eight &amp;quot;
## [17] &amp;quot;7&amp;quot;             &amp;quot;214&amp;quot;           &amp;quot;6&amp;quot;             &amp;quot;0.7&amp;quot;          
## [21] &amp;quot;6&amp;quot;             &amp;quot;2&amp;#39;33&amp;quot;          &amp;quot;612&amp;quot;           &amp;quot;1,70&amp;quot;         
## [25] &amp;quot;87&amp;quot;            &amp;quot;5&amp;#39;7.5&amp;quot;         &amp;quot;5&amp;#39;7.5&amp;quot;         &amp;quot;111&amp;quot;          
## [29] &amp;quot;5&amp;#39; 7.78&amp;quot;       &amp;quot;12&amp;quot;            &amp;quot;6&amp;quot;             &amp;quot;yyy&amp;quot;          
## [33] &amp;quot;89&amp;quot;            &amp;quot;34&amp;quot;            &amp;quot;25&amp;quot;            &amp;quot;6&amp;quot;            
## [37] &amp;quot;6&amp;quot;             &amp;quot;22&amp;quot;            &amp;quot;684&amp;quot;           &amp;quot;6&amp;quot;            
## [41] &amp;quot;1&amp;quot;             &amp;quot;1&amp;quot;             &amp;quot;6*12&amp;quot;          &amp;quot;87&amp;quot;           
## [45] &amp;quot;6&amp;quot;             &amp;quot;1.6&amp;quot;           &amp;quot;120&amp;quot;           &amp;quot;120&amp;quot;          
## [49] &amp;quot;23&amp;quot;            &amp;quot;1.7&amp;quot;           &amp;quot;6&amp;quot;             &amp;quot;5&amp;quot;            
## [53] &amp;quot;69&amp;quot;            &amp;quot;5&amp;#39; 9 &amp;quot;         &amp;quot;5 &amp;#39; 9 &amp;quot;        &amp;quot;6&amp;quot;            
## [57] &amp;quot;6&amp;quot;             &amp;quot;86&amp;quot;            &amp;quot;708,661&amp;quot;       &amp;quot;5 &amp;#39; 6 &amp;quot;       
## [61] &amp;quot;6&amp;quot;             &amp;quot;649,606&amp;quot;       &amp;quot;10000&amp;quot;         &amp;quot;1&amp;quot;            
## [65] &amp;quot;728,346&amp;quot;       &amp;quot;0&amp;quot;             &amp;quot;6&amp;quot;             &amp;quot;6&amp;quot;            
## [69] &amp;quot;6&amp;quot;             &amp;quot;100&amp;quot;           &amp;quot;88&amp;quot;            &amp;quot;6&amp;quot;            
## [73] &amp;quot;170 cm&amp;quot;        &amp;quot;7,283,465&amp;quot;     &amp;quot;5&amp;quot;             &amp;quot;5&amp;quot;            
## [77] &amp;quot;34&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Four clear patterns arise:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Many students measuring exactly 5 or 6 feet did not enter any inches, for example &lt;code&gt;6&#39;&lt;/code&gt;, and our pattern requires that inches be included.&lt;/li&gt;
&lt;li&gt;Some students measuring exactly 5 or 6 feet entered just that number.&lt;/li&gt;
&lt;li&gt;Some of the inches were entered with decimal points. For example &lt;code&gt;5&#39;7.5&#39;&#39;&lt;/code&gt;. Our pattern only looks for two digits.&lt;/li&gt;
&lt;li&gt;Some entries have spaces at the end, for example &lt;code&gt;5 &#39; 9&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Although not as common, we also see the following problems:&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Some entries are in meters and some of these use European decimals: &lt;code&gt;1.6&lt;/code&gt;, &lt;code&gt;1,70&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Two students added &lt;code&gt;cm&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;A student spelled out the numbers: &lt;code&gt;Five foot eight inches&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is not necessarily clear that it is worth writing code to handle these last three cases since they might be rare enough. However, some of them provide us with an opportunity to learn a few more regex techniques, so we will build a fix.&lt;/p&gt;
&lt;p&gt;For case 1, if we add a &lt;code&gt;&#39;0&lt;/code&gt; after the first digit, for example, convert all &lt;code&gt;6&lt;/code&gt; to &lt;code&gt;6&#39;0&lt;/code&gt;, then our previously defined pattern will match. This can be done using groups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yes &amp;lt;- c(&amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;5&amp;quot;)
no &amp;lt;- c(&amp;quot;5&amp;#39;&amp;quot;, &amp;quot;5&amp;#39;&amp;#39;&amp;quot;, &amp;quot;5&amp;#39;4&amp;quot;)
s &amp;lt;- c(yes, no)
str_replace(s, &amp;quot;^([4-7])$&amp;quot;, &amp;quot;\\1&amp;#39;0&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;5&amp;#39;0&amp;quot; &amp;quot;6&amp;#39;0&amp;quot; &amp;quot;5&amp;#39;0&amp;quot; &amp;quot;5&amp;#39;&amp;quot;  &amp;quot;5&amp;#39;&amp;#39;&amp;quot; &amp;quot;5&amp;#39;4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pattern says it has to start (&lt;code&gt;^&lt;/code&gt;) with a digit between 4 and 7 and end there (&lt;code&gt;$&lt;/code&gt;). The parenthesis defines the group that we pass as &lt;code&gt;\\1&lt;/code&gt; to generate the replacement regex string.&lt;/p&gt;
&lt;p&gt;We can adapt this code slightly to handle the case 2 as well, which covers the entry &lt;code&gt;5&#39;&lt;/code&gt;. Note &lt;code&gt;5&#39;&lt;/code&gt; is left untouched. This is because the extra &lt;code&gt;&#39;&lt;/code&gt; makes the pattern not match since we have to end with a 5 or 6. We want to permit the 5 or 6 to be followed by 0 or 1 feet sign. So we can simply add &lt;code&gt;&#39;{0,1}&lt;/code&gt; after the &lt;code&gt;&#39;&lt;/code&gt; to do this. However, we can use the none or once special character &lt;code&gt;?&lt;/code&gt;. As we saw above, this is different from &lt;code&gt;*&lt;/code&gt; which is none or more. We now see that the fourth case is also converted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_replace(s, &amp;quot;^([56])&amp;#39;?$&amp;quot;, &amp;quot;\\1&amp;#39;0&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;5&amp;#39;0&amp;quot; &amp;quot;6&amp;#39;0&amp;quot; &amp;quot;5&amp;#39;0&amp;quot; &amp;quot;5&amp;#39;0&amp;quot; &amp;quot;5&amp;#39;&amp;#39;&amp;quot; &amp;quot;5&amp;#39;4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we only permit 5 and 6, but not 4 and 7. This is because 5 and 6 feet tall is quite common, so we assume those that typed 5 or 6 really meant &lt;code&gt;60&lt;/code&gt; or &lt;code&gt;72&lt;/code&gt; inches. However, &lt;code&gt;4&lt;/code&gt; and &lt;code&gt;7&lt;/code&gt; feet tall are so rare that, although we accept &lt;code&gt;84&lt;/code&gt; as a valid entry, we assume &lt;code&gt;7&lt;/code&gt; was entered in error.&lt;/p&gt;
&lt;p&gt;We can use quantifiers to deal with &lt;strong&gt;case 3&lt;/strong&gt;. These entries are not matched because the inches include decimals and our pattern does not permit this. We need to allow the second group to include decimals not just digits. This means we must permit zero or one period &lt;code&gt;.&lt;/code&gt; then zero or more digits. So we will be using both &lt;code&gt;?&lt;/code&gt; and &lt;code&gt;*&lt;/code&gt;.
Also remember that, for this particular case, the period needs to be escaped since it is a special character (it means any character except line break). Here is a simple example of how we can use &lt;code&gt;*&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So we can adapt our pattern, currently &lt;code&gt;^[4-7]\\s*&#39;\\s*\\d{1,2}$&lt;/code&gt; to permit a decimal at the end:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;^[4-7]\\s*&amp;#39;\\s*(\\d+\\.?\\d*)$&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Case 4, meters using commas, we can approach similarly to how we converted the &lt;code&gt;x.y&lt;/code&gt; to &lt;code&gt;x&#39;y&lt;/code&gt;. A difference is that we require that the first digit be 1 or 2:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yes &amp;lt;- c(&amp;quot;1,7&amp;quot;, &amp;quot;1, 8&amp;quot;, &amp;quot;2, &amp;quot; )
no &amp;lt;- c(&amp;quot;5,8&amp;quot;, &amp;quot;5,3,2&amp;quot;, &amp;quot;1.7&amp;quot;)
s &amp;lt;- c(yes, no)
str_replace(s, &amp;quot;^([12])\\s*,\\s*(\\d*)$&amp;quot;, &amp;quot;\\1\\.\\2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1.7&amp;quot;   &amp;quot;1.8&amp;quot;   &amp;quot;2.&amp;quot;    &amp;quot;5,8&amp;quot;   &amp;quot;5,3,2&amp;quot; &amp;quot;1.7&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will later check if the entries are meters using their numeric values. We will come back to the case study after introducing two widely used functions in string processing that will come in handy when developing our final solution for the self-reported heights.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trimming&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trimming&lt;/h2&gt;
&lt;p&gt;In general, spaces at the start or end of the string are uninformative.
These can be particularly deceptive because sometimes they can be hard to see:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- &amp;quot;Hi &amp;quot;
cat(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Hi&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;identical(s, &amp;quot;Hi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a general enough problem that there is a function dedicated to removing them:
&lt;code&gt;str_trim&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_trim(&amp;quot;5 &amp;#39; 9 &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;5 &amp;#39; 9&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;changing-lettercase&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Changing lettercase&lt;/h2&gt;
&lt;p&gt;Notice that regex is case sensitive. Often we want to match a word regardless of case. One approach to doing this is to first change everything to lower case and then proceeding ignoring case. As an example, note that one of the entries writes out numbers as words &lt;code&gt;Five foot eight inches&lt;/code&gt;. Although not efficient, we could add 13 extra &lt;code&gt;str_replace&lt;/code&gt; calls to convert &lt;code&gt;zero&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;one&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt;, and so on. To avoid having to write two separate operations for &lt;code&gt;Zero&lt;/code&gt; and &lt;code&gt;zero&lt;/code&gt;, &lt;code&gt;One&lt;/code&gt; and &lt;code&gt;one&lt;/code&gt;, etc., we can use the &lt;code&gt;str_to_lower&lt;/code&gt; function to make all works lower case first:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- c(&amp;quot;Five feet eight inches&amp;quot;)
str_to_lower(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;five feet eight inches&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other related functions are &lt;code&gt;str_to_upper&lt;/code&gt; and &lt;code&gt;str_to_title&lt;/code&gt;. We are now ready to define a procedure that converts all the problematic cases to inches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-2-self-reported-heights-continued&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study 2: self-reported heights (continued)&lt;/h2&gt;
&lt;p&gt;We now put all of what we have learned together into a function that takes a string vector and tries to convert as many strings as possible to one format. We write a function that puts together what we have done above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;convert_format &amp;lt;- function(s){
  s %&amp;gt;%
    str_replace(&amp;quot;feet|foot|ft&amp;quot;, &amp;quot;&amp;#39;&amp;quot;) %&amp;gt;%
    str_replace_all(&amp;quot;inches|in|&amp;#39;&amp;#39;|\&amp;quot;|cm|and&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$&amp;quot;, &amp;quot;\\1&amp;#39;\\2&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;^([56])&amp;#39;?$&amp;quot;, &amp;quot;\\1&amp;#39;0&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;^([12])\\s*,\\s*(\\d*)$&amp;quot;, &amp;quot;\\1\\.\\2&amp;quot;) %&amp;gt;%
    str_trim()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also write a function that converts words to numbers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(english)
words_to_numbers &amp;lt;- function(s){
  s &amp;lt;- str_to_lower(s)
  for(i in 0:11)
    s &amp;lt;- str_replace_all(s, words(i), as.character(i))
  s
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we can perform the above operation more efficiently with the function &lt;code&gt;recode&lt;/code&gt;, which we learn about in Section &lt;a href=&#34;#recode&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;.
Now we can see which problematic entries remain:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;converted &amp;lt;- problems %&amp;gt;% words_to_numbers() %&amp;gt;% convert_format()
remaining_problems &amp;lt;- converted[not_inches_or_cm(converted)]
pattern &amp;lt;- &amp;quot;^[4-7]\\s*&amp;#39;\\s*\\d+\\.?\\d*$&amp;quot;
index &amp;lt;- str_detect(remaining_problems, pattern)
remaining_problems[!index]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;511&amp;quot;       &amp;quot;2&amp;quot;         &amp;quot;&amp;gt;9000&amp;quot;     &amp;quot;11111&amp;quot;     &amp;quot;103.2&amp;quot;     &amp;quot;19&amp;quot;       
##  [7] &amp;quot;300&amp;quot;       &amp;quot;7&amp;quot;         &amp;quot;214&amp;quot;       &amp;quot;0.7&amp;quot;       &amp;quot;2&amp;#39;33&amp;quot;      &amp;quot;612&amp;quot;      
## [13] &amp;quot;1.70&amp;quot;      &amp;quot;87&amp;quot;        &amp;quot;111&amp;quot;       &amp;quot;12&amp;quot;        &amp;quot;yyy&amp;quot;       &amp;quot;89&amp;quot;       
## [19] &amp;quot;34&amp;quot;        &amp;quot;25&amp;quot;        &amp;quot;22&amp;quot;        &amp;quot;684&amp;quot;       &amp;quot;1&amp;quot;         &amp;quot;1&amp;quot;        
## [25] &amp;quot;6*12&amp;quot;      &amp;quot;87&amp;quot;        &amp;quot;1.6&amp;quot;       &amp;quot;120&amp;quot;       &amp;quot;120&amp;quot;       &amp;quot;23&amp;quot;       
## [31] &amp;quot;1.7&amp;quot;       &amp;quot;86&amp;quot;        &amp;quot;708,661&amp;quot;   &amp;quot;649,606&amp;quot;   &amp;quot;10000&amp;quot;     &amp;quot;1&amp;quot;        
## [37] &amp;quot;728,346&amp;quot;   &amp;quot;0&amp;quot;         &amp;quot;100&amp;quot;       &amp;quot;88&amp;quot;        &amp;quot;7,283,465&amp;quot; &amp;quot;34&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;apart from the cases reported as meters, which we will fix below, they all seem to be cases that are impossible to fix.&lt;/p&gt;
&lt;div id=&#34;the-extract-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;extract&lt;/code&gt; function&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;extract&lt;/code&gt; function is a useful &lt;strong&gt;tidyverse&lt;/strong&gt; function for string processing that we will use in our final solution, so we introduce it here. In a previous section, we constructed a regex that lets us identify which elements of a character vector match the feet and inches pattern. However, we want to do more. We want to extract and save the feet and number values so that we can convert them to inches when appropriate.&lt;/p&gt;
&lt;p&gt;If we have a simpler case like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- c(&amp;quot;5&amp;#39;10&amp;quot;, &amp;quot;6&amp;#39;1&amp;quot;)
tab &amp;lt;- data.frame(x = s)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In Section &lt;a href=&#34;#separate&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we learned about the &lt;code&gt;separate&lt;/code&gt; function, which can be used to achieve our current goal:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab %&amp;gt;% separate(x, c(&amp;quot;feet&amp;quot;, &amp;quot;inches&amp;quot;), sep = &amp;quot;&amp;#39;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   feet inches
## 1    5     10
## 2    6      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;extract&lt;/code&gt; function from the &lt;strong&gt;tidyr&lt;/strong&gt; package lets us use regex groups to extract the desired values. Here is the equivalent to the code above using &lt;code&gt;separate&lt;/code&gt; but using &lt;code&gt;extract&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
tab %&amp;gt;% tidyr::extract(x, c(&amp;quot;feet&amp;quot;, &amp;quot;inches&amp;quot;), regex = &amp;quot;(\\d)&amp;#39;(\\d{1,2})&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   feet inches
## 1    5     10
## 2    6      1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So why do we even need the new function &lt;code&gt;extract&lt;/code&gt;? We have seen how small changes can throw off exact pattern matching. Groups in regex give us more flexibility. For example, if we define:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- c(&amp;quot;5&amp;#39;10&amp;quot;, &amp;quot;6&amp;#39;1\&amp;quot;&amp;quot;,&amp;quot;5&amp;#39;8inches&amp;quot;)
tab &amp;lt;- data.frame(x = s)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we only want the numbers, &lt;code&gt;separate&lt;/code&gt; fails:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab %&amp;gt;% separate(x, c(&amp;quot;feet&amp;quot;,&amp;quot;inches&amp;quot;), sep = &amp;quot;&amp;#39;&amp;quot;, fill = &amp;quot;right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   feet  inches
## 1    5      10
## 2    6      1&amp;quot;
## 3    5 8inches&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, we can use &lt;code&gt;extract&lt;/code&gt;. The regex here is a bit more complicated since we have to permit &lt;code&gt;&#39;&lt;/code&gt; with spaces and &lt;code&gt;feet&lt;/code&gt;. We also do not want the &lt;code&gt;&#34;&lt;/code&gt; included in the value, so we do not include that in the group:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab %&amp;gt;% tidyr::extract(x, c(&amp;quot;feet&amp;quot;, &amp;quot;inches&amp;quot;), regex = &amp;quot;(\\d)&amp;#39;(\\d{1,2})&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   feet inches
## 1    5     10
## 2    6      1
## 3    5      8&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-all-together&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Putting it all together&lt;/h3&gt;
&lt;p&gt;We are now ready to put it all together and wrangle our reported heights data to try to recover as many heights as possible. The code is complex, but we will break it down into parts.&lt;/p&gt;
&lt;p&gt;We start by cleaning up the &lt;code&gt;height&lt;/code&gt; column so that the heights are closer to a feet’inches format. We added an original heights column so we can compare before and after.&lt;/p&gt;
&lt;p&gt;Now we are ready to wrangle our reported heights dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pattern &amp;lt;- &amp;quot;^([4-7])\\s*&amp;#39;\\s*(\\d+\\.?\\d*)$&amp;quot;

smallest &amp;lt;- 50
tallest &amp;lt;- 84
new_heights &amp;lt;- reported_heights %&amp;gt;%
  dplyr::mutate(original = height,
         height = words_to_numbers(height) %&amp;gt;% convert_format()) %&amp;gt;%
  tidyr::extract(height, c(&amp;quot;feet&amp;quot;, &amp;quot;inches&amp;quot;), regex = pattern, remove = FALSE) %&amp;gt;%
  dplyr::mutate_at(c(&amp;quot;height&amp;quot;, &amp;quot;feet&amp;quot;, &amp;quot;inches&amp;quot;), as.numeric) %&amp;gt;%
  dplyr::mutate(guess = 12 * feet + inches) %&amp;gt;%
  dplyr::mutate(height = case_when(
    is.na(height) ~ as.numeric(NA),
    between(height, smallest, tallest) ~ height,  #inches
    between(height/2.54, smallest, tallest) ~ height/2.54, #cm
    between(height*100/2.54, smallest, tallest) ~ height*100/2.54, #meters
    TRUE ~ as.numeric(NA))) %&amp;gt;%
  dplyr::mutate(height = ifelse(is.na(height) &amp;amp;
                           inches &amp;lt; 12 &amp;amp; between(guess, smallest, tallest),
                         guess, height)) %&amp;gt;%
  dplyr::select(-guess)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can check all the entries we converted by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_heights %&amp;gt;%
  dplyr::filter(not_inches(original)) %&amp;gt;%
  dplyr::select(original, height) %&amp;gt;%
  arrange(height) %&amp;gt;%
  View()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A final observation is that if we look at the shortest students in our course:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_heights %&amp;gt;% arrange(height) %&amp;gt;% head(n=7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            time_stamp    sex height feet inches original
## 1 2017-07-04 01:30:25   Male  50.00   NA     NA       50
## 2 2017-09-07 10:40:35   Male  50.00   NA     NA       50
## 3 2014-09-02 15:18:30 Female  51.00   NA     NA       51
## 4 2016-06-05 14:07:20 Female  52.00   NA     NA       52
## 5 2016-06-05 14:07:38 Female  52.00   NA     NA       52
## 6 2014-09-23 03:39:56 Female  53.00   NA     NA       53
## 7 2015-01-07 08:57:29   Male  53.77   NA     NA    53.77&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see heights of 53, 54, and 55. In the originals, we also have 51 and 52. These short heights are rare and it is likely that the students actually meant &lt;code&gt;5&#39;1&lt;/code&gt;, &lt;code&gt;5&#39;2&lt;/code&gt;, &lt;code&gt;5&#39;3&lt;/code&gt;, &lt;code&gt;5&#39;4&lt;/code&gt;, and &lt;code&gt;5&#39;5&lt;/code&gt;. Because we are not completely sure, we will leave them as reported. The object &lt;code&gt;new_heights&lt;/code&gt; contains our final solution for this case study.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;string-splitting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;String splitting&lt;/h2&gt;
&lt;p&gt;Another very common data wrangling operation is string splitting. To illustrate how this comes up, we start with an illustrative example. Suppose we did not have the function &lt;code&gt;read_csv&lt;/code&gt; or &lt;code&gt;read.csv&lt;/code&gt; available to us. We instead have to read a csv file using the base R function &lt;code&gt;readLines&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filename &amp;lt;- system.file(&amp;quot;extdata/murders.csv&amp;quot;, package = &amp;quot;dslabs&amp;quot;)
lines &amp;lt;- readLines(filename)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function reads-in the data line-by-line to create a vector of strings. In this case, one string for each row in the spreadsheet. The first six lines are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lines %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;state,abb,region,population,total&amp;quot; &amp;quot;Alabama,AL,South,4779736,135&amp;quot;     
## [3] &amp;quot;Alaska,AK,West,710231,19&amp;quot;          &amp;quot;Arizona,AZ,West,6392017,232&amp;quot;      
## [5] &amp;quot;Arkansas,AR,South,2915918,93&amp;quot;      &amp;quot;California,CA,West,37253956,1257&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to extract the values that are separated by a comma for each string in the vector. The command &lt;code&gt;str_split&lt;/code&gt; does exactly this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- str_split(lines, &amp;quot;,&amp;quot;)
x %&amp;gt;% head(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;state&amp;quot;      &amp;quot;abb&amp;quot;        &amp;quot;region&amp;quot;     &amp;quot;population&amp;quot; &amp;quot;total&amp;quot;     
## 
## [[2]]
## [1] &amp;quot;Alabama&amp;quot; &amp;quot;AL&amp;quot;      &amp;quot;South&amp;quot;   &amp;quot;4779736&amp;quot; &amp;quot;135&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the first entry has the column names, so we can separate that out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;col_names &amp;lt;- x[[1]]
x &amp;lt;- x[-1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To convert our list into a data frame, we can use a shortcut provided by the &lt;code&gt;map&lt;/code&gt; functions in the &lt;strong&gt;purrr&lt;/strong&gt; package. The map function applies the same function to each element in a list. So if we want to extract the first entry of each element in &lt;code&gt;x&lt;/code&gt;, we can write:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)
map(x, function(y) y[1]) %&amp;gt;% head(2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;Alabama&amp;quot;
## 
## [[2]]
## [1] &amp;quot;Alaska&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, because this is such a common task, &lt;strong&gt;purrr&lt;/strong&gt; provides a shortcut. If the second argument receives an integer instead of a function, it assumes we want that entry. So the code above can be written more efficiently like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(x, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To force &lt;code&gt;map&lt;/code&gt; to return a character vector instead of a list, we can use &lt;code&gt;map_chr&lt;/code&gt;. Similarly, &lt;code&gt;map_int&lt;/code&gt; returns integers. So to create our data frame, we can use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- tibble(map_chr(x, 1),
              map_chr(x, 2),
              map_chr(x, 3),
              map_chr(x, 4),
              map_chr(x, 5)) %&amp;gt;%
  mutate_all(parse_guess) %&amp;gt;%
  setNames(col_names)
dat %&amp;gt;% head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 5
##   state      abb   region population total
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Alabama    AL    South     4779736   135
## 2 Alaska     AK    West       710231    19
## 3 Arizona    AZ    West      6392017   232
## 4 Arkansas   AR    South     2915918    93
## 5 California CA    West     37253956  1257
## 6 Colorado   CO    West      5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you learn more about the &lt;strong&gt;purrr&lt;/strong&gt; package, you will learn that you perform the above with the following, more efficient, code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- x %&amp;gt;%
  transpose() %&amp;gt;%
  map( ~ parse_guess(unlist(.))) %&amp;gt;%
  setNames(col_names) %&amp;gt;%
  as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns out that we can avoid all the work shown above after the call to &lt;code&gt;str_split&lt;/code&gt;. Specifically, if we know that the data we are extracting can be represented as a table, we can use the argument &lt;code&gt;simplify=TRUE&lt;/code&gt; and &lt;code&gt;str_split&lt;/code&gt; returns a matrix instead of a list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- str_split(lines, &amp;quot;,&amp;quot;, simplify = TRUE)
col_names &amp;lt;- x[1,]
x &amp;lt;- x[-1,]
colnames(x) &amp;lt;- col_names
x %&amp;gt;% as_tibble() %&amp;gt;%
  mutate_all(parse_guess) %&amp;gt;%
  head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 5
##   state      abb   region population total
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Alabama    AL    South     4779736   135
## 2 Alaska     AK    West       710231    19
## 3 Arizona    AZ    West      6392017   232
## 4 Arkansas   AR    South     2915918    93
## 5 California CA    West     37253956  1257&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-3-extracting-tables-from-a-pdf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study 3: extracting tables from a PDF&lt;/h2&gt;
&lt;p&gt;One of the datasets provided in &lt;strong&gt;dslabs&lt;/strong&gt; shows scientific funding rates by gender in the Netherlands:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;research_funding_rates&amp;quot;)
research_funding_rates %&amp;gt;%
  dplyr::select(&amp;quot;discipline&amp;quot;, &amp;quot;success_rates_men&amp;quot;, &amp;quot;success_rates_women&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            discipline success_rates_men success_rates_women
## 1   Chemical sciences              26.5                25.6
## 2   Physical sciences              19.3                23.1
## 3             Physics              26.9                22.2
## 4          Humanities              14.3                19.3
## 5  Technical sciences              15.9                21.0
## 6   Interdisciplinary              11.4                21.8
## 7 Earth/life sciences              24.4                14.3
## 8     Social sciences              15.3                11.5
## 9    Medical sciences              18.8                11.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data comes from a paper published in the Proceedings of the National Academy of Science (PNAS)&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;, a widely read scientific journal. However, the data is not provided in a spreadsheet; it is in a table in a PDF document. Here is a screenshot of the table:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./12-content_files/pnas-table-s1.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: Romy van der Lee and Naomi Ellemers, PNAS 2015 112 (40) 12349-12353&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;We could extract the numbers by hand, but this could lead to human error. Instead, we can try to wrangle the data using R. We start by downloading the pdf document, then importing into R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;pdftools&amp;quot;)
temp_file &amp;lt;- tempfile()
url &amp;lt;- paste0(&amp;quot;https://www.pnas.org/content/suppl/2015/09/16/&amp;quot;,
              &amp;quot;1510159112.DCSupplemental/pnas.201510159SI.pdf&amp;quot;)
download.file(url, temp_file)
txt &amp;lt;- pdf_text(temp_file)
file.remove(temp_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we examine the object text, we notice that it is a character vector with an entry for each page. So we keep the page we want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raw_data_research_funding_rates &amp;lt;- txt[2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The steps above can actually be skipped because we include this raw data in the &lt;strong&gt;dslabs&lt;/strong&gt; package as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;raw_data_research_funding_rates&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Examining the object &lt;code&gt;raw_data_research_funding_rates&lt;/code&gt;
we see that it is a long string and each line on the page, including the table rows, are separated by the symbol for newline: &lt;code&gt;\n&lt;/code&gt;. We therefore can create a list with the lines of the text as elements as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- str_split(raw_data_research_funding_rates, &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we start off with just one element in the string, we end up with a list with just one entry.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- tab[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By examining &lt;code&gt;tab&lt;/code&gt; we see that the information for the column names is the third and fourth entries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_names_1 &amp;lt;- tab[3]
the_names_2 &amp;lt;- tab[4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first of these rows looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                                                       Applications, n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Awards, n                      Success rates, %&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to create one vector with one name for each column. Using some of the functions we have just learned, we do this. Let’s start with &lt;code&gt;the_names_1&lt;/code&gt;, shown above. We want to remove the leading space and anything following the comma. We use regex for the latter. Then we can obtain the elements by splitting strings separated by space. We want to split only when there are 2 or more spaces to avoid splitting &lt;code&gt;Success rates&lt;/code&gt;. So we use the regex &lt;code&gt;\\s{2,}&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_names_1 &amp;lt;- the_names_1 %&amp;gt;%
  str_trim() %&amp;gt;%
  str_replace_all(&amp;quot;,\\s.&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
  str_split(&amp;quot;\\s{2,}&amp;quot;, simplify = TRUE)
the_names_1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]           [,2]     [,3]           
## [1,] &amp;quot;Applications&amp;quot; &amp;quot;Awards&amp;quot; &amp;quot;Success rates&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will look at &lt;code&gt;the_names_2&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                         Discipline              Total     Men      Women&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Total    Men       Women          Total    Men      Women&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we want to trim the leading space and then split by space as we did for the first line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_names_2 &amp;lt;- the_names_2 %&amp;gt;%
  str_trim() %&amp;gt;%
  str_split(&amp;quot;\\s+&amp;quot;, simplify = TRUE)
the_names_2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]         [,2]    [,3]  [,4]    [,5]    [,6]  [,7]    [,8]    [,9] 
## [1,] &amp;quot;Discipline&amp;quot; &amp;quot;Total&amp;quot; &amp;quot;Men&amp;quot; &amp;quot;Women&amp;quot; &amp;quot;Total&amp;quot; &amp;quot;Men&amp;quot; &amp;quot;Women&amp;quot; &amp;quot;Total&amp;quot; &amp;quot;Men&amp;quot;
##      [,10]  
## [1,] &amp;quot;Women&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then join these to generate one name for each column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp_names &amp;lt;- str_c(rep(the_names_1, each = 3), the_names_2[-1], sep = &amp;quot;_&amp;quot;)
the_names &amp;lt;- c(the_names_2[1], tmp_names) %&amp;gt;%
  str_to_lower() %&amp;gt;%
  str_replace_all(&amp;quot;\\s&amp;quot;, &amp;quot;_&amp;quot;)
the_names&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;discipline&amp;quot;          &amp;quot;applications_total&amp;quot;  &amp;quot;applications_men&amp;quot;   
##  [4] &amp;quot;applications_women&amp;quot;  &amp;quot;awards_total&amp;quot;        &amp;quot;awards_men&amp;quot;         
##  [7] &amp;quot;awards_women&amp;quot;        &amp;quot;success_rates_total&amp;quot; &amp;quot;success_rates_men&amp;quot;  
## [10] &amp;quot;success_rates_women&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we are ready to get the actual data. By examining the &lt;code&gt;tab&lt;/code&gt; object, we notice that the information is in lines 6 through 14. We can use &lt;code&gt;str_split&lt;/code&gt; again to achieve our goal:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_research_funding_rates &amp;lt;- tab[6:14] %&amp;gt;%
  str_trim %&amp;gt;%
  str_split(&amp;quot;\\s{2,}&amp;quot;, simplify = TRUE) %&amp;gt;%
  data.frame(stringsAsFactors = FALSE) %&amp;gt;%
  setNames(the_names) %&amp;gt;%
  mutate_at(-1, parse_number)
new_research_funding_rates %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 10
##   discipline    applications_tot… applications_men applications_wo… awards_total
##   &amp;lt;chr&amp;gt;                     &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 Chemical sci…               122               83               39           32
## 2 Physical sci…               174              135               39           35
## 3 Physics                      76               67                9           20
## 4 Humanities                  396              230              166           65
## 5 Technical sc…               251              189               62           43
## 6 Interdiscipl…               183              105               78           29
## 7 Earth/life s…               282              156              126           56
## 8 Social scien…               834              425              409          112
## 9 Medical scie…               505              245              260           75
## # … with 5 more variables: awards_men &amp;lt;dbl&amp;gt;, awards_women &amp;lt;dbl&amp;gt;,
## #   success_rates_total &amp;lt;dbl&amp;gt;, success_rates_men &amp;lt;dbl&amp;gt;,
## #   success_rates_women &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the objects are identical:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;identical(research_funding_rates, new_research_funding_rates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;recode&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recoding&lt;/h2&gt;
&lt;p&gt;Another common operation involving strings is recoding the names of categorical variables. Let’s say you have really long names for your levels and you will be displaying them in plots, you might want to use shorter versions of these names. For example, in character vectors with country names, you might want to change “United States of America” to “USA” and “United Kingdom” to UK, and so on. We can do this with &lt;code&gt;case_when&lt;/code&gt;, although the &lt;strong&gt;tidyverse&lt;/strong&gt; offers an option that is specifically designed for this task: the &lt;code&gt;recode&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Here is an example that shows how to rename countries with long names:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;gapminder&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Suppose we want to show life expectancy time series by country for the Caribbean:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  dplyr::filter(region == &amp;quot;Caribbean&amp;quot;) %&amp;gt;%
  ggplot(aes(year, life_expectancy, color = country)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/14-content_files/figure-html/caribbean-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot is what we want, but much of the space is wasted to accommodate some of the long country names.
&lt;!--

```r
gapminder %&gt;%
  dplyr::filter(region == &#34;Caribbean&#34;) %&gt;%
  dplyr::filter(str_length(country) &gt;= 12) %&gt;%
  distinct(country)
```

```
##                          country
## 1            Antigua and Barbuda
## 2             Dominican Republic
## 3 St. Vincent and the Grenadines
## 4            Trinidad and Tobago
```
--&gt;
We have four countries with names longer than 12 characters. These names appear once for each year in the Gapminder dataset. Once we pick nicknames, we need to change them all consistently. The &lt;code&gt;recode&lt;/code&gt; function can be used to do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;% dplyr::filter(region==&amp;quot;Caribbean&amp;quot;) %&amp;gt;%
  mutate(country = recode(country,
                          `Antigua and Barbuda` = &amp;quot;Barbuda&amp;quot;,
                          `Dominican Republic` = &amp;quot;DR&amp;quot;,
                          `St. Vincent and the Grenadines` = &amp;quot;St. Vincent&amp;quot;,
                          `Trinidad and Tobago` = &amp;quot;Trinidad&amp;quot;)) %&amp;gt;%
  ggplot(aes(year, life_expectancy, color = country)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/14-content_files/figure-html/caribbean-with-nicknames-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are other similar functions in other R packages, such as &lt;code&gt;recode_factor&lt;/code&gt; and &lt;code&gt;fct_recoder&lt;/code&gt; in the &lt;strong&gt;forcats&lt;/strong&gt; package.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Complete all lessons and exercises in the &lt;a href=&#34;https://regexone.com/&#34;&gt;https://regexone.com/&lt;/a&gt; online interactive tutorial.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the &lt;code&gt;extdata&lt;/code&gt; directory of the &lt;strong&gt;dslabs&lt;/strong&gt; package, you will find a PDF file containing daily mortality data for Puerto Rico from Jan 1, 2015 to May 31, 2018. You can find the file like this:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fn &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, &amp;quot;RD-Mortality-Report_2015-18-180531.pdf&amp;quot;,
                  package=&amp;quot;dslabs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Find and open the file or open it directly from RStudio. On a Mac, you can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;open&amp;quot;, args = fn)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and on Windows, you can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system(&amp;quot;cmd.exe&amp;quot;, input = paste(&amp;quot;start&amp;quot;, fn))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which of the following best describes this file:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;It is a table. Extracting the data will be easy.&lt;/li&gt;
&lt;li&gt;It is a report written in prose. Extracting the data will be impossible.&lt;/li&gt;
&lt;li&gt;It is a report combining graphs and tables. Extracting the data seems possible.&lt;/li&gt;
&lt;li&gt;It shows graphs of the data. Extracting the data will be difficult.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We are going to create a tidy dataset with each row representing one observation. The variables in this dataset will be year, month, day, and deaths.
Start by installing and loading the &lt;strong&gt;pdftools&lt;/strong&gt; package:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;pdftools&amp;quot;)
library(pdftools)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now read-in &lt;code&gt;fn&lt;/code&gt; using the &lt;code&gt;pdf_text&lt;/code&gt; function and store the results in an object called &lt;code&gt;txt&lt;/code&gt;. Which of the following best describes what you see in &lt;code&gt;txt&lt;/code&gt;?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;A table with the mortality data.&lt;/li&gt;
&lt;li&gt;A character string of length 12. Each entry represents the text in each page. The mortality data is in there somewhere.&lt;/li&gt;
&lt;li&gt;A character string with one entry containing all the information in the PDF file.&lt;/li&gt;
&lt;li&gt;An html document.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Extract the ninth page of the PDF file from the object &lt;code&gt;txt&lt;/code&gt;, then use the &lt;code&gt;str_split&lt;/code&gt; from the &lt;strong&gt;stringr&lt;/strong&gt; package so that you have each line in a different entry. Call this string vector &lt;code&gt;s&lt;/code&gt;. Then look at the result and choose the one that best describes what you see.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;It is an empty string.&lt;/li&gt;
&lt;li&gt;I can see the figure shown in page 1.&lt;/li&gt;
&lt;li&gt;It is a tidy table.&lt;/li&gt;
&lt;li&gt;I can see the table! But there is a bunch of other stuff we need to get rid of.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What kind of object is &lt;code&gt;s&lt;/code&gt; and how many entries does it have?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We see that the output is a list with one component. Redefine &lt;code&gt;s&lt;/code&gt; to be the first entry of the list. What kind of object is &lt;code&gt;s&lt;/code&gt; and how many entries does it have?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When inspecting the string we obtained above, we see a common problem: white space before and after the other characters. Trimming is a common first step in string processing. These extra spaces will eventually make splitting the strings hard so we start by removing them. We learned about the command &lt;code&gt;str_trim&lt;/code&gt; that removes spaces at the start or end of the strings. Use this function to trim &lt;code&gt;s&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We want to extract the numbers from the strings stored in &lt;code&gt;s&lt;/code&gt;. However, there are many non-numeric characters that will get in the way. We can remove these, but before doing this we want to preserve the string with the column header, which includes the month abbreviation.
Use the &lt;code&gt;str_which&lt;/code&gt; function to find the rows with a header. Save these results to &lt;code&gt;header_index&lt;/code&gt;. Hint: find the first string that matches the pattern &lt;code&gt;2015&lt;/code&gt; using the &lt;code&gt;str_which&lt;/code&gt; function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we are going to define two objects: &lt;code&gt;month&lt;/code&gt; will store the month and &lt;code&gt;header&lt;/code&gt; will store the column names. Identify which row contains the header of the table. Save the content of the row into an object called &lt;code&gt;header&lt;/code&gt;, then use &lt;code&gt;str_split&lt;/code&gt; to help define the two objects we need. Hints: the separator here is one or more spaces. Also, consider using the &lt;code&gt;simplify&lt;/code&gt; argument.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Notice that towards the end of the page you see a &lt;em&gt;totals&lt;/em&gt; row followed by rows with other summary statistics. Create an object called &lt;code&gt;tail_index&lt;/code&gt; with the index of the &lt;em&gt;totals&lt;/em&gt; entry.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Because our PDF page includes graphs with numbers, some of our rows have just one number (from the y-axis of the plot). Use the &lt;code&gt;str_count&lt;/code&gt; function to create an object &lt;code&gt;n&lt;/code&gt; with the number of numbers in each each row. Hint: you can write a regex for number like this &lt;code&gt;\\d+&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We are now ready to remove entries from rows that we know we don’t need. The entry &lt;code&gt;header_index&lt;/code&gt; and everything before it should be removed. Entries for which &lt;code&gt;n&lt;/code&gt; is 1 should also be removed, and the entry &lt;code&gt;tail_index&lt;/code&gt; and everything that comes after it should be removed as well.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we are ready to remove all the non-numeric entries. Do this using regex and the &lt;code&gt;str_remove_all&lt;/code&gt; function. Hint: remember that in regex, using the upper case version of a special character usually means the opposite. So &lt;code&gt;\\D&lt;/code&gt; means “not a digit”. Remember you also want to keep spaces.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To convert the strings into a table, use the &lt;code&gt;str_split_fixed&lt;/code&gt; function. Convert &lt;code&gt;s&lt;/code&gt; into a data matrix with just the day and death count data. Hints: note that the separator is one or more spaces. Make the argument &lt;code&gt;n&lt;/code&gt; a value that limits the number of columns to the values in the 4 columns and the last column captures all the extra stuff. Then keep only the first four columns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now you are almost ready to finish. Add column names to the matrix, including one called &lt;code&gt;day&lt;/code&gt;. Also, add a column with the month. Call the resulting object &lt;code&gt;dat&lt;/code&gt;. Finally, make sure the day is an integer not a character. Hint: use only the first five columns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now finish it up by tidying &lt;code&gt;tab&lt;/code&gt; with the gather function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make a plot of deaths versus day with color to denote year. Exclude 2018 since we do not have data for the entire year.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now that we have wrangled this data step-by-step, put it all together in one R chunk, using the pipe as much as possible. Hint: first define the indexes, then write one line of code that does all the string processing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Advanced: let’s return to the MLB Payroll example from the web scraping section. Use what you have learned in the web scraping and string processing chapters to extract the payroll for the New York Yankees, Boston Red Sox, and Oakland A’s and plot them as a function of time.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;web-scraping&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Web scraping&lt;/h1&gt;
&lt;p&gt;The data we need to answer a question is not always in a spreadsheet ready for us to read. For example, the US murders dataset we used in the R Basics chapter originally comes from this Wikipedia page:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- paste0(&amp;quot;https://en.wikipedia.org/w/index.php?title=&amp;quot;,
              &amp;quot;Gun_violence_in_the_United_States_by_state&amp;quot;,
              &amp;quot;&amp;amp;direction=prev&amp;amp;oldid=810166167&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the data table when you visit the webpage:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./12-content_files/murders-data-wiki-page.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Web page courtesy of Wikipedia&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;. CC-BY-SA-3.0 license&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. Screenshot of part of the page.)&lt;/p&gt;
&lt;p&gt;Unfortunately, there is no link to a data file. To make the data frame that is loaded when we type &lt;code&gt;data(murders)&lt;/code&gt;, we had to do some &lt;em&gt;web scraping&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Web scraping&lt;/em&gt;, or &lt;em&gt;web harvesting&lt;/em&gt;, is the term we use to describe the process of extracting data from a website. The reason we can do this is because the information used by a browser to render webpages is received as a text file from a server. The text is code written in hyper text markup language (HTML). Every browser has a way to show the html source code for a page, each one different. On Chrome, you can use Control-U on a PC and command+alt+U on a Mac. You will see something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./12-content_files/html-code.png&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;html&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;HTML&lt;/h2&gt;
&lt;p&gt;Because this code is accessible, we can download the HTML file, import it into R, and then write programs to extract the information we need from the page. However, once we look at HTML code, this might seem like a daunting task. But we will show you some convenient tools to facilitate the process. To get an idea of how it works, here are a few lines of code from the Wikipedia page that provides the US murders data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;table class=&amp;quot;wikitable sortable&amp;quot;&amp;gt;
&amp;lt;tr&amp;gt;
&amp;lt;th&amp;gt;State&amp;lt;/th&amp;gt;
&amp;lt;th&amp;gt;&amp;lt;a href=&amp;quot;/wiki/List_of_U.S._states_and_territories_by_population&amp;quot;
title=&amp;quot;List of U.S. states and territories by population&amp;quot;&amp;gt;Population&amp;lt;/a&amp;gt;&amp;lt;br /&amp;gt;
&amp;lt;small&amp;gt;(total inhabitants)&amp;lt;/small&amp;gt;&amp;lt;br /&amp;gt;
&amp;lt;small&amp;gt;(2015)&amp;lt;/small&amp;gt; &amp;lt;sup id=&amp;quot;cite_ref-1&amp;quot; class=&amp;quot;reference&amp;quot;&amp;gt;
&amp;lt;a href=&amp;quot;#cite_note-1&amp;quot;&amp;gt;[1]&amp;lt;/a&amp;gt;&amp;lt;/sup&amp;gt;&amp;lt;/th&amp;gt;
&amp;lt;th&amp;gt;Murders and Nonnegligent
&amp;lt;p&amp;gt;Manslaughter&amp;lt;br /&amp;gt;
&amp;lt;small&amp;gt;(total deaths)&amp;lt;/small&amp;gt;&amp;lt;br /&amp;gt;
&amp;lt;small&amp;gt;(2015)&amp;lt;/small&amp;gt; &amp;lt;sup id=&amp;quot;cite_ref-2&amp;quot; class=&amp;quot;reference&amp;quot;&amp;gt;
&amp;lt;a href=&amp;quot;#cite_note-2&amp;quot;&amp;gt;[2]&amp;lt;/a&amp;gt;&amp;lt;/sup&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/th&amp;gt;
&amp;lt;th&amp;gt;Murder and Nonnegligent
&amp;lt;p&amp;gt;Manslaughter Rate&amp;lt;br /&amp;gt;
&amp;lt;small&amp;gt;(per 100,000 inhabitants)&amp;lt;/small&amp;gt;&amp;lt;br /&amp;gt;
&amp;lt;small&amp;gt;(2015)&amp;lt;/small&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/th&amp;gt;
&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;
&amp;lt;td&amp;gt;&amp;lt;a href=&amp;quot;/wiki/Alabama&amp;quot; title=&amp;quot;Alabama&amp;quot;&amp;gt;Alabama&amp;lt;/a&amp;gt;&amp;lt;/td&amp;gt;
&amp;lt;td&amp;gt;4,853,875&amp;lt;/td&amp;gt;
&amp;lt;td&amp;gt;348&amp;lt;/td&amp;gt;
&amp;lt;td&amp;gt;7.2&amp;lt;/td&amp;gt;
&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;
&amp;lt;td&amp;gt;&amp;lt;a href=&amp;quot;/wiki/Alaska&amp;quot; title=&amp;quot;Alaska&amp;quot;&amp;gt;Alaska&amp;lt;/a&amp;gt;&amp;lt;/td&amp;gt;
&amp;lt;td&amp;gt;737,709&amp;lt;/td&amp;gt;
&amp;lt;td&amp;gt;59&amp;lt;/td&amp;gt;
&amp;lt;td&amp;gt;8.0&amp;lt;/td&amp;gt;
&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can actually see the data, except data values are surrounded by html code such as &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt;. We can also see a pattern of how it is stored. If you know HTML, you can write programs that leverage knowledge of these patterns to extract what we want. We also take advantage of a language widely used to make webpages look “pretty” called Cascading Style Sheets (CSS). We say more about this in Section &lt;a href=&#34;#css-selectors&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although we provide tools that make it possible to scrape data without knowing HTML, as a data scientist it is quite useful to learn some HTML and CSS. Not only does this improve your scraping skills, but it might come in handy if you are creating a webpage to showcase your work. There are plenty of online courses and tutorials for learning these. Two examples are Codeacademy&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; and W3schools&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-rvest-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The rvest package&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;tidyverse&lt;/strong&gt; provides a web harvesting package called &lt;strong&gt;rvest&lt;/strong&gt;. The first step using this package is to import the webpage into R. The package makes this quite simple:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rvest)
h &amp;lt;- read_html(url)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the entire Murders in the US Wikipedia webpage is now contained in &lt;code&gt;h&lt;/code&gt;. The class of this object is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(h)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;xml_document&amp;quot; &amp;quot;xml_node&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;rvest&lt;/strong&gt; package is actually more general; it handles XML documents. XML is a general markup language (that’s what the ML stands for) that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing webpages. Here we focus on HTML documents.&lt;/p&gt;
&lt;p&gt;Now, how do we extract the table from the object &lt;code&gt;h&lt;/code&gt;? If we print &lt;code&gt;h&lt;/code&gt;, we don’t really see much:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {html_document}
## &amp;lt;html class=&amp;quot;client-nojs&amp;quot; lang=&amp;quot;en&amp;quot; dir=&amp;quot;ltr&amp;quot;&amp;gt;
## [1] &amp;lt;head&amp;gt;\n&amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset=UTF-8 ...
## [2] &amp;lt;body class=&amp;quot;mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see all the code that defines the downloaded webpage using the &lt;code&gt;html_text&lt;/code&gt; function like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html_text(h)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We don’t show the output here because it includes thousands of characters, but if we look at it, we can see the data we are after are stored in an HTML table: you can see this in this line of the HTML code above &lt;code&gt;&amp;lt;table class=&#34;wikitable sortable&#34;&amp;gt;&lt;/code&gt;. The different parts of an HTML document, often defined with a message in between &lt;code&gt;&amp;lt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&lt;/code&gt; are referred to as &lt;em&gt;nodes&lt;/em&gt;. The &lt;strong&gt;rvest&lt;/strong&gt; package includes functions to extract nodes of an HTML document: &lt;code&gt;html_nodes&lt;/code&gt; extracts all nodes of different types and &lt;code&gt;html_node&lt;/code&gt; extracts the first one. To extract the tables from the html code we use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- h %&amp;gt;% html_nodes(&amp;quot;table&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, instead of the entire webpage, we just have the html code for the tables in the page:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (2)}
## [1] &amp;lt;table class=&amp;quot;wikitable sortable&amp;quot;&amp;gt;&amp;lt;tbody&amp;gt;\n&amp;lt;tr&amp;gt;\n&amp;lt;th&amp;gt;State\n&amp;lt;/th&amp;gt;\n&amp;lt;th&amp;gt;\n ...
## [2] &amp;lt;table class=&amp;quot;nowraplinks hlist mw-collapsible mw-collapsed navbox-inner&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The table we are interested is the first one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {html_node}
## &amp;lt;table class=&amp;quot;wikitable sortable&amp;quot;&amp;gt;
## [1] &amp;lt;tbody&amp;gt;\n&amp;lt;tr&amp;gt;\n&amp;lt;th&amp;gt;State\n&amp;lt;/th&amp;gt;\n&amp;lt;th&amp;gt;\n&amp;lt;a href=&amp;quot;/wiki/List_of_U.S._states ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, &lt;strong&gt;rvest&lt;/strong&gt; includes a function just for converting HTML tables into data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- tab[[1]] %&amp;gt;% html_table
class(tab)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;tbl_df&amp;quot;     &amp;quot;tbl&amp;quot;        &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now much closer to having a usable data table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- tab %&amp;gt;% setNames(c(&amp;quot;state&amp;quot;, &amp;quot;population&amp;quot;, &amp;quot;total&amp;quot;, &amp;quot;murder_rate&amp;quot;))
head(tab)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 4
##   state      population total murder_rate
##   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Alabama    4,853,875  348           7.2
## 2 Alaska     737,709    59            8  
## 3 Arizona    6,817,565  309           4.5
## 4 Arkansas   2,977,853  181           6.1
## 5 California 38,993,940 1,861         4.8
## 6 Colorado   5,448,819  176           3.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;css-selectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CSS selectors&lt;/h2&gt;
&lt;p&gt;The default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS to define the look and style of webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as &lt;em&gt;selectors&lt;/em&gt;. An example of such a pattern, which we used above, is &lt;code&gt;table&lt;/code&gt;, but there are many, many more.&lt;/p&gt;
&lt;p&gt;If we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the &lt;code&gt;html_nodes&lt;/code&gt; function. However, knowing which selector can be quite complicated.
In fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the nodes that define a particular piece of data. However, selector gadgets actually make this possible.&lt;/p&gt;
&lt;p&gt;SelectorGadget&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables from html pages, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including &lt;strong&gt;rvest&lt;/strong&gt; author Hadley Wickham’s
vignette&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt; and other tutorials based on the vignette&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;json&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;JSON&lt;/h2&gt;
&lt;p&gt;Sharing data on the internet has become more and more common. Unfortunately, providers use different formats, which makes it harder for data scientists to wrangle data into R. Yet there are some standards that are also becoming more common. Currently, a format that is widely being adopted is the JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. This JSON file looks more like the code you use to define a list. Here is an example of information stored in a JSON format:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;jsonlite&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     flatten&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [
##   {
##     &amp;quot;name&amp;quot;: &amp;quot;Miguel&amp;quot;,
##     &amp;quot;student_id&amp;quot;: 1,
##     &amp;quot;exam_1&amp;quot;: 85,
##     &amp;quot;exam_2&amp;quot;: 86
##   },
##   {
##     &amp;quot;name&amp;quot;: &amp;quot;Sofia&amp;quot;,
##     &amp;quot;student_id&amp;quot;: 2,
##     &amp;quot;exam_1&amp;quot;: 94,
##     &amp;quot;exam_2&amp;quot;: 93
##   },
##   {
##     &amp;quot;name&amp;quot;: &amp;quot;Aya&amp;quot;,
##     &amp;quot;student_id&amp;quot;: 3,
##     &amp;quot;exam_1&amp;quot;: 87,
##     &amp;quot;exam_2&amp;quot;: 88
##   },
##   {
##     &amp;quot;name&amp;quot;: &amp;quot;Cheng&amp;quot;,
##     &amp;quot;student_id&amp;quot;: 4,
##     &amp;quot;exam_1&amp;quot;: 90,
##     &amp;quot;exam_2&amp;quot;: 91
##   }
## ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file above actually represents a data frame. To read it, we can use the function &lt;code&gt;fromJSON&lt;/code&gt; from the &lt;strong&gt;jsonlite&lt;/strong&gt; package. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and obtain data. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(jsonlite)
res &amp;lt;- fromJSON(&amp;#39;http://ergast.com/api/f1/2004/1/results.json&amp;#39;)

citi_bike &amp;lt;- fromJSON(&amp;quot;http://citibikenyc.com/stations/json&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This downloads a list. The first argument tells you when you downloaded it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;citi_bike$executionTime&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the second is a data table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;citi_bike$stationBeanList %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can learn much more by examining tutorials and help files from the &lt;strong&gt;jsonlite&lt;/strong&gt; package. This package is intended for relatively simple tasks such as converting data into tables. For more flexibility, we recommend &lt;code&gt;rjson&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.regular-expressions.info/tutorial.html&#34; class=&#34;uri&#34;&gt;https://www.regular-expressions.info/tutorial.html&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf&#34; class=&#34;uri&#34;&gt;https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf&#34; class=&#34;uri&#34;&gt;https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.pnas.org/content/112/40/12349.abstract&#34; class=&#34;uri&#34;&gt;http://www.pnas.org/content/112/40/12349.abstract&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.pnas.org/content/112/40/12349&#34; class=&#34;uri&#34;&gt;http://www.pnas.org/content/112/40/12349&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Gun_violence_in_the_United_States_by_state&amp;amp;direction=prev&amp;amp;oldid=810166167&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/w/index.php?title=Gun_violence_in_the_United_States_by_state&amp;amp;direction=prev&amp;amp;oldid=810166167&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.codecademy.com/learn/learn-html&#34; class=&#34;uri&#34;&gt;https://www.codecademy.com/learn/learn-html&lt;/a&gt;&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.w3schools.com/&#34; class=&#34;uri&#34;&gt;https://www.w3schools.com/&lt;/a&gt;&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://selectorgadget.com/&#34; class=&#34;uri&#34;&gt;http://selectorgadget.com/&lt;/a&gt;&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html&lt;/a&gt;&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/&#34; class=&#34;uri&#34;&gt;https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/&lt;/a&gt;&lt;a href=&#34;#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/&#34; class=&#34;uri&#34;&gt;https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/&lt;/a&gt;&lt;a href=&#34;#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Geospatial with R</title>
      <link>https://ssc442.netlify.app/content/13-content/</link>
      <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/13-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geospatial-in-r&#34;&gt;Geospatial in R&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#vector-vs.-raster&#34;&gt;Vector vs. Raster&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#vectors-are-scalable.-rasters-are-not&#34;&gt;Vectors are scalable. Rasters are not&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors-points-lines-and-polygons&#34;&gt;Vectors: points, lines, and polygons&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#making-polygons&#34;&gt;Making polygons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-points-on-a-plot&#34;&gt;Getting points on a plot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#making-lines&#34;&gt;Making lines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reading-spatial-data&#34;&gt;Reading spatial data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#where-to-find-spatial-data&#34;&gt;Where to find spatial data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loading-the-data&#34;&gt;Loading the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#projections-briefly&#34;&gt;Projections, briefly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#spatial-merges&#34;&gt;Spatial merges&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#point-to-polygon-merges&#34;&gt;Point-to-polygon merges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mapview&#34;&gt;Mapview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#polygon-to-polygon-merges&#34;&gt;Polygon-to-polygon merges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summarizing&#34;&gt;Summarizing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cropping-vs.-merging&#34;&gt;Cropping vs. merging&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bounding-boxes&#34;&gt;Bounding boxes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distance-matrices&#34;&gt;Distance matrices&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#st_nearest_feature&#34;&gt;st_nearest_feature&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-resources&#34;&gt;Other resources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page. Sort of.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What are the building blocks of geospatial data?&lt;/li&gt;
&lt;li&gt;How do we handle uniquely geospatial properties like distance or spatial correlation?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;geospatial-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Geospatial in R&lt;/h1&gt;
&lt;p&gt;We will need a handful of new packages for our introduction to geospatial analysis in R. The primary package we will interact with is the &lt;code&gt;sf&lt;/code&gt; package. &lt;code&gt;sf&lt;/code&gt; stands for “simple features.” It has become the standard for geospatial work in R, and relies on the &lt;code&gt;rgeos&lt;/code&gt; and &lt;code&gt;rgdal&lt;/code&gt; libraries (which are themselves &lt;code&gt;R&lt;/code&gt; compilations of the &lt;code&gt;geos&lt;/code&gt; and &lt;code&gt;gdal&lt;/code&gt; libraries). Documentation of sf &lt;a href=&#34;https://r-spatial.github.io/sf/&#34;&gt;can be found here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We will also use the &lt;code&gt;mapview&lt;/code&gt; package, as well as the &lt;code&gt;tmaptools&lt;/code&gt; package. Plus, we’ll use &lt;code&gt;tigris&lt;/code&gt; to get state boundaries and &lt;code&gt;tidycensus&lt;/code&gt; to pull down census maps. Install those, and any of the many dependencies that they also install.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sf)
library(mapview)
library(tigris)
library(tidycensus)
library(tidyverse)
library(tmaptools)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;vector-vs.-raster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vector vs. Raster&lt;/h2&gt;
&lt;p&gt;There are two ways of storing 2-D mapped spatial data, &lt;em&gt;raster&lt;/em&gt; and &lt;em&gt;vector&lt;/em&gt;. A &lt;em&gt;vector&lt;/em&gt; representation of a 2-D shape is best described as an irregular polygon with points defining vertices. A square plotted in cartesian coordinates is a vector representation. Conversely, a &lt;em&gt;raster&lt;/em&gt; image is a grid of cells where each cell is defined as “in” or “out” of the square. Most computer graphics like JPEG and TIFF are raster graphics and each pixel has an assigned color. To make a raster image of a blue square, we’d make a big grid of pixels, and then color some blue based on their location. To make a blue square in vector form, we’d record &lt;em&gt;just the location of the corners&lt;/em&gt; and add instructions to color inside the polygon formed by those corners blue.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vector-conversions.com/images/vector_vs_raster.jpg&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;vectors-are-scalable.-rasters-are-not&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vectors are scalable. Rasters are not&lt;/h3&gt;
&lt;p&gt;Rasters are great for detail, like pixels in a picture, but they do not scale up very well. Vectors are great for things that do need to scale up. They are also smaller and easier to work with when they aren’t trying to replicate photo-realistic images. Vectors can handle curves by recording the properties of the curve (e.g. bezier curves), while rasters have to approximate curves along the grid of cells, so if you want a smooth curve, you need lots of cells.&lt;/p&gt;
&lt;p&gt;Geospatial work is almost always done in vectors because (1) it is easier to store data as vectors, and (2) it is easier to manipulate, project, intersect, or connect vector points, lines, and polygons.&lt;/p&gt;
&lt;p&gt;We are going to work entirely with vectors today.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors-points-lines-and-polygons&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Vectors: points, lines, and polygons&lt;/h1&gt;
&lt;p&gt;Most everything we would want to map can be represented as a point, a line, or a polygon. Points could be the location of power plants in the US, or the location of COVID cases, or the location of major intersections. Lines could be the location of train tracks, the shortest distance between someone’s house and the nearest restaurants, or a major road. Polygons could be county boundaries, landowner’s lot lines, or bodies of water.&lt;/p&gt;
&lt;p&gt;We can start by making some points, then turning them into a polygon. We’ll just use arbitrary coordinates for now, but will move into GPS latitude-longitude coordinates shortly. We’ll use &lt;code&gt;st_multipoint&lt;/code&gt; to create our points object. It takes a numeric matrix only.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myPoints = tribble(~X, ~Y,
                   0, 0,
                   0, 4,
                   1, 4,
                   1, 1,
                   .5, 0,
                   0, 0)

myPoints = st_multipoint(as.matrix(myPoints))
plot(myPoints)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/13-content_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;making-polygons&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making polygons&lt;/h3&gt;
&lt;p&gt;We’ve begun making our first spatial object! Now, we can turn it into a polygon under one condition: the polygon has to “close” in order for R to know which side is the inside. In &lt;code&gt;myPoints&lt;/code&gt;, the &lt;em&gt;last&lt;/em&gt; point is identical to the &lt;em&gt;first&lt;/em&gt; point, so R will “close” it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(st_polygon(list(myPoints)), col = &amp;#39;darkgreen&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/13-content_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s just one polylgon. Let’s add another one. When we created the polygon, we put the points object, &lt;code&gt;myPoints&lt;/code&gt;, into a list. If we have a list of, say, two points objects, then we’ll get two polygons:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myPoints2 = tribble(~X, ~Y,
                    1,1,
                    2,1,
                    2,2,
                    1,2,
                    1,1)

myPoints2 = st_multipoint(as.matrix(myPoints2))

myPolygons = st_polygon(list(myPoints, myPoints2))
plot(myPolygons, col = &amp;#39;lightblue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/13-content_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can see two polygons. Looking at the &lt;code&gt;str&lt;/code&gt;ucture of the polygons:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(myPolygons)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 2
##  $ : &amp;#39;XY&amp;#39; num [1:6, 1:2] 0 0 1 1 0.5 0 0 4 4 1 ...
##   ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. ..$ : NULL
##   .. ..$ : chr [1:2] &amp;quot;X&amp;quot; &amp;quot;Y&amp;quot;
##  $ : &amp;#39;XY&amp;#39; num [1:5, 1:2] 1 2 2 1 1 1 1 2 2 1
##   ..- attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   .. ..$ : NULL
##   .. ..$ : chr [1:2] &amp;quot;X&amp;quot; &amp;quot;Y&amp;quot;
##  - attr(*, &amp;quot;class&amp;quot;)= chr [1:3] &amp;quot;XY&amp;quot; &amp;quot;POLYGON&amp;quot; &amp;quot;sfg&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that one of the classes is &lt;code&gt;sfg&lt;/code&gt;. This is a &lt;code&gt;sf&lt;/code&gt; package-defined spatial object.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-points-on-a-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting points on a plot&lt;/h3&gt;
&lt;p&gt;One little-known trick in R is super helpful in spatial work. If you &lt;code&gt;plot(myPolygons)&lt;/code&gt; in your own R-studio console (so it appears in your “Plots” pane, not knit into your document), you can use &lt;code&gt;click(n)&lt;/code&gt; to interactively get &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; spatial points in the coordinate system of your plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myClicks = click(n = 3)
myClicks = rbind(myClicks, myClicks[1,])  # copy the first point to the last point to &amp;quot;close&amp;quot;
myNewPolygon = st_polygon(list(st_multipoint(myClicks)))
plot(myPolygons, col = &amp;#39;lightblue&amp;#39;)
plot(myNewPolygon, col = &amp;#39;green&amp;#39;, add=T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myClicks = as.matrix(data.frame(x = c(.37, 1.06, 1.31),
                                y = c(2.71, .47, 2.66)))

myClicks = rbind(myClicks, myClicks[1,])  # copy the first point to the last point to &amp;quot;close&amp;quot;
myNewPolygon = st_polygon(list(st_multipoint(myClicks)))
plot(myPolygons, col = &amp;#39;lightblue&amp;#39;)
plot(myNewPolygon, col = &amp;#39;green&amp;#39;, add=T)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;making-lines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making lines&lt;/h3&gt;
&lt;p&gt;We could also create a line with our points. I’ll leave off the one point we added to “close” the polygon. Note that the line is colored blue, not the (uncompleted) polygon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myLine = st_linestring(myPoints[1:4,])
plot(myLine, col = &amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-spatial-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reading spatial data&lt;/h1&gt;
&lt;p&gt;While it’s fun to draw our own shapes (caution: my definition of fun &lt;span class=&#34;math inline&#34;&gt;\(\neq\)&lt;/span&gt; your definition of fun), we’re probably most interested in finding and using existing spatial data. Let’s talk briefly about the types of spatial data out there:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shapefiles
&lt;ul&gt;
&lt;li&gt;Shapefiles are not actually single files - they’re usually 4-6 files with similar names and different suffixes like .dbf, .shx, etc. This is because the shapefile format kind of pre-dates our current way of thinking of file storage. The most common program for reading or making shapefiles is ESRI’s ArcGIS. It is expensive, cumbersome, and some may say bloated. Our goal in this section is to be able to rescue shapefiles from the clutches of ArcGIS and open them in R&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;GEOJSON
&lt;ul&gt;
&lt;li&gt;JSON is a way of structuring text data (like a .csv) but with the potential for nests in the data (like our &lt;code&gt;list&lt;/code&gt; object) where each nest has a different data structure. GEOJSON pairs this with &lt;strong&gt;WKT&lt;/strong&gt; or Well-Known Text representations of coordinates and takes care of making sure that each observation in the JSON data has a corresponding polygon in WKT coordinates.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;KML
&lt;ul&gt;
&lt;li&gt;Bare-bones storage of coordinates and basic data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;.RDS
&lt;ul&gt;
&lt;li&gt;Okay, this is just R’s native data type for storage, but it’s really helpful for storing &lt;code&gt;sf&lt;/code&gt; objects&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Comma separated values (.csv)
&lt;ul&gt;
&lt;li&gt;Just like the CSV’s we’ve been using, but with Latitude and Longitude columns. Only works for points (one point per .csv line), but is very commonly found. We can use &lt;code&gt;st_as_sf&lt;/code&gt; to tell R which columns are the latitude and longitude.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can open and use any one of these filetypes. I will cover Shapefiles and GEOJSON as the latter has become a very popular way of sharing spatial datasets.&lt;/p&gt;
&lt;div id=&#34;where-to-find-spatial-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Where to find spatial data&lt;/h3&gt;
&lt;p&gt;Spatial data is all around us! Try searching google for a topic + “spatial shapefile”. One of my favorite sources for spatial data is the DHS &lt;a href=&#34;https://hifld-geoplatform.opendata.arcgis.com/&#34;&gt;HIFLD Open database&lt;/a&gt;, which has lots of government datasets that are well-organized by category. Click through, and when you find something you like, click the “Download” button. If there is a GEOJSON or KML file available, &lt;strong&gt;right-click&lt;/strong&gt; and copy the link address. Then, use that with &lt;code&gt;st_read()&lt;/code&gt;. On many maps (including this one), the GeoJSON link is shown under the API drop-down. Use GeoJSON over KML as some systems have issues importing the data fields in KML.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loading the data&lt;/h3&gt;
&lt;p&gt;We’ll use the &lt;code&gt;sf&lt;/code&gt; package’s &lt;code&gt;st_read&lt;/code&gt; to open spatial data. Here, I’m loading the Natural Gas Processing Plants data from the Energy section of HIFLD. I’m using the &lt;strong&gt;GeoJSON&lt;/strong&gt; option, which &lt;code&gt;st_read()&lt;/code&gt; knows how to handle:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gasplants = st_read(&amp;#39;https://opendata.arcgis.com/datasets/ca984888f8154c63bf3a023f0a1f9ac2_0.geojson&amp;#39;) %&amp;gt;%
  dplyr::select(name = NAME)

head(gasplants)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;sf&lt;/code&gt; data type holds both the data (which here is just the name of the plant) &lt;em&gt;and&lt;/em&gt; the “geometry”, which are the points. It’s tidy data - one row is one observation of one plant, and each row has a set of coordinates telling us where to find the plant.&lt;/p&gt;
&lt;p&gt;We can use ggplot with &lt;code&gt;geom_sf()&lt;/code&gt; to plot these points. They’re just scattered across the country and we don’t automatically get a background map, but here are the points&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gasplants) + geom_sf() + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, we’re missing some context here – we can kind of make out the point of Texas down there, but it’s hard to tell anything about where these plants are located. Let’s use &lt;code&gt;tidycensus&lt;/code&gt; to get a map of the US, then plot it plus the points. Note we use different &lt;code&gt;data =&lt;/code&gt; in each of the &lt;code&gt;geom_sf()&lt;/code&gt; calls:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;US &amp;lt;- states(cb = TRUE, progress_bar = FALSE)  # tidycensus maps

ggplot() + geom_sf(data = US, col = &amp;#39;gray50&amp;#39;) +
  geom_sf(data = gasplants) + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Getting there. The problem now is that the &lt;code&gt;tigris&lt;/code&gt; data covers all US territories, which are really spread out! Let’s drop down to just Michigan. We can use good old &lt;code&gt;filter&lt;/code&gt; just like with regular data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MI = states(cb = TRUE) %&amp;gt;% dplyr::filter(STUSPS==&amp;#39;MI&amp;#39;)

ggplot() + geom_sf(data = MI, col = &amp;#39;gray50&amp;#39;) +
  geom_sf(data = gasplants) + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, now we have a different problem. We want to have only the &lt;code&gt;gasplants&lt;/code&gt; that are &lt;code&gt;over&lt;/code&gt; the state of Michigan. That requires a &lt;strong&gt;spatial join&lt;/strong&gt;. Luckily, our &lt;code&gt;tidyverse&lt;/code&gt; syntax &lt;strong&gt;works pretty seamlessly on &lt;code&gt;sf&lt;/code&gt; objects&lt;/strong&gt;. First, we have to take care of a little issue with spatial data. The &lt;strong&gt;projection&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;projections-briefly&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Projections, briefly&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;projection&lt;/em&gt; for spatial data is the translation from a 3-D object (e.g. the globe) to a 2-D space (a map, or the cartesian x-y coordinates of our screen). This is no simple matter! There are entire PhD programs dedicated to forming and processing projections and datum (which refer to the shape of the globe, which is not actually round). It can all be a nightmare. Worst of all, projections determine the definition of your coordinates, so you may be at -81 latitude, +30 longitude, but in another projection, you might be 1245349m above some reference point, and -2452849m to the left of that point. Projections define the distance along the X and Y axis, the scale of the coordinates, and a lot of other stuff about your 2-D polygons.&lt;/p&gt;
&lt;p&gt;Luckily, over the last few years, very smart people have been working on regularizing “projections”. Now, we really need to know three things:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;em&gt;projection&lt;/em&gt; of your data’s coordinates when you read it in&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;projection&lt;/em&gt; you want your data to be in when you map it&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;projection&lt;/em&gt; of other spatial data you may want to combine.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;importing-projected-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Importing projected data&lt;/h4&gt;
&lt;p&gt;GEOJSON, shapefiles, and KML files usually come with embedded projections stored as &lt;strong&gt;EPSG&lt;/strong&gt; numbers like ‘4326’ (incidentally, ‘4326’ is the usual projection for GPS coordinates). Thus, the first one is usually already taken care of. If your data doesnt have a PROJ4 or EPSG number but the coordinates are all between -180 and +180, it’s likely in EPSG=4326. If none of those, then the data creator should have &lt;em&gt;metadata&lt;/em&gt; stating the proejction. It might take some googling and some trial and error.&lt;/p&gt;
&lt;p&gt;For mapping, you might need to &lt;em&gt;transform&lt;/em&gt; your data between projections (or “reproject” it, same thing). We use &lt;code&gt;st_transform&lt;/code&gt; for this. We only need to give &lt;code&gt;R&lt;/code&gt; the EPSG (Geodetic Parameter Dataset) of the projection you want to end up in. As long as it’s already in a known projection, &lt;code&gt;R&lt;/code&gt; can re-project it. The projection is refered to by the Coordinate Reference System (CRS). &lt;code&gt;st_crs&lt;/code&gt; will tell us the projection (EPSG number and a lot more) of any spatial object. If they do not match, then &lt;code&gt;R&lt;/code&gt; will give an error or, worse, plot them on totally different scales - sometimes you end up with points from the US landing in the middle of the Indian Ocean! In fact, look back at our map of gas plants and the US.&lt;/p&gt;
&lt;div class=&#34;Notice&#34;&gt;
&lt;p&gt;Did you notice that a lot of the US gas plants mapped to…Canada? Those might be Alaskan plants, but the &lt;em&gt;projections&lt;/em&gt; were not identical, so the definition of coordinates were different between our US map and our gas plants maps. Neither is “right” or “wrong” - they just have to be the same. Differences in projections tend to be worse at the extreme latitudes.&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;st_crs(gasplants)
st_crs(MI)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One is in 4326, the other in 4269. We can use &lt;code&gt;st_transform&lt;/code&gt; on the &lt;code&gt;gasplants&lt;/code&gt; data, which will reproject the points (and won’t change the data at all). The data won’t be any different, and the points won’t look too much different&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gasplants = gasplants %&amp;gt;%
  st_transform(st_crs(MI))

ggplot(gasplants) + geom_sf() + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;importing-unprojected-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Importing unprojected data&lt;/h4&gt;
&lt;p&gt;Sometimes, we have data only in .csv format, but with X and Y coordinates (e.g. Longitude and Latitude). To import this data, we do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read in from .csv, .xls, etc.&lt;/li&gt;
&lt;li&gt;Determine the CRS of the data (usually 4326 for gps coordinates)&lt;/li&gt;
&lt;li&gt;Set the spatial coordinates and CRS&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We know how to do the first, and the 2nd and 3rd are done in one step. We’ll make a data.frame of city names and use the &lt;code&gt;tmaptools&lt;/code&gt; package’s &lt;code&gt;geocode_OSM&lt;/code&gt; to get the latitudes and longitudes of the city centers. This function uses open-source Open Street Maps instead of the google API (which is used by &lt;code&gt;ggmap&lt;/code&gt;). This way, we don’t need an API key.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ourCities = geocode_OSM(c(&amp;#39;Detroit&amp;#39;,&amp;#39;Lansing&amp;#39;,&amp;#39;Grand Rapids&amp;#39;,&amp;#39;Kalamazoo&amp;#39;,&amp;#39;Traverse City&amp;#39;,&amp;#39;Marquette&amp;#39;)) %&amp;gt;%
  select(City = query, lat = lat, lon = lon)

head(ourCities)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since these are GPS-type coordinates, we are going to assume the CRS is EPSG=4326. Longitude is the “x” axis, and latitude is the “y” axis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ourCities.spatial = st_as_sf(ourCities, coords = c(&amp;#39;lon&amp;#39;,&amp;#39;lat&amp;#39;), crs = 4326)
head(ourCities.spatial)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have the point geometries! We can map this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  geom_sf(data = MI, fill = &amp;#39;gray90&amp;#39;) +
  geom_sf(data = ourCities.spatial, col = &amp;#39;blue&amp;#39;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;spatial-merges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Spatial merges&lt;/h2&gt;
&lt;p&gt;Combining spatial data is the strength of geospatial analysis. We have our map of MI, and we have out points. Let’s “merge” the points to the map, meaning let’s connect the elements in our map (the state of MI) to the elements in our points (gas plants). This is a point-to-polygon merge.&lt;/p&gt;
&lt;div id=&#34;point-to-polygon-merges&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Point-to-polygon merges&lt;/h3&gt;
&lt;p&gt;We’ll use &lt;code&gt;st_join&lt;/code&gt; to produce an inner join, so we keep only those points that are “in” (spatially) the state of Michigan. I’m specifying &lt;code&gt;join = st_intersects&lt;/code&gt; though this is the default. Note that all the points that remain in the merged &lt;code&gt;MI.gasplants&lt;/code&gt; are in Michigan, and note that all the data columns from &lt;code&gt;MI&lt;/code&gt; are now in &lt;code&gt;gasplants&lt;/code&gt;. We’ll use a county map of MI here so we will have the &lt;em&gt;county&lt;/em&gt; data for each county containing a gas plant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MI.counties = counties(state = &amp;#39;MI&amp;#39;, cb = TRUE, progress_bar = FALSE)

MI.gasplants = gasplants %&amp;gt;%
  st_transform(st_crs(MI.counties)) %&amp;gt;%
  st_join(MI.counties, left = FALSE,
          join = st_intersects)

head(MI.gasplants)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can plot the counties map with the gasplants over it. We can even use &lt;code&gt;aes(...)&lt;/code&gt; to &lt;code&gt;fill&lt;/code&gt; the counties:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  geom_sf(data = MI.counties, aes(fill = NAME), show.legend = F) +
  geom_sf(data = MI.gasplants) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Huh. Most gas plants in Michigan are to the north of here. Interesting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mapview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mapview&lt;/h3&gt;
&lt;p&gt;Sometimes, we want to be able to zoom in and out. &lt;code&gt;ggplot&lt;/code&gt; is static, so that won’t work too well. Thanks to the &lt;code&gt;leaflet&lt;/code&gt; engine, the &lt;code&gt;mapview&lt;/code&gt; packages is stellar for exploration of spatial data. You can specify &lt;code&gt;zcol = Name&lt;/code&gt; if you want to color by the &lt;code&gt;Name&lt;/code&gt; field. I can’t embed this in the website, but you can run this at home. It will appear in the “Viewer” pane, not in the “Plots” pane. Unlike the static image here, you will be able to zoom and pan.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mapview(MI.gasplants)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::include_graphics(&amp;#39;img/MIgas.png&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In an actual mapview window (not this static image here), clicking on the points or polygons will bring up a pop-up of the data for that row. Mapview is very useful for exploring your spatial data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polygon-to-polygon-merges&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polygon-to-polygon merges&lt;/h3&gt;
&lt;p&gt;The gas plants and state merge, above, was very simple because points are always either within or not within a polygon. Worst that can happen is some of your points are not over any polygon at all (resulting in &lt;code&gt;NA&lt;/code&gt; values). But what if you’re merging polygons to polygons?&lt;/p&gt;
&lt;p&gt;First, let’s load some (overlapping) polygons. We can load up all of our states again (dropping the territories). We’ll also use a map of watersheds (which cross state boundaries). This is the HUC-4 map of the Rockies from the &lt;a href=&#34;https://hub.arcgis.com/datasets/7f8632f3e3114623b4f5c8f97d935dca_1?geometry=-157.095%2C32.305%2C-73.379%2C44.324&#34;&gt;US Geological Survey&lt;/a&gt;. The HUC-4 is a definition of a watershed where the area of the HUC-4 is the area drained by a major tributary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;US = states(cb=TRUE) %&amp;gt;%
  dplyr::filter(!STUSPS %in% c(&amp;#39;PR&amp;#39;,&amp;#39;GU&amp;#39;,&amp;#39;VI&amp;#39;,&amp;#39;MP&amp;#39;,&amp;#39;AS&amp;#39;,&amp;#39;AK&amp;#39;,&amp;#39;HI&amp;#39;))

HUC4 = st_read(&amp;#39;https://opendata.arcgis.com/datasets/7f8632f3e3114623b4f5c8f97d935dca_1.kml&amp;#39;) %&amp;gt;%
  st_transform(st_crs(US)) %&amp;gt;%
  dplyr::mutate(randomData = rpois(n(), 20))

ggplot() +
  geom_sf(data = US, col = &amp;#39;gray50&amp;#39;) +
  geom_sf(data = HUC4, aes(fill = Name), show.legend = FALSE) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These watersheds clearly overlap state boundaries. So what happens if we merge them? &lt;code&gt;sf&lt;/code&gt; will create a new obsveration (row) for every HUC-4 / State combination&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;poly.merge = HUC4 %&amp;gt;%
  st_join(US, left = TRUE)

head(poly.merge)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, every HUC-4 like “Lower Colorado” has multiple observations, one for each STUSPS that it touches. When we plot it, though, each of those observations are still attached to the same HUC-4 polygon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(poly.merge) + geom_sf() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have another option in our join - we can ask &lt;code&gt;st_join&lt;/code&gt; to keep just the &lt;code&gt;largest&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;poly.merge.largest = HUC4 %&amp;gt;%
  st_join(US, left = TRUE, largest = TRUE)

head(poly.merge.largest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, there is only &lt;em&gt;one&lt;/em&gt; observation per HUC-4, and it corresponds to the state that has the most overlap area-wise. For Lower Colorado, Arizona has the most overlap. There are lots of things besides &lt;code&gt;st_intersect&lt;/code&gt; we can use to call two things “joined”. &lt;code&gt;?st_join&lt;/code&gt; tells you about them. For instance, we can use &lt;code&gt;join = st_covers&lt;/code&gt; and we will only get a merge when HUC-4 completely covers the state.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HUC4 %&amp;gt;%
  st_join(US, left = TRUE, join = st_covers) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;None of the HUC-4’s completely cover a state, so we get &lt;code&gt;NA&lt;/code&gt; for all the state data.&lt;/p&gt;
&lt;p&gt;The other thing we can do is ask &lt;code&gt;R&lt;/code&gt; to create separate polygons - one for every HUC-4 / state combination. That isn’t a merge, but it plays a similar role. Note this uses &lt;code&gt;st_intersection&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;poly.int = HUC4 %&amp;gt;%
  st_intersection(US) %&amp;gt;%
  arrange(Name)

head(poly.int)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we have a unique polygon for every combination of HUC-4 and State:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  geom_sf(data = US, fill = &amp;#39;gray50&amp;#39;) +
  geom_sf(data = poly.int, aes(fill = STUSPS), show.legend = FALSE) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, I’ve set the fill to the state, but you can see that the HUC-4’s have boundaries at the state line.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summarizing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summarizing&lt;/h3&gt;
&lt;p&gt;Our &lt;code&gt;summarize&lt;/code&gt; function let us collapse by groups and calculate interseting things like average (over a group or region). The neat part is that &lt;em&gt;it works on spatial data as well&lt;/em&gt;. Let’s look at the data again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(poly.int %&amp;gt;%
       dplyr::select(Name, randomData, STUSPS) %&amp;gt;%
       arrange(STUSPS))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So AZ has two HUC-4’s in it - Lower Colorado and Lower Coloardo - Lake Mead (you can see them above). Summarize on geospatial data works just like regular data - we can &lt;code&gt;group_by(STUSPS)&lt;/code&gt;, and we can &lt;code&gt;summarize()&lt;/code&gt; any of the data. I threw some random data into &lt;code&gt;HUC-4&lt;/code&gt; so we can summarize that.&lt;/p&gt;
&lt;p&gt;But how do we combine data specific to each HUC-4 in AZ? We could:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Just take the average of all of the &lt;code&gt;randomData&lt;/code&gt; values within the state.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take a weighted average of &lt;code&gt;randomData&lt;/code&gt; where the &lt;em&gt;area&lt;/em&gt; is the weight&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take some other function (min, max, etc.) of &lt;code&gt;randomData&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can implement any of these using &lt;code&gt;sf&lt;/code&gt;. Let’s do the second since it nests the first. First, we’ll add the area of the State x HUC-4 using &lt;code&gt;st_area&lt;/code&gt;, which gives a &lt;code&gt;units&lt;/code&gt; object. We can turn that into a numeric:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;poly.int.summary = poly.int %&amp;gt;%
  dplyr::mutate(State.HUC.area = as.numeric(st_area(.))) %&amp;gt;%
  group_by(STUSPS) %&amp;gt;%
  dplyr::summarize(mean.randomData = weighted.mean(randomData, w = State.HUC.area))

head(poly.int.summary)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;sf&lt;/code&gt; with the &lt;code&gt;tidyverse&lt;/code&gt; makes it really easy to apply spatial versions of &lt;code&gt;summarize&lt;/code&gt; and &lt;code&gt;mutate&lt;/code&gt;. Very useful.&lt;/p&gt;
&lt;p&gt;If we had wanted to just take the average (ignoring area), we’d just leave out the &lt;code&gt;w = State.HUC.area&lt;/code&gt; or just used &lt;code&gt;mean&lt;/code&gt;. If we had wanted to take, say, the minimum, we would use &lt;code&gt;min(randomData)&lt;/code&gt; instead of &lt;code&gt;weighted.mean&lt;/code&gt;. We can use whatever function we want in &lt;code&gt;summarize&lt;/code&gt;, just as we did with non-spatial data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cropping-vs.-merging&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cropping vs. merging&lt;/h2&gt;
&lt;p&gt;Sometimes, we wish to only crop to a region rather than merging. &lt;code&gt;sf&lt;/code&gt; has the &lt;code&gt;st_crop&lt;/code&gt; function to do this. Let’s crop our &lt;code&gt;HUC-4&lt;/code&gt; data to just the &lt;strong&gt;bounding box&lt;/strong&gt; of the state of Nevada&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HUC4.nv = HUC4 %&amp;gt;%
  st_crop(US %&amp;gt;% dplyr::filter(STUSPS==&amp;#39;NV&amp;#39;))

ggplot(HUC4.nv) + geom_sf(aes(fill = Name), show.legend = F) +
  geom_sf(data = US %&amp;gt;% dplyr::filter(STUSPS==&amp;#39;NV&amp;#39;),  fill = NA, col = &amp;#39;gray20&amp;#39;, lwd = 3 ) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;bounding-boxes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bounding boxes&lt;/h3&gt;
&lt;p&gt;Notice that this forms a box around Nevada and uses that to crop. To get the shape of Nevada, we would have to use &lt;code&gt;st_intersection&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But this introduces a useful concept: the &lt;em&gt;bounding box&lt;/em&gt;. The bounding box is defined by the closest 4 points that form a box that perfectly encloses the object (even when the object is not a rectangle). The extent of the above plot is the bounding box for Nevada.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Nevada.bbox = st_bbox(US %&amp;gt;% dplyr::filter(STUSPS==&amp;#39;NV&amp;#39;))
Nevada.bbox&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The bounding box can be used to frame a “window” in a &lt;code&gt;ggplot&lt;/code&gt; using &lt;code&gt;geom_sf()&lt;/code&gt;. That is, sometimes, we want to &lt;em&gt;plot&lt;/em&gt; just a subsection of a map, but we still want the data to be the whole map. Here’s an example using the HOLC Redlining Maps, which were created in the 1930’s and were used to segregate US housing up until the 1970’s. They are available at the &lt;a href=&#34;https://dsl.richmond.edu/panorama/redlining&#34;&gt;University of Richmond’s &lt;em&gt;Mapping Inequality&lt;/em&gt; site&lt;/a&gt;. We can load Lansing and Detroit using the code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lansing = st_read(&amp;#39;https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/MILansing19XX.geojson&amp;#39;)

detroit = st_read(&amp;#39;https://dsl.richmond.edu/panorama/redlining/static/downloads/geojson/MIDetroit1939.geojson&amp;#39;)

mi.redlining = bind_rows(lansing, detroit) %&amp;gt;%
  st_transform(st_crs(MI))

ggplot(mi.redlining) +
  geom_sf(aes(fill = holc_grade, col = holc_grade)) +
  geom_sf(data = MI, fill = NA, col = &amp;#39;gray50&amp;#39;) +
  scale_fill_manual(values = c(&amp;#39;A&amp;#39; = &amp;#39;green&amp;#39;, &amp;#39;B&amp;#39; = &amp;#39;blue&amp;#39;, &amp;#39;C&amp;#39; = &amp;#39;yellow&amp;#39;, &amp;#39;D&amp;#39; = &amp;#39;red&amp;#39;),
                    aesthetics = c(&amp;#39;color&amp;#39;,&amp;#39;fill&amp;#39;)) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can tell that our polygons have plotted, but since we have the whole state of Michigan, they’re almost unreadable. We need to set our window over the lower part of the lower peninsula. We’ll use &lt;code&gt;coord_sf&lt;/code&gt; to do this, but first we need to define a window. Since windows are almost always rectangular, we can use the &lt;code&gt;st_bbox(mi.redlining)&lt;/code&gt;, but we have to pull out the xlim (xmin, xmax) and ylim (ymin, ymax):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mi.redlining) +
  geom_sf(aes(fill = holc_grade, col = holc_grade)) +
  geom_sf(data = MI, fill = NA, col = &amp;#39;gray50&amp;#39;) +
  scale_fill_manual(values = c(&amp;#39;A&amp;#39; = &amp;#39;green&amp;#39;, &amp;#39;B&amp;#39; = &amp;#39;blue&amp;#39;, &amp;#39;C&amp;#39; = &amp;#39;yellow&amp;#39;, &amp;#39;D&amp;#39; = &amp;#39;red&amp;#39;),
                    aesthetics = c(&amp;#39;color&amp;#39;,&amp;#39;fill&amp;#39;)) +
  theme_minimal() +
  coord_sf(xlim = st_bbox(mi.redlining)[c(1, 3)],
           ylim = st_bbox(mi.redlining)[c(2, 4)])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;distance-matrices&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Distance matrices&lt;/h2&gt;
&lt;p&gt;One of the most common spatial statistics we’d use in data analytics is the &lt;em&gt;distance&lt;/em&gt; matrix. If we have a set of points and we think that we can explain some data about those points (unemployment, ag production, murders per capita) based on the distance to some explanatory source (gas plants, superfund site, etc.), then we might want to include &lt;em&gt;distance to gas plants&lt;/em&gt; in our model as a predictor. Frequently, we’ll use inverse distance, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{d}\)&lt;/span&gt;, so that closer things can have more of an impact. To do this, we need a distance matrix.&lt;/p&gt;
&lt;p&gt;Let’s combine our &lt;code&gt;gasplants&lt;/code&gt; and our &lt;code&gt;ourCities&lt;/code&gt; to get the distance from each of our cities to the nearest gas plant. Maybe we have city-level data on student achievements and we want to see if gas plants lower student achievement. While we would need a lot more information to make this model, we can look at what we have for now.&lt;/p&gt;
&lt;p&gt;We will use &lt;code&gt;st_distance&lt;/code&gt;, which will generate a special type of object that contains the distance information. In our &lt;em&gt;distance matrix&lt;/em&gt;, each row is an object in &lt;code&gt;ourCities&lt;/code&gt; and each column is an object in &lt;code&gt;MI.gasplants&lt;/code&gt;. We are going to take only a few &lt;code&gt;MI.gasplants&lt;/code&gt; so we can easily view the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ourCities.spatial = ourCities.spatial %&amp;gt;%
  st_transform(st_crs(MI.gasplants))

MI.gasplants.small = MI.gasplants[1:4,]

ourDistance = st_distance(x = ourCities.spatial, y = MI.gasplants.small)
ourDistance&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a &lt;code&gt;units&lt;/code&gt; matrix, which has extra properties that allow us to convert the units. The units will be in whatever the CRS of the objects is in - &lt;code&gt;st_crs(ourCities.spatial)&lt;/code&gt; tells us the units are meters.&lt;/p&gt;
&lt;p&gt;What if we wanted to find the closest gas plant to each city? That is akin to looking at each row, and finding the column that is the smallest, right? We will use &lt;code&gt;apply&lt;/code&gt;, and we will note that the order of the columns is the same as the order in &lt;code&gt;MI.gasplants.small&lt;/code&gt;, so we can use &lt;code&gt;MI.gasplants.small$name&lt;/code&gt; to tell us the name of the closest gas plant. We will &lt;code&gt;apply&lt;/code&gt; over each row (&lt;code&gt;MAR=1&lt;/code&gt;) and use the &lt;code&gt;which.min&lt;/code&gt; function, which returns the &lt;em&gt;index&lt;/em&gt; number of the maximum column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max.index = apply(ourDistance, MAR = 1, which.min)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can combine this index with the &lt;code&gt;MI.gasplants.small&lt;/code&gt; object to get the names of the closest gas plant for each of the cities. We’ll make a nice, neat tibble with the city name (in the order from ourCities.spatial), the closest gas plant name, and the distance to that plant:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(City = ourCities.spatial$City,
       Closest.gasplant = MI.gasplants.small$name[max.index],
       Distance.to.closest = ourDistance[cbind(1:length(max.index), max.index)])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But wait, what is going on in the last line there? Well, recall our distance matrix and &lt;code&gt;max.index&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ourDistance
#
max.index&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we want to select from our distance matrix the 1st row, 3rd column; the 2nd row, 3rd column; 3rd row, 1st column; 4th row, 3rd column; 5th row, 4th column; and 6th row, 4th column. This means the row index and column index are not ranges, but are paired. Using &lt;code&gt;cbind(1:6, max.index)&lt;/code&gt; makes them paired entries, and we can select specific row x column combinations that way.&lt;/p&gt;
&lt;div id=&#34;st_nearest_feature&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;st_nearest_feature&lt;/h3&gt;
&lt;p&gt;As is common in R, there is a function that will get the closest points between to spatial objects. &lt;code&gt;st_nearest_points&lt;/code&gt; takes two geometries, and returns the neaarest point in &lt;code&gt;y&lt;/code&gt; for every point in &lt;code&gt;x&lt;/code&gt;, which is what we did with the MI gas plants.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;st_nearest_feature(x = ourCities.spatial, y = MI.gasplants.small)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is exactly our &lt;code&gt;max.index&lt;/code&gt; and can be used *on the &lt;code&gt;y&lt;/code&gt; object, &lt;code&gt;MI.gasplants.small&lt;/code&gt;, to pull the names, subset, get distances etc.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;st_nearest_feature&lt;/code&gt; also works for points and polygons, or polygons and polygons, where it returns the index of the polygon that contains the nearest point to the features in &lt;code&gt;x&lt;/code&gt;. Let’s find the nearest Great Lake for each of our cities using a &lt;a href=&#34;https://hub.arcgis.com/datasets/wi-dnr::great-lakes?geometry=-127.301%2C37.038%2C-43.585%2C48.298&#34;&gt;KML shapefile of the Great Lakes from WI DNR&lt;/a&gt;. Note that the GeoJSON link is under “API” here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GL = st_read(&amp;#39;https://opendata.arcgis.com/datasets/a8bb79fc10e64eee8c3a9db97cc5dc80_4.geojson&amp;#39;) %&amp;gt;%
  st_transform(st_crs(ourCities.spatial))

closest.GL.index = st_nearest_feature(x = ourCities.spatial, y = GL)
ourCities %&amp;gt;% dplyr::mutate(Closest.GreatLake = GL$FEAT_NAME[closest.GL.index])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;other-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Claudia Engel’s “&lt;a href=&#34;https://cengel.github.io/R-spatial/&#34;&gt;Using Spatial Data with R&lt;/a&gt;” is a very useful resource. It covers &lt;code&gt;sf&lt;/code&gt; and an older geospatial library called &lt;code&gt;sp&lt;/code&gt; that has similar functionality but was not tidyverse-friendly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;a href=&#34;https://github.com/rstudio/cheatsheets/raw/master/sf.pdf&#34;&gt;Rstudio Spatial Cheat Sheet&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are &lt;a href=&#34;https://www.rstudio.com/resources/cheatsheets/&#34;&gt;lots of useful RStudio cheat sheets, actually.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>https://ssc442.netlify.app/content/12-content/</link>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/12-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-data-wrangling&#34;&gt;Introduction to data wrangling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reshaping-data&#34;&gt;Reshaping data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gather&#34;&gt;&lt;code&gt;gather&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#spread&#34;&gt;&lt;code&gt;spread&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#separate&#34;&gt;&lt;code&gt;separate&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unite&#34;&gt;&lt;code&gt;unite&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#joining-tables&#34;&gt;Joining tables&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#joins&#34;&gt;Joins&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#left-join&#34;&gt;Left join&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#right-join&#34;&gt;Right join&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inner-join&#34;&gt;Inner join&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#full-join&#34;&gt;Full join&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#semi-join&#34;&gt;Semi join&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#anti-join&#34;&gt;Anti join&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binding&#34;&gt;Binding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#binding-columns&#34;&gt;Binding columns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binding-by-rows&#34;&gt;Binding by rows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#set-operators&#34;&gt;Set operators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intersect&#34;&gt;Intersect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#union&#34;&gt;Union&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setdiff&#34;&gt;&lt;code&gt;setdiff&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setequal&#34;&gt;&lt;code&gt;setequal&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-note-about-merging-and-dupliacted-rows&#34;&gt;A note about merging and dupliacted rows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parsing-dates-and-times&#34;&gt;Parsing dates and times&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-date-data-type&#34;&gt;The date data type&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lubridate&#34;&gt;The lubridate package&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How can we reshape data into a useable &lt;code&gt;tidy&lt;/code&gt; form?&lt;/li&gt;
&lt;li&gt;What is a &lt;em&gt;join&lt;/em&gt; and why is it a common data wrangling maneuver?&lt;/li&gt;
&lt;li&gt;What is a &lt;em&gt;primary key&lt;/em&gt; and why is it important to think about our data in this way?&lt;/li&gt;
&lt;li&gt;How do we deal with messy date variables?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction-to-data-wrangling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to data wrangling&lt;/h1&gt;
&lt;p&gt;Many of the datasets used in this class have been made available to you as &lt;code&gt;R&lt;/code&gt; objects, specifically as data frames. The US murders data, the reported heights data, and the Gapminder data were all data frames. Furthermore, much of the data is available in what is referred to as &lt;code&gt;tidy&lt;/code&gt; form. The tidyverse packages and functions assume that the data is &lt;code&gt;tidy&lt;/code&gt; and this assumption is a big part of the reason these packages work so well together.&lt;/p&gt;
&lt;p&gt;However, very rarely in a data science project is data easily available as part of a package. People did quite a bit of work “behind the scenes” to get the original raw data into the &lt;em&gt;tidy&lt;/em&gt; tables. Much more typical is for the data to be in a file, a database, or extracted from a document, including web pages, tweets, or PDFs. In these cases, the first step is to import the data into &lt;code&gt;R&lt;/code&gt; and, when using the &lt;strong&gt;tidyverse&lt;/strong&gt;, tidy up the data. This initial step in the data analysis process usually involves several, often complicated, steps to convert data from its raw form to the &lt;em&gt;tidy&lt;/em&gt; form that greatly facilitates the rest of the analysis. We refer to this process as &lt;code&gt;data wrangling&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here we cover several common steps of the data wrangling process including tidying data, string processing, html parsing, working with dates and times, and text mining. Rarely are &lt;strong&gt;all&lt;/strong&gt; these wrangling steps necessary in a single analysis, but data scientists will likely face them all at some point.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reshaping-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Reshaping data&lt;/h1&gt;
&lt;p&gt;As we have seen through the class, having data in &lt;em&gt;tidy&lt;/em&gt; format is what makes the tidyverse flow. After the first step in the data analysis process, importing data, a common next step is to reshape the data into a form that facilitates the rest of the analysis. The &lt;strong&gt;tidyr&lt;/strong&gt; package includes several functions that are useful for tidying data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
path &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, package=&amp;quot;dslabs&amp;quot;)
filename &amp;lt;- file.path(path, &amp;quot;fertility-two-countries-example.csv&amp;quot;)
wide_data &amp;lt;- read_csv(filename)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;gather&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;gather&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;One of the most used functions in the &lt;strong&gt;tidyr&lt;/strong&gt; package is &lt;code&gt;gather&lt;/code&gt;, which is useful for converting wide data into tidy data.&lt;/p&gt;
&lt;p&gt;As with most tidyverse functions, the &lt;code&gt;gather&lt;/code&gt; function’s first argument is the data frame that will be converted. Here we want to reshape the &lt;code&gt;wide_data&lt;/code&gt; dataset so that each row represents a fertility observation, which implies we need three columns to store the year, country, and the observed value. In its current form, data from different years are in different columns with the year values stored in the column names. Through the second and third argument we will tell &lt;code&gt;gather&lt;/code&gt; the column names we want to assign to the columns containing the current column names and observations, respectively. In this case a good choice for these two arguments would be &lt;code&gt;year&lt;/code&gt; and &lt;code&gt;fertility&lt;/code&gt;. Note that nowhere in the data file does it tell us this is fertility data. Instead, we deciphered this from the file name. Through the fourth argument we specify the columns containing observed values; these are the columns that will be &lt;em&gt;gathered&lt;/em&gt;. The default is to gather all columns so, in most cases, we have to specify the columns. In our example we want columns &lt;code&gt;1960&lt;/code&gt;, &lt;code&gt;1961&lt;/code&gt; up to &lt;code&gt;2015&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The code to gather the fertility data therefore looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_tidy_data &amp;lt;- gather(wide_data, year, fertility, `1960`:`2015`)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use the pipe like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_tidy_data &amp;lt;- wide_data %&amp;gt;% gather(year, fertility, `1960`:`2015`)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the data have been converted to tidy format with columns &lt;code&gt;year&lt;/code&gt; and &lt;code&gt;fertility&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(new_tidy_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 3
##   country     year  fertility
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Germany     1960       2.41
## 2 South Korea 1960       6.16
## 3 Germany     1961       2.44
## 4 South Korea 1961       5.99
## 5 Germany     1962       2.47
## 6 South Korea 1962       5.79&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and that each year resulted in two rows since we have two countries and this column was not gathered.
A somewhat quicker way to write this code is to specify which column will &lt;strong&gt;not&lt;/strong&gt; be gathered, rather than all the columns that will be gathered:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_tidy_data &amp;lt;- wide_data %&amp;gt;%
  gather(year, fertility, -country)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;new_tidy_data&lt;/code&gt; object looks like the original &lt;code&gt;tidy_data&lt;/code&gt; we defined this way&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;gapminder&amp;quot;)
tidy_data &amp;lt;- gapminder %&amp;gt;%
  dplyr::filter(country %in% c(&amp;quot;South Korea&amp;quot;, &amp;quot;Germany&amp;quot;) &amp;amp; !is.na(fertility)) %&amp;gt;%
  dplyr::select(country, year, fertility)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;with just one minor difference. Can you spot it? Look at the data type of the year column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(tidy_data$year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(new_tidy_data$year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;gather&lt;/code&gt; function assumes that column names are characters. So we need a bit more wrangling before we are ready to make a plot. We need to convert the year column to be numbers. The &lt;code&gt;gather&lt;/code&gt; function includes the &lt;code&gt;convert&lt;/code&gt; argument for this purpose:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_tidy_data &amp;lt;- wide_data %&amp;gt;%
  gather(year, fertility, -country, convert = TRUE)
class(new_tidy_data$year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we could have also used the &lt;code&gt;mutate&lt;/code&gt; and &lt;code&gt;as.numeric&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now that the data is tidy, we can use this relatively simple ggplot code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_tidy_data %&amp;gt;% ggplot(aes(year, fertility, color = country)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;spread&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;spread&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;As we will see in later examples, it is sometimes useful for data wrangling purposes to convert tidy data into wide data. We often use this as an intermediate step in tidying up data. The &lt;code&gt;spread&lt;/code&gt; function is basically the inverse of &lt;code&gt;gather&lt;/code&gt;. The first argument is for the data, but since we are using the pipe, we don’t show it. The second argument tells &lt;code&gt;spread&lt;/code&gt; which variable will be used as the column names. The third argument specifies which variable to use to fill out the cells:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_wide_data &amp;lt;- new_tidy_data %&amp;gt;% spread(year, fertility)
dplyr::select(new_wide_data, country, `1960`:`1967`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 9
##   country     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967`
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 Germany       2.41   2.44   2.47   2.49   2.49   2.48   2.44   2.37
## 2 South Korea   6.16   5.99   5.79   5.57   5.36   5.16   4.99   4.85&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following diagram can help remind you how these two functions work:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./11-content_files/gather-spread.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Image courtesy of RStudio&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. CC-BY-4.0 license&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. Cropped from original.)
&lt;!-- (Source: RStudio. The image is a section of this [cheat sheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf).)--&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;separate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;separate&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The data wrangling shown above was simple compared to what is usually required. In our example spreadsheet files, we include an illustration that is slightly more complicated. It contains two variables: life expectancy and fertility. However, the way it is stored is not tidy and, as we will explain, not optimal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path &amp;lt;- system.file(&amp;quot;extdata&amp;quot;, package = &amp;quot;dslabs&amp;quot;)

filename &amp;lt;- &amp;quot;life-expectancy-and-fertility-two-countries-example.csv&amp;quot;
filename &amp;lt;-  file.path(path, filename)

raw_dat &amp;lt;- read_csv(filename)
dplyr::select(raw_dat, 1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   country     `1960_fertility` `1960_life_expe… `1961_fertility` `1961_life_expe…
##   &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1 Germany                 2.41             69.3             2.44             69.8
## 2 South Korea             6.16             53.0             5.99             53.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, note that the data is in wide format. Second, notice that this table includes values for two variables, fertility and life expectancy, with the column name encoding which column represents which variable. Encoding information in the column names is not recommended but, unfortunately, it is quite common. We will put our wrangling skills to work to extract this information and store it in a tidy fashion.&lt;/p&gt;
&lt;p&gt;We can start the data wrangling with the &lt;code&gt;gather&lt;/code&gt; function, but we should no longer use the column name &lt;code&gt;year&lt;/code&gt; for the new column since it also contains the variable type. We will call it &lt;code&gt;key&lt;/code&gt;, the default, for now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- raw_dat %&amp;gt;% gather(key, value, -country)
head(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 3
##   country     key                  value
##   &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;
## 1 Germany     1960_fertility        2.41
## 2 South Korea 1960_fertility        6.16
## 3 Germany     1960_life_expectancy 69.3 
## 4 South Korea 1960_life_expectancy 53.0 
## 5 Germany     1961_fertility        2.44
## 6 South Korea 1961_fertility        5.99&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is not exactly what we refer to as tidy since each observation (year-country combination) is associated with two, not one, rows. We want to have the values from the two variables, fertility and life expectancy, in two separate columns. The first challenge to achieve this is to separate the &lt;code&gt;key&lt;/code&gt; column into the year and the variable type. Notice that the entries in this column separate the year from the variable name with an underscore:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat$key[1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1960_fertility&amp;quot;       &amp;quot;1960_fertility&amp;quot;       &amp;quot;1960_life_expectancy&amp;quot;
## [4] &amp;quot;1960_life_expectancy&amp;quot; &amp;quot;1961_fertility&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Encoding multiple variables in a column name is such a common problem that the &lt;strong&gt;tidyverse&lt;/strong&gt; package includes a function to separate these columns into two or more. Apart from the data, the &lt;code&gt;separate&lt;/code&gt; function takes three arguments: the name of the column to be separated, the names to be used for the new columns, and the character that separates the variables. So, a first attempt at this is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% separate(col = key, into = c(&amp;quot;year&amp;quot;, &amp;quot;variable_name&amp;quot;), sep = &amp;quot;_&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function does separate the values, but we run into a new problem. We receive the warning &lt;code&gt;Additional pieces discarded in 112 rows [3, 4, 7,...]&lt;/code&gt;. (Earlier versions may give the error &lt;code&gt;Too many values at 112 locations:&lt;/code&gt;) and that the &lt;code&gt;life_expectancy&lt;/code&gt; variable is truncated to &lt;code&gt;life&lt;/code&gt;. This is because the &lt;code&gt;_&lt;/code&gt; is used to separate &lt;code&gt;life&lt;/code&gt; and &lt;code&gt;expectancy&lt;/code&gt;, not just year and variable name! We could add a third column to catch this and let the &lt;code&gt;separate&lt;/code&gt; function know which column to &lt;em&gt;fill in&lt;/em&gt; with missing values, &lt;code&gt;NA&lt;/code&gt;, when there is no third value. Here we tell it to fill the column on the right:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% separate(key, into = c(&amp;quot;year&amp;quot;, &amp;quot;first_variable_name&amp;quot;, &amp;quot;second_variable_name&amp;quot;), fill = &amp;quot;right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 224 × 5
##    country     year  first_variable_name second_variable_name value
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;
##  1 Germany     1960  fertility           &amp;lt;NA&amp;gt;                  2.41
##  2 South Korea 1960  fertility           &amp;lt;NA&amp;gt;                  6.16
##  3 Germany     1960  life                expectancy           69.3 
##  4 South Korea 1960  life                expectancy           53.0 
##  5 Germany     1961  fertility           &amp;lt;NA&amp;gt;                  2.44
##  6 South Korea 1961  fertility           &amp;lt;NA&amp;gt;                  5.99
##  7 Germany     1961  life                expectancy           69.8 
##  8 South Korea 1961  life                expectancy           53.8 
##  9 Germany     1962  fertility           &amp;lt;NA&amp;gt;                  2.47
## 10 South Korea 1962  fertility           &amp;lt;NA&amp;gt;                  5.79
## # … with 214 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, if we read the &lt;code&gt;separate&lt;/code&gt; help file, we find that a better approach is to merge the last two variables when there is an extra separation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% separate(key, into = c(&amp;quot;year&amp;quot;, &amp;quot;variable_name&amp;quot;), extra = &amp;quot;merge&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 224 × 4
##    country     year  variable_name   value
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
##  1 Germany     1960  fertility        2.41
##  2 South Korea 1960  fertility        6.16
##  3 Germany     1960  life_expectancy 69.3 
##  4 South Korea 1960  life_expectancy 53.0 
##  5 Germany     1961  fertility        2.44
##  6 South Korea 1961  fertility        5.99
##  7 Germany     1961  life_expectancy 69.8 
##  8 South Korea 1961  life_expectancy 53.8 
##  9 Germany     1962  fertility        2.47
## 10 South Korea 1962  fertility        5.79
## # … with 214 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This achieves the separation we wanted. However, we are not done yet. We need to create a column for each variable. As we learned, the &lt;code&gt;spread&lt;/code&gt; function can do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  separate(key, c(&amp;quot;year&amp;quot;, &amp;quot;variable_name&amp;quot;), extra = &amp;quot;merge&amp;quot;) %&amp;gt;%
  spread(variable_name, value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 112 × 4
##    country year  fertility life_expectancy
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;
##  1 Germany 1960       2.41            69.3
##  2 Germany 1961       2.44            69.8
##  3 Germany 1962       2.47            70.0
##  4 Germany 1963       2.49            70.1
##  5 Germany 1964       2.49            70.7
##  6 Germany 1965       2.48            70.6
##  7 Germany 1966       2.44            70.8
##  8 Germany 1967       2.37            71.0
##  9 Germany 1968       2.28            70.6
## 10 Germany 1969       2.17            70.5
## # … with 102 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is now in tidy format with one row for each observation with three variables: year, fertility, and life expectancy.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unite&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;unite&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;It is sometimes useful to do the inverse of &lt;code&gt;separate&lt;/code&gt;, unite two columns into one. To demonstrate how to use &lt;code&gt;unite&lt;/code&gt;, we show code that, although &lt;em&gt;not&lt;/em&gt; the optimal approach, serves as an illustration. Suppose that we did not know about &lt;code&gt;extra&lt;/code&gt; and used this command to separate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  separate(key, c(&amp;quot;year&amp;quot;, &amp;quot;first_variable_name&amp;quot;, &amp;quot;second_variable_name&amp;quot;), fill = &amp;quot;right&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 224 × 5
##    country     year  first_variable_name second_variable_name value
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;
##  1 Germany     1960  fertility           &amp;lt;NA&amp;gt;                  2.41
##  2 South Korea 1960  fertility           &amp;lt;NA&amp;gt;                  6.16
##  3 Germany     1960  life                expectancy           69.3 
##  4 South Korea 1960  life                expectancy           53.0 
##  5 Germany     1961  fertility           &amp;lt;NA&amp;gt;                  2.44
##  6 South Korea 1961  fertility           &amp;lt;NA&amp;gt;                  5.99
##  7 Germany     1961  life                expectancy           69.8 
##  8 South Korea 1961  life                expectancy           53.8 
##  9 Germany     1962  fertility           &amp;lt;NA&amp;gt;                  2.47
## 10 South Korea 1962  fertility           &amp;lt;NA&amp;gt;                  5.79
## # … with 214 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can achieve the same final result by uniting the second and third columns, then spreading the columns and renaming &lt;code&gt;fertility_NA&lt;/code&gt; to &lt;code&gt;fertility&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  separate(key, c(&amp;quot;year&amp;quot;, &amp;quot;first_variable_name&amp;quot;, &amp;quot;second_variable_name&amp;quot;), fill = &amp;quot;right&amp;quot;) %&amp;gt;%
  unite(variable_name, first_variable_name, second_variable_name) %&amp;gt;%
  spread(variable_name, value) %&amp;gt;%
  rename(fertility = fertility_NA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 112 × 4
##    country year  fertility life_expectancy
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;
##  1 Germany 1960       2.41            69.3
##  2 Germany 1961       2.44            69.8
##  3 Germany 1962       2.47            70.0
##  4 Germany 1963       2.49            70.1
##  5 Germany 1964       2.49            70.7
##  6 Germany 1965       2.48            70.6
##  7 Germany 1966       2.44            70.8
##  8 Germany 1967       2.37            71.0
##  9 Germany 1968       2.28            70.6
## 10 Germany 1969       2.17            70.5
## # … with 102 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Run the following command to define the &lt;code&gt;co2_wide&lt;/code&gt; object using the &lt;code&gt;co2&lt;/code&gt; data built in to R (see &lt;code&gt;?co2&lt;/code&gt;):&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co2_wide &amp;lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) %&amp;gt;%
  setNames(1:12) %&amp;gt;%
  mutate(year = as.character(1959:1997))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the gather function to wrangle this into a tidy dataset. Call the column with the CO2 measurements &lt;code&gt;co2&lt;/code&gt; and call the month column &lt;code&gt;month&lt;/code&gt;. Call the resulting object &lt;code&gt;co2_tidy&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Plot CO2 versus month with a different curve for each year using this code:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co2_tidy %&amp;gt;% ggplot(aes(month, co2, color = year)) + geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the expected plot is not made, it is probably because &lt;code&gt;co2_tidy$month&lt;/code&gt; is not numeric:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(co2_tidy$month)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rewrite the call to gather using an argument that assures the month column will be numeric. Then make the plot.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What do we learn from this plot?&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;CO2 measures increase monotonically from 1959 to 1997.&lt;/li&gt;
&lt;li&gt;CO2 measures are higher in the summer and the yearly average increased from 1959 to 1997.&lt;/li&gt;
&lt;li&gt;CO2 measures appear constant and random variability explains the differences.&lt;/li&gt;
&lt;li&gt;CO2 measures do not have a seasonal trend.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Now load the &lt;code&gt;admissions&lt;/code&gt; data set, which contains admission information for men and women across six majors and keep only the admitted percentage column:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(admissions)
dat &amp;lt;- admissions %&amp;gt;% dplyr::select(-applicants)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we think of an observation as a major, and that each observation has two variables (men admitted percentage and women admitted percentage) then this is not tidy. Use the &lt;code&gt;spread&lt;/code&gt; function to wrangle into tidy shape: one row for each major.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Now we will try a more advanced wrangling challenge. We want to wrangle the admissions data so that for each major we have 4 observations: &lt;code&gt;admitted_men&lt;/code&gt;, &lt;code&gt;admitted_women&lt;/code&gt;, &lt;code&gt;applicants_men&lt;/code&gt; and &lt;code&gt;applicants_women&lt;/code&gt;. The &lt;em&gt;trick&lt;/em&gt; we perform here is actually quite common: first gather to generate an intermediate data frame and then spread to obtain the tidy data we want. We will go step by step in this and the next two exercises.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Use the gather function to create a &lt;code&gt;tmp&lt;/code&gt; data.frame with a column containing the type of observation &lt;code&gt;admitted&lt;/code&gt; or &lt;code&gt;applicants&lt;/code&gt;. Call the new columns &lt;code&gt;key&lt;/code&gt; and value.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Now you have an object &lt;code&gt;tmp&lt;/code&gt; with columns &lt;code&gt;major&lt;/code&gt;, &lt;code&gt;gender&lt;/code&gt;, &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt;. Note that if you combine the key and gender, we get the column names we want: &lt;code&gt;admitted_men&lt;/code&gt;, &lt;code&gt;admitted_women&lt;/code&gt;, &lt;code&gt;applicants_men&lt;/code&gt; and &lt;code&gt;applicants_women&lt;/code&gt;. Use the function &lt;code&gt;unite&lt;/code&gt; to create a new column called &lt;code&gt;column_name&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the &lt;code&gt;spread&lt;/code&gt; function to generate the tidy data with four variables for each major.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the pipe to write a line of code that turns &lt;code&gt;admissions&lt;/code&gt; to the table produced in the previous exercise.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;joining-tables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Joining tables&lt;/h1&gt;
&lt;p&gt;The information we need for a given analysis may not be just in one table. For example, when forecasting elections we used the function &lt;code&gt;left_join&lt;/code&gt; to combine the information from two tables. We also saw this in action using the &lt;code&gt;WDI&lt;/code&gt; function (technically, the &lt;code&gt;WDI&lt;/code&gt; API) in Project 2. Here we use a simpler example to illustrate the general challenge of combining tables.&lt;/p&gt;
&lt;p&gt;Suppose we want to explore the relationship between population size for US states and electoral votes. We have the population size in this table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
data(murders)
head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total
## 1    Alabama  AL  South    4779736   135
## 2     Alaska  AK   West     710231    19
## 3    Arizona  AZ   West    6392017   232
## 4   Arkansas  AR  South    2915918    93
## 5 California  CA   West   37253956  1257
## 6   Colorado  CO   West    5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and electoral votes in this one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(polls_us_election_2016)
head(results_us_election_2016)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          state electoral_votes clinton trump others
## 1   California              55    61.7  31.6    6.7
## 2        Texas              38    43.2  52.2    4.5
## 3      Florida              29    47.8  49.0    3.2
## 4     New York              29    59.0  36.5    4.5
## 5     Illinois              20    55.8  38.8    5.4
## 6 Pennsylvania              20    47.9  48.6    3.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just concatenating these two tables together will not work since the order of the states is not the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;identical(results_us_election_2016$state, murders$state)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;join&lt;/em&gt; functions, described below, are designed to handle this challenge.&lt;/p&gt;
&lt;div id=&#34;joins&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Joins&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;join&lt;/em&gt; functions in the &lt;strong&gt;dplyr&lt;/strong&gt; package (part of the tidyverse) make sure that the tables are combined so that matching rows are together. If you know SQL, you will see that the approach and syntax is very similar. The general idea is that one needs to identify one or more columns that will serve to match the two tables. Then a new table with the combined information is returned. Notice what happens if we join the two tables above by state using &lt;code&gt;left_join&lt;/code&gt; (we will remove the &lt;code&gt;others&lt;/code&gt; column and rename &lt;code&gt;electoral_votes&lt;/code&gt; so that the tables fit on the page):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- left_join(murders, results_us_election_2016, by = &amp;quot;state&amp;quot;) %&amp;gt;%
  dplyr::select(-others) %&amp;gt;%
  rename(ev = electoral_votes)
head(tab)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total ev clinton trump
## 1    Alabama  AL  South    4779736   135  9    34.4  62.1
## 2     Alaska  AK   West     710231    19  3    36.6  51.3
## 3    Arizona  AZ   West    6392017   232 11    45.1  48.7
## 4   Arkansas  AR  South    2915918    93  6    33.7  60.6
## 5 California  CA   West   37253956  1257 55    61.7  31.6
## 6   Colorado  CO   West    5029196    65  9    48.2  43.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data has been successfully joined and we can now, for example, make a plot to explore the relationship:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggrepel)
tab %&amp;gt;% ggplot(aes(population/10^6, ev, label = abb)) +
  geom_point() +
  geom_text_repel() +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;) +
  scale_y_continuous(trans = &amp;quot;log2&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/12-content_files/figure-html/ev-vs-population-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see the relationship is close to linear with about 2 electoral votes for every million persons, but with very small states getting higher ratios.&lt;/p&gt;
&lt;p&gt;In practice, it is not always the case that each row in one table has a matching row in the other. For this reason, we have several versions of join. To illustrate this challenge, we will take subsets of the tables above. We create the tables &lt;code&gt;tab1&lt;/code&gt; and &lt;code&gt;tab2&lt;/code&gt; so that they have some states in common but not all:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 &amp;lt;- slice(murders, 1:6) %&amp;gt;% dplyr::select(state, population)
tab_1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state population
## 1    Alabama    4779736
## 2     Alaska     710231
## 3    Arizona    6392017
## 4   Arkansas    2915918
## 5 California   37253956
## 6   Colorado    5029196&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_2 &amp;lt;- results_us_election_2016 %&amp;gt;%
  dplyr::filter(state%in%c(&amp;quot;Alabama&amp;quot;, &amp;quot;Alaska&amp;quot;, &amp;quot;Arizona&amp;quot;,
                    &amp;quot;California&amp;quot;, &amp;quot;Connecticut&amp;quot;, &amp;quot;Delaware&amp;quot;)) %&amp;gt;%
  dplyr::select(state, electoral_votes) %&amp;gt;% rename(ev = electoral_votes)
tab_2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         state ev
## 1  California 55
## 2     Arizona 11
## 3     Alabama  9
## 4 Connecticut  7
## 5      Alaska  3
## 6    Delaware  3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use these two tables as examples in the next sections.&lt;/p&gt;
&lt;div id=&#34;left-join&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Left join&lt;/h3&gt;
&lt;p&gt;Suppose we want a table like &lt;code&gt;tab_1&lt;/code&gt;, but adding electoral votes to whatever states we have available. For this, we use &lt;code&gt;left_join&lt;/code&gt; with &lt;code&gt;tab_1&lt;/code&gt; as the first argument. We specify which column to use to match with the &lt;code&gt;by&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;left_join(tab_1, tab_2, by = &amp;quot;state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state population ev
## 1    Alabama    4779736  9
## 2     Alaska     710231  3
## 3    Arizona    6392017 11
## 4   Arkansas    2915918 NA
## 5 California   37253956 55
## 6   Colorado    5029196 NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that &lt;code&gt;NA&lt;/code&gt;s are added to the two states not appearing in &lt;code&gt;tab_2&lt;/code&gt;. Also, notice that this function, as well as all the other joins, can receive the first arguments through the pipe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 %&amp;gt;% left_join(tab_2, by = &amp;quot;state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;right-join&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Right join&lt;/h3&gt;
&lt;p&gt;If instead of a table with the same rows as first table, we want one with the same rows as second table, we can use &lt;code&gt;right_join&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 %&amp;gt;% right_join(tab_2, by = &amp;quot;state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         state population ev
## 1     Alabama    4779736  9
## 2      Alaska     710231  3
## 3     Arizona    6392017 11
## 4  California   37253956 55
## 5 Connecticut         NA  7
## 6    Delaware         NA  3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the NAs are in the column coming from &lt;code&gt;tab_1&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;inner-join&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inner join&lt;/h3&gt;
&lt;p&gt;If we want to keep only the rows that have information in both tables, we use &lt;code&gt;inner_join&lt;/code&gt;. You can think of this as an intersection:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inner_join(tab_1, tab_2, by = &amp;quot;state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state population ev
## 1    Alabama    4779736  9
## 2     Alaska     710231  3
## 3    Arizona    6392017 11
## 4 California   37253956 55&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;full-join&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Full join&lt;/h3&gt;
&lt;p&gt;If we want to keep all the rows and fill the missing parts with NAs, we can use &lt;code&gt;full_join&lt;/code&gt;. You can think of this as a union:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;full_join(tab_1, tab_2, by = &amp;quot;state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         state population ev
## 1     Alabama    4779736  9
## 2      Alaska     710231  3
## 3     Arizona    6392017 11
## 4    Arkansas    2915918 NA
## 5  California   37253956 55
## 6    Colorado    5029196 NA
## 7 Connecticut         NA  7
## 8    Delaware         NA  3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;semi-join&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Semi join&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;semi_join&lt;/code&gt; function lets us keep the part of first table for which we have information in the second. It does not add the columns of the second. It isn’t often used:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semi_join(tab_1, tab_2, by = &amp;quot;state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state population
## 1    Alabama    4779736
## 2     Alaska     710231
## 3    Arizona    6392017
## 4 California   37253956&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives the same result as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 %&amp;gt;%
  filter(state %in% tab_2$state)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state population
## 1    Alabama    4779736
## 2     Alaska     710231
## 3    Arizona    6392017
## 4 California   37253956&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;anti-join&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Anti join&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;anti_join&lt;/code&gt; is the opposite of &lt;code&gt;semi_join&lt;/code&gt;. It keeps the elements of the first table for which there is no information in the second:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anti_join(tab_1, tab_2, by = &amp;quot;state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      state population
## 1 Arkansas    2915918
## 2 Colorado    5029196&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following diagram summarizes the above joins:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./11-content_files/joins.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/joins.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Image courtesy of RStudio&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. CC-BY-4.0 license&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Cropped from original.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;binding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binding&lt;/h2&gt;
&lt;p&gt;Although we have yet to use it in this book, another common way in which datasets are combined is by &lt;em&gt;binding&lt;/em&gt; them. Unlike the join function, the binding functions do not try to match by a variable, but instead simply combine datasets. If the datasets don’t match by the appropriate dimensions, one obtains an error.&lt;/p&gt;
&lt;div id=&#34;binding-columns&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binding columns&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;dplyr&lt;/strong&gt; function &lt;em&gt;bind_cols&lt;/em&gt; binds two objects by making them columns in a tibble. For example, we quickly want to make a data frame consisting of numbers we can use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_cols(a = 1:3, b = 4:6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 2
##       a     b
##   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
## 1     1     4
## 2     2     5
## 3     3     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function requires that we assign names to the columns. Here we chose &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note that there is an R-base function &lt;code&gt;cbind&lt;/code&gt; with the exact same functionality. An important difference is that &lt;code&gt;cbind&lt;/code&gt; can create different types of objects, while &lt;code&gt;bind_cols&lt;/code&gt; always produces a data frame.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;bind_cols&lt;/code&gt; can also bind two different data frames. For example, here we break up the &lt;code&gt;tab&lt;/code&gt; data frame and then bind them back together:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 &amp;lt;- tab[, 1:3]
tab_2 &amp;lt;- tab[, 4:6]
tab_3 &amp;lt;- tab[, 7:8]
new_tab &amp;lt;- bind_cols(tab_1, tab_2, tab_3)
head(new_tab)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total ev clinton trump
## 1    Alabama  AL  South    4779736   135  9    34.4  62.1
## 2     Alaska  AK   West     710231    19  3    36.6  51.3
## 3    Arizona  AZ   West    6392017   232 11    45.1  48.7
## 4   Arkansas  AR  South    2915918    93  6    33.7  60.6
## 5 California  CA   West   37253956  1257 55    61.7  31.6
## 6   Colorado  CO   West    5029196    65  9    48.2  43.3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;binding-by-rows&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binding by rows&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;bind_rows&lt;/code&gt; function is similar to &lt;code&gt;bind_cols&lt;/code&gt;, but binds rows instead of columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 &amp;lt;- tab[1:2,]
tab_2 &amp;lt;- tab[3:4,]
bind_rows(tab_1, tab_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      state abb region population total ev clinton trump
## 1  Alabama  AL  South    4779736   135  9    34.4  62.1
## 2   Alaska  AK   West     710231    19  3    36.6  51.3
## 3  Arizona  AZ   West    6392017   232 11    45.1  48.7
## 4 Arkansas  AR  South    2915918    93  6    33.7  60.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is based on an R-base function &lt;code&gt;rbind&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;set-operators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set operators&lt;/h2&gt;
&lt;p&gt;Another set of commands useful for combining datasets are the set operators. When applied to vectors, these behave as their names suggest. Examples are &lt;code&gt;intersect&lt;/code&gt;, &lt;code&gt;union&lt;/code&gt;, &lt;code&gt;setdiff&lt;/code&gt;, and &lt;code&gt;setequal&lt;/code&gt;. However, if the &lt;strong&gt;tidyverse&lt;/strong&gt;, or more specifically &lt;strong&gt;dplyr&lt;/strong&gt;, is loaded, these functions can be used on data frames as opposed to just on vectors.&lt;/p&gt;
&lt;div id=&#34;intersect&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Intersect&lt;/h3&gt;
&lt;p&gt;You can take intersections of vectors of any type, such as numeric:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;intersect(1:10, 6:15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or characters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;intersect(c(&amp;quot;a&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;c&amp;quot;), c(&amp;quot;b&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;d&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;dplyr&lt;/strong&gt; package includes an &lt;code&gt;intersect&lt;/code&gt; function that can be applied to tables with the same column names. This function returns the rows in common between two tables. To make sure we use the &lt;strong&gt;dplyr&lt;/strong&gt; version of &lt;code&gt;intersect&lt;/code&gt; rather than the base package version, we can use &lt;code&gt;dplyr::intersect&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 &amp;lt;- tab[1:5,]
tab_2 &amp;lt;- tab[3:7,]
dplyr::intersect(tab_1, tab_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total ev clinton trump
## 1    Arizona  AZ   West    6392017   232 11    45.1  48.7
## 2   Arkansas  AR  South    2915918    93  6    33.7  60.6
## 3 California  CA   West   37253956  1257 55    61.7  31.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;union&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Union&lt;/h3&gt;
&lt;p&gt;Similarly &lt;em&gt;union&lt;/em&gt; takes the union of vectors. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;union(1:10, 6:15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;union(c(&amp;quot;a&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;c&amp;quot;), c(&amp;quot;b&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;d&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;d&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;dplyr&lt;/strong&gt; package includes a version of &lt;code&gt;union&lt;/code&gt; that combines all the rows of two tables with the same column names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 &amp;lt;- tab[1:5,]
tab_2 &amp;lt;- tab[3:7,]
dplyr::union(tab_1, tab_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         state abb    region population total ev clinton trump
## 1     Alabama  AL     South    4779736   135  9    34.4  62.1
## 2      Alaska  AK      West     710231    19  3    36.6  51.3
## 3     Arizona  AZ      West    6392017   232 11    45.1  48.7
## 4    Arkansas  AR     South    2915918    93  6    33.7  60.6
## 5  California  CA      West   37253956  1257 55    61.7  31.6
## 6    Colorado  CO      West    5029196    65  9    48.2  43.3
## 7 Connecticut  CT Northeast    3574097    97  7    54.6  40.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we get 7 unique rows from this. We do not get duplicated rows from the overlap in &lt;code&gt;1:5&lt;/code&gt; and &lt;code&gt;3:7&lt;/code&gt;. If we were to &lt;code&gt;bind_rows&lt;/code&gt; on the two subsets, we would get duplicates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setdiff&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;setdiff&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The set difference between a first and second argument can be obtained with &lt;code&gt;setdiff&lt;/code&gt;. Unlike &lt;code&gt;intersect&lt;/code&gt; and &lt;code&gt;union&lt;/code&gt;, this function is not symmetric:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setdiff(1:10, 6:15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setdiff(6:15, 1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11 12 13 14 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with the functions shown above, &lt;strong&gt;dplyr&lt;/strong&gt; has a version for data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 &amp;lt;- tab[1:5,]
tab_2 &amp;lt;- tab[3:7,]
dplyr::setdiff(tab_1, tab_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     state abb region population total ev clinton trump
## 1 Alabama  AL  South    4779736   135  9    34.4  62.1
## 2  Alaska  AK   West     710231    19  3    36.6  51.3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;setequal&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;setequal&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Finally, the function &lt;code&gt;setequal&lt;/code&gt; tells us if two sets are the same, regardless of order. So notice that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setequal(1:5, 1:6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setequal(1:5, 5:1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When applied to data frames that are not equal, regardless of order, the &lt;strong&gt;dplyr&lt;/strong&gt; version provides a useful message letting us know how the sets are different:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dplyr::setequal(tab_1, tab_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Install and load the &lt;strong&gt;Lahman&lt;/strong&gt; library. This database includes data related to baseball teams. It includes summary statistics about how the players performed on offense and defense for several years. It also includes personal information about the players.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;Batting&lt;/code&gt; data frame contains the offensive statistics for all players for many years. You can see, for example, the top 10 hitters by running this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Lahman)

top &amp;lt;- Batting %&amp;gt;%
  dplyr::filter(yearID == 2016) %&amp;gt;%
  arrange(desc(HR)) %&amp;gt;%
  slice(1:10)

top %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But who are these players? We see an ID, but not the names. The player names are in this table&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Master %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see column names &lt;code&gt;nameFirst&lt;/code&gt; and &lt;code&gt;nameLast&lt;/code&gt;. Use the &lt;code&gt;left_join&lt;/code&gt; function to create a table of the top home run hitters. The table should have &lt;code&gt;playerID&lt;/code&gt;, first name, last name, and number of home runs (HR). Rewrite the object &lt;code&gt;top&lt;/code&gt; with this new table.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Now use the &lt;code&gt;Salaries&lt;/code&gt; data frame to add each player’s salary to the table you created in exercise 1. Note that salaries are different every year so make sure to filter for the year 2016, then use &lt;code&gt;right_join&lt;/code&gt;. This time show first name, last name, team, HR, and salary.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In a previous exercise, we created a tidy version of the &lt;code&gt;co2&lt;/code&gt; dataset:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co2_wide &amp;lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) %&amp;gt;%
  setNames(1:12) %&amp;gt;%
  mutate(year = 1959:1997) %&amp;gt;%
  gather(month, co2, -year, convert = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to see if the monthly trend is changing so we are going to remove the year effects and then plot the results. We will first compute the year averages. Use the &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarize&lt;/code&gt; to compute the average co2 for each year. Save in an object called &lt;code&gt;yearly_avg&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Now use the &lt;code&gt;left_join&lt;/code&gt; function to add the yearly average to the &lt;code&gt;co2_wide&lt;/code&gt; dataset. Then compute the residuals: observed co2 measure - yearly average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make a plot of the seasonal trends by year but only after removing the year effect.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-note-about-merging-and-dupliacted-rows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A note about merging and dupliacted rows&lt;/h2&gt;
&lt;p&gt;When we merge data, we have to be very careful about duplicated rows. Specifically, we have to be certain that the fields we use to join on are unique &lt;strong&gt;or&lt;/strong&gt; that we know they aren’t unique and intend to duplicate rows.&lt;/p&gt;
&lt;p&gt;When we have tidy data, we have one row per observation. When we join data that has more than one row per observation, the new dataset will no longer be tidy. For instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidyData &amp;lt;- bind_cols( state = c(&amp;#39;MI&amp;#39;,&amp;#39;CA&amp;#39;,&amp;#39;MI&amp;#39;,&amp;#39;CA&amp;#39;),
                       year = c(2001, 2002, 2001, 2002),
                       Arrests = c(10, 21, 30, 12))

notTidyData &amp;lt;- bind_cols(state = c(&amp;#39;MI&amp;#39;,&amp;#39;MI&amp;#39;,&amp;#39;MI&amp;#39;,&amp;#39;CA&amp;#39;,&amp;#39;CA&amp;#39;,&amp;#39;CA&amp;#39;),
                         County = c(&amp;#39;Ingham&amp;#39;,&amp;#39;Clinton&amp;#39;,&amp;#39;Wayne&amp;#39;,&amp;#39;Orange&amp;#39;,&amp;#39;Los Angeles&amp;#39;,&amp;#39;Kern&amp;#39;),
                         InNOut_locations = c(0,0,0,20, 31, 8))

head(tidyData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 3
##   state  year Arrests
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 MI     2001      10
## 2 CA     2002      21
## 3 MI     2001      30
## 4 CA     2002      12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(notTidyData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 3
##   state County      InNOut_locations
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;
## 1 MI    Ingham                     0
## 2 MI    Clinton                    0
## 3 MI    Wayne                      0
## 4 CA    Orange                    20
## 5 CA    Los Angeles               31
## 6 CA    Kern                       8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use the only common field, &lt;code&gt;state&lt;/code&gt; to merge &lt;code&gt;tidyData&lt;/code&gt;, which is unique on &lt;code&gt;state&lt;/code&gt; and &lt;code&gt;year&lt;/code&gt;, to &lt;code&gt;notTidyData&lt;/code&gt;, which is unique on &lt;code&gt;county&lt;/code&gt;, then every time we see &lt;code&gt;state&lt;/code&gt; in our “left” data, we will get &lt;strong&gt;all three counties in that state and for that year&lt;/strong&gt;. Even worse, we will get the &lt;code&gt;InNOut_locations&lt;/code&gt; tally repeated for every matching &lt;code&gt;state&lt;/code&gt;!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;joinedData &amp;lt;- left_join(tidyData, notTidyData, by = c(&amp;#39;state&amp;#39;))
joinedData&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 × 5
##    state  year Arrests County      InNOut_locations
##    &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;
##  1 MI     2001      10 Ingham                     0
##  2 MI     2001      10 Clinton                    0
##  3 MI     2001      10 Wayne                      0
##  4 CA     2002      21 Orange                    20
##  5 CA     2002      21 Los Angeles               31
##  6 CA     2002      21 Kern                       8
##  7 MI     2001      30 Ingham                     0
##  8 MI     2001      30 Clinton                    0
##  9 MI     2001      30 Wayne                      0
## 10 CA     2002      12 Orange                    20
## 11 CA     2002      12 Los Angeles               31
## 12 CA     2002      12 Kern                       8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we asked for all columns of &lt;code&gt;tidyData&lt;/code&gt; that matched (on &lt;code&gt;state&lt;/code&gt;) in &lt;code&gt;notTidyData&lt;/code&gt;, we get replicated &lt;code&gt;Arrests&lt;/code&gt; - look at MI in 2001 in the first three rows. The original data had 10 Arrests in Michigan in 2001. Now, &lt;em&gt;for every MI County in &lt;code&gt;notTidyData&lt;/code&gt;, we have replicated the statewide arrests!&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(joinedData$Arrests)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 219&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(tidyData$Arrests)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 73&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yikes! We now have 3x the number of arrests in our data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(joinedData$InNOut_locations)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 118&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(notTidyData$InNOut_locations)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 59&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And while we might like that we have 3x the In-N-Out locations, we definitely think our data shouldn’t suddenly have more.&lt;/p&gt;
&lt;p&gt;The reason this happens is that we do not have a unique &lt;em&gt;key&lt;/em&gt; variable. In the &lt;code&gt;WDI&lt;/code&gt; tip in Project 2, we didn’t have a single unique key variable - there were multiple values of &lt;code&gt;iso2&lt;/code&gt; country code in the data because we had multiple years for each &lt;code&gt;iso2&lt;/code&gt;. Thus, when we merged, use used &lt;code&gt;by = c(&#39;iso2&#39;,&#39;year&#39;)&lt;/code&gt; because &lt;strong&gt;the &lt;code&gt;iso2&lt;/code&gt; x &lt;code&gt;year&lt;/code&gt; combination was the unique key&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The lesson is this: always know what your join is doing. Know your unique keys. Use &lt;code&gt;sum(duplicated(tidyData$key))&lt;/code&gt; to see if all values are unique, or &lt;code&gt;NROW(unique(tidyData %&amp;gt;% dplyr::select(key1, key2)))&lt;/code&gt; to see if all rows are unique over the 2 keys (replacing “key1” and “key2” with your key fields).&lt;/p&gt;
&lt;p&gt;If it shouldn’t add rows, then make sure the new data has the same number of rows as the old one, or use &lt;code&gt;setequal&lt;/code&gt; to check.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;parsing-dates-and-times&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parsing dates and times&lt;/h1&gt;
&lt;div id=&#34;the-date-data-type&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The date data type&lt;/h2&gt;
&lt;p&gt;We have described three main types of vectors: numeric, character, and logical. In data science projects, we very often encounter variables that are dates. Although we can represent a date with a string, for example &lt;code&gt;November 2, 2017&lt;/code&gt;, once we pick a reference day, referred to as the &lt;em&gt;epoch&lt;/em&gt;, they can be converted to numbers by calculating the number of days since the epoch. Computer languages usually use January 1, 1970, as the epoch. So, for example, January 2, 2017 is day 1, December 31, 1969 is day -1, and November 2, 2017, is day 17,204.&lt;/p&gt;
&lt;p&gt;Now how should we represent dates and times when analyzing data in R? We could just use days since the epoch, but then it is almost impossible to interpret. If I tell you it’s November 2, 2017, you know what this means immediately. If I tell you it’s day 17,204, you will be quite confused. Similar problems arise with times and even more complications can appear due to time zones.&lt;/p&gt;
&lt;p&gt;For this reason, &lt;code&gt;R&lt;/code&gt; defines a data type just for dates and times. We saw an example in the polls data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
data(&amp;quot;polls_us_election_2016&amp;quot;)
polls_us_election_2016$startdate %&amp;gt;% head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2016-11-03&amp;quot; &amp;quot;2016-11-01&amp;quot; &amp;quot;2016-11-02&amp;quot; &amp;quot;2016-11-04&amp;quot; &amp;quot;2016-11-03&amp;quot;
## [6] &amp;quot;2016-11-03&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These look like strings, but they are not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(polls_us_election_2016$startdate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at what happens when we convert them to numbers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(polls_us_election_2016$startdate) %&amp;gt;% head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 17108 17106 17107 17109 17108 17108&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns them into days since the epoch. The &lt;code&gt;as.Date&lt;/code&gt; function can convert a character into a date. So to see that the epoch is day 0 we can type&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.Date(&amp;quot;1970-01-01&amp;quot;) %&amp;gt;% as.numeric&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting functions, such as those in ggplot, are aware of the date format. This means that, for example, a scatterplot can use the numeric representation to decide on the position of the point, but include the string in the labels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls_us_election_2016 %&amp;gt;% dplyr::filter(pollster == &amp;quot;Ipsos&amp;quot; &amp;amp; state ==&amp;quot;U.S.&amp;quot;) %&amp;gt;%
  ggplot(aes(startdate, rawpoll_trump)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/12-content_files/figure-html/rawpolls-vs-time-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note in particular that the month names are displayed, a very convenient feature.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lubridate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The lubridate package&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;tidyverse&lt;/strong&gt; includes functionality for dealing with dates through the &lt;strong&gt;lubridate&lt;/strong&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will take a random sample of dates to show some of the useful things one can do:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2002)
dates &amp;lt;- sample(polls_us_election_2016$startdate, 10) %&amp;gt;% sort
dates&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;2016-05-31&amp;quot; &amp;quot;2016-08-08&amp;quot; &amp;quot;2016-08-19&amp;quot; &amp;quot;2016-09-22&amp;quot; &amp;quot;2016-09-27&amp;quot;
##  [6] &amp;quot;2016-10-12&amp;quot; &amp;quot;2016-10-24&amp;quot; &amp;quot;2016-10-26&amp;quot; &amp;quot;2016-10-29&amp;quot; &amp;quot;2016-10-30&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The functions &lt;code&gt;year&lt;/code&gt;, &lt;code&gt;month&lt;/code&gt; and &lt;code&gt;day&lt;/code&gt; extract those values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(date = dates,
       month = month(dates),
       day = day(dates),
       year = year(dates))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 4
##    date       month   day  year
##    &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1 2016-05-31     5    31  2016
##  2 2016-08-08     8     8  2016
##  3 2016-08-19     8    19  2016
##  4 2016-09-22     9    22  2016
##  5 2016-09-27     9    27  2016
##  6 2016-10-12    10    12  2016
##  7 2016-10-24    10    24  2016
##  8 2016-10-26    10    26  2016
##  9 2016-10-29    10    29  2016
## 10 2016-10-30    10    30  2016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also extract the month labels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;month(dates, label = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] May Aug Aug Sep Sep Oct Oct Oct Oct Oct
## 12 Levels: Jan &amp;lt; Feb &amp;lt; Mar &amp;lt; Apr &amp;lt; May &amp;lt; Jun &amp;lt; Jul &amp;lt; Aug &amp;lt; Sep &amp;lt; ... &amp;lt; Dec&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another useful set of functions are the &lt;em&gt;parsers&lt;/em&gt; that convert strings into dates. The function &lt;code&gt;ymd&lt;/code&gt; assumes the dates are in the format YYYY-MM-DD and tries to parse as well as possible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(20090101, &amp;quot;2009-01-02&amp;quot;, &amp;quot;2009 01 03&amp;quot;, &amp;quot;2009-1-4&amp;quot;,
       &amp;quot;2009-1, 5&amp;quot;, &amp;quot;Created on 2009 1 6&amp;quot;, &amp;quot;200901 !!! 07&amp;quot;)
ymd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2009-01-01&amp;quot; &amp;quot;2009-01-02&amp;quot; &amp;quot;2009-01-03&amp;quot; &amp;quot;2009-01-04&amp;quot; &amp;quot;2009-01-05&amp;quot;
## [6] &amp;quot;2009-01-06&amp;quot; &amp;quot;2009-01-07&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A further complication comes from the fact that dates often come in different formats in which the order of year, month, and day are different. The preferred format is to show year (with all four digits), month (two digits), and then day, or what is called the ISO 8601. Specifically we use YYYY-MM-DD so that if we order the string, it will be ordered by date. You can see the function &lt;code&gt;ymd&lt;/code&gt; returns them in this format.&lt;/p&gt;
&lt;p&gt;But, what if you encounter dates such as “09/01/02”? This could be September 1, 2002 or January 2, 2009 or January 9, 2002.
In these cases, examining the entire vector of dates will help you determine what format it is by process of elimination. Once you know, you can use the many parses provided by &lt;strong&gt;lubridate&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For example, if the string is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- &amp;quot;09/01/02&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;ymd&lt;/code&gt; function assumes the first entry is the year, the second is the month, and the third is the day, so it converts it to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ymd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2009-01-02&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;mdy&lt;/code&gt; function assumes the first entry is the month, then the day, then the year:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mdy(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2002-09-01&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;lubridate&lt;/em&gt; package provides a function for every possibility:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ydm(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2009-02-01&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2001-09-02&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dmy(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2002-01-09&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dym(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2001-02-09&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;lubridate&lt;/strong&gt; package is also useful for dealing with times. In R base, you can get the current time typing &lt;code&gt;Sys.time()&lt;/code&gt;. The &lt;strong&gt;lubridate&lt;/strong&gt; package provides a slightly more advanced function, &lt;code&gt;now&lt;/code&gt;, that permits you to define the time zone:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;now()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2021-12-07 12:19:53 EST&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;now(&amp;quot;GMT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2021-12-07 17:19:53 GMT&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see all the available time zones with &lt;code&gt;OlsonNames()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;We can also extract hours, minutes, and seconds:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;now() %&amp;gt;% hour()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;now() %&amp;gt;% minute()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 19&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;now() %&amp;gt;% second()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 53.18362&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The package also includes a function to parse strings into times as well as parsers for time objects that include dates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(&amp;quot;12:34:56&amp;quot;)
hms(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;12H 34M 56S&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- &amp;quot;Nov/2/2012 12:34:56&amp;quot;
mdy_hms(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2012-11-02 12:34:56 UTC&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This package has many other useful functions. We describe two of these here that we find particularly useful.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;make_date&lt;/code&gt; function can be used to quickly create a date object. It takes three arguments: year, month, day, hour, minute, seconds, and time zone defaulting to the epoch values on UTC time. So create an date object representing, for example, July 6, 2019 we write:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_date(2019, 7, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2019-07-06&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make a vector of January 1 for the 80s we write:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_date(1980:1989)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;1980-01-01&amp;quot; &amp;quot;1981-01-01&amp;quot; &amp;quot;1982-01-01&amp;quot; &amp;quot;1983-01-01&amp;quot; &amp;quot;1984-01-01&amp;quot;
##  [6] &amp;quot;1985-01-01&amp;quot; &amp;quot;1986-01-01&amp;quot; &amp;quot;1987-01-01&amp;quot; &amp;quot;1988-01-01&amp;quot; &amp;quot;1989-01-01&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another very useful function is the &lt;code&gt;round_date&lt;/code&gt;. It can be used to &lt;em&gt;round&lt;/em&gt; dates to nearest year, quarter, month, week, day, hour, minutes, or seconds. So if we want to group all the polls by week of the year we can do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls_us_election_2016 %&amp;gt;%
  mutate(week = round_date(startdate, &amp;quot;week&amp;quot;)) %&amp;gt;%
  group_by(week) %&amp;gt;%
  summarize(margin = mean(rawpoll_clinton - rawpoll_trump)) %&amp;gt;%
  qplot(week, margin, data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/12-content_files/figure-html/poll-margin-versus-week-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Date objects can be added to and subtracted from with &lt;code&gt;hours&lt;/code&gt;, &lt;code&gt;minutes&lt;/code&gt;, etc.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;startDate &amp;lt;- ymd_hms(&amp;#39;2021-06-14 12:20:57&amp;#39;)

startDate + seconds(4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2021-06-14 12:21:01 UTC&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;startDate + hours(1) + days(2) - seconds(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2021-06-16 13:20:47 UTC&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can even calculate time differences in specific units:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;endDate = ymd_hms(&amp;#39;2021-06-15 01:00:00&amp;#39;)

endDate - startDate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 12.65083 hours&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;difftime(endDate, startDate, units = &amp;#39;days&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 0.5271181 days&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that both of these result in a &lt;code&gt;difftime&lt;/code&gt; object. You can use &lt;code&gt;as.numeric(difftime(endDate, startDate))&lt;/code&gt; to get the numeric difference in times.&lt;/p&gt;
&lt;p&gt;Sequences can be created as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(from = startDate, to = endDate, by = &amp;#39;hour&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;2021-06-14 12:20:57 UTC&amp;quot; &amp;quot;2021-06-14 13:20:57 UTC&amp;quot;
##  [3] &amp;quot;2021-06-14 14:20:57 UTC&amp;quot; &amp;quot;2021-06-14 15:20:57 UTC&amp;quot;
##  [5] &amp;quot;2021-06-14 16:20:57 UTC&amp;quot; &amp;quot;2021-06-14 17:20:57 UTC&amp;quot;
##  [7] &amp;quot;2021-06-14 18:20:57 UTC&amp;quot; &amp;quot;2021-06-14 19:20:57 UTC&amp;quot;
##  [9] &amp;quot;2021-06-14 20:20:57 UTC&amp;quot; &amp;quot;2021-06-14 21:20:57 UTC&amp;quot;
## [11] &amp;quot;2021-06-14 22:20:57 UTC&amp;quot; &amp;quot;2021-06-14 23:20:57 UTC&amp;quot;
## [13] &amp;quot;2021-06-15 00:20:57 UTC&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Weekly Writing</title>
      <link>https://ssc442.netlify.app/assignment/weekly3/</link>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/weekly3/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#weekly-writing-prompt&#34;&gt;Weekly Writing Prompt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;weekly-writing-prompt&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Weekly Writing Prompt&lt;/h3&gt;
&lt;p&gt;We’re nearing the end of weekly writings.&lt;/p&gt;
&lt;p&gt;This will be the last one (except a catch-up at the VERY end).&lt;/p&gt;
&lt;p&gt;You must find two datasets on bls.gov (link &lt;a href=&#34;https://www.bls.gov/data/&#34;&gt;here&lt;/a&gt; ) and join them together. &lt;em&gt;Hint: try joining by State!&lt;/em&gt;. You must then make a single visualization.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Classification</title>
      <link>https://ssc442.netlify.app/content/11-content/</link>
      <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/11-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#classification-overview&#34;&gt;Overview&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#visualization-for-classification&#34;&gt;Visualization for Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-simple-classifier&#34;&gt;A Simple Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#metrics-for-classification&#34;&gt;Metrics for Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic Regression&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linear-regression-and-binary-responses&#34;&gt;Linear Regression and Binary Responses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bayes-classifier&#34;&gt;Bayes Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression-with-glm&#34;&gt;Logistic Regression with &lt;code&gt;glm()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#roc-curves&#34;&gt;ROC Curves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multinomial-logistic-regression&#34;&gt;Multinomial Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf&#34;&gt;Chapter 4&lt;/a&gt; in &lt;em&gt;Introduction to Statistical Learning with Applications in R&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How do we make predictions about binary responses?&lt;/li&gt;
&lt;li&gt;Why should we be concerned about using simple linear regression?&lt;/li&gt;
&lt;li&gt;What is the right way to assess the accuracy of such a model?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt; is a form of &lt;strong&gt;supervised learning&lt;/strong&gt; where the response variable is categorical, as opposed to numeric for regression. &lt;em&gt;Our goal is to find a rule, algorithm, or function which takes as input a feature vector, and outputs a category which is the true category as often as possible.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./10-content_files/classification.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is, the classifier &lt;span class=&#34;math inline&#34;&gt;\(\hat{C}(x)\)&lt;/span&gt; returns the predicted category &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{y}(x) = \hat{C}(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To build our first classifier, we will use the &lt;code&gt;Default&lt;/code&gt; dataset from the &lt;code&gt;ISLR&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ISLR)
library(tibble)
as_tibble(Default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,000 × 4
##    default student balance income
##    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 No      No         730. 44362.
##  2 No      Yes        817. 12106.
##  3 No      No        1074. 31767.
##  4 No      No         529. 35704.
##  5 No      No         786. 38463.
##  6 No      Yes        920.  7492.
##  7 No      No         826. 24905.
##  8 No      Yes        809. 17600.
##  9 No      No        1161. 37469.
## 10 No      No           0  29275.
## # … with 9,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our goal is to properly classify individuals as defaulters based on student status, credit card balance, and income. Be aware that the response &lt;code&gt;default&lt;/code&gt; is a factor, as is the predictor &lt;code&gt;student&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(Default$default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(Default$student)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we did with regression, we test-train split our data. In this case, using 50% for each.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42069)
default_idx   = sample(nrow(Default), 5000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;visualization-for-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualization for Classification&lt;/h2&gt;
&lt;p&gt;Often, some simple visualizations can suggest simple classification rules. To quickly create some useful visualizations, we use the &lt;code&gt;featurePlot()&lt;/code&gt; function from the &lt;code&gt;caret()&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A density plot can often suggest a simple split based on a numeric predictor. Essentially this plot graphs a density estimate&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{f}_{X_i}(x_i \mid Y = k)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for each numeric predictor &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and each category &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of the response &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;featurePlot(x = default_trn[, c(&amp;quot;balance&amp;quot;, &amp;quot;income&amp;quot;)],
            y = default_trn$default,
            plot = &amp;quot;density&amp;quot;,
            scales = list(x = list(relation = &amp;quot;free&amp;quot;),
                          y = list(relation = &amp;quot;free&amp;quot;)),
            adjust = 1.5,
            pch = &amp;quot;|&amp;quot;,
            layout = c(2, 1),
            auto.key = list(columns = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/11-content_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some notes about the arguments to this function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is a data frame containing only &lt;strong&gt;numeric predictors&lt;/strong&gt;. It would be nonsensical to estimate a density for a categorical predictor.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; is the response variable. It needs to be a factor variable. If coded as &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, you will need to coerce to factor for plotting.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;plot&lt;/code&gt; specifies the type of plot, here &lt;code&gt;density&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scales&lt;/code&gt; defines the scale of the axes for each plot. By default, the axis of each plot would be the same, which often is not useful, so the arguments here, a different axis for each plot, will almost always be used.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;adjust&lt;/code&gt; specifies the amount of smoothing used for the density estimate.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pch&lt;/code&gt; specifies the &lt;strong&gt;p&lt;/strong&gt;lot &lt;strong&gt;ch&lt;/strong&gt;aracter used for the bottom of the plot.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;layout&lt;/code&gt; places the individual plots into rows and columns. For some odd reason, it is given as (col, row).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;auto.key&lt;/code&gt; defines the key at the top of the plot. The number of columns should be the number of categories.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It seems that the income variable by itself is not particularly useful. However, there seems to be a big difference in default status at a &lt;code&gt;balance&lt;/code&gt; of about 1400. We will use this information shortly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;featurePlot(x = default_trn[, c(&amp;quot;balance&amp;quot;, &amp;quot;income&amp;quot;)],
            y = default_trn$student,
            plot = &amp;quot;density&amp;quot;,
            scales = list(x = list(relation = &amp;quot;free&amp;quot;),
                          y = list(relation = &amp;quot;free&amp;quot;)),
            adjust = 1.5,
            pch = &amp;quot;|&amp;quot;,
            layout = c(2, 1),
            auto.key = list(columns = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/11-content_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, we create a similar plot, except with &lt;code&gt;student&lt;/code&gt; as the response. We see that students often carry a slightly larger balance, and have far lower income. This will be useful to know when making more complicated classifiers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;featurePlot(x = default_trn[, c(&amp;quot;student&amp;quot;, &amp;quot;balance&amp;quot;, &amp;quot;income&amp;quot;)],
            y = default_trn$default,
            plot = &amp;quot;pairs&amp;quot;,
            auto.key = list(columns = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/11-content_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use &lt;code&gt;plot = &#34;pairs&#34;&lt;/code&gt; to consider multiple variables at the same time. This plot reinforces using &lt;code&gt;balance&lt;/code&gt; to create a classifier, and again shows that &lt;code&gt;income&lt;/code&gt; seems not that useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ellipse)
featurePlot(x = default_trn[, c(&amp;quot;balance&amp;quot;, &amp;quot;income&amp;quot;)],
            y = default_trn$default,
            plot = &amp;quot;ellipse&amp;quot;,
            auto.key = list(columns = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/11-content_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar to &lt;code&gt;pairs&lt;/code&gt; is a plot of type &lt;code&gt;ellipse&lt;/code&gt;, which requires the &lt;code&gt;ellipse&lt;/code&gt; package. Here we only use numeric predictors, as essentially we are assuming multivariate normality. The ellipses mark points of equal density. This will be useful later when discussing LDA and QDA.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simple-classifier&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Simple Classifier&lt;/h2&gt;
&lt;p&gt;A very simple classifier is a rule based on a boundary &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; for a particular input variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; x &amp;gt; b \\
      0 &amp;amp; x \leq b
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Based on the first plot, we believe we can use &lt;code&gt;balance&lt;/code&gt; to create a reasonable classifier. In particular,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(\texttt{balance}) =
\begin{cases}
      \text{Yes} &amp;amp; \texttt{balance} &amp;gt; 1400 \\
      \text{No} &amp;amp; \texttt{balance} \leq 1400
   \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we predict an individual is a defaulter if their &lt;code&gt;balance&lt;/code&gt; is above 1400, and not a defaulter if the balance is 1400 or less.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simple_class = function(x, boundary, above = 1, below = 0) {
  ifelse(x &amp;gt; boundary, above, below)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We write a simple &lt;code&gt;R&lt;/code&gt; function that compares a variable to a boundary, then use it to make predictions on the train and test sets with our chosen variable and boundary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;default_trn_pred = simple_class(x = default_trn$balance,
                                boundary = 1400, above = &amp;quot;Yes&amp;quot;, below = &amp;quot;No&amp;quot;)
default_tst_pred = simple_class(x = default_tst$balance,
                                boundary = 1400, above = &amp;quot;Yes&amp;quot;, below = &amp;quot;No&amp;quot;)
head(default_tst_pred, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;metrics-for-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metrics for Classification&lt;/h2&gt;
&lt;p&gt;In the classification setting, there are a large number of metrics to assess how well a classifier is performing.&lt;/p&gt;
&lt;p&gt;One of the most obvious things to do is arrange predictions and true values in a cross table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(trn_tab = table(predicted = default_trn_pred, actual = default_trn$default))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          actual
## predicted   No  Yes
##       No  4361   24
##       Yes  464  151&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(tst_tab = table(predicted = default_tst_pred, actual = default_tst$default))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          actual
## predicted   No  Yes
##       No  4319   28
##       Yes  523  130&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Often we give specific names to individual cells of these tables, and in the predictive setting, we would call this table a &lt;a href=&#34;https://en.wikipedia.org/wiki/Confusion_matrix&#34;&gt;&lt;strong&gt;confusion matrix&lt;/strong&gt;&lt;/a&gt;. Be aware, that the placement of Actual and Predicted values affects the names of the cells, and often the matrix may be presented transposed.&lt;/p&gt;
&lt;p&gt;In statistics, we label the errors Type I and Type II, but these are hard to remember. False Positive and False Negative are more descriptive, so we choose to use these.&lt;/p&gt;
&lt;!-- ![](images/confusion.png) --&gt;
&lt;p&gt;The &lt;code&gt;confusionMatrix()&lt;/code&gt; function from the &lt;code&gt;caret&lt;/code&gt; package can be used to obtain a wealth of additional information, which we see output below for the test data. Note that we specify which category is considered “positive.”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trn_con_mat  = confusionMatrix(trn_tab, positive = &amp;quot;Yes&amp;quot;)
tst_con_mat = confusionMatrix(tst_tab, positive = &amp;quot;Yes&amp;quot;)
tst_con_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##          actual
## predicted   No  Yes
##       No  4319   28
##       Yes  523  130
##                                           
##                Accuracy : 0.8898          
##                  95% CI : (0.8808, 0.8984)
##     No Information Rate : 0.9684          
##     P-Value [Acc &amp;gt; NIR] : 1               
##                                           
##                   Kappa : 0.2842          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : &amp;lt;2e-16          
##                                           
##             Sensitivity : 0.8228          
##             Specificity : 0.8920          
##          Pos Pred Value : 0.1991          
##          Neg Pred Value : 0.9936          
##              Prevalence : 0.0316          
##          Detection Rate : 0.0260          
##    Detection Prevalence : 0.1306          
##       Balanced Accuracy : 0.8574          
##                                           
##        &amp;#39;Positive&amp;#39; Class : Yes             
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most common, and most important metric is the &lt;strong&gt;classification error rate&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{err}(\hat{C}, \text{Data}) = \frac{1}{n}\sum_{i = 1}^{n}I(y_i \neq \hat{C}(x_i))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; is an indicator function, so we are essentially calculating the proportion of predicted classes that match the true class.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
I(y_i \neq \hat{C}(x)) =
\begin{cases}
  1 &amp;amp; y_i \neq \hat{C}(x) \\
  0 &amp;amp; y_i = \hat{C}(x) \\
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is also common to discuss the &lt;strong&gt;accuracy&lt;/strong&gt;, which is simply one minus the error.&lt;/p&gt;
&lt;p&gt;Like regression, we often split the data, and then consider Train (Classification) Error and Test (Classification) Error will be used as a measure of how well a classifier will work on unseen future data.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{err}_{\texttt{trn}}(\hat{C}, \text{Train Data}) = \frac{1}{n_{\texttt{trn}}}\sum_{i \in \texttt{trn}}^{}I(y_i \neq \hat{C}(x_i))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{err}_{\texttt{tst}}(\hat{C}, \text{Test Data}) = \frac{1}{n_{\texttt{tst}}}\sum_{i \in \texttt{tst}}^{}I(y_i \neq \hat{C}(x_i))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Accuracy values can be found by calling &lt;code&gt;confusionMatrix()&lt;/code&gt;, or, if stored, can be accessed directly. Here, we use them to obtain &lt;strong&gt;error rates&lt;/strong&gt; (1-Accuracy).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; 1 - trn_con_mat$overall[&amp;quot;Accuracy&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Accuracy 
##   0.0976&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Note, R doesn&amp;#39;t know to rename the result &amp;quot;err&amp;quot;, so it keeps the name &amp;quot;Accuracy&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - tst_con_mat$overall[&amp;quot;Accuracy&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Accuracy 
##   0.1102&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Note, R doesn&amp;#39;t know to rename the result &amp;quot;err&amp;quot;, so it keeps the name &amp;quot;Accuracy&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can go back to the &lt;code&gt;tst_con_mat&lt;/code&gt; table before and hand-calculate accuracy&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - ((4319 + 130) / 5000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1102&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First some notation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; is the total number of actual positives&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(TP\)&lt;/span&gt; is the total number of actual positives predicted to be positive&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the total number of actual negatives&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(TN\)&lt;/span&gt; is the total number of actual negatives predicted to be negative&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(FP\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(FN\)&lt;/span&gt; are the total number of false positives and false negatives&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Which means…&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(P = TP + FN\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N = TN + FP\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sometimes guarding against making certain errors, FP or FN, are more important than simply finding the best accuracy. Thus, sometimes we will consider &lt;strong&gt;sensitivity&lt;/strong&gt; and &lt;strong&gt;specificity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Sensitivity} = \text{True Positive Rate} = \frac{\text{TP}}{\text{P}} = \frac{\text{TP}}{\text{TP + FN}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tst_con_mat$byClass[&amp;quot;Sensitivity&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sensitivity 
##   0.8227848&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 130/(130+28)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the &lt;em&gt;share of actually-“yes” observations that were predicted by the model to be “yes”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Specificity} = \text{True Negative Rate} = \frac{\text{TN}}{\text{N}} = \frac{\text{TN}}{\text{TN + FP}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tst_con_mat$byClass[&amp;quot;Specificity&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Specificity 
##   0.8919868&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 4319/(4319/523)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specificity is the &lt;em&gt;share of actually-“no” observations that were predicted by the model to be “no”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Like accuracy, these can easily be found using &lt;code&gt;confusionMatrix()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When considering how well a classifier is performing, often, it is understandable to assume that any accuracy in a binary classification problem above 0.50 is a reasonable classifier. This however is not the case. We need to consider the &lt;strong&gt;balance&lt;/strong&gt; of the classes. To do so, we look at the &lt;strong&gt;prevalence&lt;/strong&gt; of positive cases.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Prev} = \frac{\text{P}}{\text{Total Obs}}= \frac{\text{TP + FN}}{\text{Total Obs}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trn_con_mat$byClass[&amp;quot;Prevalence&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Prevalence 
##      0.035&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tst_con_mat$byClass[&amp;quot;Prevalence&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Prevalence 
##     0.0316&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# (28+130)/5000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we see an extremely low prevalence, which suggests an even simpler classifier than our current based on &lt;code&gt;balance&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(\texttt{balance}) =
\begin{cases}
      \text{No} &amp;amp; \texttt{balance} &amp;gt; 1400 \\
      \text{No} &amp;amp; \texttt{balance} \leq 1400
   \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This classifier simply classifies all observations as negative cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_all_no = simple_class(default_tst$balance,
                           boundary = 1400, above = &amp;quot;No&amp;quot;, below = &amp;quot;No&amp;quot;)
table(predicted = pred_all_no, actual = default_tst$default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          actual
## predicted   No  Yes
##        No 4842  158&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;confusionMatrix()&lt;/code&gt; function won’t even accept this table as input, because it isn’t a full matrix, only one row, so we calculate error rates directly. To do so, we write a function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_class_err(actual = default_tst$default,
               predicted = pred_all_no)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0316&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see that the error rate is exactly the prevelance of the minority class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(default_tst$default) / length(default_tst$default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##     No    Yes 
## 0.9684 0.0316&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This classifier does better than the previous. But the point is, in reality, to create a good classifier, we should obtain a test error better than 0.033, which is obtained by simply manipulating the prevalences. Next section, we’ll introduce much better classifiers which should have no problem accomplishing this task. Point is, think carefully about what you’re putting your classifier up against. Last March when we were very worried about COVID test accuracy, and when &lt;em&gt;prevalance&lt;/em&gt; was, say, 1%, it was pointed out that we could make a 99% accurate COVID test by simply returning “No COVID” for every test! We’d be the new Theranos!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Logistic Regression&lt;/h1&gt;
&lt;p&gt;In this section, we continue our discussion of classification. We introduce our first model for classification, logistic regression. To begin, we return to the &lt;code&gt;Default&lt;/code&gt; dataset from above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ISLR)
library(tibble)
as_tibble(Default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,000 × 4
##    default student balance income
##    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 No      No         730. 44362.
##  2 No      Yes        817. 12106.
##  3 No      No        1074. 31767.
##  4 No      No         529. 35704.
##  5 No      No         786. 38463.
##  6 No      Yes        920.  7492.
##  7 No      No         826. 24905.
##  8 No      Yes        809. 17600.
##  9 No      No        1161. 37469.
## 10 No      No           0  29275.
## # … with 9,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also repeat the test-train split from above (you need not repeat this step if you have this saved).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
default_idx = sample(nrow(Default), 5000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;linear-regression-and-binary-responses&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Regression and Binary Responses&lt;/h2&gt;
&lt;p&gt;Before moving on to logistic regression, why not plain, old, linear regression?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;default_trn_lm = default_trn
default_tst_lm = default_tst&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since linear regression expects a numeric response variable, we coerce the response to be numeric. (Notice that we also shift the results, as we require &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, not &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt;.) Notice we have also copied the dataset so that we can return the original data with factors later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;default_trn_lm$default = as.numeric(default_trn_lm$default) - 1
default_tst_lm$default = as.numeric(default_tst_lm$default) - 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why would we think this should work? Recall that,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mathbb{E}}[Y \mid X = x] = X\hat{\beta}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is limited to values of &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[Y \mid X = x] = P(Y = 1 \mid X = x).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It would then seem reasonable that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\hat{\beta}\)&lt;/span&gt; is a reasonable estimate of &lt;span class=&#34;math inline&#34;&gt;\(P(Y = 1 \mid X = x)\)&lt;/span&gt;. We test this on the &lt;code&gt;Default&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_lm = lm(default ~ balance, data = default_trn_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything seems to be working, until we plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(default ~ balance, data = default_trn_lm,
     col = &amp;quot;darkorange&amp;quot;, pch = &amp;quot;|&amp;quot;, ylim = c(-0.2, 1),
     main = &amp;quot;Using Linear Regression for Classification&amp;quot;)
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
abline(model_lm, lwd = 3, col = &amp;quot;dodgerblue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/11-content_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Two issues arise. First, all of the predicted probabilities are below 0.5. That means, we would classify every observation as a &lt;code&gt;&#34;No&#34;&lt;/code&gt;. This is certainly possible, but not what we would expect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(predict(model_lm) &amp;lt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next, and bigger issue, is predicted probabilities less than 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;any(predict(model_lm) &amp;lt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-classifier&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes Classifier&lt;/h2&gt;
&lt;p&gt;Why are we using a predicted probability of 0.5 as the cutoff for classification? Recall, the Bayes Classifier, which minimizes the classification error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
C^B(x) = \underset{g}{\mathrm{argmax}} \ P(Y = g \mid  X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, in the binary classification problem, we will use predicted probabilities&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid { X = x})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{P}(Y = 0 \mid { X = x})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and then classify to the larger of the two. We actually only need to consider a single probability, usually &lt;span class=&#34;math inline&#34;&gt;\(\hat{P}(Y = 1 \mid { X = x})\)&lt;/span&gt;. Since we use it so often, we give it the shorthand notation, &lt;span class=&#34;math inline&#34;&gt;\(\hat{p}(x)\)&lt;/span&gt;. Then the classifier is written,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; \hat{p}(x) &amp;gt; 0.5 \\
      0 &amp;amp; \hat{p}(x) \leq 0.5
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This classifier is essentially estimating the Bayes Classifier - it takes the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, figures out which is larger, the &lt;span class=&#34;math inline&#34;&gt;\(\hat{P}(Y=1|X=x)\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\hat{P}(Y=0 | X=x)\)&lt;/span&gt;, and returns the classification &lt;span class=&#34;math inline&#34;&gt;\(\hat{C}(x)\)&lt;/span&gt; as whichever probability is larger. Since there are only two values for &lt;span class=&#34;math inline&#34;&gt;\(Y\in\{0,1\}\)&lt;/span&gt;, the larger is always the one greater than &lt;span class=&#34;math inline&#34;&gt;\(.50\)&lt;/span&gt;. Thus, since this is a Bayes Classificer, it minimizes classification errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression-with-glm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic Regression with &lt;code&gt;glm()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;To better estimate the probability&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(x) = P(Y = 1 \mid {X = x})
\]&lt;/span&gt;
we turn to logistic regression. The model is written&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\log\left(\frac{p(x)}{1 - p(x)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Rearranging, we see the probabilities can be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p)}} = \sigma(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice, we use the sigmoid function as shorthand notation, which appears often in deep learning literature. It takes any real input, and outputs a number between 0 and 1. How useful! (This is actualy a particular sigmoid function called the logistic function, but since it is by far the most popular sigmoid function, often sigmoid function is used to refer to the logistic function)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The model is fit by numerically maximizing the likelihood, which we will let &lt;code&gt;R&lt;/code&gt; take care of. Essentially, &lt;code&gt;R&lt;/code&gt; is going to try a whole bunch of guesses for &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\beta}\)&lt;/span&gt; and choose the one that best explains the data we have.&lt;/p&gt;
&lt;p&gt;We start with a single predictor example, again using &lt;code&gt;balance&lt;/code&gt; as our single predictor. Note that &lt;code&gt;default_trn&lt;/code&gt; has a factor variable for &lt;code&gt;default&lt;/code&gt; (No/Yes). Since &lt;code&gt;R&lt;/code&gt; represents factor variables as numbers (here, 1 and 2), &lt;code&gt;glm&lt;/code&gt; figures out that you mean &lt;code&gt;No&lt;/code&gt; and &lt;code&gt;Yes&lt;/code&gt; for &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glm = glm(default ~ balance, data = default_trn, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fitting this model looks very similar to fitting a simple linear regression. Instead of &lt;code&gt;lm()&lt;/code&gt; we use &lt;code&gt;glm()&lt;/code&gt;. The only other difference is the use of &lt;code&gt;family = &#34;binomial&#34;&lt;/code&gt; which indicates that we have a two-class categorical response. Using &lt;code&gt;glm()&lt;/code&gt; with &lt;code&gt;family = &#34;gaussian&#34;&lt;/code&gt; would perform the usual linear regression.&lt;/p&gt;
&lt;p&gt;First, we can obtain the fitted coefficients the same way we did with linear regression.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(model_glm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)       balance 
## -10.493158288   0.005424994&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next thing we should understand is how the &lt;code&gt;predict()&lt;/code&gt; function works with &lt;code&gt;glm()&lt;/code&gt;. So, let’s look at some predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_glm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2369      5273      9290      1252      8826       356 
## -5.376670 -4.875653 -5.018746 -4.007664 -6.538414 -6.601582&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, &lt;code&gt;predict.glm()&lt;/code&gt; uses &lt;code&gt;type = &#34;link&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_glm, type = &amp;quot;link&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2369      5273      9290      1252      8826       356 
## -5.376670 -4.875653 -5.018746 -4.007664 -6.538414 -6.601582&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is, &lt;code&gt;R&lt;/code&gt; is returning&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p
\]&lt;/span&gt;
for each observation.&lt;/p&gt;
&lt;p&gt;Importantly, these are &lt;strong&gt;not&lt;/strong&gt; predicted probabilities. To obtain the predicted probabilities&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we need to use &lt;code&gt;type = &#34;response&#34;&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_glm, type = &amp;quot;response&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        2369        5273        9290        1252        8826         356 
## 0.004601914 0.007572331 0.006569370 0.017851333 0.001444691 0.001356375&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that these are probabilities, &lt;strong&gt;not&lt;/strong&gt; classifications. To obtain classifications, we will need to compare to the correct cutoff value with an &lt;code&gt;ifelse()&lt;/code&gt; statement.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glm_pred = ifelse(predict(model_glm, type = &amp;quot;link&amp;quot;) &amp;gt; 0, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)
# model_glm_pred = ifelse(predict(model_glm, type = &amp;quot;response&amp;quot;) &amp;gt; 0.5, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The line that is run is performing&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; \hat{f}(x) &amp;gt; 0 \\
      0 &amp;amp; \hat{f}(x) \leq 0
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{f}(x) =\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The commented line, which would give the same results, is performing&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; \hat{p}(x) &amp;gt; 0.5 \\
      0 &amp;amp; \hat{p}(x) \leq 0.5
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid X = x).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Once we have classifications, we can calculate metrics such as the training classification error rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_class_err(actual = default_trn$default, predicted = model_glm_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0284&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we saw previously, the &lt;code&gt;table()&lt;/code&gt; and &lt;code&gt;confusionMatrix()&lt;/code&gt; functions can be used to quickly obtain many more metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_tab = table(predicted = model_glm_pred, actual = default_trn$default)
library(caret)
train_con_mat = confusionMatrix(train_tab, positive = &amp;quot;Yes&amp;quot;)
c(train_con_mat$overall[&amp;quot;Accuracy&amp;quot;],
  train_con_mat$byClass[&amp;quot;Sensitivity&amp;quot;],
  train_con_mat$byClass[&amp;quot;Specificity&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Accuracy Sensitivity Specificity 
##   0.9716000   0.2941176   0.9954451&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also write a custom function for the error for use with trained logistic regression models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_logistic_error = function(mod, data, res = &amp;quot;y&amp;quot;, pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = &amp;quot;response&amp;quot;)
  preds = ifelse(probs &amp;gt; cut, pos, neg)
  calc_class_err(actual = data[, res], predicted = preds)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function will be useful later when calculating train and test errors for several models at the same time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_logistic_error(model_glm, data = default_trn,
                   res = &amp;quot;default&amp;quot;, pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0284&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see how much better logistic regression is for this task, we create the same plot we used for linear regression.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(default ~ balance, data = default_trn_lm,
     col = &amp;quot;darkorange&amp;quot;, pch = &amp;quot;|&amp;quot;, ylim = c(-0.2, 1),
     main = &amp;quot;Using Logistic Regression for Classification&amp;quot;)
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
curve(predict(model_glm, data.frame(balance = x), type = &amp;quot;response&amp;quot;),
      add = TRUE, lwd = 3, col = &amp;quot;dodgerblue&amp;quot;)
abline(v = -coef(model_glm)[1] / coef(model_glm)[2], lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/11-content_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot contains a wealth of information.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The orange &lt;code&gt;|&lt;/code&gt; characters are the data, &lt;span class=&#34;math inline&#34;&gt;\((x_i, y_i)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The blue “curve” is the predicted probabilities given by the fitted logistic regression. That is,
&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid { X = x})
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The solid vertical black line represents the &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_boundary&#34;&gt;decision boundary&lt;/a&gt;&lt;/strong&gt;, the &lt;code&gt;balance&lt;/code&gt; that obtains a predicted probability of 0.5. In this case &lt;code&gt;balance&lt;/code&gt; = 1934.2247145.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The decision boundary is found by solving for points that satisfy&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid { X = x}) = 0.5
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is equivalent to point that satisfy&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 = 0.
\]&lt;/span&gt;
Thus, for logistic regression with a single predictor, the decision boundary is given by the &lt;em&gt;point&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_1 = \frac{-\hat{\beta}_0}{\hat{\beta}_1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following is not run, but an alternative way to add the logistic curve to the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid = seq(0, max(default_trn$balance), by = 0.01)

sigmoid = function(x) {
  1 / (1 + exp(-x))
}

lines(grid, sigmoid(coef(model_glm)[1] + coef(model_glm)[2] * grid), lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the usual formula syntax, it is easy to add or remove complexity from logistic regressions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_1 = glm(default ~ 1, data = default_trn, family = &amp;quot;binomial&amp;quot;)
model_2 = glm(default ~ ., data = default_trn, family = &amp;quot;binomial&amp;quot;)
model_3 = glm(default ~ . ^ 2 + I(balance ^ 2),
              data = default_trn, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, using polynomial transformations of predictors will allow a linear model to have non-linear decision boundaries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_list = list(model_1, model_2, model_3)
train_errors = sapply(model_list, get_logistic_error, data = default_trn,
                      res = &amp;quot;default&amp;quot;, pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.5)
test_errors  = sapply(model_list, get_logistic_error, data = default_tst,
                      res = &amp;quot;default&amp;quot;, pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.5)

knitr::kable(cbind(train_errors, test_errors))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;train_errors&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;test_errors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.0340&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0326&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.0274&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0258&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.0274&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0264&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here we see the misclassification error rates for each model. The train (weakly) decreases, and the test decreases, until it starts to increases. Everything we learned about the bias-variance tradeoff for regression also applies here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff(train_errors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.0066  0.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff(test_errors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.0068  0.0006&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We call &lt;code&gt;model_2&lt;/code&gt; the &lt;strong&gt;additive&lt;/strong&gt; logistic model, which we will use quite often.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curves&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ROC Curves&lt;/h2&gt;
&lt;p&gt;Let’s return to our simple model with only balance as a predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glm = glm(default ~ balance, data = default_trn, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We write a function which allows use to make predictions based on different probability cutoffs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_logistic_pred = function(mod, data, res = &amp;quot;y&amp;quot;, pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = &amp;quot;response&amp;quot;)
  ifelse(probs &amp;gt; cut, pos, neg)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; \hat{p}(x) &amp;gt; c \\
      0 &amp;amp; \hat{p}(x) \leq c
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s use this to obtain predictions using a low, medium, and high cutoff. (0.1, 0.5, and 0.9)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_pred_10 = get_logistic_pred(model_glm, data = default_tst, res = &amp;quot;default&amp;quot;,
                                 pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.1)
test_pred_50 = get_logistic_pred(model_glm, data = default_tst, res = &amp;quot;default&amp;quot;,
                                 pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.5)
test_pred_90 = get_logistic_pred(model_glm, data = default_tst, res = &amp;quot;default&amp;quot;,
                                 pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we evaluate accuracy, sensitivity, and specificity for these classifiers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_tab_10 = table(predicted = test_pred_10, actual = default_tst$default)
test_tab_50 = table(predicted = test_pred_50, actual = default_tst$default)
test_tab_90 = table(predicted = test_pred_90, actual = default_tst$default)

test_con_mat_10 = confusionMatrix(test_tab_10, positive = &amp;quot;Yes&amp;quot;)
test_con_mat_50 = confusionMatrix(test_tab_50, positive = &amp;quot;Yes&amp;quot;)
test_con_mat_90 = confusionMatrix(test_tab_90, positive = &amp;quot;Yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics = rbind(

  c(test_con_mat_10$overall[&amp;quot;Accuracy&amp;quot;],
    test_con_mat_10$byClass[&amp;quot;Sensitivity&amp;quot;],
    test_con_mat_10$byClass[&amp;quot;Specificity&amp;quot;]),

  c(test_con_mat_50$overall[&amp;quot;Accuracy&amp;quot;],
    test_con_mat_50$byClass[&amp;quot;Sensitivity&amp;quot;],
    test_con_mat_50$byClass[&amp;quot;Specificity&amp;quot;]),

  c(test_con_mat_90$overall[&amp;quot;Accuracy&amp;quot;],
    test_con_mat_90$byClass[&amp;quot;Sensitivity&amp;quot;],
    test_con_mat_90$byClass[&amp;quot;Specificity&amp;quot;])

)

rownames(metrics) = c(&amp;quot;c = 0.10&amp;quot;, &amp;quot;c = 0.50&amp;quot;, &amp;quot;c = 0.90&amp;quot;)
metrics&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Accuracy Sensitivity Specificity
## c = 0.10   0.9328  0.71779141   0.9400455
## c = 0.50   0.9730  0.31288344   0.9952450
## c = 0.90   0.9688  0.04294479   1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see then sensitivity decreases as the cutoff is increased. Conversely, specificity increases as the cutoff increases. This is useful if we are more interested in a particular error, instead of giving them equal weight.&lt;/p&gt;
&lt;p&gt;Note that usually the best accuracy will be seen near &lt;span class=&#34;math inline&#34;&gt;\(c = 0.50\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead of manually checking cutoffs, we can create an ROC curve (receiver operating characteristic curve) which will sweep through all possible cutoffs, and plot the sensitivity and specificity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Where on this curve would you think is the “best” place to be? Why?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Where on this curve would you think is the “worst” place to be? Why?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pROC)
test_prob = predict(model_glm, newdata = default_tst, type = &amp;quot;response&amp;quot;)
test_roc = roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/11-content_files/figure-html/unnamed-chunk-53-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(test_roc$auc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9492866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The AUC is the “area under the curve”. One interpretation of the AUC is that it is “the probability that the model ranks a randomly selected positive more highly than a randomly selected negative.” A good model will have a high AUC. A high AUC has a high sensitivity and a high specificity over all of the cutoff values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multinomial-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multinomial Logistic Regression&lt;/h2&gt;
&lt;p&gt;What if the response contains more than two categories? For that we need multinomial logistic regression.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(Y = k \mid { X = x}) = \frac{e^{\beta_{0k} + \beta_{1k} x_1 + \cdots +  + \beta_{pk} x_p}}{\sum_{g = 1}^{G} e^{\beta_{0g} + \beta_{1g} x_1 + \cdots + \beta_{pg} x_p}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will omit the details, as ISL has as well. If you are interested, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multinomial_logistic_regression&#34;&gt;Wikipedia page&lt;/a&gt; provides a rather thorough coverage. Also note that the above is an example of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Softmax_function&#34;&gt;softmax function&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As an example of a dataset with a three category response, we use the &lt;code&gt;iris&lt;/code&gt; dataset, which is so famous, it has its own &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;Wikipedia entry&lt;/a&gt;. It is also a default dataset in &lt;code&gt;R&lt;/code&gt;, so no need to load it.&lt;/p&gt;
&lt;p&gt;Before proceeding, we test-train split this data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(430)
iris_obs = nrow(iris)
iris_idx = sample(iris_obs, size = trunc(0.50 * iris_obs))
iris_trn = iris[iris_idx, ]
iris_test = iris[-iris_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To perform multinomial logistic regression, we use the &lt;code&gt;multinom&lt;/code&gt; function from the &lt;code&gt;nnet&lt;/code&gt; package. Training using &lt;code&gt;multinom()&lt;/code&gt; is done using similar syntax to &lt;code&gt;lm()&lt;/code&gt; and &lt;code&gt;glm()&lt;/code&gt;. We add the &lt;code&gt;trace = FALSE&lt;/code&gt; argument to suppress information about updates to the optimization routine as the model is trained.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nnet)
model_multi = multinom(Species ~ ., data = iris_trn, trace = FALSE)
summary(model_multi)$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width
## versicolor    16.77474    -7.855576   -13.98668     25.13860    4.270375
## virginica    -33.94895   -37.519645   -94.22846     97.82691   73.487162&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice we are only given coefficients for two of the three class, much like only needing coefficients for one class in logistic regression.&lt;/p&gt;
&lt;p&gt;A difference between &lt;code&gt;glm()&lt;/code&gt; and &lt;code&gt;multinom()&lt;/code&gt; is how the &lt;code&gt;predict()&lt;/code&gt; function operates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_multi, newdata = iris_trn))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] setosa     versicolor versicolor setosa     virginica  versicolor
## Levels: setosa versicolor virginica&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_multi, newdata = iris_trn, type = &amp;quot;prob&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           setosa   versicolor     virginica
## 1   1.000000e+00 1.910554e-16 6.118616e-176
## 92  8.542846e-22 1.000000e+00  1.372168e-18
## 77  8.343856e-23 1.000000e+00  2.527471e-14
## 38  1.000000e+00 1.481126e-16 5.777917e-180
## 108 1.835279e-73 1.403654e-36  1.000000e+00
## 83  1.256090e-16 1.000000e+00  2.223689e-32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that by default, classifications are returned. When obtaining probabilities, we are given the predicted probability for &lt;strong&gt;each&lt;/strong&gt; class.&lt;/p&gt;
&lt;p&gt;Interestingly, you’ve just fit a neural network, and you didn’t even know it! (Hence the &lt;code&gt;nnet&lt;/code&gt; package.) Later we will discuss the connections between logistic regression, multinomial logistic regression, and simple neural networks.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonparametric Regression</title>
      <link>https://ssc442.netlify.app/content/10-content/</link>
      <pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/10-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-ideas&#34;&gt;Guiding Ideas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nonparametric-regression&#34;&gt;Nonparametric Regression&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-setup&#34;&gt;R Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mathematical-setup&#34;&gt;Mathematical Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbors&#34;&gt;k-Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decision-trees&#34;&gt;Decision Trees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-credit-card-data&#34;&gt;Example: Credit Card Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page. That’s it.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-ideas&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How to use &lt;strong&gt;k-nearest neighbors&lt;/strong&gt; for regression through the use of the &lt;code&gt;knnreg()&lt;/code&gt; function from the &lt;code&gt;caret&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;How to use &lt;strong&gt;decision trees&lt;/strong&gt; for regression through the use of the &lt;code&gt;rpart()&lt;/code&gt; function from the &lt;code&gt;rpart&lt;/code&gt; package.&lt;/li&gt;
&lt;li&gt;How “making predictions” can be thought of as &lt;strong&gt;estimating the regression function&lt;/strong&gt;, that is, the conditional mean of the response given values of the features.&lt;/li&gt;
&lt;li&gt;The difference between &lt;strong&gt;parametric&lt;/strong&gt; and &lt;strong&gt;nonparametric&lt;/strong&gt; methods.&lt;/li&gt;
&lt;li&gt;The difference between &lt;strong&gt;model parameters&lt;/strong&gt; and &lt;strong&gt;tuning parameters&lt;/strong&gt; methods.&lt;/li&gt;
&lt;li&gt;How these nonparametric methods deal with &lt;strong&gt;categorical variables&lt;/strong&gt; and &lt;strong&gt;interactions&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;What is &lt;strong&gt;model flexibility&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;What is &lt;strong&gt;overfitting&lt;/strong&gt; and how do we avoid it?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;nonparametric-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nonparametric Regression&lt;/h1&gt;
&lt;p&gt;In the next weeks, we will continue to explore models for making &lt;strong&gt;predictions&lt;/strong&gt;, but now we will introduce &lt;strong&gt;nonparametric models&lt;/strong&gt; that will contrast the &lt;strong&gt;parametric models&lt;/strong&gt; that we have used previously.&lt;/p&gt;
&lt;p&gt;This content is currently &lt;strong&gt;under construction&lt;/strong&gt;. You can expect it to be a lot less polished than other sections.&lt;/p&gt;
&lt;div id=&#34;r-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R Setup&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tibble)     # data frame printing
library(dplyr)      # data manipulation

library(caret)      # fitting knn
library(rpart)      # fitting trees
library(rpart.plot) # plotting trees

library(knitr)      # creating tables
library(kableExtra) # styling tables&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mathematical-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mathematical Setup&lt;/h2&gt;
&lt;p&gt;Let’s return to the setup we defined in the previous lectures. Consider a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; which represents a &lt;strong&gt;response&lt;/strong&gt; variable, and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; &lt;strong&gt;feature&lt;/strong&gt; variables &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{X} = (X_1, X_2, \ldots, X_p)\)&lt;/span&gt;. We assume that the response variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is some function of the features, plus some random noise.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = f(\boldsymbol{X}) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our goal is to find some &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(f(\boldsymbol{X})\)&lt;/span&gt; is close to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. More specifically we want to minimize the risk under squared error loss.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}_{\boldsymbol{X}, Y} \left[ (Y - f(\boldsymbol{X})) ^ 2 \right] = \mathbb{E}_{\boldsymbol{X}} \mathbb{E}_{Y \mid \boldsymbol{X}} \left[ ( Y - f(\boldsymbol{X}) ) ^ 2 \mid \boldsymbol{X} = \boldsymbol{x} \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We saw previously (see the slides from last two content days) that this risk is minimized by the &lt;strong&gt;conditional mean&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{X}\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu(\boldsymbol{x}) \triangleq \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which we call the &lt;strong&gt;regression function&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Our goal then is to &lt;strong&gt;estimate&lt;/strong&gt; this &lt;strong&gt;regression function&lt;/strong&gt;. Let’s use an example where we know the true probability model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = 1 - 2x - 3x ^ 2 + 5x ^ 3 + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon \sim \text{N}(0, \sigma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recall that this implies that the regression function is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}] = 1 - 2x - 3x ^ 2 + 5x ^ 3
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s also pretend that we do not actually know this information, but instead have some data, &lt;span class=&#34;math inline&#34;&gt;\((x_i, y_i)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i = 1, 2, \ldots, n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We simulate enough data to make the “pattern” clear-ish to recognize.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we use a linear model, we first need to make an &lt;strong&gt;assumption&lt;/strong&gt; about the form of the regression function.&lt;/p&gt;
&lt;p&gt;For example, we could assume that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}] = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is fit in R using the &lt;code&gt;lm()&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ x + I(x ^ 2) + I(x ^ 3), data = sim_slr_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + I(x^2) + I(x^3), data = sim_slr_data)
## 
## Coefficients:
## (Intercept)            x       I(x^2)       I(x^3)  
##      0.8397      -2.7257      -2.3752       6.0906&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that what is returned are (maximum likelihood or least squares) estimates of the unknown &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; coefficients. That is, the “learning” that takes place with a linear models is “learning” the values of the coefficients.&lt;/p&gt;
&lt;p&gt;For this reason, we call linear regression models &lt;strong&gt;parametric&lt;/strong&gt; models. They have unknown &lt;strong&gt;model parameters&lt;/strong&gt;, in this case the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; coefficients that must be learned from the data. The form of the regression function is assumed.&lt;/p&gt;
&lt;p&gt;What if we don’t want to make an assumption about the form of the regression function? While in this case, you might look at the plot and arrive at a reasonable guess of assuming a third order polynomial, what if it isn’t so clear? What if you have 100 features? Making strong assumptions might not work well.&lt;/p&gt;
&lt;p&gt;Enter &lt;strong&gt;nonparametric&lt;/strong&gt; models. We will consider two examples: &lt;strong&gt;k-nearest neighbors&lt;/strong&gt; and &lt;strong&gt;decision trees&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;k-Nearest Neighbors&lt;/h2&gt;
&lt;p&gt;We’ll start with &lt;strong&gt;k-nearest neighbors&lt;/strong&gt; which is possibly a more intuitive procedure than linear models.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If our goal is to estimate the mean function,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the most natural approach would be to use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{average}(\{ y_i : x_i = x \}).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That is, to estimate the conditional mean at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, average the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values for each data point where &lt;span class=&#34;math inline&#34;&gt;\(x_i = x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;While this sounds nice, it has an obvious flaw. For most values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; there will not be any &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in the data where &lt;span class=&#34;math inline&#34;&gt;\(x_i = x\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;So what’s the next best thing? Pick values of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; that are “close” to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{average}( \{ y_i : x_i \text{ equal to (or very close to) x} \} ).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the main idea behind many nonparametric approaches. The details often just amount to very specifically defining what “close” means.&lt;/p&gt;
&lt;p&gt;In the case of k-nearest neighbors we use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mu}_k(x) = \frac{1}{k} \sum_{ \{i \ : \ x_i \in \mathcal{N}_k(x, \mathcal{D}) \} } y_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;as our estimate of the regression function at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. While this looks complicated, it is actually very simple. Here, we are using an average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; nearest neighbors to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; “nearest” neighbors are the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; data points &lt;span class=&#34;math inline&#34;&gt;\((x_i, y_i)\)&lt;/span&gt; that have &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; values that are nearest to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We can define “nearest” using any distance we like, but unless otherwise noted, we are referring to euclidean distance.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; We are using the notation &lt;span class=&#34;math inline&#34;&gt;\(\{i \ : \ x_i \in \mathcal{N}_k(x, \mathcal{D}) \}\)&lt;/span&gt; to define the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; observations that have &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; values that are nearest to the value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in a dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, in other words, the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; nearest neighbors.&lt;/p&gt;
&lt;p&gt;The plots below begin to illustrate this idea.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the left plot, to estimate the mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = -0.5\)&lt;/span&gt; we use the three nearest neighbors, which are highlighted with green. Our estimate is the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of these three points indicated by the black x.&lt;/li&gt;
&lt;li&gt;In the middle plot, to estimate the mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = 0\)&lt;/span&gt; we use the five nearest neighbors, which are highlighted with green. Our estimate is the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of these five points indicated by the black x.&lt;/li&gt;
&lt;li&gt;In the right plot, to estimate the mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = 0.75\)&lt;/span&gt; we use the nine nearest neighbors, which are highlighted with green. Our estimate is the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of these nine points indicated by the black x.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might begin to notice a bit of an issue here. We have to do a new calculation each time we want to estimate the regression function at a different value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;! For this reason, k-nearest neighbors is often said to be “fast to train” and “slow to predict.” Training, is instant. You just memorize the data! Prediction involves finding the distance between the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; considered and all &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in the data!&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, how then, do we choose the value of the &lt;strong&gt;tuning&lt;/strong&gt; parameter &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;? We &lt;em&gt;&lt;strong&gt;validate&lt;/strong&gt;&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;First, let’s take a look at what happens with this data if we consider three different values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For each plot, the black dashed curve is the true mean function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the left plot we use &lt;span class=&#34;math inline&#34;&gt;\(k = 25\)&lt;/span&gt;. The red “curve” is the estimate of the mean function for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; shown in the plot.&lt;/li&gt;
&lt;li&gt;In the left plot we use &lt;span class=&#34;math inline&#34;&gt;\(k = 5\)&lt;/span&gt;. The blue “curve” is the estimate of the mean function for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; shown in the plot.&lt;/li&gt;
&lt;li&gt;In the left plot we use &lt;span class=&#34;math inline&#34;&gt;\(k = 1\)&lt;/span&gt;. The green “curve” is the estimate of the mean function for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; shown in the plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some things to notice here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The left plot with &lt;span class=&#34;math inline&#34;&gt;\(k = 25\)&lt;/span&gt; is performing poorly. The estimated “curve” does not “move” enough. This is an example of an &lt;strong&gt;inflexible&lt;/strong&gt; model.&lt;/li&gt;
&lt;li&gt;The right plot with &lt;span class=&#34;math inline&#34;&gt;\(k = 1\)&lt;/span&gt; might not perform too well. The estimated “curve” seems to “move” too much. (Notice, that it goes through each point. We’ve fit to the noise.) This is an example of a &lt;strong&gt;flexible&lt;/strong&gt; model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the middle plot with &lt;span class=&#34;math inline&#34;&gt;\(k = 5\)&lt;/span&gt; is not “perfect” it seems to roughly capture the “motion” of the true regression function. We can begin to see that if we generated new data, this estimated regression function would perform better than the other two.&lt;/p&gt;
&lt;p&gt;But remember, in practice, we won’t know the true regression function, so we will need to determine how our model performs using only the available data!&lt;/p&gt;
&lt;p&gt;This &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, the number of neighbors, is an example of a &lt;strong&gt;tuning parameter&lt;/strong&gt;. Instead of being learned from the data, like model parameters such as the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; coefficients in linear regression, a tuning parameter tells us &lt;em&gt;how&lt;/em&gt; to learn from data. It is user-specified. To determine the value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; that should be used, many models are fit to the estimation data, then evaluated on the validation. Using the information from the validation data, a value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is chosen. (More on this in a bit.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Model parameters&lt;/strong&gt; are “learned” using the same data that was used to fit the model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tuning parameters&lt;/strong&gt; are “chosen” using data not used to fit the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tuning parameter &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; also defines the &lt;strong&gt;flexibility&lt;/strong&gt; of the model. In KNN, a small value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is a flexible model, while a large value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is inflexible.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Before moving to an example of tuning a KNN model, we will first introduce decision trees.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Decision Trees&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Decision trees&lt;/strong&gt; are similar to k-nearest neighbors but instead of looking for neighbors, decision trees create neighborhoods. We won’t explore the full details of trees, but just start to understand the basic concepts, as well as learn to fit them in R.&lt;/p&gt;
&lt;p&gt;Neighborhoods are created via recursive binary partitions. In simpler terms, pick a feature and a possible cutoff value. Data that have a value less than the cutoff for the selected feature are in one neighborhood (the left) and data that have a value greater than the cutoff are in another (the right). Within these two neighborhoods, repeat this procedure until a stopping rule is satisfied. To make a prediction, check which neighborhood a new piece of data would belong to and predict the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of data in that neighborhood.&lt;/p&gt;
&lt;p&gt;With the data above, which has a single feature &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, consider three possible cutoffs: -0.5, 0.0, and 0.75.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For each plot, the black vertical line defines the neighborhoods. The green horizontal lines are the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values for the points in the left neighborhood. The red horizontal lines are the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values for the points in the right neighborhood.&lt;/p&gt;
&lt;p&gt;What makes a cutoff good? Large differences in the average &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; between the two neighborhoods. More formally we want to find a cutoff value that minimizes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i \in N_L} \left( y_i - \hat{\mu}_{N_L} \right) ^ 2 + \sum_{i \in N_R} \left(y_i - \hat{\mu}_{N_R} \right) ^ 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N_L\)&lt;/span&gt; are the data in the left neighborhood, that is &lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; c\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N_R\)&lt;/span&gt; are the data in the right neighborhood, that is &lt;span class=&#34;math inline&#34;&gt;\(x &amp;gt; c\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_{N_L}\)&lt;/span&gt; is the mean of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; for data in the left neighborhood&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_{N_R}\)&lt;/span&gt; is the mean of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; for data in the right neighborhood&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This quantity is the sum of two sum of squared errors, one for the left neighborhood, and one for the right neighborhood.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Cutoff
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Total SSE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Left SSE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Right SSE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45.02
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.74
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58.94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.68
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.26
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
56.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
55.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.25
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The table above summarizes the results of the three potential splits. We see that (of the splits considered, which are not exhaustive&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;) the split based on a cutoff of &lt;span class=&#34;math inline&#34;&gt;\(x = -0.50\)&lt;/span&gt; creates the best partitioning of the space.&lt;/p&gt;
&lt;p&gt;Now let’s consider building a full tree.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the plot above, the true regression function is the dashed black curve, and the solid orange curve is the estimated regression function using a decision tree. We see that there are two splits, which we can visualize as a tree.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above “tree”&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; shows the splits that were made. It informs us of the variable used, the cutoff value, and some summary of the resulting neighborhood. In “tree” terminology the resulting neighborhoods are “terminal nodes” of the tree. In contrast, “internal nodes” are neighborhoods that are created, but then further split.&lt;/p&gt;
&lt;p&gt;The “root node” is the neighborhood contains all observations, before any splitting, and can be seen at the top of the image above. We see that this node represents 100% of the data. The other number, 0.21, is the mean of the response variable, in this case, &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Looking at a terminal node, for example the bottom left node, we see that 23% of the data is in this node. The average value of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in this node is -1, which can be seen in the plot above.&lt;/p&gt;
&lt;p&gt;We also see that the first split is based on the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; variable, and a cutoff of &lt;span class=&#34;math inline&#34;&gt;\(x = -0.52\)&lt;/span&gt;. Note that because there is only one variable here, all splits are based on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, but in the future, we will have multiple features that can be split and neighborhoods will no longer be one-dimensional. However, this is hard to plot.&lt;/p&gt;
&lt;p&gt;Let’s build a bigger, more flexible tree.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are two tuning parameters at play here which we will call by their names in R which we will see soon:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cp&lt;/code&gt; or the “complexity parameter” as it is called.&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; This parameter determines which splits are accepted. A split must improve the performance of the tree by more than &lt;code&gt;cp&lt;/code&gt; in order to be used. When we get to R, we will see that the default value is 0.1.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;minsplit&lt;/code&gt;, the minimum number of observations in a node (neighborhood) in order to consider splitting within a neighborhood.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are actually many more possible tuning parameters for trees, possibly differing depending on who wrote the code you’re using. We will limit discussion to these two.&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; Note that they effect each other, and they effect other parameters which we are not discussing. The main takeaway should be how they effect model flexibility.&lt;/p&gt;
&lt;p&gt;First let’s look at what happens for a fixed &lt;code&gt;minsplit&lt;/code&gt; by variable &lt;code&gt;cp&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that as &lt;code&gt;cp&lt;/code&gt; &lt;em&gt;decreases&lt;/em&gt;, model flexibility &lt;strong&gt;increases&lt;/strong&gt;. We see more splits, because the increase in performance needed to accept a split is smaller as &lt;code&gt;cp&lt;/code&gt; is reduced.&lt;/p&gt;
&lt;p&gt;Now the reverse, fix &lt;code&gt;cp&lt;/code&gt; and vary &lt;code&gt;minsplit&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that as &lt;code&gt;minsplit&lt;/code&gt; &lt;em&gt;decreases&lt;/em&gt;, model flexibility &lt;strong&gt;increases&lt;/strong&gt;. By allowing splits of neighborhoods with fewer observations, we obtain more splits, which results in a more flexible model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-credit-card-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Credit Card Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load data, coerce to tibble
crdt = as_tibble(ISLR::Credit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we are using the &lt;code&gt;Credit&lt;/code&gt; data form the &lt;code&gt;ISLR&lt;/code&gt; package. Note: &lt;strong&gt;this is not real data.&lt;/strong&gt; It has been simulated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data prep
crdt = crdt %&amp;gt;%
  select(-ID) %&amp;gt;%
  select(-Rating, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We remove the &lt;code&gt;ID&lt;/code&gt; variable as it should have no predictive power. We also move the &lt;code&gt;Rating&lt;/code&gt; variable to the last column with a clever &lt;code&gt;dplyr&lt;/code&gt; trick. This is in no way necessary, but is useful in creating some plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test-train split
set.seed(1)
crdt_trn_idx = sample(nrow(crdt), size = 0.8 * nrow(crdt))
crdt_trn = crdt[crdt_trn_idx, ]
crdt_tst = crdt[-crdt_trn_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimation-validation split
crdt_est_idx = sample(nrow(crdt_trn), size = 0.8 * nrow(crdt_trn))
crdt_est = crdt_trn[crdt_est_idx, ]
crdt_val = crdt_trn[-crdt_est_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After train-test (with 80% of the data) and estimation-validation splitting the data, we look at the train data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check data
head(crdt_trn, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 11
##    Income Limit Cards   Age Education Gender   Student Married Ethnicity Balance
##     &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;       &amp;lt;int&amp;gt;
##  1  183.  13913     4    98        17 &amp;quot; Male&amp;quot;  No      Yes     Caucasian    1999
##  2   35.7  2880     2    35        15 &amp;quot; Male&amp;quot;  No      No      African …       0
##  3  123.   8376     2    89        17 &amp;quot; Male&amp;quot;  Yes     No      African …    1259
##  4   20.8  2672     1    70        18 &amp;quot;Female&amp;quot; No      No      African …       0
##  5   39.1  5565     4    48        18 &amp;quot;Female&amp;quot; No      Yes     Caucasian     772
##  6   36.5  3806     2    52        13 &amp;quot; Male&amp;quot;  No      No      African …     188
##  7   45.1  3762     3    80         8 &amp;quot; Male&amp;quot;  No      Yes     Caucasian      70
##  8   43.5  2906     4    69        11 &amp;quot; Male&amp;quot;  No      No      Caucasian       0
##  9   23.1  3476     2    50        15 &amp;quot;Female&amp;quot; No      No      Caucasian     209
## 10   53.2  4943     2    46        16 &amp;quot;Female&amp;quot; No      Yes     Asian         382
## # … with 1 more variable: Rating &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this simulated data, we would like to predict the &lt;code&gt;Rating&lt;/code&gt; variable. For now, let’s try to use only demographic information as predictors.&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; In particular, let’s focus on &lt;code&gt;Age&lt;/code&gt; (numeric), &lt;code&gt;Gender&lt;/code&gt; (categorical), and &lt;code&gt;Student&lt;/code&gt; (categorical).&lt;/p&gt;
&lt;p&gt;Let’s fit KNN models with these features, and various values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. To do so, we use the &lt;code&gt;knnreg()&lt;/code&gt; function from the &lt;code&gt;caret&lt;/code&gt; package.&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; Use &lt;code&gt;?knnreg&lt;/code&gt; for documentation and details.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 1)
crdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 10)
crdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we fit three models to the estimation data. We supply the variables that will be used as features as we would with &lt;code&gt;lm()&lt;/code&gt;. We also specify how many neighbors to consider via the &lt;code&gt;k&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;But wait a second, what is the distance from non-student to student? From male to female? In other words, how does KNN handle categorical variables? It doesn’t! Like &lt;code&gt;lm()&lt;/code&gt; it creates dummy variables under the hood.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To this point, and until we specify otherwise, we will always coerce categorical variables to be factor variables in R. We will then let modeling functions such as &lt;code&gt;lm()&lt;/code&gt; or &lt;code&gt;knnreg()&lt;/code&gt; deal with the creation of dummy variables internally.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(crdt_knn_10$learn$X)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Age GenderFemale StudentYes
## 1  30            0          0
## 2  25            0          0
## 3  44            0          0
## 4  73            1          0
## 5  44            0          1
## 6  71            0          0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once these dummy variables have been created, we have a numeric &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; matrix, which makes distance calculations easy.&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; For example, the distance between the 3rd and 4th observation here is 29.017.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist(head(crdt_knn_10$learn$X))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           1         2         3         4         5
## 2  5.000000                                        
## 3 14.000000 19.000000                              
## 4 43.011626 48.010416 29.017236                    
## 5 14.035669 19.026298  1.000000 29.034462          
## 6 41.000000 46.000000 27.000000  2.236068 27.018512&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(sum((crdt_knn_10$learn$X[3, ] - crdt_knn_10$learn$X[4, ]) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 29.01724&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about interactions? Basically, you’d have to create them the same way as you do for linear models. We only mention this to contrast with trees in a bit.&lt;/p&gt;
&lt;p&gt;OK, so of these three models, which one performs best? (Where for now, “best” is obtaining the lowest validation RMSE.)&lt;/p&gt;
&lt;p&gt;First, note that we return to the &lt;code&gt;predict()&lt;/code&gt; function as we did with &lt;code&gt;lm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(crdt_knn_10, crdt_val[1:5, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 337.7857 356.0000 295.7692 360.8182 306.8000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This uses the 10-NN (10 nearest neighbors) model to make predictions (estimate the regression function) given the first five observations of the validation data. &lt;strong&gt;Note:&lt;/strong&gt; We did not name the second argument to &lt;code&gt;predict()&lt;/code&gt;. Again, you’ve been warned.&lt;/p&gt;
&lt;p&gt;Now that we know how to use the &lt;code&gt;predict()&lt;/code&gt; function, let’s calculate the validation RMSE for each of these models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knn_mod_list = list(
  crdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 1),
  crdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 10),
  crdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 25)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knn_val_pred = lapply(knn_mod_list, predict, crdt_val)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sapply(knn_val_pred, calc_rmse, crdt_val$Rating)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## crdt_knn_01 crdt_knn_10 crdt_knn_25 
##    182.3469    149.2172    138.6527&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, of these three values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, the model with &lt;span class=&#34;math inline&#34;&gt;\(k = 25\)&lt;/span&gt; achieves the lowest validation RMSE.&lt;/p&gt;
&lt;p&gt;This process, fitting a number of models with different values of the &lt;em&gt;tuning parameter&lt;/em&gt;, in this case &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, and then finding the “best” tuning parameter value based on performance on the validation data is called &lt;strong&gt;tuning&lt;/strong&gt;. In practice, we would likely consider more values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, but this should illustrate the point.&lt;/p&gt;
&lt;p&gt;In the next lectures, we will discuss the details of model flexibility and model tuning, and how these concepts are tied together. However, even though we will present some theory behind this relationship, in practice, &lt;strong&gt;you must tune and validate your models&lt;/strong&gt;. There is no theory that will inform you ahead of tuning and validation which model will be the best. By teaching you &lt;em&gt;how&lt;/em&gt; to fit KNN models in R and how to calculate validation RMSE, you already have all a set of tools you can use to find a good model.&lt;/p&gt;
&lt;p&gt;Let’s turn to decision trees which we will fit with the &lt;code&gt;rpart()&lt;/code&gt; function from the &lt;code&gt;rpart&lt;/code&gt; package. Use &lt;code&gt;?rpart&lt;/code&gt; and &lt;code&gt;?rpart.control&lt;/code&gt; for documentation and details. In particular, &lt;code&gt;?rpart.control&lt;/code&gt; will detail the many tuning parameters of this implementation of decision tree models in R.&lt;/p&gt;
&lt;p&gt;We’ll start by using default tuning parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_tree = rpart(Rating ~ Age + Gender + Student, data = crdt_est)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_tree&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## n= 256 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 256 6667400.0 357.0781  
##    2) Age&amp;lt; 82.5 242 5865419.0 349.3719  
##      4) Age&amp;gt;=69.5 52 1040678.0 313.0385 *
##      5) Age&amp;lt; 69.5 190 4737307.0 359.3158  
##       10) Age&amp;lt; 38.5 55  700013.2 326.6000 *
##       11) Age&amp;gt;=38.5 135 3954443.0 372.6444  
##         22) Student=Yes 14  180764.4 297.7857 *
##         23) Student=No 121 3686148.0 381.3058  
##           46) Age&amp;gt;=50.5 64 1881299.0 359.2344  
##             92) Age&amp;lt; 53.5 9   48528.0 278.3333 *
##             93) Age&amp;gt;=53.5 55 1764228.0 372.4727 *
##           47) Age&amp;lt; 50.5 57 1738665.0 406.0877 *
##    3) Age&amp;gt;=82.5 14  539190.9 490.2857 *&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Above we see the resulting tree printed, however, this is difficult to read. Instead, we use the &lt;code&gt;rpart.plot()&lt;/code&gt; function from the &lt;code&gt;rpart.plot&lt;/code&gt; package to better visualize the tree.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(crdt_tree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At each split, the variable used to split is listed together with a condition. If the condition is true for a data point, send it to the left neighborhood. Although the &lt;code&gt;Gender&lt;/code&gt; available for creating splits, we only see splits based on &lt;code&gt;Age&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt;. This hints at the relative importance of these variables for prediction. More on this much later.&lt;/p&gt;
&lt;p&gt;Categorical variables are split based on potential categories! This is &lt;em&gt;excellent&lt;/em&gt;. This means that trees naturally handle categorical features without needing to convert to numeric under the hood. We see a split that puts students into one neighborhood, and non-students into another.&lt;/p&gt;
&lt;p&gt;Notice that the splits happen in order. So for example, the third terminal node (with an average rating of 298) is based on splits of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Age &amp;lt; 83&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Age &amp;lt; 70&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Age &amp;gt; 39&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Student = Yes&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, individuals in this terminal node are students who are between the ages of 39 and 70. (Only 5% of the data is represented here.) This is basically an interaction between &lt;code&gt;Age&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt; without any need to directly specify it! What a great feature of trees.&lt;/p&gt;
&lt;p&gt;To recap:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trees do not make assumptions about the form of the regression function.&lt;/li&gt;
&lt;li&gt;Trees automatically handle categorical features.&lt;/li&gt;
&lt;li&gt;Trees naturally incorporate interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let’s fit another tree that is more flexible by relaxing some tuning parameters. Recall that by default, &lt;code&gt;cp = 0.1&lt;/code&gt; and &lt;code&gt;minsplit = 20&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_tree_big = rpart(Rating ~ Age + Gender + Student, data = crdt_est,
                      cp = 0.0, minsplit = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(crdt_tree_big)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To make the tree even bigger, we could reduce &lt;code&gt;minsplit&lt;/code&gt;, but in practice we mostly consider the &lt;code&gt;cp&lt;/code&gt; parameter. Since &lt;code&gt;minsplit&lt;/code&gt; has been kept the same, but &lt;code&gt;cp&lt;/code&gt; was reduced, we see the same splits as the smaller tree, but many additional splits.&lt;/p&gt;
&lt;p&gt;Now let’s fit a bunch of trees, with different values of &lt;code&gt;cp&lt;/code&gt;, for tuning.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tree_mod_list = list(
  crdt_tree_0000 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.000),
  crdt_tree_0001 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.001),
  crdt_tree_0010 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.010),
  crdt_tree_0100 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.100)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tree_val_pred = lapply(tree_mod_list, predict, crdt_val)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sapply(tree_val_pred, calc_rmse, crdt_val$Rating)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## crdt_tree_0000 crdt_tree_0001 crdt_tree_0010 crdt_tree_0100 
##       156.3527       155.4262       151.9081       140.0806&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see the least flexible model, with &lt;code&gt;cp = 0.100&lt;/code&gt;, performs best.&lt;/p&gt;
&lt;p&gt;Note that by only using these three features, we are severely limiting our models performance. Let’s quickly assess using all available predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_tree_all = rpart(Rating ~ ., data = crdt_est)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(crdt_tree_all)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/10-content_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that this model &lt;strong&gt;only&lt;/strong&gt; splits based on &lt;code&gt;Limit&lt;/code&gt; despite using all features. This should be a big hint about which variables are useful for prediction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_rmse(
  actual = crdt_val$Rating,
  predicted = predict(crdt_tree_all, crdt_val)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 28.8498&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model performs much better. You should try something similar with the KNN models above. Also, consider comparing this result to results from last lectures using linear models.&lt;/p&gt;
&lt;p&gt;Notice that we’ve been using that trusty &lt;code&gt;predict()&lt;/code&gt; function here again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(crdt_tree_all, crdt_val[1:5, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1        2        3        4        5 
## 292.8182 467.5152 467.5152 467.5152 772.4000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does this code do? It estimates the mean &lt;code&gt;Rating&lt;/code&gt; given the feature information (the “x” values) from the first five observations from the validation data using a decision tree model with default tuning parameters. Hopefully a theme is emerging.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;We chose to start with linear regression because most students the social sciences should already be familiar with the basic notion.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The usual distance when you hear distance. That is, unless you drive a &lt;a href=&#34;https://en.wikipedia.org/wiki/Taxicab_geometry&#34;&gt;taxicab&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;For this reason, KNN is often not used in practice, but it is very useful learning tool.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Many texts use the term complex instead of flexible. We feel this is confusing as complex is often associated with difficult. KNN with &lt;span class=&#34;math inline&#34;&gt;\(k = 1\)&lt;/span&gt; is actually a very simple model to understand, but it is very flexible as defined here.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;To exhaust all possible splits of a variable, we would need to consider the midpoint between each of the order statistics of the variable. To exhaust all possible splits, we would need to do this for each of the feature variables.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;It’s really an upside tree isn’t it?&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Flexibility parameter would be a better name.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;The &lt;code&gt;rpart&lt;/code&gt; function in R would allow us to use others, but we will always just leave their values as the default values.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;There is a question of whether or not we &lt;em&gt;should&lt;/em&gt; use these variables. For example, should men and women be given different ratings when all other variables are the same? Using the &lt;code&gt;Gender&lt;/code&gt; variable allows for this to happen. Also, you might think, just don’t use the &lt;code&gt;Gender&lt;/code&gt; variable. Unfortunately, it’s not that easy. There is an increasingly popular field of study centered around these ideas called &lt;a href=&#34;https://en.wikipedia.org/wiki/Fairness_(machine_learning)&#34;&gt;machine learning &lt;strong&gt;fairness&lt;/strong&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;There are many other KNN functions in &lt;code&gt;R&lt;/code&gt;. However, the operation and syntax of &lt;code&gt;knnreg()&lt;/code&gt; better matches other functions we use in this course.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;Wait. Doesn’t this sort of create an arbitrary distance between the categories? Why &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and not &lt;span class=&#34;math inline&#34;&gt;\(-42\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(51\)&lt;/span&gt;? Good question. This hints at the notion of pre-processing. We’re going to hold off on this for now, but, often when performing k-nearest neighbors, you should try scaling all of the features to have mean &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Bias-Variance Tradeoff</title>
      <link>https://ssc442.netlify.app/content/09-content/</link>
      <pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/09-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-biasvariance-tradeoff&#34;&gt;The Bias–Variance Tradeoff&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-setup-and-source&#34;&gt;R Setup and Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-regression-setup&#34;&gt;The Regression Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reducible-and-irreducible-error&#34;&gt;Reducible and Irreducible Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-variance-decomposition&#34;&gt;Bias-Variance Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-simulation-to-estimate-bias-and-variance&#34;&gt;Using Simulation to Estimate Bias and Variance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-expected-prediction-error&#34;&gt;Estimating Expected Prediction Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-flexibility&#34;&gt;Model Flexibility&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linear-models&#34;&gt;Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbors&#34;&gt;k-Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decision-trees&#34;&gt;Decision Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf&#34;&gt;Chapter 2&lt;/a&gt; in &lt;em&gt;Introduction to Statistical Learning with Applications in R&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the relationship between &lt;strong&gt;bias&lt;/strong&gt;, &lt;strong&gt;variance&lt;/strong&gt;, and &lt;strong&gt;mean squared error?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;What is the relationship between &lt;strong&gt;model flexibility&lt;/strong&gt; and training error?&lt;/li&gt;
&lt;li&gt;What is the relationship between &lt;strong&gt;model flexibility&lt;/strong&gt; and validation (or test) error?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-biasvariance-tradeoff&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Bias–Variance Tradeoff&lt;/h1&gt;
&lt;p&gt;This lecture will begin to dig into some theoretical details of estimating regression functions, in particular how the &lt;strong&gt;bias-variance tradeoff&lt;/strong&gt; helps explain the relationship between &lt;strong&gt;model flexibility&lt;/strong&gt; and the errors a model makes.&lt;/p&gt;
&lt;p&gt;This content is currently &lt;strong&gt;under construction&lt;/strong&gt;. You can expect it to be less polished than other sections.&lt;/p&gt;
&lt;p&gt;Don’t freak out if this seems mathematically overwhelming. We’ll walk through relatively slowly. It’s not super important to follow the nitty-gritty details; but the broad takeaways are quite important.&lt;/p&gt;
&lt;div id=&#34;r-setup-and-source&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R Setup and Source&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tibble)     # data frame printing
library(dplyr)      # data manipulation

library(caret)      # fitting knn
library(rpart)      # fitting trees
library(rpart.plot) # plotting trees&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-regression-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Regression Setup&lt;/h2&gt;
&lt;p&gt;Consider the general regression setup where we are given a random pair &lt;span class=&#34;math inline&#34;&gt;\((X, Y) \in \mathbb{R}^p \times \mathbb{R}\)&lt;/span&gt;. We would like to “predict” &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; with some function of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, say, &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To clarify what we mean by “predict,” we specify that we would like &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; to be “close” to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. To further clarify what we mean by “close,” we define the &lt;strong&gt;squared error loss&lt;/strong&gt; of estimating &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L(Y, f(X)) \triangleq (Y - f(X)) ^ 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the &lt;strong&gt;risk&lt;/strong&gt; of estimating &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R(Y, f(X)) \triangleq \mathbb{E}[L(Y, f(X))] = \mathbb{E}_{X, Y}[(Y - f(X)) ^ 2]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Before attempting to minimize the risk, we first re-write the risk after conditioning on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}_{X, Y} \left[ (Y - f(X)) ^ 2 \right] = \mathbb{E}_{X} \mathbb{E}_{Y \mid X} \left[ ( Y - f(X) ) ^ 2 \mid X = x \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Minimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to &lt;span class=&#34;math inline&#34;&gt;\(Y \mid X\)&lt;/span&gt;, essentially minimizing the risk pointwise, for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It turns out, that the risk is minimized by setting &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; to be equal the conditional mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(x) = \mathbb{E}(Y \mid X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which we call the &lt;strong&gt;regression function&lt;/strong&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L(Y, f(X)) \triangleq | Y - f(X) |
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The risk would then be minimized setting &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; equal to the conditional median.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(x) = \text{median}(Y \mid X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Despite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations.&lt;/p&gt;
&lt;p&gt;Now, given data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}\)&lt;/span&gt;, our goal becomes finding some &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; that is a good estimate of the regression function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. We’ll see that this amounts to minimizing what we call the reducible error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reducible-and-irreducible-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reducible and Irreducible Error&lt;/h2&gt;
&lt;p&gt;Suppose that we obtain some &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;, how well does it estimate &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;? We define the &lt;strong&gt;expected prediction error&lt;/strong&gt; of predicting &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt;. A good &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; will have a low expected prediction error.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}(X)\right) \triangleq \mathbb{E}_{X, Y, \mathcal{D}} \left[  \left( Y - \hat{f}(X) \right)^2 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This expectation is over &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and also &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;. The estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; is actually random depending on the data, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, used to estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;. We could actually write &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X, \mathcal{D})\)&lt;/span&gt; to make this dependence explicit, but our notation will become cumbersome enough as it is.&lt;/p&gt;
&lt;p&gt;Like before, we’ll condition on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. This results in the expected prediction error of predicting &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}(x)\right) =
\mathbb{E}_{Y \mid X, \mathcal{D}} \left[  \left(Y - \hat{f}(X) \right)^2 \mid X = x \right] =
\underbrace{\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]}_\textrm{reducible error} +
\underbrace{\mathbb{V}_{Y \mid X} \left[ Y \mid X = x \right]}_\textrm{irreducible error}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A number of things to note here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The expected prediction error is for a random &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given a fixed &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a random &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;. As such, the expectation is over &lt;span class=&#34;math inline&#34;&gt;\(Y \mid X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;. Our estimated function &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; is random depending on the data, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, which is used to perform the estimation.&lt;/li&gt;
&lt;li&gt;The expected prediction error of predicting &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt; has been decomposed into two errors:
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;reducible error&lt;/strong&gt;, which is the expected squared error loss of estimation &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; at a fixed point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The only thing that is random here is &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, the data used to obtain &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;. (Both &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; are fixed.) We’ll often call this reducible error the &lt;strong&gt;mean squared error&lt;/strong&gt; of estimating &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; at a fixed point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.
&lt;span class=&#34;math display&#34;&gt;\[ \text{MSE}\left(f(x), \hat{f}(x)\right) \triangleq \mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;irreducible error&lt;/strong&gt;. This is simply the variance of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given that &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt;, essentially noise that we do not want to learn. This is also called the &lt;strong&gt;Bayes error&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As the name suggests, the reducible error is the error that we have some control over. But how do we control this error?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-variance-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bias-Variance Decomposition&lt;/h2&gt;
&lt;p&gt;After decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error.&lt;/p&gt;
&lt;p&gt;Recall the definition of the &lt;strong&gt;bias&lt;/strong&gt; of an estimator.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{bias}(\hat{\theta}) \triangleq \mathbb{E}\left[\hat{\theta}\right] - \theta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also recall the definition of the &lt;strong&gt;variance&lt;/strong&gt; of an estimator.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{V}(\hat{\theta}) = \text{var}(\hat{\theta}) \triangleq \mathbb{E}\left [ ( \hat{\theta} -\mathbb{E}\left[\hat{\theta}\right] )^2 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using this, we further decompose the reducible error (mean squared error) into bias squared and variance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE}\left(f(x), \hat{f}(x)\right) =
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] =
\underbrace{\left(f(x) - \mathbb{E} \left[ \hat{f}(x) \right]  \right)^2}_{\text{bias}^2 \left(\hat{f}(x) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}(x) - \mathbb{E} \left[ \hat{f}(x) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}(x) \right)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; at some point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE}\left(f(x), \hat{f}(x)\right) = \text{bias}^2 \left(\hat{f}(x) \right) + \text{var} \left(\hat{f}(x) \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a perfect world, we would be able to find some &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; which is &lt;strong&gt;unbiased&lt;/strong&gt;, that is &lt;span class=&#34;math inline&#34;&gt;\(\text{bias}\left(\hat{f}(x) \right) = 0\)&lt;/span&gt;, which also has low variance. In practice, this isn’t always possible.&lt;/p&gt;
&lt;p&gt;It turns out, there is a &lt;strong&gt;bias-variance tradeoff&lt;/strong&gt;. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance.&lt;/p&gt;
&lt;p&gt;In the context of regression, models are biased when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parametric: The form of the model &lt;a href=&#34;https://en.wikipedia.org/wiki/Omitted-variable_bias&#34;&gt;does not incorporate all the necessary variables&lt;/a&gt;, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic.&lt;/li&gt;
&lt;li&gt;Non-parametric: The model provides too much smoothing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the context of regression, models are variable when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parametric: The form of the model incorporates too many variables, or the form of the relationship is too flexible. For example, a parametric model assumes a cubic relationship, but the true relationship is linear.&lt;/li&gt;
&lt;li&gt;Non-parametric: The model does not provide enough smoothing. It is very, “wiggly.”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data.&lt;/p&gt;
&lt;p&gt;Recall that when fitting models, we’ve seen that train RMSE decreases as model flexibility is increasing. (Technically it is non-increasing.) For validation RMSE, we expect to see a U-shaped curve. Importantly, validation RMSE decreases, until a certain flexibility, then begins to increase.&lt;/p&gt;
&lt;p&gt;Now we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the flexibility that minimizes the prediction error.&lt;/p&gt;
&lt;p&gt;To summarize, if we assume that irreducible error can be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{V}[Y \mid X = x] = \sigma ^ 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;then we can write the full decomposition of the expected prediction error of predicting &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}(x)\right) =
\underbrace{\text{bias}^2\left(\hat{f}(x)\right) + \text{var}\left(\hat{f}(x)\right)}_\textrm{reducible error} + \sigma^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As model flexibility increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model flexibility to find a model that will predict well on unseen observations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/error-vs-flex-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Tying this all together, the above image shows how we “expect” training and validation error to behavior in relation to model flexibility.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; In practice, we won’t always see such a nice “curve” in the validation error, but we expect to see the general trends.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-simulation-to-estimate-bias-and-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using Simulation to Estimate Bias and Variance&lt;/h2&gt;
&lt;p&gt;We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f = function(x) {
  x ^ 2
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More specifically, we’d like to predict an observation, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, given that &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt; by using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[Y \mid X = x] = f(x) = x^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{V}[Y \mid X = x] = \sigma ^ 2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we could write this as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = f(X) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[\epsilon] = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{V}[\epsilon] = \sigma ^ 2\)&lt;/span&gt;. In this formulation, we call &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; the &lt;strong&gt;signal&lt;/strong&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; the &lt;strong&gt;noise&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_sim_data = function(f, sample_size = 100) {
  x = runif(n = sample_size, min = 0, max = 1)
  y = rnorm(n = sample_size, mean = f(x), sd = 0.3)
  tibble(x, y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also note that if you prefer to think of this situation using the &lt;span class=&#34;math inline&#34;&gt;\(Y = f(X) + \epsilon\)&lt;/span&gt; formulation, the following code represents the same data generating process.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_sim_data = function(f, sample_size = 100) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = 0.75)
  y = f(x) + eps
  tibble(x, y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To completely specify the data generating process, we have made more model assumptions than simply &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[Y \mid X = x] = x^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{V}[Y \mid X = x] = \sigma ^ 2\)&lt;/span&gt;. In particular,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from a uniform distribution over &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; are independent.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from the conditional normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y \mid X \sim N(f(x), \sigma^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using this setup, we will generate datasets, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, with a sample size &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt; and fit four models.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\texttt{predict(fit0, x)} &amp;amp;= \hat{f}_0(x) = \hat{\beta}_0\\
\texttt{predict(fit1, x)} &amp;amp;= \hat{f}_1(x) = \hat{\beta}_0 + \hat{\beta}_1 x \\
\texttt{predict(fit2, x)} &amp;amp;= \hat{f}_2(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \\
\texttt{predict(fit9, x)} &amp;amp;= \hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
sim_data = gen_sim_data(f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that technically we’re being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes.&lt;/p&gt;
&lt;p&gt;Plotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.&lt;/p&gt;
&lt;p&gt;While the ninth degree model doesn’t appear to be correct for any of these three simulations, we’ll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn’t the only goal we’re after, since in practice, we’ll only have a single dataset. This is why we’d also like our models to exhibit low variance.&lt;/p&gt;
&lt;p&gt;We could have also fit &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-nearest neighbors models to these three datasets.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see that when &lt;span class=&#34;math inline&#34;&gt;\(k = 100\)&lt;/span&gt; we have a biased model with very low variance.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; When &lt;span class=&#34;math inline&#34;&gt;\(k = 5\)&lt;/span&gt;, we again have a highly variable model.&lt;/p&gt;
&lt;p&gt;These two sets of plots reinforce our intuition about the bias-variance tradeoff. Flexible models (ninth degree polynomial and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and &lt;span class=&#34;math inline&#34;&gt;\(k = 100\)&lt;/span&gt;) are very biased, but have extremely low variance.&lt;/p&gt;
&lt;p&gt;We will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; given by these four models at the point &lt;span class=&#34;math inline&#34;&gt;\(x = 0.90\)&lt;/span&gt;. We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = gen_sim_data(f)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this is one of many ways we could have accomplished this task using &lt;code&gt;R&lt;/code&gt;. For example we could have used a combination of &lt;code&gt;replicate()&lt;/code&gt; and &lt;code&gt;*apply()&lt;/code&gt; functions. Alternatively, we could have used a &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; approach, which likely would have used some combination of &lt;a href=&#34;http://dplyr.tidyverse.org/&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;http://tidyr.tidyverse.org/&#34;&gt;&lt;code&gt;tidyr&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;http://purrr.tidyverse.org/&#34;&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our approach, which would be considered a &lt;code&gt;base&lt;/code&gt; &lt;code&gt;R&lt;/code&gt; approach, was chosen to make it as clear as possible what is being done. The &lt;code&gt;tidyverse&lt;/code&gt; approach is rapidly gaining popularity in the &lt;code&gt;R&lt;/code&gt; community, but might make it more difficult to see what is happening here, unless you are already familiar with that approach.&lt;/p&gt;
&lt;p&gt;Also of note, while it may seem like the output stored in &lt;code&gt;predictions&lt;/code&gt; would meet the definition of &lt;a href=&#34;http://vita.had.co.nz/papers/tidy-data.html&#34;&gt;tidy data&lt;/a&gt; given by &lt;a href=&#34;http://hadley.nz/&#34;&gt;Hadley Wickham&lt;/a&gt; since each row represents a simulation, it actually falls slightly short. For our data to be tidy, a row should store the simulation number, the model, and the resulting prediction. We’ve actually already aggregated one level above this. Our observational unit is a simulation (with four predictions), but for tidy data, it should be a single prediction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, &lt;span class=&#34;math inline&#34;&gt;\(f(x = 0.90) = (0.9)^2 = 0.81\)&lt;/span&gt;, is given by the solid black horizontal line.&lt;/p&gt;
&lt;p&gt;Two things are immediately clear:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As flexibility &lt;em&gt;increases&lt;/em&gt;, &lt;strong&gt;bias decreases&lt;/strong&gt;. The mean of a model’s predictions is closer to the truth.&lt;/li&gt;
&lt;li&gt;As flexibility &lt;em&gt;increases&lt;/em&gt;, &lt;strong&gt;variance increases&lt;/strong&gt;. The variance about the mean of a model’s predictions increases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal of this simulation study is to show that the following holds true for each of the four models.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE}\left(f(0.90), \hat{f}_k(0.90)\right) =
\underbrace{\left(\mathbb{E} \left[ \hat{f}_k(0.90) \right] - f(0.90) \right)^2}_{\text{bias}^2 \left(\hat{f}_k(0.90) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}_k(0.90) - \mathbb{E} \left[ \hat{f}_k(0.90) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}_k(0.90) \right)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ll use the empirical results of our simulations to estimate these quantities. (Yes, we’re using estimation to justify facts about estimation.) Note that we’ve actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we’ve performed just enough simulations to obtain the desired results. (Since we’re estimating estimation, the bigger the sample size, the better.)&lt;/p&gt;
&lt;p&gt;To estimate the mean squared error of our predictions, we’ll use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\widehat{\text{MSE}}\left(f(0.90), \hat{f}_k(0.90)\right) = \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}} \left(f(0.90) - \hat{f}_k^{[i]}(0.90) \right)^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}_k^{[i]}(0.90)\)&lt;/span&gt; is the estimate of &lt;span class=&#34;math inline&#34;&gt;\(f(0.90)\)&lt;/span&gt; using the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th from the polynomial degree &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; model.&lt;/p&gt;
&lt;p&gt;We also write an accompanying &lt;code&gt;R&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_mse = function(truth, estimate) {
  mean((estimate - truth) ^ 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, for the bias of our predictions we use,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\widehat{\text{bias}} \left(\hat{f}(0.90) \right)  = \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}} \left(\hat{f}_k^{[i]}(0.90) \right) - f(0.90)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And again, we write an accompanying &lt;code&gt;R&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_bias = function(estimate, truth) {
  mean(estimate) - truth
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, for the variance of our predictions we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\widehat{\text{var}} \left(\hat{f}(0.90) \right) = \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}} \left(\hat{f}_k^{[i]}(0.90) - \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}}\hat{f}_k^{[i]}(0.90) \right)^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While there is already &lt;code&gt;R&lt;/code&gt; function for variance, the following is more appropriate in this situation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_var = function(estimate) {
  mean((estimate - mean(estimate)) ^ 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To quickly obtain these results for each of the four models, we utilize the &lt;code&gt;apply()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bias = apply(predictions, 2, get_bias, truth = f(x = 0.90))
variance = apply(predictions, 2, get_var)
mse = apply(predictions, 2, get_mse, truth = f(x = 0.90))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We summarize these results in the following table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Degree&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Mean Squared Error&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Bias Squared&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Variance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.22643&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.22476&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00829&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00508&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00322&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00387&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00005&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00381&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.01019&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00002&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.01017&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A number of things to notice here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as flexibility increases.&lt;/li&gt;
&lt;li&gt;The squared bias trend which we see here is &lt;strong&gt;decreasing&lt;/strong&gt; as flexibility increases, which we expect to see in general.&lt;/li&gt;
&lt;li&gt;The exact opposite is true of variance. As model flexibility increases, variance &lt;strong&gt;increases&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can check for these trends with the &lt;code&gt;diff()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(diff(bias ^ 2) &amp;lt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(diff(variance) &amp;gt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff(mse) &amp;lt; 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     1     2     9 
##  TRUE  TRUE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance.&lt;/p&gt;
&lt;p&gt;Models with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, since least squares estimation is unbiased, importantly,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}\left[\hat{\beta}_d\right] = \beta_d = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(d = 3, 4, \ldots 9\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}\left[\hat{f}_9(x)\right] = \beta_0 + \beta_1 x + \beta_2 x^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can finally verify the bias-variance decomposition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bias ^ 2 + variance == mse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     0     1     2     9 
## FALSE FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But wait, this says it isn’t true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(bias ^ 2 + variance, mse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See &lt;code&gt;?all.equal()&lt;/code&gt; for details.&lt;/p&gt;
&lt;p&gt;So far, we’ve focused our efforts on looking at the mean squared error of estimating &lt;span class=&#34;math inline&#34;&gt;\(f(0.90)\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(0.90)\)&lt;/span&gt;. We could also look at the expected prediction error of using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0.90\)&lt;/span&gt; to estimate &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}_k(0.90)\right) =
\mathbb{E}_{Y \mid X, \mathcal{D}} \left[  \left(Y - \hat{f}_k(X) \right)^2 \mid X = 0.90 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can estimate this quantity for each of the four models using the simulation study we already performed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_epe = function(realized, estimate) {
  mean((realized - estimate) ^ 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y = rnorm(n = nrow(predictions), mean = f(x = 0.9), sd = 0.3)
epe = apply(predictions, 2, get_epe, realized = y)
epe&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         0         1         2         9 
## 0.3180470 0.1104055 0.1095955 0.1205570&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about the unconditional expected prediction error. That is, for any &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, not just &lt;span class=&#34;math inline&#34;&gt;\(0.90\)&lt;/span&gt;. Specifically, the expected prediction error of estimating &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt;. The following (new) simulation study provides an estimate of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}_k(X)\right) = \mathbb{E}_{X, Y, \mathcal{D}} \left[  \left( Y - \hat{f}_k(X) \right)^2 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for the quadratic model, that is &lt;span class=&#34;math inline&#34;&gt;\(k = 2\)&lt;/span&gt; as we have defined &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
n_sims = 2500
X = runif(n = n_sims, min = 0, max = 1)
Y = rnorm(n = n_sims, mean = f(X), sd = 0.3)

f_hat_X = rep(0, length(X))

for (i in seq_along(X)) {
  sim_data = gen_sim_data(f)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  f_hat_X[i] = predict(fit_2, newdata = data.frame(x = X[i]))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# truth
0.3 ^ 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# via simulation
mean((Y - f_hat_X) ^ 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09566445&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in practice, we should use many more simulations in this study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-expected-prediction-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating Expected Prediction Error&lt;/h2&gt;
&lt;p&gt;While previously, we only decomposed the expected prediction error conditionally, a similar argument holds unconditionally.&lt;/p&gt;
&lt;p&gt;Assuming&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{V}[Y \mid X = x] = \sigma ^ 2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}(X)\right) =
\mathbb{E}_{X, Y, \mathcal{D}} \left[  (Y - \hat{f}(X))^2 \right] =
\underbrace{\mathbb{E}_{X} \left[\text{bias}^2\left(\hat{f}(X)\right)\right] + \mathbb{E}_{X} \left[\text{var}\left(\hat{f}(X)\right)\right]}_\textrm{reducible error} + \sigma^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we note that if&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{D} = \mathcal{D}_{\texttt{trn}} \cup \mathcal{D}_{\texttt{tst}} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i = 1, 2, \ldots n
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{D}_{\texttt{trn}} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i \in \texttt{trn}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{D}_{\texttt{tst}} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i \in \texttt{tst}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, if we have a model fit to the training data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{\texttt{trn}}\)&lt;/span&gt;, we can use the test mean squared error&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i \in \texttt{tst}}\left(y_i - \hat{f}(x_i)\right) ^ 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;as an estimate of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}_{X, Y, \mathcal{D}} \left[  (Y - \hat{f}(X))^2 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the expected prediction error.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;How good is this estimate? Well, if &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; is a random sample from &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt;, and the &lt;span class=&#34;math inline&#34;&gt;\(\texttt{tst}\)&lt;/span&gt; data are randomly sampled observations randomly sampled from &lt;span class=&#34;math inline&#34;&gt;\(i = 1, 2, \ldots, n\)&lt;/span&gt;, then it is a reasonable estimate. However, it is rather variable due to the randomness of selecting the observations for the test set, if the test set is small.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-flexibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Flexibility&lt;/h2&gt;
&lt;p&gt;Let’s return to the simiulated dataset we used occaisionally in the linear regression content. Recall there was a single feature &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; with the following properties:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define regression function
cubic_mean = function(x) {
  1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then generated some data around this function with some added noise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define full data generating process
gen_slr_data = function(sample_size = 100, mu) {
  x = runif(n = sample_size, min = -1, max = 1)
  y = mu(x) + rnorm(n = sample_size)
  tibble(x, y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After defining the data generating process, we generate and split the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate entire dataset
set.seed(3)
sim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean)

# test-train split
slr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data))
slr_trn = sim_slr_data[slr_trn_idx, ]
slr_tst = sim_slr_data[-slr_trn_idx, ]

# estimation-validation split
slr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn))
slr_est = slr_trn[slr_est_idx, ]
slr_val = slr_trn[-slr_est_idx, ]

# check data
head(slr_trn, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 × 2
##         x      y
##     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1  0.573 -1.18 
##  2  0.807  0.576
##  3  0.272 -0.973
##  4 -0.813 -1.78 
##  5 -0.161  0.833
##  6  0.736  1.07 
##  7 -0.242  2.97 
##  8  0.520 -1.64 
##  9 -0.664  0.269
## 10 -0.777 -2.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For validating models, we will use RMSE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# helper function for calculating RMSE
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/error-vs-flex-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This picture is an idealized version of what we expect to see, but we’ll illustrate the sorts of validate “curves” that we might see in practice.&lt;/p&gt;
&lt;p&gt;Note that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details.&lt;/p&gt;
&lt;div id=&#34;linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Linear Models&lt;/h3&gt;
&lt;p&gt;First up, linear models. We will fit polynomial models with degree from one to nine, and then validate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit polynomial models
poly_mod_est_list = list(
  poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est),
  poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est),
  poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est),
  poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est),
  poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est),
  poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est),
  poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est),
  poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est),
  poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below visualizes the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What do we see here? As the polynomial degree &lt;em&gt;increases&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The training error &lt;em&gt;decreases&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The validation error &lt;em&gt;decreases&lt;/em&gt;, then &lt;em&gt;increases&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This more of less matches the idealized version above, but the validation “curve” is much more jagged. This is something that we can expect in practice.&lt;/p&gt;
&lt;p&gt;We have previously noted that training error isn’t particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;k-Nearest Neighbors&lt;/h3&gt;
&lt;p&gt;Next up, k-nearest neighbors. We will consider values for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; that are odd and between &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(45\)&lt;/span&gt; inclusive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# helper function for fitting knn models
fit_knn_mod = function(neighbors) {
  knnreg(y ~ x, data = slr_est, k = neighbors)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define values of tuning parameter k to evaluate
k_to_try = seq(from = 1, to = 45, by = 2)

# fit knn models
knn_mod_est_list = lapply(k_to_try, fit_knn_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below visualizes the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see the “opposite” of the usual plot. Why? Because with k-nearest neighbors, a small value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; generates a flexible model compared to larger values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. So visually, this plot is flipped. That is we see that as &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; &lt;em&gt;increases&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The training error &lt;em&gt;increases&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The validation error &lt;em&gt;decreases&lt;/em&gt;, then &lt;em&gt;increases&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Important to note here: the pattern above only holds “in general,” that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Decision Trees&lt;/h3&gt;
&lt;p&gt;Lastly, we evaluate some decision tree models. We choose some arbitrary values of &lt;code&gt;cp&lt;/code&gt; to evaluate, while holding &lt;code&gt;minsplit&lt;/code&gt; constant at &lt;code&gt;5&lt;/code&gt;. There are arbitrary choices that produce a plot that is useful for discussion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# helper function for fitting decision tree models
tree_knn_mod = function(flex) {
  rpart(y ~ x, data = slr_est, cp = flex, minsplit = 5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define values of tuning parameter cp to evaluate
cp_to_try = c(0.5, 0.3, 0.1, 0.05, 0.01, 0.001, 0.0001)

# fit decision tree models
tree_mod_est_list = lapply(cp_to_try, tree_knn_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below visualizes the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/09-content_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on this plot, how is &lt;code&gt;cp&lt;/code&gt; related to model flexibility?&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Note that in this section, we will refer to &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; as the regression function instead of &lt;span class=&#34;math inline&#34;&gt;\(\mu(x)\)&lt;/span&gt; for unimportant and arbitrary reasons.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Someday, someone will tell you this is a lie. They aren’t wrong. In modern deep learning, there is a concept called &lt;a href=&#34;https://openai.com/blog/deep-double-descent/&#34;&gt;Deep Double Descent&lt;/a&gt;. See also &lt;span class=&#34;citation&#34;&gt;@belkin2018reconciling&lt;/span&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;It’s actually the same as the 0 predictor linear model. Can you see why?&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;In practice we prefer RMSE to MSE for comparing models and reporting because of the units.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;In practice, if you already know how your model’s flexibility works, by checking that the training error goes down as you increase flexibility, you can check that you have done your coding and model training correctly.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;As &lt;code&gt;cp&lt;/code&gt; increases, model flexibility decreases.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression III</title>
      <link>https://ssc442.netlify.app/content/08-content/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/08-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-question&#34;&gt;Guiding Question&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#association-is-not-causation&#34;&gt;Association is not causation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#spurious-correlation&#34;&gt;Spurious correlation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#outliers&#34;&gt;Outliers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reversing-cause-and-effect&#34;&gt;Reversing cause and effect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confounders&#34;&gt;Confounders&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-uc-berkeley-admissions&#34;&gt;Example: UC Berkeley admissions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confounding-explained-graphically&#34;&gt;Confounding explained graphically&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-after-stratifying&#34;&gt;Average after stratifying&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simpsons-paradox&#34;&gt;Simpson’s paradox&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code&gt;## Loading required package: knitr&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Question&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;When can we make causal claims about the relationship between variables?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;association-is-not-causation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Association is not causation&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Association is not causation&lt;/em&gt; is perhaps the most important lesson one learns in a statistics class. &lt;em&gt;Correlation is not causation&lt;/em&gt; is another way to say this. Throughout the previous parts of this class, we have described tools useful for quantifying associations between variables. However, we must be careful not to over-interpret these associations.&lt;/p&gt;
&lt;p&gt;There are many reasons that a variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can be correlated with a variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; without having any direct effect on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Here we examine four common ways that can lead to misinterpreting data.&lt;/p&gt;
&lt;div id=&#34;spurious-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Spurious correlation&lt;/h2&gt;
&lt;p&gt;The following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/divorce-versus-margarine-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Does this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a &lt;em&gt;spurious correlation&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You can see many more absurd examples on the Spurious Correlations website&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The cases presented in the spurious correlation site are all instances of what is generally called &lt;em&gt;data dredging&lt;/em&gt;, &lt;em&gt;data fishing&lt;/em&gt;, or &lt;em&gt;data snooping&lt;/em&gt;. It’s basically a form of what in the US they call &lt;em&gt;cherry picking&lt;/em&gt;. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.&lt;/p&gt;
&lt;p&gt;A Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 25
g &amp;lt;- 1000000
sim_data &amp;lt;- tibble(group = rep(1:g, each=N),
                   x = rnorm(N * g),
                   y = rnorm(N * g))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first column denotes group. We created groups and for each one we generated a pair of independent vectors, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are not correlated.&lt;/p&gt;
&lt;p&gt;Next, we compute the correlation between &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; for each group and look at the max:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- sim_data %&amp;gt;%
  group_by(group) %&amp;gt;%
  summarize(r = cor(x, y)) %&amp;gt;%
  arrange(desc(r))
res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,000,000 × 2
##     group     r
##     &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1 272719 0.828
##  2 514283 0.816
##  3 441525 0.771
##  4 559260 0.763
##  5 111062 0.758
##  6 747042 0.758
##  7 980182 0.753
##  8 172204 0.750
##  9 632943 0.750
## 10 402582 0.748
## # … with 999,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see a maximum correlation of 0.828 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are in fact correlated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data %&amp;gt;% filter(group == res$group[which.max(res$r)]) %&amp;gt;%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/dredging-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Remember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res %&amp;gt;% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/null-corr-hist-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.204135, the largest one will be close to 1.&lt;/p&gt;
&lt;p&gt;If we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)
sim_data %&amp;gt;%
  filter(group == res$group[which.max(res$r)]) %&amp;gt;%
  do(tidy(lm(y ~ x, data = .))) %&amp;gt;%
  filter(term == &amp;quot;x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 5
##   term  estimate std.error statistic     p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 x         1.03     0.145      7.08 0.000000326&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, imagine that instead of a whole lot of simulated data, you had a whole lot of actual data and waded through enough of it to find two unrelated variables that happened to show up as correlated (like divorce rates and pounds of margarine consumed). This particular form of data dredging is referred to as &lt;em&gt;p-hacking&lt;/em&gt;. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and a lot of different variables that represent exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you can learn methods to adjust for these multiple comparisons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outliers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outliers&lt;/h2&gt;
&lt;p&gt;Suppose we take measurements from two independent outcomes, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1985)
x &amp;lt;- rnorm(100,100,1)
y &amp;lt;- rnorm(100,84,1)
x[-23] &amp;lt;- scale(x[-23])
y[-23] &amp;lt;- scale(y[-23])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/outlier-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not surprisingly, the correlation is very high:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x,y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9878382&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x[-23], y[-23])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.04419032&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Previously, we (briefly) described alternatives to the average and standard deviation that are robust to outliers. There is also an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called &lt;em&gt;Spearman correlation&lt;/em&gt;. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(rank(x), rank(y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/scatter-plot-of-ranks-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The outlier is no longer associated with a very large value and the correlation comes way down:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(rank(x), rank(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.002508251&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spearman correlation can also be calculated like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x, y, method = &amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.002508251&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber &amp;amp; Elvezio M. Ronchetti.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reversing-cause-and-effect&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reversing cause and effect&lt;/h2&gt;
&lt;p&gt;Another way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.&lt;/p&gt;
&lt;p&gt;A form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. Consider this quote from the article:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.&lt;/p&gt;
&lt;p&gt;We can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_i = \beta_0 + \beta_1 y_i + \varepsilon_i, i=1, \dots, N\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to the father and son height data, with &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; the father height and &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; the son height, we do get a statistically significant result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HistData)
data(&amp;quot;GaltonFamilies&amp;quot;)
GaltonFamilies %&amp;gt;%
  filter(childNum == 1 &amp;amp; gender == &amp;quot;male&amp;quot;) %&amp;gt;%
  select(father, childHeight) %&amp;gt;%
  rename(son = childHeight) %&amp;gt;%
  do(tidy(lm(father ~ son, data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)   34.0      4.57        7.44 4.31e-12
## 2 son            0.499    0.0648      7.70 9.47e-13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confounders&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confounders&lt;/h2&gt;
&lt;p&gt;Confounders are perhaps the most common reason that leads to associations begin misinterpreted.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are correlated, we call &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; a &lt;em&gt;confounder&lt;/em&gt; if changes in &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; causes changes in both &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.&lt;/p&gt;
&lt;p&gt;Incorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.&lt;/p&gt;
&lt;div id=&#34;example-uc-berkeley-admissions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example: UC Berkeley admissions&lt;/h3&gt;
&lt;p&gt;Admission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and calculate the “headline” number:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(admissions)
admissions %&amp;gt;% group_by(gender) %&amp;gt;%
  summarize(percentage =
              round(sum(admitted*applicants)/sum(applicants),1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   gender percentage
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 men          44.5
## 2 women        30.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The chi-squared test compares two groups with binary outcomes (like “admit” and “nonadmit”). The null hypothesis is that the groups are not differently distributed between the outcomes. A low p-value rejects this hypothesis. Here, the test clearly rejects the hypothesis that gender and admission are independent:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;admissions %&amp;gt;% group_by(gender) %&amp;gt;%
  summarize(total_admitted = round(sum(admitted / 100 * applicants)),
            not_admitted = sum(applicants) - sum(total_admitted)) %&amp;gt;%
  select(-gender) %&amp;gt;%
  do(tidy(chisq.test(.))) %&amp;gt;% .$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.055797e-21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But closer inspection shows a paradoxical result. Here are the percent admissions by major:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;admissions %&amp;gt;% select(major, gender, admitted) %&amp;gt;%
  spread(gender, admitted) %&amp;gt;%
  mutate(women_minus_men = women - men)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   major men women women_minus_men
## 1     A  62    82              20
## 2     B  63    68               5
## 3     C  37    34              -3
## 4     D  33    35               2
## 5     E  28    24              -4
## 6     F   6     7               1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Four out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.&lt;/p&gt;
&lt;p&gt;The paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.&lt;/p&gt;
&lt;p&gt;So let’s define three variables: &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is 1 for men and 0 for women, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is 1 for those admitted and 0 otherwise, and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; quantifies the selectivity of the major. A gender bias claim would be based on the fact that &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(Y=1 | X = x)\)&lt;/span&gt; is higher for &lt;span class=&#34;math inline&#34;&gt;\(x=1\)&lt;/span&gt; than &lt;span class=&#34;math inline&#34;&gt;\(x=0\)&lt;/span&gt;. However, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is an important confounder to consider. Clearly &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is associated with &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, as the more selective a major, the lower &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(Y=1 | Z = z)\)&lt;/span&gt;. But is major selectivity &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; associated with gender &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;One way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;admissions %&amp;gt;%
  group_by(major) %&amp;gt;%
  summarize(major_selectivity = sum(admitted * applicants)/sum(applicants),
            percent_women_applicants = sum(applicants * (gender==&amp;quot;women&amp;quot;)) /
                                             sum(applicants) * 100) %&amp;gt;%
  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +
  geom_text()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/uc-berkeley-majors-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There seems to be association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confounding-explained-graphically&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confounding explained graphically&lt;/h3&gt;
&lt;p&gt;The following plot shows the number of applicants that were admitted and those that were not by:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/confounding-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;!--

```r
admissions %&gt;%
  mutate(percent_admitted = admitted * applicants/sum(applicants)) %&gt;%
  ggplot(aes(gender, y = percent_admitted, fill = major)) +
  geom_bar(stat = &#34;identity&#34;, position = &#34;stack&#34;)
```

&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/confounding-2-1.png&#34; width=&#34;672&#34; /&gt;
--&gt;
&lt;p&gt;It also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-after-stratifying&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Average after stratifying&lt;/h3&gt;
&lt;p&gt;In this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;admissions %&amp;gt;%
  ggplot(aes(major, admitted, col = gender, size = applicants)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/admission-by-major-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.&lt;/p&gt;
&lt;p&gt;If we average the difference by major, we find that the percent is actually 3.5% higher for women.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;admissions %&amp;gt;%  group_by(gender) %&amp;gt;% summarize(average = mean(admitted))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 2
##   gender average
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 men       38.2
## 2 women     41.7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simpsons-paradox&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simpson’s paradox&lt;/h2&gt;
&lt;p&gt;The case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; and that we observe realizations of these. Here is a plot of simulated observations for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; along with the sample correlation:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/simpsons-paradox-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are negatively correlated. However, once we stratify by &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; (shown in different colors below) another pattern emerges:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/08-content_files/figure-html/simpsons-paradox-explained-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is really &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; that is negatively correlated with &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. If we stratify by &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are actually positively correlated as seen in the plot above.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://tylervigen.com/spurious-correlations&#34; class=&#34;uri&#34;&gt;http://tylervigen.com/spurious-correlations&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated&#34; class=&#34;uri&#34;&gt;https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Project 2</title>
      <link>https://ssc442.netlify.app/assignment/project2/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/project2/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#part-1-instructions&#34;&gt;Part 1: Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-2-hypotheses&#34;&gt;Part 2: Hypotheses&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-cleaning-code&#34;&gt;Data cleaning code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-to-possibly-use-in-your-plot&#34;&gt;Data to possibly use in your plot&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#country-totals-over-time&#34;&gt;Country totals over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cumulative-country-totals-over-time&#34;&gt;Cumulative country totals over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continent-totals-over-time&#34;&gt;Continent totals over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cumulative-continent-totals-over-time&#34;&gt;Cumulative continent totals over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-about-other-data&#34;&gt;What about other data?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;As with Project 1, each member of the group should submit an &lt;strong&gt;identical&lt;/strong&gt; copy of the project to D2L (for ease of evaluation and to ensure communication across the group). You must write your group number all group members’ names across the top. Upload the components (see below) to the Project 2 assignment on D2L by 11:59pm on November 7, 2021.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The United States has resettled more than 600,000 refugees from 60 different countries since 2006.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/assignments/refugees_welcome.jpg&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this project, you will use &lt;strong&gt;R, ggplot&lt;/strong&gt; and some form of graphics editor to explore where these refugees have come from.&lt;/p&gt;
&lt;div id=&#34;part-1-instructions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 1: Instructions&lt;/h2&gt;
&lt;p&gt;Here’s what you need to do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; the Department of Homeland Security’s annual count of people granted refugee status between 2006-2015:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/refugee_status.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; DHS refugees, 2006-2015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Save this somewhere on your computer (you might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text). This data was originally &lt;a href=&#34;https://www.kaggle.com/dhs/refugee-report&#34;&gt;uploaded by the Department of Homeland Security to Kaggle&lt;/a&gt;, and is provided with a public domain license.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Clean&lt;/strong&gt; the data using the code we’ve given you below. As always, this code is presented without guarantee. You may need to deal with a few issues, depending on your computer’s setup.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Summarize&lt;/strong&gt; the data somehow. There is data for 60 countries over 10 years, so you’ll probably need to aggregate or reshape the data somehow (unless you do a 60-country sparkline). I’ve included some examples down below.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Create&lt;/strong&gt; an appropriate time-based visualization based on the data. I’ve shown a few different ways to summarize the data so that it’s plottable down below. Don’t just calculate overall averages or totals per country—the visualization needs to deal with change over time, and it needs to illustrate something non-obvious and insightful in the data. &lt;em&gt;Do as much polishing and refining in R&lt;/em&gt;—make adjustments to the colors, scales, labels, grid lines, and even fonts, etc. You may have more than one visualization, but only one is required. If you have more than one, they must be visually consistent (same appearance, coordinated colors, etc.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Refine and polish&lt;/strong&gt; the saved image, adding annotations, changing colors, and otherwise enhancing it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Design and create&lt;/strong&gt; an infographic “poster”. Your poster should look like a polished image that you might see on a newspaper website like &lt;a href=&#34;https://www.nytimes.com/interactive/2020/04/11/business/economy/coronavirus-us-economy-spending.html&#34;&gt;the NYT&lt;/a&gt;. Your infographic “poster” should include an eye-catching title, your plot, the caption describing the plot, and 2-4 short paragraphs succinctly describing the insights you are sharing about the data. You can (and should consider) integrating other images like national flags or arrows to convey some semantic meaning. *You may do the layout of the infographic “poster” in any software you choose - Publisher (do people still use that?), Adobe Illustrator, etc. Again, the idea is to have a polished plot with an interesting insight from the data, a polished layout to make it attractive, and a polished 2-4 paragraphs that sets up the plot and elaborates on your insight.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Upload&lt;/strong&gt; the following outputs to D2L:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your code (.Rmd) that generates the graphic.&lt;/li&gt;
&lt;li&gt;Your final poster, saved as a PDF.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this assignment, we are less concerned with the code (that’s why we gave most of it to you), and more concerned with the &lt;em&gt;design&lt;/em&gt;. Choose good colors based on palettes. Choose good, clean fonts. Use the heck out of &lt;code&gt;theme()&lt;/code&gt;. Add informative design elements. Make it look beautiful. Refer to &lt;a href=&#34;https://ssc442.netlify.app/resource/design/&#34;&gt;the design resources here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please seek out help when you need it!&lt;/strong&gt; You know enough R (and have enough examples of code from class and your readings) to be able to do this. &lt;em&gt;You can do this,&lt;/em&gt; and you’ll feel like a budding dataviz witch/wizard when you’re done.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-hypotheses&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 2: Hypotheses&lt;/h2&gt;
&lt;p&gt;For this part of the assignment, you need to provide five hypotheses about the relationship between variables in a dataset. You can (and should) consider making hypotheses about the dataset that you plan to use for your final project. However, this is not a requirement. All that is required is that you provide five hypotheses about some data. Your write-up should have an enumerated list of questions (e.g., “1. Are there more murders in states that have high unemployment.”). You will receive 2 points for each hypothesis.&lt;/p&gt;
&lt;div id=&#34;evaluation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;I will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 1&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Technical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (15 points)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Professionalism of visuals: Does the visualizations look like something you might see on TV or in the newspaper? (15 points)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Poster clarity: Does your poster clearly convey some point? (10 points)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Part 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each hypothesis is worth 2 points. (This is intended to be some free points for all; 10 points)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-cleaning-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data cleaning code&lt;/h2&gt;
&lt;p&gt;The data isn’t perfectly clean and tidy, but it’s real world data, so this is normal. Because the emphasis for this assignment is on design, not code, we’ve provided code to help you clean up the data.&lt;/p&gt;
&lt;p&gt;These are the main issues with the data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There are non-numeric values in the data, like &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt;. The data isn’t very well documented; we’re assuming &lt;code&gt;-&lt;/code&gt; indicates a missing value, but we’re not sure what &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;D&lt;/code&gt; mean, so for this assignment, we’ll just assume they’re also missing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The data generally includes rows for dozens of countries, but there are also rows for some continents, “unknown,” “other,” and a total row. Because &lt;a href=&#34;https://twitter.com/africasacountry&#34;&gt;Africa is not a country&lt;/a&gt;, and neither are the other continents, we want to exclude all non-countries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Maintaining consistent country names across different datasets is &lt;em&gt;literally&lt;/em&gt; the woooooooorst. Countries have different formal official names and datasets are never consistent in how they use those names.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; It’s such a tricky problem that social scientists have spent their careers just figuring out how to properly name and code countries. Really.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; There are international standards for country codes, though, like &lt;a href=&#34;https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3&#34;&gt;ISO 3166-1 alpha 3&lt;/a&gt; (my favorite), also known as ISO3. It’s not perfect—it omits microstates (some Polynesian countries) and gray area states (Palestine, Kosovo)—but it’s an international standard, so it has that going for it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To ensure that country names are consistent in this data, we use the &lt;strong&gt;countrycode&lt;/strong&gt; package (install it if you don’t have it), which is amazing. The &lt;code&gt;countrycode()&lt;/code&gt; function will take a country name in a given coding scheme and convert it to a different coding scheme using this syntax:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  countrycode(variable, &amp;quot;current-coding-scheme&amp;quot;, &amp;quot;new-coding-scheme&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It also does a farily good job at guessing and parsing inconsistent country names (e.g. it will recognize “Congo, Democratic Republic”, even though it should technically be “Democratic Republic of the Congo”). Here, we use &lt;code&gt;countrycode()&lt;/code&gt; to convert the inconsistent country names into ISO3 codes. We then create a cleaner version of the &lt;code&gt;origin_country&lt;/code&gt; column by converting the ISO3 codes back into country names. Note that the function chokes on North Korea initially, since it’s included as “Korea, North”—we use the &lt;code&gt;custom_match&lt;/code&gt; argument to help the function out.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The data isn’t tidy—there are individual columns for each year. &lt;code&gt;gather()&lt;/code&gt; takes every column and changes it to a row. We exclude the country, region, continent, and ISO3 code from the change-into-rows transformation with &lt;code&gt;-origin_country, -iso3, -origin_region, -origin_continent&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Currently, the year is being treated as a number, but it’s helpful to also treat it as an actual date. We create a new variable named &lt;code&gt;year_date&lt;/code&gt; that converts the raw number (e.g. 2009) into a date. The date needs to have at least a month, day, and year, so we actually convert it to January 1, 2009 with &lt;code&gt;ymd(paste0(year, &#34;-01-01&#34;))&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)    # For ggplot, dplyr, and friends
library(countrycode)  # For dealing with country names, abbreviations, and codes
library(lubridate)    # For dealing with dates
library(WDI)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_raw &amp;lt;- read_csv(&amp;quot;data/refugee_status.csv&amp;quot;, na = c(&amp;quot;-&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;D&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_countries &amp;lt;- c(&amp;quot;Africa&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;Europe&amp;quot;, &amp;quot;North America&amp;quot;, &amp;quot;Oceania&amp;quot;,
                   &amp;quot;South America&amp;quot;, &amp;quot;Unknown&amp;quot;, &amp;quot;Other&amp;quot;, &amp;quot;Total&amp;quot;)

refugees_clean &amp;lt;- refugees_raw %&amp;gt;%
  # Make this column name easier to work with
  rename(origin_country = `Continent/Country of Nationality`) %&amp;gt;%
  # Get rid of non-countries
  filter(!(origin_country %in% non_countries)) %&amp;gt;%
  # Convert country names to ISO3 codes
  mutate(iso3 = countrycode(origin_country, &amp;quot;country.name&amp;quot;, &amp;quot;iso3c&amp;quot;,
                            custom_match = c(&amp;quot;Korea, North&amp;quot; = &amp;quot;PRK&amp;quot;))) %&amp;gt;%
  # Convert ISO3 codes to country names, regions, and continents
  mutate(origin_country = countrycode(iso3, &amp;quot;iso3c&amp;quot;, &amp;quot;country.name&amp;quot;),
         origin_region = countrycode(iso3, &amp;quot;iso3c&amp;quot;, &amp;quot;region&amp;quot;),
         origin_continent = countrycode(iso3, &amp;quot;iso3c&amp;quot;, &amp;quot;continent&amp;quot;)) %&amp;gt;%
  # Make this data tidy
  gather(year, number, -origin_country, -iso3, -origin_region, -origin_continent) %&amp;gt;%
  # Make sure the year column is numeric + make an actual date column for years
  mutate(year = as.numeric(year),
         year_date = ymd(paste0(year, &amp;quot;-01-01&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-to-possibly-use-in-your-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data to possibly use in your plot&lt;/h2&gt;
&lt;p&gt;Here are some possible summaries of the data you might use…&lt;/p&gt;
&lt;div id=&#34;country-totals-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Country totals over time&lt;/h3&gt;
&lt;p&gt;This is just the &lt;code&gt;refugees_clean&lt;/code&gt; data frame I gave you. You’ll want to filter it and select specific countries, though—you won’t really be able to plot 60 countries all at once unless you use sparklines.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 7
##   origin_country iso3  origin_region         origin_continent  year number year_date 
##   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;date&amp;gt;    
## 1 Afghanistan    AFG   South Asia            Asia              2006    651 2006-01-01
## 2 Angola         AGO   Sub-Saharan Africa    Africa            2006     13 2006-01-01
## 3 Armenia        ARM   Europe &amp;amp; Central Asia Asia              2006     87 2006-01-01
## 4 Azerbaijan     AZE   Europe &amp;amp; Central Asia Asia              2006     77 2006-01-01
## 5 Belarus        BLR   Europe &amp;amp; Central Asia Europe            2006    350 2006-01-01
## 6 Bhutan         BTN   South Asia            Asia              2006      3 2006-01-01&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cumulative-country-totals-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cumulative country totals over time&lt;/h3&gt;
&lt;p&gt;Note the &lt;code&gt;cumsum()&lt;/code&gt; function—it calculates the cumulative sum of a column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_countries_cumulative &amp;lt;- refugees_clean %&amp;gt;%
  arrange(year_date) %&amp;gt;%
  group_by(origin_country) %&amp;gt;%
  mutate(cumulative_total = cumsum(number))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 7
## # Groups:   origin_country [1]
##   origin_country iso3  origin_continent  year number year_date  cumulative_total
##   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;date&amp;gt;                &amp;lt;dbl&amp;gt;
## 1 Afghanistan    AFG   Asia              2006    651 2006-01-01              651
## 2 Afghanistan    AFG   Asia              2007    441 2007-01-01             1092
## 3 Afghanistan    AFG   Asia              2008    576 2008-01-01             1668
## 4 Afghanistan    AFG   Asia              2009    349 2009-01-01             2017
## 5 Afghanistan    AFG   Asia              2010    515 2010-01-01             2532
## 6 Afghanistan    AFG   Asia              2011    428 2011-01-01             2960&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;continent-totals-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Continent totals over time&lt;/h3&gt;
&lt;p&gt;Note the &lt;code&gt;na.rm = TRUE&lt;/code&gt; argument in &lt;code&gt;sum()&lt;/code&gt;. This makes R ignore any missing data when calculating the total. Without it, if R finds a missing value in the column, it will mark the final sum as &lt;code&gt;NA&lt;/code&gt; too, which we don’t want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_continents &amp;lt;- refugees_clean %&amp;gt;%
  group_by(origin_continent, year_date) %&amp;gt;%
  summarize(total = sum(number, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;origin_continent&amp;#39;. You can override using the `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 3
## # Groups:   origin_continent [1]
##   origin_continent year_date  total
##   &amp;lt;chr&amp;gt;            &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Africa           2006-01-01 18116
## 2 Africa           2007-01-01 17473
## 3 Africa           2008-01-01  8931
## 4 Africa           2009-01-01  9664
## 5 Africa           2010-01-01 13303
## 6 Africa           2011-01-01  7677&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cumulative-continent-totals-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cumulative continent totals over time&lt;/h3&gt;
&lt;p&gt;Note that there are two &lt;code&gt;group_by()&lt;/code&gt; functions here. First we get the total number of refugees per continent per year, then we group by continent only to get the cumulative sum of refugees across continents.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_continents_cumulative &amp;lt;- refugees_clean %&amp;gt;%
  group_by(origin_continent, year_date) %&amp;gt;%
  summarize(total = sum(number, na.rm = TRUE)) %&amp;gt;%
  arrange(year_date) %&amp;gt;%
  group_by(origin_continent) %&amp;gt;%
  mutate(cumulative_total = cumsum(total))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;origin_continent&amp;#39;. You can override using the `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 4
## # Groups:   origin_continent [1]
##   origin_continent year_date  total cumulative_total
##   &amp;lt;chr&amp;gt;            &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1 Africa           2006-01-01 18116            18116
## 2 Africa           2007-01-01 17473            35589
## 3 Africa           2008-01-01  8931            44520
## 4 Africa           2009-01-01  9664            54184
## 5 Africa           2010-01-01 13303            67487
## 6 Africa           2011-01-01  7677            75164&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-other-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What about other data?&lt;/h3&gt;
&lt;p&gt;In your prerequisite course, you learned how to use the &lt;code&gt;merge&lt;/code&gt; function (and possibly its Tidyverse cousin, &lt;code&gt;left_join&lt;/code&gt;). Since our refugee data has a standardized country code, we can find other datasets that might have useful information.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;WDI&lt;/code&gt; package acts as an interface to the World Bank Development Inidcators dataset, which has a &lt;em&gt;lot&lt;/em&gt; of information on the countries in our data. To merge WDI data to our refugee data, we’ll need to make a little change to our cleaning code - &lt;code&gt;WDI&lt;/code&gt; needs the iso2 country code, which is a unique 2-letter code instead of the iso3 3-letter code we have. Here’s the cleaning data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_clean &amp;lt;- refugees_raw %&amp;gt;%
  rename(origin_country = `Continent/Country of Nationality`) %&amp;gt;%
  dplyr::filter(!(origin_country %in% non_countries)) %&amp;gt;%
  mutate(iso2 = countrycode(origin_country, &amp;quot;country.name&amp;quot;, &amp;quot;iso2c&amp;quot;,
                            custom_match = c(&amp;quot;Korea, North&amp;quot; = &amp;quot;KP&amp;quot;))) %&amp;gt;%
  mutate(origin_country = countrycode(iso2, &amp;quot;iso2c&amp;quot;, &amp;quot;country.name&amp;quot;),
         origin_region = countrycode(iso2, &amp;quot;iso2c&amp;quot;, &amp;quot;region&amp;quot;),
         origin_continent = countrycode(iso2, &amp;quot;iso2c&amp;quot;, &amp;quot;continent&amp;quot;)) %&amp;gt;%
  gather(year, number, -origin_country, -iso2, -origin_region, -origin_continent) %&amp;gt;%
  mutate(year = as.numeric(year),
         year_date = ymd(paste0(year, &amp;quot;-01-01&amp;quot;)),
         iso2c = iso2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only difference here is that we now have iso2 instead of iso3. If you use your iso3 field in your code, you’ll have to create both.&lt;/p&gt;
&lt;p&gt;Now, the &lt;code&gt;WDI&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(WDI)
myData = WDI(country = refugees_clean$iso2, indicator = &amp;#39;SP.POP.TOTL&amp;#39;, start = 2006, end = 2015)  %&amp;gt;%
      dplyr::select(-country)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function needs four arguments (see &lt;code&gt;?WDI&lt;/code&gt; after installing the &lt;code&gt;WDI&lt;/code&gt; package for more). The first has to be list of iso2 codes, which we have…in the form of the &lt;code&gt;iso2&lt;/code&gt; column. So we pass that column as the first argument, and &lt;code&gt;WDI&lt;/code&gt; will give us data for every country in that column.&lt;/p&gt;
&lt;p&gt;The second argument, &lt;code&gt;indicator&lt;/code&gt;, tells &lt;code&gt;WDI&lt;/code&gt; what data you want. The &lt;a href=&#34;https://databank.worldbank.org/metadataglossary/World-Development-Indicators/series&#34;&gt;WDI data dictionary is here&lt;/a&gt;. Use the drop-down under “Select Database” and choose just the World Development Indicators option to simplify. Then, use the search box to search for intersting indicators. The website shows the “code” of each of the indicators. That’s the code you use. For instance, &lt;code&gt;SP.POP.TOTL&lt;/code&gt; is the total population by country, so you can generate per-capita measures of refugees if you want. Finally, &lt;code&gt;WDI&lt;/code&gt; needs the start and end years. Our data is 2006 to 2015, so that makes the most sense.&lt;/p&gt;
&lt;p&gt;Now, once you have &lt;code&gt;myData&lt;/code&gt; retrieved from &lt;code&gt;WDI&lt;/code&gt;, take a look at it. We need to see which columns to merge on - that is, which columns should R match to put the data together? We want to merge on &lt;code&gt;&#39;iso2c&#39;&lt;/code&gt; and on &lt;code&gt;&#39;year&#39;&lt;/code&gt; because &lt;code&gt;WDI&lt;/code&gt; gives us a tidy data frame by country-year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_clean_merged = left_join(refugees_clean, myData, by = c(&amp;#39;iso2c&amp;#39;,&amp;#39;year&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We didn’t have an &lt;code&gt;iso2c&lt;/code&gt; column before, just an &lt;code&gt;iso2&lt;/code&gt; column, so you might notice that the new cleaning code I gave you added &lt;code&gt;iso2c = iso2&lt;/code&gt; at the end using &lt;code&gt;mutate&lt;/code&gt;. That way, the column names that we’re using to merge will match.&lt;/p&gt;
&lt;p&gt;You can search the WDI data dictionary and find the “Code” for the indicator you’d like to use, then just merge it into your data. Look at your data closely before you merge, and make sure you know what you’re merging in. Use the slack if you get stuck. Happy data hunting!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(refugees_clean_merged %&amp;gt;% dplyr::select(origin_country, number, iso2, year, SP.POP.TOTL))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 5
##   origin_country number iso2   year SP.POP.TOTL
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Afghanistan       651 AF     2006    26433058
## 2 Angola             13 AO     2006    20149905
## 3 Armenia            87 AM     2006     2958301
## 4 Azerbaijan         77 AZ     2006     8484550
## 5 Belarus           350 BY     2006     9604924
## 6 Bhutan              3 BT     2006      657404&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For instance, “North Korea”, “Korea, North”, “DPRK”, “Korea, Democratic People’s Republic of”, and “Democratic People’s Republic of Korea”, and “Korea (DPRK)” are all perfectly normal versions of the country’s name and you’ll find them all in the wild.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;See Gleditsch, Kristian S. &amp;amp; Michael D. Ward. 1999. &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/03050629908434958&#34;&gt;“Interstate System Membership: A Revised List of the Independent States since 1816.”&lt;/a&gt; &lt;em&gt;International Interactions&lt;/em&gt; 25: 393-413; or the &lt;a href=&#34;http://www.paulhensel.org/icownames.html&#34;&gt;“ICOW Historical State Names Data Set”&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Project 1</title>
      <link>https://ssc442.netlify.app/assignment/project1/</link>
      <pubDate>Fri, 08 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/project1/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#groups&#34;&gt;Groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-1-rats-rats-rats.&#34;&gt;Part 1: Rats, rats, rats.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#instructions&#34;&gt;Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#starter-code&#34;&gt;Starter code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-2-data-hunting&#34;&gt;Part 2: Data Hunting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluations&#34;&gt;Evaluations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Groups&lt;/h3&gt;
&lt;p&gt;Your groups are listed &lt;a href=&#34;https://ssc442.netlify.app/projects/grouplist.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Each member of the group must submit a copy of the project to D2L. Please write your group number and the group members’ names across the top.&lt;/p&gt;
&lt;p&gt;Turn in your copies by &lt;strong&gt;11:59pm on October 8th&lt;/strong&gt; (I have extended everyone’s deadline due to slow rollout of groups).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-1-rats-rats-rats.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 1: Rats, rats, rats.&lt;/h2&gt;
&lt;p&gt;New York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PeJUqcbool4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;NYC keeps incredibly detailed data regarding animal sightings, including rats, and &lt;a href=&#34;https://www.kaggle.com/new-york-city/nyc-rat-sightings/data&#34;&gt;it makes this data publicly available&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For this first project, you will use &lt;strong&gt;R and ggplot2&lt;/strong&gt; to tell an interesting story hidden in the data. You must create a story by looking carefully at the data.&lt;/p&gt;
&lt;div id=&#34;instructions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Instructions&lt;/h3&gt;
&lt;p&gt;Here’s what you need to do:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; New York City’s database of rat sightings since 2010:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Rat_sightings.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Rat_sightings.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Summarize&lt;/strong&gt; the data somehow. The raw data has more than 100,000 rows, which means you’ll need to aggregate the data (&lt;code&gt;filter()&lt;/code&gt;, &lt;code&gt;group_by()&lt;/code&gt;, and &lt;code&gt;summarize()&lt;/code&gt; will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2016.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Create&lt;/strong&gt; an appropriate visualization based on the data you summarized.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Write&lt;/strong&gt; a memo explaining your process. We are specifically looking for a discussion of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What story are you telling with your new graphic?&lt;/li&gt;
&lt;li&gt;How have you applied reasonable standards in visual storytelling?&lt;/li&gt;
&lt;li&gt;What policy implication is there (if any)?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Upload&lt;/strong&gt; the following outputs to D2L:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A PDF file of your memo with your final code and graphic embedded in it.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; This means you’ll need to do all your coding in an &lt;code&gt;R&lt;/code&gt; Markdown file and embed your code in chunks. Note that Part 2 of this project should be included in this PDF (see below).&lt;/li&gt;
&lt;li&gt;A standalone PDF version of your graphic. Use &lt;code&gt;ggsave(plot_name, filename = &#34;output/blah.pdf&#34;, width = XX, height = XX)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;starter-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Starter code&lt;/h3&gt;
&lt;p&gt;I’ve provided some starter code below. A couple comments about it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By default, &lt;code&gt;read_csv()&lt;/code&gt; treats cells that are empty or “NA” as missing values. This rat dataset uses “N/A” to mark missing values, so we need to add that as a possible marker of missingness (hence &lt;code&gt;na = c(&#34;&#34;, &#34;NA&#34;, &#34;N/A&#34;)&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;To make life easier, I’ve renamed some of the key variables you might work with. You can rename others if you want.&lt;/li&gt;
&lt;li&gt;I’ve also created a few date-related variables (&lt;code&gt;sighting_year&lt;/code&gt;, &lt;code&gt;sighting_month&lt;/code&gt;, &lt;code&gt;sighting_day&lt;/code&gt;, and &lt;code&gt;sighting_weekday&lt;/code&gt;). You don’t have to use them, but they’re there if you need them. The functions that create these, like &lt;code&gt;year()&lt;/code&gt; and &lt;code&gt;wday()&lt;/code&gt; are part of the &lt;strong&gt;lubridate&lt;/strong&gt; library.&lt;/li&gt;
&lt;li&gt;The date/time variables are formatted like &lt;code&gt;04/03/2017 12:00:00 AM&lt;/code&gt;, which R is not able to automatically parse as a date when reading the CSV file. You can use the &lt;code&gt;mdy_hms()&lt;/code&gt; function in the &lt;strong&gt;lubridate&lt;/strong&gt; library to parse dates that are structured as “month-day-year-hour-minute”. There are also a bunch of other iterations of this function, like &lt;code&gt;ymd()&lt;/code&gt;, &lt;code&gt;dmy()&lt;/code&gt;, etc., for other date formats.&lt;/li&gt;
&lt;li&gt;There’s one row with an unspecified borough, so I filter that out.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
rats_raw &amp;lt;- read_csv(&amp;quot;data/Rat_Sightings.csv&amp;quot;, na = c(&amp;quot;&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;N/A&amp;quot;))
# If you get an error that says &amp;quot;All formats failed to parse. No formats
# found&amp;quot;, it&amp;#39;s because the mdy_hms function couldn&amp;#39;t parse the date. The date
# variable *should* be in this format: &amp;quot;04/03/2017 12:00:00 AM&amp;quot;, but in some
# rare instances, it might load without the seconds as &amp;quot;04/03/2017 12:00 AM&amp;quot;.
# If there are no seconds, use mdy_hm() instead of mdy_hms().
rats_clean &amp;lt;- rats_raw %&amp;gt;%
  rename(created_date = `Created Date`,
         location_type = `Location Type`,
         borough = Borough) %&amp;gt;%
  mutate(created_date = mdy_hms(created_date)) %&amp;gt;%
  mutate(sighting_year = year(created_date),
         sighting_month = month(created_date),
         sighting_day = day(created_date),
         sighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %&amp;gt;%
  filter(borough != &amp;quot;Unspecified&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll summarize the data with functions from &lt;strong&gt;dplyr&lt;/strong&gt;, including stuff like &lt;code&gt;count()&lt;/code&gt;, &lt;code&gt;arrange()&lt;/code&gt;, &lt;code&gt;filter()&lt;/code&gt;, &lt;code&gt;group_by()&lt;/code&gt;, &lt;code&gt;summarize()&lt;/code&gt;, and &lt;code&gt;mutate()&lt;/code&gt;. Here are some examples of ways to summarize the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# See the count of rat sightings by weekday
rats_clean %&amp;gt;%
  count(sighting_weekday)
# Assign a summarized data frame to an object to use it in a plot
rats_by_weekday &amp;lt;- rats_clean %&amp;gt;%
  count(sighting_weekday, sighting_year)
ggplot(rats_by_weekday, aes(x = fct_rev(sighting_weekday), y = n)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ sighting_year)
# See the count of rat sightings by weekday and borough
rats_clean %&amp;gt;%
  count(sighting_weekday, borough, sighting_year)
# An alternative to count() is to specify the groups with group_by() and then
# be explicit about how you&amp;#39;re summarizing the groups, such as calculating the
# mean, standard deviation, or number of observations (we do that here with
# `n()`).
rats_clean %&amp;gt;%
  group_by(sighting_weekday, borough) %&amp;gt;%
  summarize(n = n())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-data-hunting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 2: Data Hunting&lt;/h2&gt;
&lt;p&gt;For the second part of the project, your task is simple. Your group must identify three different data sources&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; for potential use in your final project. You are not bound to this decision.&lt;/p&gt;
&lt;p&gt;For each, you must write a single paragraph about what about this data interests you. Add this to the memo from Part 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluations&lt;/h2&gt;
&lt;p&gt;I will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 1&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Technical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (10 points)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Appropriateness of visuals: Do the visualizations tell a clear story? Have we learned something? (10 points)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Storytelling: Does your memo clearly convey what you’re doing and why? (9 points)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Part 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each piece of data (and description) is worth 7 points. (21 points total)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;You can approach this in a couple different ways—you can write the memo and then include the full figure and code at the end, &lt;a href=&#34;https://rud.is/b/2017/09/18/mapping-fall-foliage-with-sf/&#34;&gt;similar to this blog post&lt;/a&gt;, or you can write the memo in an incremental way, describing the different steps of creating the figure, ultimately arriving at a clean final figure, &lt;a href=&#34;https://rudeboybert.github.io/fivethirtyeight/articles/bechdel.html&#34;&gt;like this blog post&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The three different sources need not be different websites or from different organizations. For example, three different tables from the US Census would be sufficient&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Regression</title>
      <link>https://ssc442.netlify.app/content/06-content/</link>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/06-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regression&#34;&gt;Introduction to Linear Regression&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-is-height-hereditary&#34;&gt;Case study: is height hereditary?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#corr-coef&#34;&gt;The correlation coefficient&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sample-correlation-is-a-random-variable&#34;&gt;Sample correlation is a random variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-is-not-always-a-useful-summary&#34;&gt;Correlation is not always a useful summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-expectation&#34;&gt;Conditional expectations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-regression-line&#34;&gt;The regression line&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#regression-improves-precision&#34;&gt;Regression improves precision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bivariate-normal-distribution-advanced&#34;&gt;Bivariate normal distribution (advanced)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance-explained&#34;&gt;Variance explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#warning-there-are-two-regression-lines&#34;&gt;Warning: there are two regression lines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ### Supplemental Readings --&gt;
&lt;!-- - Coming soon. --&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How can we express a simple relationship between variables?&lt;/li&gt;
&lt;li&gt;What is a geometric intuition behind “linear regression”?&lt;/li&gt;
&lt;li&gt;How might we visualize a linear regression?&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s lecture will ask you to touch real data during the lecture. Please download the following dataset and load it into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This dataset is from houses in Ames, Iowa. (Thrilling!) We will use this dataset during the lecture to illustrate some of the points discussed below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to Linear Regression&lt;/h1&gt;
&lt;p&gt;Up to this point, this class has focused mainly on single variables. However, in data analytics applications, it is very common to be interested in the &lt;strong&gt;relationship&lt;/strong&gt; between two or more variables. For instance, in the coming days we will use a data-driven approach that examines the relationship between player statistics and success to guide the building of a baseball team with a limited budget. Before delving into this more complex example, we introduce necessary concepts needed to understand regression using a simpler illustration. We actually use the dataset from which regression was born.&lt;/p&gt;
&lt;p&gt;The example is from genetics. Francis Galton&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity. While doing this, he developed the concepts of correlation and regression, as well as a connection to pairs of data that follow a normal distribution. Of course, at the time this data was collected our knowledge of genetics was quite limited compared to what we know today. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height? The technique he developed to answer this question, regression, can also be applied to our baseball question. Regression can be applied in many other circumstances as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Historical note&lt;/strong&gt;: Galton made important contributions to statistics and genetics, but he was also one of the first proponents of eugenics, a scientifically flawed philosophical movement favored by many biologists of Galton’s time but with horrific historical consequences. These consequences still reverberate to this day, and form the basis for much of the Western world’s racist policies. You can read more about it here: &lt;a href=&#34;https://pged.org/history-eugenics-and-genetics/&#34;&gt;https://pged.org/history-eugenics-and-genetics/&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;case-study-is-height-hereditary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: is height hereditary?&lt;/h2&gt;
&lt;p&gt;We have access to Galton’s family height data through the &lt;strong&gt;HistData&lt;/strong&gt; package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(HistData)
data(&amp;quot;GaltonFamilies&amp;quot;)

set.seed(1983)
galton_heights &amp;lt;- GaltonFamilies %&amp;gt;%
  filter(gender == &amp;quot;male&amp;quot;) %&amp;gt;%
  group_by(family) %&amp;gt;%
  sample_n(1) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(father, childHeight) %&amp;gt;%
  rename(son = childHeight)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the exercises, we will look at other relationships including mothers and daughters.&lt;/p&gt;
&lt;p&gt;Suppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;%
  summarize(mean(father), sd(father), mean(son), sd(son))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 4
##   `mean(father)` `sd(father)` `mean(son)` `sd(son)`
##            &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1           69.1         2.55        69.2      2.71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;% ggplot(aes(father, son)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/scatterplot-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will learn that the correlation coefficient is an informative summary of how two variables move together and then see how this can be used to predict one variable using the other.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;corr-coef&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The correlation coefficient&lt;/h2&gt;
&lt;p&gt;The correlation coefficient is defined for a list of pairs &lt;span class=&#34;math inline&#34;&gt;\((x_1, y_1), \dots, (x_n,y_n)\)&lt;/span&gt; as the average of the product of the standardized values:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)
\]&lt;/span&gt;
with &lt;span class=&#34;math inline&#34;&gt;\(\mu_x, \mu_y\)&lt;/span&gt; the averages of &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots, x_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_1, \dots, y_n\)&lt;/span&gt;, respectively, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x, \sigma_y\)&lt;/span&gt; the standard deviations. The Greek letter &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is commonly used in statistics books to denote the correlation. The Greek letter for &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, because it is the first letter of regression. Soon we learn about the connection between correlation and regression. We can represent the formula above with R code using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rho &amp;lt;- mean(scale(x) * scale(y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To understand why this equation does in fact summarize how two variables move together, consider the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th entry of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\left( \frac{x_i-\mu_x}{\sigma_x} \right)\)&lt;/span&gt; SDs away from the average. Similarly, the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; that is paired with &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, is &lt;span class=&#34;math inline&#34;&gt;\(\left( \frac{y_1-\mu_y}{\sigma_y} \right)\)&lt;/span&gt; SDs away from the average &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are unrelated, the product &lt;span class=&#34;math inline&#34;&gt;\(\left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)\)&lt;/span&gt; will be positive ( &lt;span class=&#34;math inline&#34;&gt;\(+ \times +\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(- \times -\)&lt;/span&gt; ) as often as negative (&lt;span class=&#34;math inline&#34;&gt;\(+ \times -\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(- \times +\)&lt;/span&gt;) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products ( &lt;span class=&#34;math inline&#34;&gt;\(+ \times +\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(- \times -\)&lt;/span&gt;) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation.&lt;/p&gt;
&lt;p&gt;The correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)^2 =
\frac{1}{\sigma_x^2} \frac{1}{n} \sum_{i=1}^n \left( x_i-\mu_x \right)^2 =
\frac{1}{\sigma_x^2} \sigma^2_x =
1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A similar derivation, but with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and its exact opposite, proves the correlation has to be bigger or equal to -1.&lt;/p&gt;
&lt;p&gt;For other pairs, the correlation is in between -1 and 1. The correlation between father and son’s heights is about 0.5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;% summarize(r = cor(father, son)) %&amp;gt;% pull(r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4334102&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see what data looks like for different values of &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, here are six examples of pairs with correlations ranging from -0.9 to 0.99:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/what-correlation-looks-like-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;sample-correlation-is-a-random-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sample correlation is a random variable&lt;/h3&gt;
&lt;p&gt;Before we continue connecting correlation to regression, let’s remind ourselves about random variability.&lt;/p&gt;
&lt;p&gt;In most data science applications, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest but rather for a random sample. As with the average and standard deviation, the &lt;em&gt;sample correlation&lt;/em&gt; is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.&lt;/p&gt;
&lt;p&gt;By way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- sample_n(galton_heights, 25, replace = TRUE) %&amp;gt;%
  summarize(r = cor(father, son)) %&amp;gt;% pull(r)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is a random variable. We can run a Monte Carlo simulation to see its distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 1000
N &amp;lt;- 25
R &amp;lt;- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %&amp;gt;%
    summarize(r=cor(father, son)) %&amp;gt;%
    pull(r)
})
qplot(R, geom = &amp;quot;histogram&amp;quot;, binwidth = 0.05, color = I(&amp;quot;black&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/sample-correlation-distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the expected value of &lt;code&gt;R&lt;/code&gt; is the population correlation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4307393&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and that it has a relatively high standard error relative to the range of values &lt;code&gt;R&lt;/code&gt; can take:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1609393&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.&lt;/p&gt;
&lt;p&gt;Also, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, the distribution of &lt;code&gt;R&lt;/code&gt; is approximately normal with expected value &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. The standard deviation, which is somewhat complex to derive, is &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{1-r^2}{N-2}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In our example, &lt;span class=&#34;math inline&#34;&gt;\(N=25\)&lt;/span&gt; does not seem to be large enough to make the approximation a good one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(aes(sample=R), data = data.frame(R)) +
  stat_qq() +
  geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/small-sample-correlation-not-normal-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you increase &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, you will see the distribution converging to normal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-is-not-always-a-useful-summary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation is not always a useful summary&lt;/h3&gt;
&lt;p&gt;Correlation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/ascombe-quartet-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Correlation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-expectation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional expectations&lt;/h2&gt;
&lt;p&gt;Suppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?&lt;/p&gt;
&lt;p&gt;It turns out that if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.&lt;/p&gt;
&lt;p&gt;In general, we call this approach &lt;em&gt;conditioning&lt;/em&gt;. The general idea is that we stratify a population into groups and compute summaries in each group. To provide a mathematical description of conditioning, consider we have a population of pairs of values &lt;span class=&#34;math inline&#34;&gt;\((x_1,y_1),\dots,(x_n,y_n)\)&lt;/span&gt;, for example all father and son heights in England. In the previous week’s content, we learned that if you take a random pair &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;, the expected value and best predictor of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}(Y) = \mu_y\)&lt;/span&gt;, the population average &lt;span class=&#34;math inline&#34;&gt;\(1/n\sum_{i=1}^n y_i\)&lt;/span&gt;. However, we are no longer interested in the general population, instead we are interested in only the subset of a population with a specific &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; value, 72 inches in our example. This subset of the population, is also a population and thus the same principles and properties we have learned apply. The &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in the subpopulation have a distribution, referred to as the &lt;em&gt;conditional distribution&lt;/em&gt;, and this distribution has an expected value referred to as the &lt;em&gt;conditional expectation&lt;/em&gt;. In our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches. The statistical notation for the conditional expectation is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}(Y \mid X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; representing the fixed value that defines that subset, for example 72 inches. Similarly, we denote the standard deviation of the strata with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SD}(Y \mid X = x) = \sqrt{\mbox{Var}(Y \mid X = x)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because the conditional expectation &lt;span class=&#34;math inline&#34;&gt;\(E(Y\mid X=x)\)&lt;/span&gt; is the best predictor for the random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for an individual in the strata defined by &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt;, many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.&lt;/p&gt;
&lt;p&gt;In the example we have been considering, we are interested in computing the average son height &lt;em&gt;conditioned&lt;/em&gt; on the father being 72 inches tall. We want to estimate &lt;span class=&#34;math inline&#34;&gt;\(E(Y|X=72)\)&lt;/span&gt; using the sample collected by Galton. We previously learned that the sample average is the preferred approach to estimating the population average. However, a challenge when using this approach to estimating conditional expectations is that for continuous data we don’t have many data points matching exactly one value in our sample. For example, we have only:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(galton_heights$father == 72)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;fathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(galton_heights$father == 72.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conditional_avg &amp;lt;- galton_heights %&amp;gt;%
  filter(round(father) == 72) %&amp;gt;%
  summarize(avg = mean(son)) %&amp;gt;%
  pull(avg)
conditional_avg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 70.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that a 72-inch father is taller than average – specifically, 72 - 69.1/2.5 =
1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son. The sons of 72-inch fathers have &lt;em&gt;regressed&lt;/em&gt; some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later lecture, this is not a coincidence.&lt;/p&gt;
&lt;p&gt;If we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;% mutate(father_strata = factor(round(father))) %&amp;gt;%
  ggplot(aes(father_strata, son)) +
  geom_boxplot() +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/boxplot-1-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not surprisingly, the centers of the groups are increasing with height.
Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/conditional-averages-follow-line-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the &lt;em&gt;regression line&lt;/em&gt;, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-regression-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The regression line&lt;/h2&gt;
&lt;p&gt;If we are predicting a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; knowing the value of another &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt; using a regression line, then we predict that for every standard deviation, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_X\)&lt;/span&gt;, that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; increases above the average &lt;span class=&#34;math inline&#34;&gt;\(\mu_X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increase &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; standard deviations &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Y\)&lt;/span&gt; above the average &lt;span class=&#34;math inline&#34;&gt;\(\mu_Y\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; the correlation between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. The formula for the regression is therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left( \frac{Y-\mu_Y}{\sigma_Y} \right) = \rho \left( \frac{x-\mu_X}{\sigma_X} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can rewrite it like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \mu_Y + \rho \left( \frac{x-\mu_X}{\sigma_X} \right) \sigma_Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; at all for the prediction and simply predict the average &lt;span class=&#34;math inline&#34;&gt;\(\mu_Y\)&lt;/span&gt;. For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.&lt;/p&gt;
&lt;p&gt;Note that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, is to the average of the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;s. This is why we call it &lt;em&gt;regression&lt;/em&gt;: the son regresses to the average height. In fact, the title of Galton’s paper was: &lt;em&gt;Regression toward mediocrity in hereditary stature&lt;/em&gt;. To add regression lines to plots, we will need the above formula in the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y= b + mx \mbox{ with slope } m = \rho \frac{\sigma_y}{\sigma_x} \mbox{ and intercept } b=\mu_y - m \mu_x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we add the regression line to the original data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu_x &amp;lt;- mean(galton_heights$father)
mu_y &amp;lt;- mean(galton_heights$son)
s_x &amp;lt;- sd(galton_heights$father)
s_y &amp;lt;- sd(galton_heights$son)
r &amp;lt;- cor(galton_heights$father, galton_heights$son)

galton_heights %&amp;gt;%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/regression-line-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. You can make same plot, but using standard units like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;%
  ggplot(aes(scale(father), scale(son))) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = r)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;regression-improves-precision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression improves precision&lt;/h3&gt;
&lt;p&gt;Let’s compare the two approaches to prediction that we have presented:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Round fathers’ heights to closest inch, stratify, and then take the average.&lt;/li&gt;
&lt;li&gt;Compute the regression line and use it to predict.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We use a Monte Carlo simulation sampling &lt;span class=&#34;math inline&#34;&gt;\(N=50\)&lt;/span&gt; families:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 1000
N &amp;lt;- 50

set.seed(1983)
conditional_avg &amp;lt;- replicate(B, {
  dat &amp;lt;- sample_n(galton_heights, N)
  dat %&amp;gt;% filter(round(father) == 72) %&amp;gt;%
    summarize(avg = mean(son)) %&amp;gt;%
    pull(avg)
  })

regression_prediction &amp;lt;- replicate(B, {
  dat &amp;lt;- sample_n(galton_heights, N)
  mu_x &amp;lt;- mean(dat$father)
  mu_y &amp;lt;- mean(dat$son)
  s_x &amp;lt;- sd(dat$father)
  s_y &amp;lt;- sd(dat$son)
  r &amp;lt;- cor(dat$father, dat$son)
  mu_y + r*(72 - mu_x)/s_x*s_y
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the expected value of these two random variables is about the same:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(conditional_avg, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 70.49368&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(regression_prediction)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 70.50941&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The standard error for the regression prediction is substantially smaller:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(conditional_avg, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9635814&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(regression_prediction)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4520833&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use &lt;code&gt;na.rm=TRUE&lt;/code&gt;. The regression always uses all the data.&lt;/p&gt;
&lt;p&gt;So why not always use the regression for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The justification, which we include in the next section, is somewhat more advanced than the rest of this reading.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-normal-distribution-advanced&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bivariate normal distribution (advanced)&lt;/h3&gt;
&lt;p&gt;Correlation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.&lt;/p&gt;
&lt;p&gt;The main way we motivate the use of correlation involves what is called the &lt;em&gt;bivariate normal distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;When a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. These ovals can be thin (high correlation) or circle-shaped (no correlation).&lt;/p&gt;
&lt;!--
&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/bivariate-ovals-1.png&#34; width=&#34;672&#34; /&gt;
--&gt;
&lt;p&gt;A more technical way to define the bivariate normal distribution is the following: if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a normally distributed random variable, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is also a normally distributed random variable, and the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt; is approximately normal, then the pair is approximately bivariate normal.&lt;/p&gt;
&lt;p&gt;If we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;%
  mutate(z_father = round((father - mean(father)) / sd(father))) %&amp;gt;%
  filter(z_father %in% -2:2) %&amp;gt;%
  ggplot() +
  stat_qq(aes(sample = son)) +
  facet_wrap( ~ z_father)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/qqnorm-of-strata-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the expected value of the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; in pairs for which &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}(Y | X=x) = \mu_Y +  \rho \frac{X-\mu_X}{\sigma_X}\sigma_Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the regression line, with slope &lt;span class=&#34;math display&#34;&gt;\[\rho \frac{\sigma_Y}{\sigma_X}\]&lt;/span&gt; and intercept &lt;span class=&#34;math inline&#34;&gt;\(\mu_y - m\mu_X\)&lt;/span&gt;. It is equivalent to the regression equation we showed earlier which can be written like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\mbox{E}(Y \mid X=x)  - \mu_Y}{\sigma_Y} = \rho \frac{x-\mu_X}{\sigma_X}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.&lt;/p&gt;
&lt;p&gt;In summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given we know the value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, is given by the regression line.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-explained&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance explained&lt;/h3&gt;
&lt;p&gt;The bivariate normal theory also tells us that the standard deviation of the &lt;em&gt;conditional&lt;/em&gt; distribution described above is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SD}(Y \mid X=x ) = \sigma_Y \sqrt{1-\rho^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To see why this is intuitive, notice that without conditioning, &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SD}(Y) = \sigma_Y\)&lt;/span&gt;, we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.&lt;/p&gt;
&lt;p&gt;Specifically, it is reduced to &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{1-\rho^2} = \sqrt{1 - 0.25}\)&lt;/span&gt; = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.&lt;/p&gt;
&lt;p&gt;The statement “&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by &lt;span class=&#34;math inline&#34;&gt;\(1-\rho^2\)&lt;/span&gt;, so we say that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; explains &lt;span class=&#34;math inline&#34;&gt;\(1- (1-\rho^2)=\rho^2\)&lt;/span&gt; (the correlation squared) of the variance.&lt;/p&gt;
&lt;p&gt;But it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;warning-there-are-two-regression-lines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Warning: there are two regression lines&lt;/h3&gt;
&lt;p&gt;We computed a regression line to predict the son’s height from father’s height. We used these calculations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu_x &amp;lt;- mean(galton_heights$father)
mu_y &amp;lt;- mean(galton_heights$son)
s_x &amp;lt;- sd(galton_heights$father)
s_y &amp;lt;- sd(galton_heights$son)
r &amp;lt;- cor(galton_heights$father, galton_heights$son)
m_1 &amp;lt;-  r * s_y / s_x
b_1 &amp;lt;- mu_y - m_1*mu_x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives us the function &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}(Y\mid X=x) =\)&lt;/span&gt; 37.3 + 0.46 &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;What if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function:
&lt;span class=&#34;math inline&#34;&gt;\(x = \{ \mbox{E}(Y\mid X=x) -\)&lt;/span&gt; 37.3 &lt;span class=&#34;math inline&#34;&gt;\(\} /\)&lt;/span&gt; 0.5.&lt;/p&gt;
&lt;p&gt;We need to compute &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}(X \mid Y=y)\)&lt;/span&gt;. Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m_2 &amp;lt;-  r * s_x / s_y
b_2 &amp;lt;- mu_x - m_2 * mu_y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we get &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}(X \mid Y=y) =\)&lt;/span&gt; 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is to the son average.&lt;/p&gt;
&lt;p&gt;Here is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = b_1, slope = m_1, col = &amp;quot;blue&amp;quot;) +
  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/06-content_files/figure-html/two-regression-lines-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Load the &lt;code&gt;GaltonFamilies&lt;/code&gt; data from the &lt;strong&gt;HistData&lt;/strong&gt;. The children in each family are listed by gender and then by height. Create a dataset called &lt;code&gt;galton_heights&lt;/code&gt; by picking a male and female at random.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Francis_Galton&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Francis_Galton&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Effective Visualizations</title>
      <link>https://ssc442.netlify.app/content/03-content/</link>
      <pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/03-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#readings&#34;&gt;Readings&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ggplot2&#34;&gt;ggplot2&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-components-of-a-graph&#34;&gt;The components of a graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ggplot-objects&#34;&gt;&lt;code&gt;ggplot&lt;/code&gt; objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geometries-briefly&#34;&gt;Geometries (briefly)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aesthetic-mappings&#34;&gt;Aesthetic mappings&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#aesthetics-in-general&#34;&gt;Aesthetics in general&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#now-back-to-aesthetic-mappings&#34;&gt;Now, back to aesthetic mappings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#legends-for-aesthetics&#34;&gt;Legends for aesthetics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#annotation-layers&#34;&gt;Annotation Layers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#global-versus-local-aesthetic-mappings&#34;&gt;Global versus local aesthetic mappings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#try-it&#34;&gt;Try it!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#some-quirky-stuff&#34;&gt;Some Quirky Stuff&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scales-and-transformations&#34;&gt;Scales and transformations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#log-transformations&#34;&gt;Log transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transforming-data-vs.-transforming-using-scale_...&#34;&gt;Transforming data vs. transforming using &lt;code&gt;scale_...&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#axis-labels-legends-and-titles&#34;&gt;Axis labels, legends, and titles&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#changing-axis-titles&#34;&gt;Changing axis titles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#titles&#34;&gt;Titles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#axis-ticks&#34;&gt;Axis ticks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#additional-geometries&#34;&gt;Additional geometries&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#geom_line&#34;&gt;&lt;code&gt;geom_line&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-different-data-with-different-geometries&#34;&gt;Using different data with different geometries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-geometries&#34;&gt;Multiple geometries&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#try-it-1&#34;&gt;Try it!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Readings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;No guiding questions today; we’re mostly learning some technical aspects of &lt;code&gt;ggplot&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ggplot2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ggplot2&lt;/h1&gt;
&lt;p&gt;Exploratory data visualization is perhaps the greatest strength of &lt;code&gt;R&lt;/code&gt;. One can quickly go from idea to data to plot with a unique balance of flexibility and ease. For example, Excel may be easier than &lt;code&gt;R&lt;/code&gt; for some plots, but it is nowhere near as flexible. &lt;code&gt;D3.js&lt;/code&gt; may be more flexible and powerful than &lt;code&gt;R&lt;/code&gt;, but it takes much longer to generate a plot. One of the reasons we use &lt;code&gt;R&lt;/code&gt; is its incredible flexibility &lt;strong&gt;and&lt;/strong&gt; ease.&lt;/p&gt;
&lt;p&gt;Throughout this course, we will be creating plots using the &lt;strong&gt;ggplot2&lt;/strong&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many other approaches are available for creating plots in &lt;code&gt;R&lt;/code&gt;. In fact, the plotting capabilities that come with a basic installation of &lt;code&gt;R&lt;/code&gt; are already quite powerful. There are also other packages for creating graphics such as &lt;strong&gt;grid&lt;/strong&gt; and &lt;strong&gt;lattice&lt;/strong&gt;. We chose to use &lt;strong&gt;ggplot2&lt;/strong&gt; in this course because it breaks plots into components in a way that permits beginners to create relatively complex and aesthetically pleasing plots using syntax that is intuitive and comparatively easy to remember.&lt;/p&gt;
&lt;p&gt;One reason &lt;strong&gt;ggplot2&lt;/strong&gt; is generally more intuitive for beginners is that it uses a so-called “grammar of graphics”&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, the letters &lt;em&gt;gg&lt;/em&gt; in &lt;strong&gt;ggplot2&lt;/strong&gt;. This is analogous to the way learning grammar can help a beginner construct hundreds of different sentences by learning just a handful of verbs, nouns and adjectives without having to memorize each specific sentence. Similarly, by learning a handful of &lt;strong&gt;ggplot2&lt;/strong&gt; building blocks and its grammar, you will be able to create hundreds of different plots.&lt;/p&gt;
&lt;p&gt;Another reason &lt;strong&gt;ggplot2&lt;/strong&gt; is easy for beginners is that its default behavior is carefully chosen to satisfy the great majority of cases and is visually pleasing. As a result, it is possible to create informative and elegant graphs with relatively simple and readable code.&lt;/p&gt;
&lt;p&gt;One limitation is that &lt;strong&gt;ggplot2&lt;/strong&gt; is designed to work exclusively with data tables in tidy format (where rows are observations and columns are variables). However, a substantial percentage of datasets that beginners work with are in, or can be converted into, this format. An advantage of this approach is that, assuming that our data is tidy, &lt;strong&gt;ggplot2&lt;/strong&gt; simplifies plotting code and the learning of grammar for a variety of plots. You should review the previous content about tidy data if you are feeling lost.&lt;/p&gt;
&lt;p&gt;To use &lt;strong&gt;ggplot2&lt;/strong&gt; you will have to learn several functions and arguments. These are hard to memorize, so we highly recommend you have the ggplot2 cheat sheet handy. You can get a copy here: &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf&#34;&gt;https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf&lt;/a&gt; or simply perform an internet search for “ggplot2 cheat sheet”.&lt;/p&gt;
&lt;div id=&#34;the-components-of-a-graph&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The components of a graph&lt;/h2&gt;
&lt;p&gt;We will eventually construct a graph that summarizes the US murders dataset that looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.&lt;/p&gt;
&lt;p&gt;This data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.&lt;/p&gt;
&lt;p&gt;The first step in learning &lt;strong&gt;ggplot2&lt;/strong&gt; is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the &lt;strong&gt;ggplot2&lt;/strong&gt; terminology. The main five components to note are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Data&lt;/strong&gt;: The US murders data table is being summarized. We refer to this as the &lt;strong&gt;data&lt;/strong&gt; component.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Geometry&lt;/strong&gt;: The plot above is a scatterplot. This is referred to as the
&lt;strong&gt;geometry&lt;/strong&gt; component. Other possible geometries are barplot, histogram, smooth densities, qqplot, boxplot, pie (ew!), and many, many more. We will learn about these later.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Aesthetic mapping&lt;/strong&gt;: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we &lt;em&gt;map&lt;/em&gt; data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the &lt;strong&gt;aesthetic mapping&lt;/strong&gt; component. How we define the mapping depends on what &lt;strong&gt;geometry&lt;/strong&gt; we are using.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Annotations&lt;/strong&gt;: These are things like axis labels, axis ticks (the lines along the axis at regular intervals or specific points of interest), axis scales (e.g. log-scale), titles, legends, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Style&lt;/strong&gt;: An overall appearance of the graph determined by fonts, color palattes, layout, blank spaces, and more.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The points are labeled with the state abbreviations.&lt;/li&gt;
&lt;li&gt;The range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.&lt;/li&gt;
&lt;li&gt;There are labels, a title, a legend, and we use the style of The Economist magazine.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of the flexibility and visualization power of &lt;code&gt;ggplot&lt;/code&gt; is contained in these four elements (plus your data)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ggplot-objects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;ggplot&lt;/code&gt; objects&lt;/h2&gt;
&lt;p&gt;We will now construct the plot piece by piece.&lt;/p&gt;
&lt;p&gt;We start by loading the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first step in creating a &lt;strong&gt;ggplot2&lt;/strong&gt; graph is to define a &lt;code&gt;ggplot&lt;/code&gt; object. We do this with the function &lt;code&gt;ggplot&lt;/code&gt;, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also pipe the data in as the first argument. So this line of code is equivalent to the one above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It renders a plot. But in this case, it renders a blank slate. The object was created and, because it was not assigned to an object, it was automatically evaluated. Of course, we can assign our plot to an object, for example like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = murders)
class(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;gg&amp;quot;     &amp;quot;ggplot&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To render the plot associated with this object, we simply print the object &lt;code&gt;p&lt;/code&gt;. The following two lines of code each produce the same plot we see above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;geometries-briefly&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geometries (briefly)&lt;/h2&gt;
&lt;p&gt;The plot above is incredibly boring (it’s only a blank slate). It’s boring because we didn’t tell &lt;code&gt;ggplot2&lt;/code&gt; what to do. In general, if &lt;code&gt;R&lt;/code&gt; is doing something unexpected, it’s because you (the user) failed to tell it what you had in &lt;em&gt;mind&lt;/em&gt;. Here, we want to make an actual plot, but we didn’t tell &lt;code&gt;R&lt;/code&gt; what sort of plot to make. In order to tell it what sort of plot we want, we use “geometries”.&lt;/p&gt;
&lt;p&gt;Specifically, in &lt;code&gt;ggplot2&lt;/code&gt; we create graphs by adding geometry &lt;em&gt;layers&lt;/em&gt;. Layers can define geometries, compute summary statistics, define what scales to use, create annotations, or even change styles. To add layers, we use the symbol &lt;code&gt;+&lt;/code&gt;. In general, a line of code will look like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;DATA %&amp;gt;% ggplot() + LAYER 1 + LAYER 2 + ... + LAYER N&lt;/code&gt;
Usually, the first &lt;strong&gt;added&lt;/strong&gt; layer after &lt;code&gt;ggplot() +&lt;/code&gt; defines the geometry. After that, we may add additional geometries, we may rescale an axis, we may add annotations and labels, or we may change the style. For now, we want to make a scatterplot. What geometry do we use?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Taking a quick look at the cheat sheet, we see that the function used to create plots with this geometry is &lt;code&gt;geom_point&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/ggplot2-cheatsheeta.png&#34; /&gt;
&lt;img src=&#34;https://ssc442.netlify.app/example/02-example_files/ggplot2-cheatsheetb.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Image courtesy of RStudio&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. CC-BY-4.0 license&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;!--(Source: [RStudio](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf))--&gt;
&lt;p&gt;Geometry function names follow the pattern: &lt;code&gt;geom_X&lt;/code&gt; where X is the name of some specific geometry. Some examples include &lt;code&gt;geom_point&lt;/code&gt;, &lt;code&gt;geom_bar&lt;/code&gt;, and &lt;code&gt;geom_histogram&lt;/code&gt;. You’ve already seen a few of these. We will start with a scatterplot created using &lt;code&gt;geom_point()&lt;/code&gt; for now, then circle back to more geometries after we cover aesthetic mappings, layers, and annotations.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;geom_point&lt;/code&gt; to run properly we need to provide data and an &lt;strong&gt;aesthetic mapping&lt;/strong&gt;. The simplest mapping for a scatter plot is to say we want one variable on the X-axis, and a different one on the Y-axis, so each point is an {X,Y} pair. That is an &lt;strong&gt;aesthetic mapping&lt;/strong&gt; because X and Y are &lt;strong&gt;aesthetics&lt;/strong&gt; in a &lt;code&gt;geom_point&lt;/code&gt; scatterplot.&lt;/p&gt;
&lt;p&gt;We have already connected the object &lt;code&gt;p&lt;/code&gt; with the &lt;code&gt;murders&lt;/code&gt; data table, and if we add the layer &lt;code&gt;geom_point&lt;/code&gt; it defaults to using this data. To find out what mappings are expected, we read the &lt;strong&gt;Aesthetics&lt;/strong&gt; section of the help file &lt;code&gt;?geom_point&lt;/code&gt; help file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; Aesthetics
&amp;gt;
&amp;gt; geom_point understands the following aesthetics (required aesthetics are in bold):
&amp;gt;
&amp;gt; **x**
&amp;gt;
&amp;gt; **y**
&amp;gt;
&amp;gt; alpha
&amp;gt;
&amp;gt; colour
&amp;gt;
&amp;gt; fill
&amp;gt;
&amp;gt; group
&amp;gt;
&amp;gt; shape
&amp;gt;
&amp;gt; size
&amp;gt;
&amp;gt; stroke&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and—although it does not show in bold above—we see that at least two arguments are required: &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. You can’t have a &lt;code&gt;geom_point&lt;/code&gt; scatterplot unless you state what you want on the X and Y axes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aesthetic-mappings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Aesthetic mappings&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Aesthetic mappings&lt;/strong&gt; describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The &lt;code&gt;aes&lt;/code&gt; function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the &lt;code&gt;aes&lt;/code&gt; function is often used as the argument of a geometry function. This example produces a scatterplot of population in millions (x-axis) versus total murders (y-axis):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot() +
  geom_point(aes(x = population/10^6, y = total))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of defining our plot from scratch, we can also add a layer to the &lt;code&gt;p&lt;/code&gt; object that was defined above as &lt;code&gt;p &amp;lt;- ggplot(data = murders)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The scales and annotations like axis labels are defined by default when adding this layer (note the x-axis label is exactly what we wrote in the function call). Like &lt;strong&gt;dplyr&lt;/strong&gt; functions, &lt;code&gt;aes&lt;/code&gt; also uses the variable names from the object component: we can use &lt;code&gt;population&lt;/code&gt; and &lt;code&gt;total&lt;/code&gt; without having to call them as &lt;code&gt;murders$population&lt;/code&gt; and &lt;code&gt;murders$total&lt;/code&gt;. The behavior of recognizing the variables from the data component is quite specific to &lt;code&gt;aes&lt;/code&gt;. With most functions, if you try to access the values of &lt;code&gt;population&lt;/code&gt; or &lt;code&gt;total&lt;/code&gt; outside of &lt;code&gt;aes&lt;/code&gt; you receive an error.&lt;/p&gt;
&lt;p&gt;Note that we did some rescaling within the &lt;code&gt;aes()&lt;/code&gt; call - we can do simple things like multiplication or division on the variable names in the &lt;code&gt;ggplot&lt;/code&gt; call. The axis labels reflect this. We will change the axis labels later.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;aesthetic mappings&lt;/strong&gt; are very powerful - changing the variable in &lt;code&gt;x=&lt;/code&gt; or &lt;code&gt;y=&lt;/code&gt; changes the meaning of the plot entirely. We’ll come back to additional &lt;strong&gt;aesthetic mappings&lt;/strong&gt; once we talk about aesthetics in general.&lt;/p&gt;
&lt;div id=&#34;aesthetics-in-general&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Aesthetics in general&lt;/h3&gt;
&lt;p&gt;Even without mappings, a plots aesthetics can be useful. Things like color, fill, alpha, and size are aesthetics that can be changed.&lt;/p&gt;
&lt;p&gt;Let’s say we want larger points in our scatterplot. The &lt;code&gt;size&lt;/code&gt; aesthetic can be used to set the size. The scale of &lt;code&gt;size&lt;/code&gt; is “multiples of the defaults” (so &lt;code&gt;size = 1&lt;/code&gt; is the default)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total), size = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;size&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; a mapping so it is &lt;strong&gt;not&lt;/strong&gt; in the &lt;code&gt;aes()&lt;/code&gt; part: whereas mappings use data from specific observations and need to be inside &lt;code&gt;aes()&lt;/code&gt;, operations we want to affect all the points the same way do not need to be included inside &lt;code&gt;aes&lt;/code&gt;. We’ll see what happens if &lt;code&gt;size&lt;/code&gt; is inside &lt;code&gt;aes(size = xxx)&lt;/code&gt; in a second.&lt;/p&gt;
&lt;p&gt;We can change the &lt;code&gt;shape&lt;/code&gt; to one of the many different base-R options found &lt;a href=&#34;http://www.sthda.com/english/wiki/r-plot-pch-symbols-the-different-point-shapes-available-in-r&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total), size = 3, shape = 17)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-5shape-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also change the &lt;code&gt;fill&lt;/code&gt; and the &lt;code&gt;color&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total), size = 4, shape = 23, fill = &amp;#39;#18453B&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-5b-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;fill&lt;/code&gt; can take a common name like &lt;code&gt;&#39;green&#39;&lt;/code&gt;, or can take a hex color like &lt;code&gt;&#39;#18453B&#39;&lt;/code&gt;, which is &lt;a href=&#34;https://brand.msu.edu/design-visual/index.html#color&#34;&gt;MSU Green according to MSU’s branding site&lt;/a&gt;. You can also find &lt;a href=&#34;https://youtu.be/0BxNHwJi1y4&#34;&gt;UM Maize&lt;/a&gt; and &lt;a href=&#34;https://youtu.be/dQw4w9WgXcQ&#34;&gt;OSU Scarlet&lt;/a&gt; on respective branding pages, or google “XXX color hex.” We’ll learn how to build a color palatte later on.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;color&lt;/code&gt; (or &lt;code&gt;colour&lt;/code&gt;, same thing because &lt;code&gt;ggplot&lt;/code&gt; creators allow both spellings) is a little tricky with points - it changes the outline of the geometry rather than the fill color, but in &lt;code&gt;geom_point()&lt;/code&gt; most shapes are only the outline, including the default. This is more useful with, say, a barplot where the outline and the fill might be different colors. Still, shapes 21-25 have both &lt;code&gt;fill&lt;/code&gt; and &lt;code&gt;color&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total), size = 5, shape = 23, fill = &amp;#39;#18453B&amp;#39;, color = &amp;#39;white&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-5c-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;color = &#39;white&#39;&lt;/code&gt; makes the outline of the shape white, which you can see if you look closely in the areas where the shapes overlap. This only works with shapes 21-25, or any other geometry that has both an outline and a fill.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;now-back-to-aesthetic-mappings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Now, back to aesthetic mappings&lt;/h3&gt;
&lt;p&gt;Now that we’ve seen a few aesthetics (and know we can find more by looking at which aesthetics work with our geometry in the help file), let’s return to the power of aesthetic mappings.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;aesthetic mapping&lt;/strong&gt; means we can vary an aesthetic (like fill or shape or size) according to some &lt;strong&gt;variable in our data&lt;/strong&gt;. This opens up a world of possibilities! Let’s try adding to our &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; aesthetics with a &lt;code&gt;color&lt;/code&gt; aesthetic (since points respond to &lt;code&gt;color&lt;/code&gt; better than &lt;code&gt;fill&lt;/code&gt;) that varies by &lt;code&gt;region&lt;/code&gt;, which is a column in our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total, color = region), size = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-color-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We include &lt;code&gt;color=region&lt;/code&gt; &lt;strong&gt;inside&lt;/strong&gt; the &lt;code&gt;aes&lt;/code&gt; call, which tells R to find a variable called &lt;code&gt;region&lt;/code&gt; and change color based on that. R will choose a somewhat ghastly color palatte, and &lt;strong&gt;every&lt;/strong&gt; unique value in the data for &lt;code&gt;region&lt;/code&gt; will get a different color if the variable is discrete. If the variable is a continuous value, then &lt;code&gt;ggplot&lt;/code&gt; will automatically make a color ramp. Thus, &lt;strong&gt;discrete&lt;/strong&gt; and &lt;strong&gt;continuous&lt;/strong&gt; values for aesthetic mappings work differently.&lt;/p&gt;
&lt;p&gt;Let’s see a useful example of a continuous aesthetic mapping to &lt;code&gt;color&lt;/code&gt;. In our data, we are making a scatterplot of population and total murders, which really just shows that states with higher populations have higher murders. What we really want is murders per capita (I think COVID taught us a lot about rates vs. levels like “cases” and “cases per 100,000 people”). We can create a variable of “murders per capita” on the fly. Since “murders per capita” is a very small number and hard to read, we’ll multiply by 100 so that we get “percent of population murdered per year”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^5, y = total, color = 100*total/population), size = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-colfill-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While the clear pattern of “more population means more murders” is still there, look at the outlier in light blue in the bottom left. With the color ramp, see how easy it is to see here that there is one location where murders per capita is quite high?&lt;/p&gt;
&lt;p&gt;Note that &lt;code&gt;size&lt;/code&gt; is outside of &lt;code&gt;aes&lt;/code&gt; and is set to an explicit value, not to a variable. What if we set size to a variable in the data?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-color2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;legends-for-aesthetics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Legends for aesthetics&lt;/h3&gt;
&lt;p&gt;Here we see yet another useful default behavior: &lt;strong&gt;ggplot2&lt;/strong&gt; automatically adds a legend that maps color to region, and size to population (which we scaled by 1,000,000). To avoid adding this legend we set the &lt;code&gt;geom_point&lt;/code&gt; argument &lt;code&gt;show.legend = FALSE&lt;/code&gt;. This removes both the &lt;code&gt;size&lt;/code&gt; and the &lt;code&gt;color&lt;/code&gt; legend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total, color = region, size = population/10^6), show.legend = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-color3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Later on, when we get to &lt;strong&gt;annotation layers&lt;/strong&gt;, we’ll talk about controlling the legend text and layout. For now, we just need to know how to turn them off.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;annotation-layers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Annotation Layers&lt;/h2&gt;
&lt;p&gt;A second layer in the plot we wish to make involves adding a label to each point to identify the state. The &lt;code&gt;geom_label&lt;/code&gt; and &lt;code&gt;geom_text&lt;/code&gt; functions permit us to add text to the plot with and without a rectangle behind the text, respectively.&lt;/p&gt;
&lt;p&gt;Because each point (each state in this case) has a label, we need an &lt;strong&gt;aesthetic mapping&lt;/strong&gt; to make the connection between points and labels. By reading the help file &lt;code&gt;?geom_text&lt;/code&gt;, we learn that we supply the mapping between point and label through the &lt;code&gt;label&lt;/code&gt; argument of &lt;code&gt;aes&lt;/code&gt;. That is, &lt;code&gt;label&lt;/code&gt; is an &lt;strong&gt;aesthetic&lt;/strong&gt; that we can map. So the code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) +
  geom_text(aes(x = population/10^6, y = total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have successfully added a second layer to the plot.&lt;/p&gt;
&lt;p&gt;As an example of the unique behavior of &lt;code&gt;aes&lt;/code&gt; mentioned above, note that this call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) +
  geom_text(aes(population/10^6, total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is fine, whereas this call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) +
  geom_text(aes(population/10^6, total), label = abb)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will give you an error since &lt;code&gt;abb&lt;/code&gt; is not found because it is outside of the &lt;code&gt;aes&lt;/code&gt; function. The layer &lt;code&gt;geom_text&lt;/code&gt; does not know where to find &lt;code&gt;abb&lt;/code&gt; since it is a column name and not a global variable, and &lt;code&gt;ggplot&lt;/code&gt; does not look for column names for non-mapped aesthetics. For a trivial example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) +
  geom_text(aes(population/10^6, total), label = &amp;#39;abb&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;global-versus-local-aesthetic-mappings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Global versus local aesthetic mappings&lt;/h3&gt;
&lt;p&gt;In the previous line of code, we define the mapping &lt;code&gt;aes(population/10^6, total)&lt;/code&gt; twice, once in each geometry. We can avoid this by using a &lt;em&gt;global&lt;/em&gt; aesthetic mapping. We can do this when we define the blank slate &lt;code&gt;ggplot&lt;/code&gt; object. Remember that the function &lt;code&gt;ggplot&lt;/code&gt; contains an argument that permits us to define aesthetic mappings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;args(ggplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (data = NULL, mapping = aes(), ..., environment = parent.frame()) 
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we define a mapping in &lt;code&gt;ggplot&lt;/code&gt;, all the geometries that are added as layers will default to this mapping. We redefine &lt;code&gt;p&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- murders %&amp;gt;% ggplot(aes(x = population/10^6, y = total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then we can simply write the following code to produce the previous plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(nudge_x = 1.5) # offsets the label&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We keep the &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;nudge_x&lt;/code&gt; arguments in &lt;code&gt;geom_point&lt;/code&gt; and &lt;code&gt;geom_text&lt;/code&gt;, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in &lt;code&gt;aes&lt;/code&gt; then they would apply to both plots. Also note that the &lt;code&gt;geom_point&lt;/code&gt; function does not need a &lt;code&gt;label&lt;/code&gt; argument and therefore ignores that aesthetic.&lt;/p&gt;
&lt;p&gt;If necessary, we can override the global mapping by defining a new mapping within each layer. These &lt;em&gt;local&lt;/em&gt; definitions override the &lt;em&gt;global&lt;/em&gt;. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(aes(x = 10, y = 800, label = &amp;quot;Hello there!&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, the second call to &lt;code&gt;geom_text&lt;/code&gt; does not use &lt;code&gt;x = population&lt;/code&gt; and &lt;code&gt;y = total&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;try-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Try it!&lt;/h2&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Let’s break in to smaller groups and try playing with some of the aesthetics and aesthetic mappings. If we’re online, we’ll use Zoom Breakout Rooms. Each of the rooms have a room number which will correspond with one of the tasks below. If we’re in person (woohoo!), we’ll form the same number of groups in class.&lt;/p&gt;
&lt;p&gt;In each group, one person should be the main coder - someone who has the packages like &lt;code&gt;dslabs&lt;/code&gt; installed and has successfully run the plots above. Each set of tasks ask you to learn about an aesthetic and put it into action with the &lt;code&gt;murder&lt;/code&gt; data. We’ll leave about 5 minutes to do the task, then have you come back and share your results with the class.&lt;/p&gt;
&lt;p&gt;For each group, we’ll start with the following code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p + geom_point(aes(x = population/10^6, y = total)) +
  geom_text(aes(x = population/10^6, y = total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;code&gt;alpha&lt;/code&gt; aesthetic mapping.
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The &lt;code&gt;alpha&lt;/code&gt; aesthetic can only take a number between 0 and 1. So first, in &lt;code&gt;murders&lt;/code&gt;, create a &lt;code&gt;murders_per_capita&lt;/code&gt; column by dividing &lt;code&gt;total&lt;/code&gt; by &lt;code&gt;population&lt;/code&gt;. Second, find the &lt;code&gt;max(murders$murders_per_capita)&lt;/code&gt; and then create another new column called &lt;code&gt;murders_per_capita_rescaled&lt;/code&gt; which divides &lt;code&gt;murders_per_capita&lt;/code&gt; by the max value. &lt;code&gt;murders_per_capita_rescaled&lt;/code&gt; will be between 0 and 1, with the value of 1 for the state with the max murder rate. This is a little hard to do on the fly in &lt;code&gt;ggplot&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;alpha&lt;/code&gt; aesthetic mapping to &lt;code&gt;murders_per_capita_rescaled&lt;/code&gt; for &lt;code&gt;geom_point&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Turn off the legend using &lt;code&gt;show.legend=FALSE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Include the &lt;code&gt;geom_text&lt;/code&gt; labels, but make sure the aesthetic mapping does &lt;strong&gt;not&lt;/strong&gt; apply to the labels.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;nudge_x = 1.5&lt;/code&gt; as before to offset the labels.&lt;/li&gt;
&lt;li&gt;Be able to explain the plot.
&lt;ul&gt;
&lt;li&gt;Does the &lt;code&gt;alpha&lt;/code&gt; aesthetic help present the data here? It’s OK if it doesn’t!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;stroke&lt;/code&gt; aesthetic mapping.
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The &lt;code&gt;stroke&lt;/code&gt; aesthetic works a bit like the &lt;code&gt;size&lt;/code&gt; aesthetic. It must be used with a plot that has both a border and a fill, like shapes 21-25, so use one of those.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;stroke&lt;/code&gt; aesthetic mapping (meaning the stroke will change according to a value in the data) to set a different stroke size based on murders &lt;em&gt;per capita&lt;/em&gt;. You can create a murders per capita variable on the fly, or add it to your &lt;code&gt;murders&lt;/code&gt; data.
&lt;ul&gt;
&lt;li&gt;Include the text labels as before and use &lt;code&gt;nudge_x = 1.5&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Make sure you’re only setting the aesthetic for the points on the scatterplot!&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;angle&lt;/code&gt; aesthetic
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Using the &lt;code&gt;?geom_text&lt;/code&gt; help, note that &lt;code&gt;geom_text&lt;/code&gt; takes an aesthetic of &lt;code&gt;angle&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;angle&lt;/code&gt; aesthetic (not aesthetic mapping) in the appropriate place (e.g. on &lt;code&gt;geom_text&lt;/code&gt; and not on other geometries) to adjust the labels on our plot.&lt;/li&gt;
&lt;li&gt;Now, try using the &lt;code&gt;angle&lt;/code&gt; aesthetic mapping by using the &lt;code&gt;total&lt;/code&gt; field as both the &lt;code&gt;y&lt;/code&gt; value &lt;strong&gt;and&lt;/strong&gt; the &lt;code&gt;angle&lt;/code&gt; value in the &lt;code&gt;geom_text&lt;/code&gt; layer.&lt;/li&gt;
&lt;li&gt;Does using &lt;code&gt;angle&lt;/code&gt; as an aesthetic help? What about as an aesthetic mapping?&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;color&lt;/code&gt; aesthetic mapping
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Set the &lt;code&gt;color&lt;/code&gt; aesthetic mapping in &lt;code&gt;geom_text&lt;/code&gt; to &lt;code&gt;total/population&lt;/code&gt;.
&lt;ul&gt;
&lt;li&gt;Use the &lt;code&gt;nudge_x = 1.5&lt;/code&gt; aesthetic in &lt;code&gt;geom_text&lt;/code&gt; still&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Try it with and without the legend using &lt;code&gt;show.legend&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Be able to explain the plot.
&lt;ul&gt;
&lt;li&gt;Does the &lt;code&gt;color&lt;/code&gt; aesthetic mapping help present the data here?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geom_label&lt;/code&gt; and the &lt;code&gt;fill&lt;/code&gt; aesthetic
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Looking at &lt;code&gt;?geom_label&lt;/code&gt; (which is the same help as &lt;code&gt;geom_text&lt;/code&gt;), we note that “The &lt;code&gt;fill&lt;/code&gt; aesthetic controls the backgreound colour of the label”.&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;fill&lt;/code&gt; aesthetic mapping to &lt;code&gt;total/population&lt;/code&gt; in &lt;code&gt;geom_label&lt;/code&gt; (replacing &lt;code&gt;geom_text&lt;/code&gt; but still using &lt;code&gt;nudge_x=1.5&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;fill&lt;/code&gt; aesthetic (not mapping) to the color of your choice.&lt;/li&gt;
&lt;li&gt;Be able to explain the plots.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Does the &lt;code&gt;fill&lt;/code&gt; aesthetic mapping help present the data here?&lt;/li&gt;
&lt;li&gt;What color did you choose for the non-mapped fill aesthetic?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-quirky-stuff&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some Quirky Stuff&lt;/h2&gt;
&lt;p&gt;Load up our &lt;code&gt;murders&lt;/code&gt; data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
library(ggplot2)
library(dplyr)
data(murders)
p &amp;lt;- ggplot(data = murders, aes(x = population, y = total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;scales-and-transformations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scales and transformations&lt;/h2&gt;
&lt;div id=&#34;log-transformations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Log transformations&lt;/h3&gt;
&lt;p&gt;Last lecture, we re-scaled our population by 10^6 (millions), but still had a lot of variation because some states are tiny and some are huge. Sometimes, we want to have one (or both) of our axes scaled non-linearly. For instance, if we wanted to have our x-axis be in log base 10, then each major tick would represent a factor of 10 over the last. This is not the default, so this change needs to be added through a &lt;em&gt;scales&lt;/em&gt; layer. A quick look at the cheat sheet reveals the &lt;code&gt;scale_x_continuous&lt;/code&gt; function lets us control the behavior of scales. We use them like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(nudge_x = 0.05) +
  scale_x_continuous(trans = &amp;quot;log10&amp;quot;) +
  scale_y_continuous(trans = &amp;quot;log10&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/ggplot-example-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A couple of things here: adding things like &lt;code&gt;scale_x_continuous(...)&lt;/code&gt; operates on the whole plot. In some cases, order matters, but it doesn’t here, so we can throw &lt;code&gt;scale_x_continuous&lt;/code&gt; anywhere. Because we have altered the whole plot’s scale to be in the log-scale now, the &lt;em&gt;nudge&lt;/em&gt; must be made smaller. It is in log-base-10 units. Using &lt;code&gt;?scale_x_continuous&lt;/code&gt; brings us to the help for both &lt;code&gt;scale_x_continuous&lt;/code&gt; and &lt;code&gt;scale_y_continuous&lt;/code&gt;, which shows us the options for transformations &lt;code&gt;trans = ...&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This particular transformation is so common that &lt;strong&gt;ggplot2&lt;/strong&gt; provides the specialized functions &lt;code&gt;scale_x_log10&lt;/code&gt; and &lt;code&gt;scale_y_log10&lt;/code&gt; which “inherit” (take the place of) the &lt;code&gt;scale_x_continuous&lt;/code&gt; functions but have log base 10 as default. We can use these to rewrite the code like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(nudge_x = 0.05) +
  scale_x_log10() +
  scale_y_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This can make a plot much easier to read, though one has to be sure to pay attention to the values on the axes. Plotting anything with very large outliers will almost always be better if done in log-scale. Adding the scale layer is an easy way to fix this.&lt;/p&gt;
&lt;p&gt;We can also use one of many built-in transformations. Of note: &lt;strong&gt;reverse&lt;/strong&gt; just inverts the scale, which can be helpful, &lt;strong&gt;log&lt;/strong&gt; uses the natural log, &lt;strong&gt;sqrt&lt;/strong&gt; takes the square root (dropping anything with a negative value), &lt;strong&gt;reciprocal&lt;/strong&gt; takes 1/x. If your x-axis is in a date format, you can also scale to &lt;strong&gt;hms&lt;/strong&gt; (hour-minute-second) or &lt;strong&gt;date&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transforming-data-vs.-transforming-using-scale_...&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Transforming data vs. transforming using &lt;code&gt;scale_...&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;We could simply take the log of population and log of total in the call and we’d get something very similar. Note that we had to override the aesthetic mapping set in &lt;code&gt;p&lt;/code&gt; in each of the geometries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(x = log(population, base=10), y = log(total, base=10)), size = 3) +
  geom_text(aes(x = log(population, base=10), y = log(total, base=10)), nudge_x = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This avoids using &lt;code&gt;scale_x_continuous&lt;/code&gt; or it’s child function &lt;code&gt;scale_x_log10&lt;/code&gt;. One advantage to using &lt;code&gt;scale_x...&lt;/code&gt; is that the axes are correctly labeled. When we transform the data directly, the axis labels only show the transformed values, so 7,000,000 becomes 7.0. This could be confusing! We could update the axis labels to say “total murders (log base 10)” and “total population (log base 10)”, but that’s cumbersome. Using &lt;code&gt;scale_x...&lt;/code&gt; is a lot more refined and easy.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;axis-labels-legends-and-titles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Axis labels, legends, and titles&lt;/h2&gt;
&lt;p&gt;But let’s say we did want to re-name our x-axis label. Or maybe we don’t like that the variable column name is lower-case “p”.&lt;/p&gt;
&lt;p&gt;As with many things in &lt;code&gt;ggplot&lt;/code&gt;, there are many ways to get the same result. We’ll go over one way of changing titles and labels, but know that there are many more.&lt;/p&gt;
&lt;div id=&#34;changing-axis-titles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Changing axis titles&lt;/h3&gt;
&lt;p&gt;We’ll use the &lt;code&gt;labs(...)&lt;/code&gt; annotation layer to do this, which is pretty straightforward. &lt;code&gt;?labs&lt;/code&gt; shows us what we can change, and while it looks pretty basic, the real meat is in the &lt;code&gt;...&lt;/code&gt; argument, which the help says is “A list of new name-value pairs”. This means we can re-define the label on anything that is an aesthetic mapping. X and Y are aesthetic mappings, so…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(nudge_x = 0.05) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = &amp;#39;Population&amp;#39;, y = &amp;#39;Total murders&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s use an aesthetic mapping that generates a legend, like &lt;code&gt;color&lt;/code&gt;, and see what &lt;code&gt;labs&lt;/code&gt; renames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(color = region), size = 3) +
  geom_text(nudge_x = 0.05) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = &amp;#39;Population&amp;#39;, y = &amp;#39;Total murders&amp;#39;, color = &amp;#39;US Region&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;
We can rename the aesthetic mapping-relevant label using &lt;code&gt;labs&lt;/code&gt;. Even if there are multiple mapped aesthetics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(color = region, size = total/population)) +
  geom_text(nudge_x = 0.05) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = &amp;#39;Population&amp;#39;, y = &amp;#39;Total murders&amp;#39;, color = &amp;#39;US Region&amp;#39;, size = &amp;#39;Murder rate&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;titles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Titles&lt;/h3&gt;
&lt;p&gt;In &lt;code&gt;?labs&lt;/code&gt;, we also see some things that look like titles and captions. We can include those:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(color = region, size = total/population)) +
  geom_text(nudge_x = 0.05) +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = &amp;#39;Population&amp;#39;, y = &amp;#39;Total murders&amp;#39;, color = &amp;#39;US Region&amp;#39;, size = &amp;#39;Murder rate&amp;#39;,
       title = &amp;#39;This is a title&amp;#39;, subtitle = &amp;#39;This is a subtitle&amp;#39;, caption = &amp;#39;This is a caption&amp;#39;, tag = &amp;#39;This is a tag&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that you know how, &lt;strong&gt;always label your plots with at least a title and have meaningful axis and legend labels&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;axis-ticks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Axis ticks&lt;/h2&gt;
&lt;p&gt;In addition to the axis labels, we may want to format or change the axis tick labels (like “1e+06” above) or even where the tick marks and lines are drawn. If we don’t specify anything, the axis labels and tick marks are drawn as best as &lt;code&gt;ggplot&lt;/code&gt; can do, but we can change this. This might be especially useful if our data has some meaningful cutoffs that aren’t found by the default, or we just don’t like where the marks fall or how they are labeled. This is easy to fix with &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To change the tick mark labels, we have to set the tick mark locations. Then we can set a label for each tick mark. Let’s go back to our &lt;code&gt;murders&lt;/code&gt; data and, for simplicity, take the log transformation off the Y axis. We’ll use &lt;code&gt;scale_y_continuous&lt;/code&gt; to tell R &lt;em&gt;where&lt;/em&gt; to put the breaks (&lt;code&gt;breaks =&lt;/code&gt;) and &lt;em&gt;what to label&lt;/em&gt; the breaks. We have to give it one label for every break. Let’s say we just want a line at the 500’s and let’s say we want to (absurdly) use written numerics for each of the Y-axis lines. Since &lt;code&gt;scale_y_log10&lt;/code&gt; &lt;strong&gt;inherits from&lt;/strong&gt; &lt;code&gt;scale_y_continuous&lt;/code&gt;, we can just use that and add the breaks and labels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(color = region), size = 3) +
  geom_text(nudge_x = .05) +
  scale_x_log10() +
  scale_y_log10(breaks = c(0,50, 100, 500,1000,1500),
                     labels = c(&amp;#39;Zero&amp;#39;,&amp;#39;Fifty&amp;#39;,&amp;#39;One hundred&amp;#39;,&amp;#39;Five hundred&amp;#39;,&amp;#39;One thousand&amp;#39;,&amp;#39;Fifteen hundred&amp;#39;)) +
  labs(x = &amp;#39;Population&amp;#39;, y = &amp;#39;Total murders&amp;#39;, color = &amp;#39;US Region&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;
We have manually set both the location and the label for the y-axis. Note that R filled in the in-between “minor” tick lines, but we can take those out. Since we are setting the location of the lines, we can do anything we want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(color = region), size = 3) +
  geom_text(nudge_x = .05) +
  scale_x_log10() +
  scale_y_log10(breaks = c(0,50, 100, 721, 1000,1500),
                     labels = c(&amp;#39;Zero&amp;#39;,&amp;#39;Fifty&amp;#39;,&amp;#39;One hundred&amp;#39;,&amp;#39;Seven hundred twenty one&amp;#39;,&amp;#39;One thousand&amp;#39;,&amp;#39;Fifteen hundred&amp;#39;),
                     minor_breaks = NULL) +
  labs(x = &amp;#39;Population&amp;#39;, y = &amp;#39;Total murders&amp;#39;, color = &amp;#39;US Region&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we can now define where axis tick lines should lie and how they should be labeled.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;additional-geometries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Additional geometries&lt;/h2&gt;
&lt;p&gt;Let’s say we are happy with our axis tick locations, but we want to add a single additional line. Maybe we want to divide at 1,000,000 population (a vertical line at 1,000,000) becuase we think those over 1,000,000 are somehow different, and we want to call attention to the data around that point. As a more general example, if we were to plot, say, car accidents by age, we would maybe want to label age 21, when people can legally purchase alcohol (and subsequently cause car accidents).&lt;/p&gt;
&lt;p&gt;This brings us to our first additional geometry beyond &lt;code&gt;geom_point&lt;/code&gt; (OK, we used &lt;code&gt;geom_text&lt;/code&gt;, but that’s more of an annotation). &lt;code&gt;geom_vline&lt;/code&gt; lets us add a single vertical line (without aesthetic mappings). If we look at &lt;code&gt;?geom_vline&lt;/code&gt; we see that it requires ones aesthetic:&lt;code&gt;xintercept&lt;/code&gt;. It also takes aesthetics like color and size, and introduces the &lt;code&gt;linetype&lt;/code&gt; aesthetic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(color = region), size = 3) +
  geom_text(nudge_x = .05) +
  geom_vline(aes(xintercept = 1000000), col = &amp;#39;red&amp;#39;, size = 2, linetype = 2) +
  scale_x_log10() +
  scale_y_log10(breaks = c(0,50, 100, 721, 1000,1500),
                     labels = c(&amp;#39;Zero&amp;#39;,&amp;#39;Fifty&amp;#39;,&amp;#39;One hundred&amp;#39;,&amp;#39;Seven hundred twenty one&amp;#39;,&amp;#39;One thousand&amp;#39;,&amp;#39;Fifteen hundred&amp;#39;),
                     minor_breaks = NULL) +
  labs(x = &amp;#39;Population&amp;#39;, y = &amp;#39;Total murders&amp;#39;, color = &amp;#39;US Region&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Combining geometries is as easy as adding the layers with &lt;code&gt;+&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;geom_line&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;geom_line&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;For a good old line plot, we use the line geometry at &lt;code&gt;geom_line&lt;/code&gt;. The help for &lt;code&gt;?geom_line&lt;/code&gt; tells us that we need an x and a y aesthetic (much like &lt;code&gt;geom_points&lt;/code&gt;). Since our &lt;code&gt;murders&lt;/code&gt; data isn’t really suited to a line graph, we’ll use a daily stock price. We’ll get this using &lt;code&gt;tidyquant&lt;/code&gt;, which pulls stock prices from Yahoo Finance and maintains the “tidy” format. You’ll need to &lt;code&gt;install.packages(&#39;tidyquant&#39;)&lt;/code&gt; before you run this the first time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyquant)
AAPL = tq_get(&amp;quot;AAPL&amp;quot;, from = &amp;#39;2009-01-01&amp;#39;, to = &amp;#39;2021-08-01&amp;#39;, get = &amp;#39;stock.prices&amp;#39;)
head(AAPL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 8
##   symbol date        open  high   low close     volume adjusted
##   &amp;lt;chr&amp;gt;  &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 AAPL   2009-01-02  3.07  3.25  3.04  3.24  746015200     2.78
## 2 AAPL   2009-01-05  3.33  3.44  3.31  3.38 1181608400     2.90
## 3 AAPL   2009-01-06  3.43  3.47  3.30  3.32 1289310400     2.85
## 4 AAPL   2009-01-07  3.28  3.30  3.22  3.25  753048800     2.79
## 5 AAPL   2009-01-08  3.23  3.33  3.22  3.31  673500800     2.84
## 6 AAPL   2009-01-09  3.33  3.34  3.22  3.24  546845600     2.77&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can plot a line graph of the Apple closing stock price over the requested date range. We want this to be a time series, so the x-axis will be the date and the y-axis will be the closing price.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(AAPL, aes(x = date, y = close)) +
  geom_line() +
  labs(x = &amp;#39;Date&amp;#39;, y = &amp;#39;Closing price&amp;#39;, title = &amp;#39;Apple stock price&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/geomPath-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;geom_line&lt;/code&gt;, R will automatically sort on the x-variable. If you don’t want this, then &lt;code&gt;geom_path&lt;/code&gt; will use whatever order the data is in. Either way, if you have multiple observations for the same value on the x-axis, then you’ll get something pretty messy because R will try to connect, in some order, all the points. Let’s see an example with two stocks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AAPLNFLX = tq_get(c(&amp;quot;AAPL&amp;quot;,&amp;quot;NFLX&amp;quot;), from = &amp;#39;2021-01-01&amp;#39;, to = &amp;#39;2021-08-01&amp;#39;, get = &amp;#39;stock.prices&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `type_convert()` only converts columns of type &amp;#39;character&amp;#39;.
## - `df` has no columns of type &amp;#39;character&amp;#39;

## Warning: `type_convert()` only converts columns of type &amp;#39;character&amp;#39;.
## - `df` has no columns of type &amp;#39;character&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(AAPLNFLX, aes(x = date, y = close)) +
  geom_line() +
  labs(x = &amp;#39;Date&amp;#39;, y = &amp;#39;Closing price&amp;#39;, title = &amp;#39;Apple and Netflix stock price&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That looks kinda strange. That’s because, for every date, we have two values - the NFLX and the AAPL value, so each day has a vertical line drawn between the two prices. This is nonsense, especially since what we want to see is the history of NFLX and AAPL over time.&lt;/p&gt;
&lt;p&gt;Aesthetics to the rescue! Remember, when we use an aesthetic mapping, we are able to separate out data by things like color or linetype. Let’s use color as the aesthetic here, and map it to the stock ticker:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AAPLNFLX = tq_get(c(&amp;quot;AAPL&amp;quot;,&amp;quot;NFLX&amp;quot;), from = &amp;#39;2021-01-01&amp;#39;, to = &amp;#39;2021-08-01&amp;#39;, get = &amp;#39;stock.prices&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `type_convert()` only converts columns of type &amp;#39;character&amp;#39;.
## - `df` has no columns of type &amp;#39;character&amp;#39;

## Warning: `type_convert()` only converts columns of type &amp;#39;character&amp;#39;.
## - `df` has no columns of type &amp;#39;character&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(AAPLNFLX, aes(x = date, y = close, color = symbol)) +
  geom_line() +
  labs(x = &amp;#39;Date&amp;#39;, y = &amp;#39;Closing price&amp;#39;, title = &amp;#39;Apple and Netflix stock price&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;
Well there we go! We can now see each stock price over time, with a convenient legend. Later on, we’ll learn how to change the color palatte. If we don’t necessarily want a different color but we do want to separate the lines, we can use the &lt;code&gt;group&lt;/code&gt; aesthetic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AAPLNFLX = tq_get(c(&amp;quot;AAPL&amp;quot;,&amp;quot;NFLX&amp;quot;), from = &amp;#39;2021-01-01&amp;#39;, to = &amp;#39;2021-08-01&amp;#39;, get = &amp;#39;stock.prices&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `type_convert()` only converts columns of type &amp;#39;character&amp;#39;.
## - `df` has no columns of type &amp;#39;character&amp;#39;

## Warning: `type_convert()` only converts columns of type &amp;#39;character&amp;#39;.
## - `df` has no columns of type &amp;#39;character&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +
  geom_line() +
  labs(x = &amp;#39;Date&amp;#39;, y = &amp;#39;Closing price&amp;#39;, title = &amp;#39;Apple and Netflix stock price&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar result as &lt;code&gt;geom_line&lt;/code&gt;, but without the color difference (which makes it rather hard to tell what you’re looking at). But if we add labels using &lt;code&gt;geom_label&lt;/code&gt;, we’ll get one label for every point, which will be overwhelming. The solution? Use some filtered data so that there is only one point for each label. But that means replacing the &lt;code&gt;data&lt;/code&gt; in &lt;code&gt;ggplot&lt;/code&gt;. Here’s how.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-different-data-with-different-geometries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using different data with different geometries&lt;/h3&gt;
&lt;p&gt;Just as we can use different aesthetic mappings on each geometry, we can use different &lt;em&gt;data&lt;/em&gt; entirely. This is useful when we want one geometry to have one set of data (like the stock prices above), but another geometry to only have a subset of the data. Why would we want that? Well, we’d like to label just &lt;em&gt;one&lt;/em&gt; part of each of the lines in our plot, right? That means we want to label a &lt;em&gt;subset&lt;/em&gt; of the stock data.&lt;/p&gt;
&lt;p&gt;To replace data in a geometry, we just need to specify the &lt;code&gt;data =&lt;/code&gt; argument separately:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +
  geom_line() +
  geom_label(data = AAPLNFLX %&amp;gt;% group_by(symbol) %&amp;gt;% slice(100),
             aes(label = symbol),
             nudge_y = 20) +
  labs(x = &amp;#39;Date&amp;#39;, y = &amp;#39;Closing price&amp;#39;, title = &amp;#39;Apple and Netflix stock price&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;geom_label&lt;/code&gt;, we specified we wanted the 100th observation from each symbol to be the label location. Then, we nudged it up along y by 20 so that it’s clear of the line.&lt;/p&gt;
&lt;p&gt;R also has a very useful &lt;code&gt;ggrepel&lt;/code&gt; package that gives us &lt;code&gt;geom_label_repel&lt;/code&gt; which takes care of the nudging for us, even in complicated situations (lots of points, lines, etc.). It does a decent job here of moving the label to a point where it doesn’t cover a lot of data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggrepel)
ggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +
  geom_line() +
  geom_label_repel(data = AAPLNFLX %&amp;gt;% group_by(symbol) %&amp;gt;% slice(100),
             aes(label = symbol)) +
  labs(x = &amp;#39;Date&amp;#39;, y = &amp;#39;Closing price&amp;#39;, title = &amp;#39;Apple and Netflix stock price&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, we don’t lose a lot of space to a legend, and we haven’t had to use color to separate the stock symbols.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-geometries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiple geometries&lt;/h3&gt;
&lt;p&gt;Since this section is about adding geometries, we &lt;em&gt;can&lt;/em&gt; combine points and lines. Since lines connect points, it will look like a giant connect-the-dots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggrepel)
ggplot(AAPLNFLX, aes(x = date, y = close, group = symbol)) +
  geom_line() +
  geom_point() +
  geom_label_repel(data = AAPLNFLX %&amp;gt;% group_by(symbol) %&amp;gt;% slice(100),
             aes(label = symbol)) +
  labs(x = &amp;#39;Date&amp;#39;, y = &amp;#39;Closing price&amp;#39;, title = &amp;#39;Apple and Netflix stock price&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/03-content_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;try-it-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Try it!&lt;/h2&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start by loading the &lt;strong&gt;dplyr&lt;/strong&gt; and &lt;strong&gt;ggplot2&lt;/strong&gt; library as well as the &lt;code&gt;murders&lt;/code&gt; and &lt;code&gt;heights&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(dslabs)
data(heights)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;With &lt;strong&gt;ggplot2&lt;/strong&gt; plots can be saved as objects. For example we can associate a dataset with a plot object like this&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because &lt;code&gt;data&lt;/code&gt; is the first argument we don’t need to spell it out&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we can also use the pipe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- murders %&amp;gt;% ggplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is class of the object &lt;code&gt;p&lt;/code&gt;?&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Remember that to print an object you can use the command &lt;code&gt;print&lt;/code&gt; or simply type the object.
Print the object &lt;code&gt;p&lt;/code&gt; defined in exercise one and describe what you see.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Nothing happens.&lt;/li&gt;
&lt;li&gt;A blank slate plot.&lt;/li&gt;
&lt;li&gt;A scatterplot.&lt;/li&gt;
&lt;li&gt;A histogram.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Using the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;, create an object &lt;code&gt;p&lt;/code&gt; but this time associated with the &lt;code&gt;heights&lt;/code&gt; dataset instead of the &lt;code&gt;murders&lt;/code&gt; dataset.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the class of the object &lt;code&gt;p&lt;/code&gt; you have just created?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we are going to add a layer and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the &lt;code&gt;murders&lt;/code&gt; data frame to remind yourself what are the names for these two variables and select the correct answer. &lt;strong&gt;Hint&lt;/strong&gt;: Look at &lt;code&gt;?murders&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;state&lt;/code&gt; and &lt;code&gt;abb&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;total_murders&lt;/code&gt; and &lt;code&gt;population_size&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;total&lt;/code&gt; and &lt;code&gt;population&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To create the scatterplot we add a layer with &lt;code&gt;geom_point&lt;/code&gt;. The aesthetic mappings require us to define the x-axis and y-axis variables, respectively. So the code looks like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot(aes(x = , y = )) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;except we have to define the two variables &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. Fill this out with the correct variable names.&lt;/p&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Note that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot(aes(population, total)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remake the plot but now with total in the x-axis and population in the y-axis.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If instead of points we want to add text, we can use the &lt;code&gt;geom_text()&lt;/code&gt; or &lt;code&gt;geom_label()&lt;/code&gt; geometries. The following code&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot(aes(population, total)) + geom_label()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will give us the error message: &lt;code&gt;Error: geom_label requires the following missing aesthetics: label&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Why is this?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;We need to map a character to each point through the label argument in aes.&lt;/li&gt;
&lt;li&gt;We need to let &lt;code&gt;geom_label&lt;/code&gt; know what character to use in the plot.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;geom_label&lt;/code&gt; geometry does not require x-axis and y-axis values.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geom_label&lt;/code&gt; is not a ggplot2 command.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;9&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Rewrite the code above to use abbreviation as the label through &lt;code&gt;aes&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Change the color of the labels to blue. How will we do this?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Adding a column called &lt;code&gt;blue&lt;/code&gt; to &lt;code&gt;murders&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Because each label needs a different color we map the colors through &lt;code&gt;aes&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;color&lt;/code&gt; argument in &lt;code&gt;ggplot&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Because we want all colors to be blue, we do not need to map colors, just use the color argument in &lt;code&gt;geom_label&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;11&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Rewrite the code above to make the labels blue.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now suppose we want to use color to represent the different regions. In this case which of the following is most appropriate:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Adding a column called &lt;code&gt;color&lt;/code&gt; to &lt;code&gt;murders&lt;/code&gt; with the color we want to use.&lt;/li&gt;
&lt;li&gt;Because each label needs a different color we map the colors through the color argument of &lt;code&gt;aes&lt;/code&gt; .&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;color&lt;/code&gt; argument in &lt;code&gt;ggplot&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Because we want all colors to be blue, we do not need to map colors, just use the color argument in &lt;code&gt;geom_label&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;13&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Rewrite the code above to make the labels’ color be determined by the state’s region.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by defining an object &lt;code&gt;p&lt;/code&gt; holding the plot we have made up to now&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- murders %&amp;gt;%
  ggplot(aes(population, total, label = abb, color = region)) +
  geom_label()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To change the y-axis to a log scale we learned about the &lt;code&gt;scale_x_log10()&lt;/code&gt; function. Add this layer to the object &lt;code&gt;p&lt;/code&gt; to change the scale and render the plot.&lt;/p&gt;
&lt;ol start=&#34;15&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Repeat the previous exercise but now change both axes to be in the log scale.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now edit the code above to add the title “Gun murder data” to the plot. Hint: use the &lt;code&gt;labs&lt;/code&gt; function or the &lt;code&gt;ggtitle&lt;/code&gt; function.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://ggplot2.tidyverse.org/&#34; class=&#34;uri&#34;&gt;https://ggplot2.tidyverse.org/&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.springer.com/us/book/9780387245447&#34; class=&#34;uri&#34;&gt;http://www.springer.com/us/book/9780387245447&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to the tidyverse</title>
      <link>https://ssc442.netlify.app/content/02-content/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/02-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#readings&#34;&gt;Readings&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-question&#34;&gt;Guiding Question&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#final-projects&#34;&gt;Final Projects&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#teams&#34;&gt;Teams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#randomness-and-data-analytics&#34;&gt;Randomness and Data Analytics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#learning-from-data&#34;&gt;Learning From Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#formalization&#34;&gt;Formalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-target-function&#34;&gt;The Target Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-estimate-an-unknown-function&#34;&gt;Why Estimate an Unknown Function?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-parable-of-the-marbles&#34;&gt;The Parable of the Marbles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#outside-the-data&#34;&gt;Outside the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hoeffdings-inequality&#34;&gt;Hoeffding’s Inequality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#an-example-of-hoeffdings-inequality&#34;&gt;An example of Hoeffding’s Inequality&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidyverse&#34;&gt;The tidyverse&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tidy-data&#34;&gt;Tidy data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#manipulating-data-frames&#34;&gt;Manipulating data frames&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#adding-a-column-with-mutate&#34;&gt;Adding a column with &lt;code&gt;mutate&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subsetting-with-filter&#34;&gt;Subsetting with &lt;code&gt;filter&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selecting-columns-with-select&#34;&gt;Selecting columns with &lt;code&gt;select&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-pipe&#34;&gt;The pipe: &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summarizing-data&#34;&gt;Summarizing data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#summarize&#34;&gt;&lt;code&gt;summarize&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pull&#34;&gt;&lt;code&gt;pull&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#group-by&#34;&gt;Group then summarize with &lt;code&gt;group_by&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sorting-data-frames&#34;&gt;Sorting data frames&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#nested-sorting&#34;&gt;Nested sorting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-top-n&#34;&gt;The top &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tibbles&#34;&gt;Tibbles&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tibbles-display-better&#34;&gt;Tibbles display better&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subsets-of-tibbles-are-tibbles&#34;&gt;Subsets of tibbles are tibbles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tibbles-can-have-complex-entries&#34;&gt;Tibbles can have complex entries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tibbles-can-be-grouped&#34;&gt;Tibbles can be grouped&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#create-a-tibble-using-tibble-instead-of-data.frame&#34;&gt;Create a tibble using &lt;code&gt;tibble&lt;/code&gt; instead of &lt;code&gt;data.frame&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-dot-operator&#34;&gt;The dot operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#do&#34;&gt;&lt;code&gt;do&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-purrr-package&#34;&gt;The &lt;strong&gt;purrr&lt;/strong&gt; package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidyverse-conditionals&#34;&gt;Tidyverse conditionals&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case_when&#34;&gt;&lt;code&gt;case_when&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#between&#34;&gt;&lt;code&gt;between&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lecture-video&#34;&gt;Lecture Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Readings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;li&gt;Chapter 1 of Introduction to Statistical Learning, available &lt;a href=&#34;https://www.statlearning.com/&#34;&gt;here.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Optional: The “Tidy Your Data” tutorial on &lt;a href=&#34;https://rstudio.cloud/learn/primers&#34;&gt;Rstudio Clould Primers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Question&lt;/h3&gt;
&lt;p&gt;For future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why do we want tidy data?&lt;/li&gt;
&lt;li&gt;What are the challenges associated with shaping things into a tidy format?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;final-projects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final Projects&lt;/h2&gt;
&lt;p&gt;Your final is a group project. Accordingly, you need to start planning soon.&lt;/p&gt;
&lt;p&gt;To aid in your planning, here are the required elements of your project (&lt;strong&gt;note: the assignment that currently exists on this site, if you find it, is old and will change a lot between now and next week&lt;/strong&gt;).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;You must find existing data to analyze. Aggregating and merging data from multiple sources is encouraged.&lt;/li&gt;
&lt;li&gt;You must visualize 3 intersting features of that data.&lt;/li&gt;
&lt;li&gt;You must come up with some analysis—using tools from this course—which relates your data to either a prediction or a policy conclusion.&lt;/li&gt;
&lt;li&gt;You must think critically about your analysis and be able to identify potential issues/&lt;/li&gt;
&lt;li&gt;You must present your analysis as if presenting to a C-suite executive.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you have any additional questions, you can find some more information in the Assignments section of this website.&lt;/p&gt;
&lt;div id=&#34;teams&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Teams&lt;/h3&gt;
&lt;p&gt;You must form teams of 3-4 people. I will send out a survey link via email to better understand your teams. If you are in a group (and everyone agrees you’re in a group) then only one of you needs to respond to the survey.&lt;/p&gt;
&lt;p&gt;If you are not listed on another person’s team and do not respond to the survey, I will interpret this as evidence that you have opted to not form a team—or you like adventure! Accordingly, you will be automatically added to the “willing to be randomly assigned” pool and will be paired with others in the class.&lt;/p&gt;
&lt;div id=&#34;more-information-on-teams&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;More Information on Teams&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;You should strongly consider coordinating your work via Github.&lt;/li&gt;
&lt;li&gt;Your team will earn the same scores on all projects. (Note that projects are not labs / writings. They are the shorter two projects and the final project.)&lt;/li&gt;
&lt;li&gt;Teams will submit only one write-up for the mini-projects and the final.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To combat additional freeloading, we will use a reporting system. We’ll discuss that a bit later.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;randomness-and-data-analytics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Randomness and Data Analytics&lt;/h2&gt;
&lt;p&gt;And the fabulous importance of probabilistic inference…&lt;/p&gt;
&lt;p&gt;This lecture is very “high-level,” which means it is talking about abstract concepts. It is also quite important. We want to discuss &lt;strong&gt;why&lt;/strong&gt; we eventually will need ot utilize tons of difficult mathematics. Why do we care so much about hypothesis tests and the like? Moreover, we can highlight why we want our data structured to behave nicely.&lt;/p&gt;
&lt;div id=&#34;learning-from-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Learning From Data&lt;/h3&gt;
&lt;p&gt;The following are the baisc requirements for statistical learning&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;A pattern exists.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This pattern is not easily expressed in a closed mathematical form.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You have data.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;formalization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Formalization&lt;/h3&gt;
&lt;p&gt;We think of our outcome-of-interest as a &lt;strong&gt;reponse&lt;/strong&gt; or &lt;strong&gt;target&lt;/strong&gt; that we wish to predict or wish to learn something about.&lt;/p&gt;
&lt;p&gt;We generically refer to the response as &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Other aspects of the data are known as &lt;strong&gt;features, inputs, predictors&lt;/strong&gt;, or &lt;strong&gt;regressors&lt;/strong&gt;. We call one of these &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The subscript &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; indicates that we have an &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; realized for every individual in our data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can refer to the input vector collectively as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X = \begin{bmatrix}
x_{11} &amp;amp; x_{12} \\
x_{21} &amp;amp; x_{22} \\
\vdots &amp;amp; \vdots \\
x_{N1} &amp;amp; x_{N2}
\end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We are seeking some unknown function that maps &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Put another way, we are seeking to explain &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y = f(X) + e\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-target-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Target Function&lt;/h3&gt;
&lt;p&gt;We call the function &lt;span class=&#34;math inline&#34;&gt;\(f: \mathcal{X} \rightarrow \mathcal{Y}\)&lt;/span&gt; the &lt;strong&gt;target function&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;How do we find the function? We don’t! We get as close as we can, though:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Observe data &lt;span class=&#34;math inline&#34;&gt;\((\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_N, y_N)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use some algorithm to approximate &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Produce final hypothesis function &lt;span class=&#34;math inline&#34;&gt;\(g \approx f\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Evaluate how well &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; approximates &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and iterate as needed.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;why-estimate-an-unknown-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why Estimate an Unknown Function?&lt;/h3&gt;
&lt;p&gt;With a good estimate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; we can make predictions of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;strong&gt;new&lt;/strong&gt; points &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can also understand which components of &lt;span class=&#34;math inline&#34;&gt;\(X = (X_1, X_2, \cdots, X_m)\)&lt;/span&gt; are important in explaining &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and which are (potentially) irrelevant&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;e.g. &lt;code&gt;GDP&lt;/code&gt; and &lt;code&gt;yearsindustrialized&lt;/code&gt; have a big impact on &lt;code&gt;emissions&lt;/code&gt; but &lt;code&gt;hydroutilization&lt;/code&gt; typically does not.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on the complexity of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, we may be able to meaningfully understand how each component of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; affects &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;(But we should be careful about assigning causal interpretations, more on this later)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-parable-of-the-marbles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Parable of the Marbles&lt;/h3&gt;
&lt;p&gt;Imagine a bag of marbles with two types of marbles: ♣️ and ♦️.&lt;/p&gt;
&lt;p&gt;We are going to pick a &lt;strong&gt;sample&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; marbles (with replacement).&lt;/p&gt;
&lt;p&gt;We want to learn something about &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, the &lt;strong&gt;objective&lt;/strong&gt; probability to pick a ♣️.&lt;/p&gt;
&lt;p&gt;In addition to defining the &lt;strong&gt;objective&lt;/strong&gt; probability of picking a ♣️, we have an observed fraction &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;, which will define as the fraction of ♣️ in the &lt;em&gt;sample&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: Can we say anything exact and for-sure about &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (outside the data) after observing &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; (the data)?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;No. It is possible for the sample to be all ♣️, ♣️, ♣️, ♣️, ♣️ even when the bag is is 50/50 ♦️&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;No matter what we draw, we can’t (based on that draw alone) eliminate the possibility of drawing a ♦️.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;And unless we assume that the only two values in the world are ♦️ and ♣️, we can’t rule out 💩&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;: Then why do we do things like polling (e.g. to predict the outcome of a presidential election)?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The bad case, that we draw something that has is completely misleading, is &lt;em&gt;possible&lt;/em&gt; but not &lt;strong&gt;probable&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;outside-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Outside the Data&lt;/h3&gt;
&lt;p&gt;Put another way, since &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; is unknown, it can take on any value outside the data we have, no matter how large the data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is called &lt;em&gt;No Free Lunch&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You cannot know anything for sure about &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; outside the data without making assumptions.&lt;/p&gt;
&lt;p&gt;Is there any hope to know anything about &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; outside the data set without making assumptions about &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Yes&lt;/strong&gt;, if we are willing to give up the “for sure”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hoeffdings-inequality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hoeffding’s Inequality&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Hoeffding’s Inequality&lt;/strong&gt; states, loosely, that &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; cannot be too far from &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbb{P}\left[|\eta - \mu| &amp;gt; \epsilon \right] \leq 2e^{-2\epsilon^2n}\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\eta \approx \mu\)&lt;/span&gt; is called &lt;strong&gt;probably approximately correct&lt;/strong&gt; (PAC) learning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-of-hoeffdings-inequality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;An example of Hoeffding’s Inequality&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: n = 1,000. Draw a sample and observe &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;$$99% of the time, &lt;span class=&#34;math inline&#34;&gt;\(\mu - .05 \leq \eta \leq \mu+.05\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is implied by setting &lt;span class=&#34;math inline&#34;&gt;\(\epsilon = 0.05\)&lt;/span&gt; and using &lt;span class=&#34;math inline&#34;&gt;\(n=1,000\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;99.9999996% of the time &lt;span class=&#34;math inline&#34;&gt;\(\mu - .10 \leq \eta \leq \mu + .10\%\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;What does this mean?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If I repeatedly pick a sample of size 1,000, observe &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; and claim that &lt;span class=&#34;math inline&#34;&gt;\(\mu \in \left[\eta - .05, \eta + .05\right]\)&lt;/span&gt; (or that the error bar is &lt;span class=&#34;math inline&#34;&gt;\(\pm 0.05\)&lt;/span&gt;), I will be right 99% of the time.&lt;/p&gt;
&lt;p&gt;On any particular sample you may be wrong, but not often.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This week’s content is split into two “halves”: the critical data manipulation information contained below and a more-entertaining discussion of visualization included in the Exercises.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tidyverse&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The tidyverse&lt;/h1&gt;
&lt;p&gt;In the first weeks’ content (or maybe that is week zero?), we demonstrated how to manipulate vectors by reordering and subsetting them through indexing. However, once we start more advanced analyses, the preferred unit for data storage is not the vector but the data frame. In this lecture, we learn to work directly with data frames, which greatly facilitate the organization of information. We will be using data frames for the majority of this class and you will use them for the majority of your data science life (however long that might be). We will focus on a specific data format referred to as &lt;em&gt;tidy&lt;/em&gt; and on specific collection of packages that are particularly helpful for working with &lt;em&gt;tidy&lt;/em&gt; data referred to as the &lt;em&gt;tidyverse&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can load all the tidyverse packages at once by installing and loading the &lt;strong&gt;tidyverse&lt;/strong&gt; package:&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will learn how to implement the tidyverse approach throughout the book, but before delving into the details, in this chapter we introduce some of the most widely used tidyverse functionality, starting with the &lt;strong&gt;dplyr&lt;/strong&gt; package for manipulating data frames and the &lt;strong&gt;purrr&lt;/strong&gt; package for working with functions. Note that the tidyverse also includes a graphing package, &lt;strong&gt;ggplot2&lt;/strong&gt;, which we introduce later in Chapter &lt;a href=&#34;#ggplot2&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; in the Data Visualization part of the book; the &lt;strong&gt;readr&lt;/strong&gt; package discussed in Chapter &lt;a href=&#34;#importing-data&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;; and many others. In this chapter, we first introduce the concept of &lt;em&gt;tidy data&lt;/em&gt; and then demonstrate how we use the tidyverse to work with data frames in this format.&lt;/p&gt;
&lt;div id=&#34;tidy-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidy data&lt;/h2&gt;
&lt;p&gt;We say that a data table is in &lt;em&gt;tidy&lt;/em&gt; format if each row represents one observation and columns represent the different variables available for each of these observations. The &lt;code&gt;murders&lt;/code&gt; dataset is an example of a tidy data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total
## 1    Alabama  AL  South    4779736   135
## 2     Alaska  AK   West     710231    19
## 3    Arizona  AZ   West    6392017   232
## 4   Arkansas  AR  South    2915918    93
## 5 California  CA   West   37253956  1257
## 6   Colorado  CO   West    5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.&lt;/p&gt;
&lt;p&gt;To see how the same information can be provided in different formats, consider the following example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;gapminder&amp;quot;)
tidy_data &amp;lt;- gapminder %&amp;gt;%
  filter(country %in% c(&amp;quot;South Korea&amp;quot;, &amp;quot;Germany&amp;quot;) &amp;amp; !is.na(fertility)) %&amp;gt;%
  select(country, year, fertility)
head(tidy_data, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       country year fertility
## 1     Germany 1960      2.41
## 2 South Korea 1960      6.16
## 3     Germany 1961      2.44
## 4 South Korea 1961      5.99
## 5     Germany 1962      2.47
## 6 South Korea 1962      5.79&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the &lt;strong&gt;dslabs&lt;/strong&gt; package. Originally, the data was in the following format:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       country 1960 1961 1962
## 1     Germany 2.41 2.44 2.47
## 2 South Korea 6.16 5.99 5.79&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into &lt;code&gt;tidy&lt;/code&gt; format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.&lt;/p&gt;
&lt;p&gt;Although not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Examine the built-in dataset &lt;code&gt;co2&lt;/code&gt;. Which of the following is true:&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;co2&lt;/code&gt; is tidy data: it has one year for each row.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;co2&lt;/code&gt; is not tidy: we need at least one column with a character vector.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;co2&lt;/code&gt; is not tidy: it is a matrix instead of a data frame.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;co2&lt;/code&gt; is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Examine the built-in dataset &lt;code&gt;ChickWeight&lt;/code&gt;. Which of the following is true:&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;ChickWeight&lt;/code&gt; is not tidy: each chick has more than one row.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ChickWeight&lt;/code&gt; is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ChickWeight&lt;/code&gt; is not tidy: we are missing the year column.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ChickWeight&lt;/code&gt; is tidy: it is stored in a data frame.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Examine the built-in dataset &lt;code&gt;BOD&lt;/code&gt;. Which of the following is true:&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;BOD&lt;/code&gt; is not tidy: it only has six rows.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BOD&lt;/code&gt; is not tidy: the first column is just an index.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BOD&lt;/code&gt; is tidy: each row is an observation with two values (time and demand)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BOD&lt;/code&gt; is tidy: all small datasets are tidy by definition.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which of the following built-in datasets is tidy (you can pick more than one):&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;BJsales&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;EuStockMarkets&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DNase&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Formaldehyde&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Orange&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UCBAdmissions&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;manipulating-data-frames&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manipulating data frames&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;dplyr&lt;/strong&gt; package from the &lt;strong&gt;tidyverse&lt;/strong&gt; introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use &lt;code&gt;mutate&lt;/code&gt;. To filter the data table to a subset of rows, we use &lt;code&gt;filter&lt;/code&gt;. Finally, to subset the data by selecting specific columns, we use &lt;code&gt;select&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;adding-a-column-with-mutate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding a column with &lt;code&gt;mutate&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;We want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function &lt;code&gt;mutate&lt;/code&gt; takes the data frame as a first argument and the name and values of the variable as a second argument using the convention &lt;code&gt;name = values&lt;/code&gt;. So, to add murder rates, we use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)
murders &amp;lt;- mutate(murders, rate = total / population * 100000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that here we used &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;population&lt;/code&gt; inside the function, which are objects that are &lt;strong&gt;not&lt;/strong&gt; defined in our workspace. But why don’t we get an error?&lt;/p&gt;
&lt;p&gt;This is one of &lt;strong&gt;dplyr&lt;/strong&gt;’s main features. Functions in this package, such as &lt;code&gt;mutate&lt;/code&gt;, know to look for variables in the data frame provided in the first argument. In the call to mutate above, &lt;code&gt;total&lt;/code&gt; will have the values in &lt;code&gt;murders$total&lt;/code&gt;. This approach makes the code much more readable.&lt;/p&gt;
&lt;p&gt;We can see that the new column is added:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total     rate
## 1    Alabama  AL  South    4779736   135 2.824424
## 2     Alaska  AK   West     710231    19 2.675186
## 3    Arizona  AZ   West    6392017   232 3.629527
## 4   Arkansas  AR  South    2915918    93 3.189390
## 5 California  CA   West   37253956  1257 3.374138
## 6   Colorado  CO   West    5029196    65 1.292453&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although we have overwritten the original &lt;code&gt;murders&lt;/code&gt; object, this does not change the object that loaded with &lt;code&gt;data(murders)&lt;/code&gt;. If we load the &lt;code&gt;murders&lt;/code&gt; data again, the original will overwrite our mutated version.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsetting-with-filter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting with &lt;code&gt;filter&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Now suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the &lt;code&gt;filter&lt;/code&gt; function, which takes the data table as the first argument and then the conditional statement as the second. Like &lt;code&gt;mutate&lt;/code&gt;, we can use the unquoted variable names from &lt;code&gt;murders&lt;/code&gt; inside the function and it will know we mean the columns and not objects in the workspace.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, rate &amp;lt;= 0.71)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state abb        region population total      rate
## 1        Hawaii  HI          West    1360301     7 0.5145920
## 2          Iowa  IA North Central    3046355    21 0.6893484
## 3 New Hampshire  NH     Northeast    1316470     5 0.3798036
## 4  North Dakota  ND North Central     672591     4 0.5947151
## 5       Vermont  VT     Northeast     625741     2 0.3196211&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-columns-with-select&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Selecting columns with &lt;code&gt;select&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Although our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the &lt;strong&gt;dplyr&lt;/strong&gt; &lt;code&gt;select&lt;/code&gt; function. In the code below we select three columns, assign this to a new object and then filter the new object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_table &amp;lt;- select(murders, state, region, rate)
filter(new_table, rate &amp;lt;= 0.71)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state        region      rate
## 1        Hawaii          West 0.5145920
## 2          Iowa North Central 0.6893484
## 3 New Hampshire     Northeast 0.3798036
## 4  North Dakota North Central 0.5947151
## 5       Vermont     Northeast 0.3196211&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the call to &lt;code&gt;select&lt;/code&gt;, the first argument &lt;code&gt;murders&lt;/code&gt; is an object, but &lt;code&gt;state&lt;/code&gt;, &lt;code&gt;region&lt;/code&gt;, and &lt;code&gt;rate&lt;/code&gt; are variable names.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the &lt;strong&gt;dplyr&lt;/strong&gt; package and the murders dataset.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can add columns using the &lt;strong&gt;dplyr&lt;/strong&gt; function &lt;code&gt;mutate&lt;/code&gt;. This function is aware of the column names and inside the function you can call them unquoted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders &amp;lt;- mutate(murders, population_in_millions = population / 10^6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can write &lt;code&gt;population&lt;/code&gt; rather than &lt;code&gt;murders$population&lt;/code&gt;. The function &lt;code&gt;mutate&lt;/code&gt; knows we are grabbing columns from &lt;code&gt;murders&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Use the function &lt;code&gt;mutate&lt;/code&gt; to add a murders column named &lt;code&gt;rate&lt;/code&gt; with the per 100,000 murder rate as in the example code above. Make sure you redefine &lt;code&gt;murders&lt;/code&gt; as done in the example code above ( murders &amp;lt;- [your code]) so we can keep using this variable.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;If &lt;code&gt;rank(x)&lt;/code&gt; gives you the ranks of &lt;code&gt;x&lt;/code&gt; from lowest to highest, &lt;code&gt;rank(-x)&lt;/code&gt; gives you the ranks from highest to lowest. Use the function &lt;code&gt;mutate&lt;/code&gt; to add a column &lt;code&gt;rank&lt;/code&gt; containing the rank, from highest to lowest murder rate. Make sure you redefine &lt;code&gt;murders&lt;/code&gt; so we can keep using this variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With &lt;strong&gt;dplyr&lt;/strong&gt;, we can use &lt;code&gt;select&lt;/code&gt; to show only certain columns. For example, with this code we would only show the states and population sizes:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select(murders, state, population) %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use &lt;code&gt;select&lt;/code&gt; to show the state names and abbreviations in &lt;code&gt;murders&lt;/code&gt;. Do not redefine &lt;code&gt;murders&lt;/code&gt;, just show the results.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;strong&gt;dplyr&lt;/strong&gt; function &lt;code&gt;filter&lt;/code&gt; is used to choose specific rows of the data frame to keep. Unlike &lt;code&gt;select&lt;/code&gt; which is for columns, &lt;code&gt;filter&lt;/code&gt; is for rows. For example, you can show just the New York row like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, state == &amp;quot;New York&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use other logical vectors to filter rows.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;filter&lt;/code&gt; to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the &lt;code&gt;rank&lt;/code&gt; column.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can remove rows using the &lt;code&gt;!=&lt;/code&gt; operator. For example, to remove Florida, we would do this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;no_florida &amp;lt;- filter(murders, state != &amp;quot;Florida&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a new data frame called &lt;code&gt;no_south&lt;/code&gt; that removes states from the South region. How many states are in this category? You can use the function &lt;code&gt;nrow&lt;/code&gt; for this.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can also use &lt;code&gt;%in%&lt;/code&gt; to filter with &lt;strong&gt;dplyr&lt;/strong&gt;. You can therefore see the data from New York and Texas like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, state %in% c(&amp;quot;New York&amp;quot;, &amp;quot;Texas&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a new data frame called &lt;code&gt;murders_nw&lt;/code&gt; with only the states from the Northeast and the West. How many states are in this category?&lt;/p&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Suppose you want to live in the Northeast or West &lt;strong&gt;and&lt;/strong&gt; want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with &lt;code&gt;filter&lt;/code&gt;. Here is an example in which we filter to keep only small states in the Northeast region.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, population &amp;lt; 5000000 &amp;amp; region == &amp;quot;Northeast&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure &lt;code&gt;murders&lt;/code&gt; has been defined with &lt;code&gt;rate&lt;/code&gt; and &lt;code&gt;rank&lt;/code&gt; and still has all states. Create a table called &lt;code&gt;my_states&lt;/code&gt; that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use &lt;code&gt;select&lt;/code&gt; to show only the state name, the rate, and the rank.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The pipe: &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;With &lt;strong&gt;dplyr&lt;/strong&gt; we can perform a series of operations, for example &lt;code&gt;select&lt;/code&gt; and then &lt;code&gt;filter&lt;/code&gt;, by sending the results of one function to another using what is called the &lt;em&gt;pipe operator&lt;/em&gt;: &lt;code&gt;%&amp;gt;%&lt;/code&gt;. Some details are included below.&lt;/p&gt;
&lt;p&gt;We wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object &lt;code&gt;new_table&lt;/code&gt;. In &lt;strong&gt;dplyr&lt;/strong&gt; we can write code that looks more like a description of what we want to do without intermediate objects:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \mbox{original data }
\rightarrow \mbox{ select }
\rightarrow \mbox{ filter } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For such an operation, we can use the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;. The code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% select(state, region, rate) %&amp;gt;% filter(rate &amp;lt;= 0.71)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state        region      rate
## 1        Hawaii          West 0.5145920
## 2          Iowa North Central 0.6893484
## 3 New Hampshire     Northeast 0.3798036
## 4  North Dakota North Central 0.5947151
## 5       Vermont     Northeast 0.3196211&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This line of code is equivalent to the two lines of code above. What is going on here?&lt;/p&gt;
&lt;p&gt;In general, the pipe &lt;em&gt;sends&lt;/em&gt; the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;16 %&amp;gt;% sqrt()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can continue to pipe values along:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;16 %&amp;gt;% sqrt() %&amp;gt;% log2()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above statement is equivalent to &lt;code&gt;log2(sqrt(16))&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Remember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;16 %&amp;gt;% sqrt() %&amp;gt;% log(base = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, when using the pipe with data frames and &lt;strong&gt;dplyr&lt;/strong&gt;, we no longer need to specify the required first argument since the &lt;strong&gt;dplyr&lt;/strong&gt; functions we have described all take the data as the first argument. In the code we wrote:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% select(state, region, rate) %&amp;gt;% filter(rate &amp;lt;= 0.71)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;murders&lt;/code&gt; is the first argument of the &lt;code&gt;select&lt;/code&gt; function, and the new data frame (formerly &lt;code&gt;new_table&lt;/code&gt;) is the first argument of the &lt;code&gt;filter&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Note that the pipe works well with functions where the first argument is the input data. Functions in &lt;strong&gt;tidyverse&lt;/strong&gt; packages like &lt;strong&gt;dplyr&lt;/strong&gt; have this format and can be used easily with the pipe.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders &amp;lt;- mutate(murders, rate =  total / population * 100000,
                  rank = rank(-rate))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the solution to the previous exercise, we did the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_states &amp;lt;- filter(murders, region %in% c(&amp;quot;Northeast&amp;quot;, &amp;quot;West&amp;quot;) &amp;amp;
                      rate &amp;lt; 1)

select(my_states, state, rate, rank)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; permits us to perform both operations sequentially without having to define an intermediate variable &lt;code&gt;my_states&lt;/code&gt;. We therefore could have mutated and selected in the same line like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mutate(murders, rate =  total / population * 100000,
       rank = rank(-rate)) %&amp;gt;%
  select(state, rate, rank)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that &lt;code&gt;select&lt;/code&gt; no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the &lt;code&gt;%&amp;gt;%&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Repeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; to do this in just one line.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Reset &lt;code&gt;murders&lt;/code&gt; to the original table by using &lt;code&gt;data(murders)&lt;/code&gt;. Use a pipe to create a new data frame called &lt;code&gt;my_states&lt;/code&gt; that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three &lt;code&gt;%&amp;gt;%&lt;/code&gt;. The code should look something like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_states &amp;lt;- murders %&amp;gt;%
  mutate SOMETHING %&amp;gt;%
  filter SOMETHING %&amp;gt;%
  select SOMETHING&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summarizing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summarizing data&lt;/h2&gt;
&lt;p&gt;An important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new &lt;strong&gt;dplyr&lt;/strong&gt; verbs that make these computations easier: &lt;code&gt;summarize&lt;/code&gt; and &lt;code&gt;group_by&lt;/code&gt;. We learn to access resulting values using the &lt;code&gt;pull&lt;/code&gt; function.&lt;/p&gt;
&lt;div id=&#34;summarize&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;summarize&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;summarize&lt;/code&gt; function in &lt;strong&gt;dplyr&lt;/strong&gt; provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The &lt;code&gt;heights&lt;/code&gt; dataset includes heights and sex reported by students in an in-class survey.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(dslabs)
data(heights)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code computes the average and standard deviation for females:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  summarize(average = mean(height), standard_deviation = sd(height))
s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    average standard_deviation
## 1 64.93942           3.760656&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use &lt;code&gt;average&lt;/code&gt; and &lt;code&gt;standard_deviation&lt;/code&gt;, but we could have used other names just the same.&lt;/p&gt;
&lt;p&gt;Because the resulting table stored in &lt;code&gt;s&lt;/code&gt; is a data frame, we can access the components with the accessor &lt;code&gt;$&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s$average&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 64.93942&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s$standard_deviation&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.760656&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with most other &lt;strong&gt;dplyr&lt;/strong&gt; functions, &lt;code&gt;summarize&lt;/code&gt; is aware of the variable names and we can use them directly. So when inside the call to the &lt;code&gt;summarize&lt;/code&gt; function we write &lt;code&gt;mean(height)&lt;/code&gt;, the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  summarize(median = median(height), minimum = min(height),
            maximum = max(height))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     median minimum maximum
## 1 64.98031      51      79&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can obtain these three values with just one line using the &lt;code&gt;quantile&lt;/code&gt; function: for example, &lt;code&gt;quantile(x, c(0,0.5,1))&lt;/code&gt; returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector &lt;code&gt;x&lt;/code&gt;. However, if we attempt to use a function like this that returns two or more values inside &lt;code&gt;summarize&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  summarize(range = quantile(height, c(0, 0.5, 1)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we will receive an error: &lt;code&gt;Error: expecting result of length one, got : 2&lt;/code&gt;. With the function &lt;code&gt;summarize&lt;/code&gt;, we can only call functions that return a single value. In Section &lt;a href=&#34;#do&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;, we will learn how to deal with functions that return more than one value.&lt;/p&gt;
&lt;p&gt;For another example of how we can use the &lt;code&gt;summarize&lt;/code&gt; function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used &lt;strong&gt;dplyr&lt;/strong&gt; to add a murder rate column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders &amp;lt;- murders %&amp;gt;% mutate(rate = total/population*100000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that the US murder rate is &lt;strong&gt;not&lt;/strong&gt; the average of the state murder rates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summarize(murders, mean(rate))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mean(rate)
## 1   2.779125&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_murder_rate &amp;lt;- murders %&amp;gt;%
  summarize(rate = sum(total) / sum(population) * 100000)
us_murder_rate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       rate
## 1 3.034555&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This computation counts larger states proportionally to their size which results in a larger value.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pull&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;pull&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;us_murder_rate&lt;/code&gt; object defined above represents just one number. Yet we are storing it in a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(us_murder_rate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;since, as most &lt;strong&gt;dplyr&lt;/strong&gt; functions, &lt;code&gt;summarize&lt;/code&gt; always returns a data frame.&lt;/p&gt;
&lt;p&gt;This might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the &lt;code&gt;pull&lt;/code&gt; function. To understand what we mean take a look at this line of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_murder_rate %&amp;gt;% pull(rate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.034555&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns the value in the &lt;code&gt;rate&lt;/code&gt; column of &lt;code&gt;us_murder_rate&lt;/code&gt; making it equivalent to &lt;code&gt;us_murder_rate$rate&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To get a number from the original data table with one line of code we can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_murder_rate &amp;lt;- murders %&amp;gt;%
  summarize(rate = sum(total) / sum(population) * 100000) %&amp;gt;%
  pull(rate)

us_murder_rate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.034555&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is now a numeric:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(us_murder_rate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;group-by&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Group then summarize with &lt;code&gt;group_by&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;A common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The &lt;code&gt;group_by&lt;/code&gt; function helps us do this.&lt;/p&gt;
&lt;p&gt;If we type this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% group_by(sex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,050 × 2
## # Groups:   sex [2]
##    sex    height
##    &amp;lt;fct&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 Male       75
##  2 Male       70
##  3 Male       68
##  4 Male       74
##  5 Male       61
##  6 Female     65
##  7 Female     66
##  8 Female     62
##  9 Female     66
## 10 Male       67
## # … with 1,040 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result does not look very different from &lt;code&gt;heights&lt;/code&gt;, except we see &lt;code&gt;Groups: sex [2]&lt;/code&gt; when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a &lt;em&gt;grouped data frame&lt;/em&gt;, and &lt;strong&gt;dplyr&lt;/strong&gt; functions, in particular &lt;code&gt;summarize&lt;/code&gt;, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  summarize(average = mean(height), standard_deviation = sd(height))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 3
##   sex    average standard_deviation
##   &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt;
## 1 Female    64.9               3.76
## 2 Male      69.3               3.61&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;summarize&lt;/code&gt; function applies the summarization to each group separately.&lt;/p&gt;
&lt;p&gt;For another example, let’s compute the median murder rate in the four regions of the country:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  group_by(region) %&amp;gt;%
  summarize(median_rate = median(rate))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 2
##   region        median_rate
##   &amp;lt;fct&amp;gt;               &amp;lt;dbl&amp;gt;
## 1 Northeast            1.80
## 2 South                3.40
## 3 North Central        1.97
## 4 West                 1.29&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting-data-frames&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sorting data frames&lt;/h2&gt;
&lt;p&gt;When examining a dataset, it is often convenient to sort the table by the different columns. We know about the &lt;code&gt;order&lt;/code&gt; and &lt;code&gt;sort&lt;/code&gt; function, but for ordering entire tables, the &lt;strong&gt;dplyr&lt;/strong&gt; function &lt;code&gt;arrange&lt;/code&gt; is useful. For example, here we order the states by population size:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  arrange(population) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  state abb        region population total       rate
## 1              Wyoming  WY          West     563626     5  0.8871131
## 2 District of Columbia  DC         South     601723    99 16.4527532
## 3              Vermont  VT     Northeast     625741     2  0.3196211
## 4         North Dakota  ND North Central     672591     4  0.5947151
## 5               Alaska  AK          West     710231    19  2.6751860
## 6         South Dakota  SD North Central     814180     8  0.9825837&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;arrange&lt;/code&gt; we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by &lt;code&gt;rate&lt;/code&gt; instead:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  arrange(rate) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state abb        region population total      rate
## 1       Vermont  VT     Northeast     625741     2 0.3196211
## 2 New Hampshire  NH     Northeast    1316470     5 0.3798036
## 3        Hawaii  HI          West    1360301     7 0.5145920
## 4  North Dakota  ND North Central     672591     4 0.5947151
## 5          Iowa  IA North Central    3046355    21 0.6893484
## 6         Idaho  ID          West    1567582    12 0.7655102&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the default behavior is to order in ascending order. In &lt;strong&gt;dplyr&lt;/strong&gt;, the function &lt;code&gt;desc&lt;/code&gt; transforms a vector so that it is in descending order. To sort the table in descending order, we can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  arrange(desc(rate))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;nested-sorting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Nested sorting&lt;/h3&gt;
&lt;p&gt;If we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by &lt;code&gt;region&lt;/code&gt;, then within region we order by murder rate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  arrange(region, rate) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state abb    region population total      rate
## 1       Vermont  VT Northeast     625741     2 0.3196211
## 2 New Hampshire  NH Northeast    1316470     5 0.3798036
## 3         Maine  ME Northeast    1328361    11 0.8280881
## 4  Rhode Island  RI Northeast    1052567    16 1.5200933
## 5 Massachusetts  MA Northeast    6547629   118 1.8021791
## 6      New York  NY Northeast   19378102   517 2.6679599&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-top-n&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The top &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;In the code above, we have used the function &lt;code&gt;head&lt;/code&gt; to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the &lt;code&gt;top_n&lt;/code&gt; function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% top_n(5, rate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  state abb        region population total      rate
## 1 District of Columbia  DC         South     601723    99 16.452753
## 2            Louisiana  LA         South    4533372   351  7.742581
## 3             Maryland  MD         South    5773552   293  5.074866
## 4             Missouri  MO North Central    5988927   321  5.359892
## 5       South Carolina  SC         South    4625364   207  4.475323&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that rows are not sorted by &lt;code&gt;rate&lt;/code&gt;, only filtered. If we want to sort, we need to use &lt;code&gt;arrange&lt;/code&gt;.
Note that if the third argument is left blank, &lt;code&gt;top_n&lt;/code&gt; filters by the last column.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the &lt;strong&gt;NHANES&lt;/strong&gt; package. Once you install the &lt;strong&gt;NHANES&lt;/strong&gt; package, you can load the data like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(NHANES)
data(NHANES)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;NHANES&lt;/strong&gt; data has many missing values. The &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sd&lt;/code&gt; functions in R will return &lt;code&gt;NA&lt;/code&gt; if any of the entries of the input vector is an &lt;code&gt;NA&lt;/code&gt;. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(na_example)
mean(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To ignore the &lt;code&gt;NA&lt;/code&gt;s we can use the &lt;code&gt;na.rm&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(na_example, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.301754&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(na_example, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.22338&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now explore the NHANES data.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. &lt;code&gt;AgeDecade&lt;/code&gt; is a categorical variable with these ages. Note that the category is coded like ” 20-29”, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the &lt;code&gt;BPSysAve&lt;/code&gt; variable? Save it to a variable called &lt;code&gt;ref&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hint: Use &lt;code&gt;filter&lt;/code&gt; and &lt;code&gt;summarize&lt;/code&gt; and use the &lt;code&gt;na.rm = TRUE&lt;/code&gt; argument when computing the average and standard deviation. You can also filter the NA values using &lt;code&gt;filter&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Using a pipe, assign the average to a numeric variable &lt;code&gt;ref_avg&lt;/code&gt;. Hint: Use the code similar to above and then &lt;code&gt;pull&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now report the min and max values for the same group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by &lt;code&gt;AgeDecade&lt;/code&gt;. Hint: rather than filtering by age and gender, filter by &lt;code&gt;Gender&lt;/code&gt; and then use &lt;code&gt;group_by&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat exercise 4 for males.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can actually combine both summaries for exercises 4 and 5 into one line of code. This is because &lt;code&gt;group_by&lt;/code&gt; permits us to group by more than one variable. Obtain one big summary table using &lt;code&gt;group_by(AgeDecade, Gender)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For males between the ages of 40-49, compare systolic blood pressure across race as reported in the &lt;code&gt;Race1&lt;/code&gt; variable. Order the resulting table from lowest to highest average systolic blood pressure.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tibbles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tibbles&lt;/h2&gt;
&lt;p&gt;Tidy data must be stored in data frames. We introduced the data frame in Section &lt;a href=&#34;#data-frames&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; and have been using the &lt;code&gt;murders&lt;/code&gt; data frame throughout the book. In Section &lt;a href=&#34;#group-by&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we introduced the &lt;code&gt;group_by&lt;/code&gt; function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% group_by(region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 51 × 6
## # Groups:   region [4]
##    state                abb   region    population total  rate
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt; &amp;lt;fct&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Alabama              AL    South        4779736   135  2.82
##  2 Alaska               AK    West          710231    19  2.68
##  3 Arizona              AZ    West         6392017   232  3.63
##  4 Arkansas             AR    South        2915918    93  3.19
##  5 California           CA    West        37253956  1257  3.37
##  6 Colorado             CO    West         5029196    65  1.29
##  7 Connecticut          CT    Northeast    3574097    97  2.71
##  8 Delaware             DE    South         897934    38  4.23
##  9 District of Columbia DC    South         601723    99 16.5 
## 10 Florida              FL    South       19687653   669  3.40
## # … with 41 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that there are no columns with this information. But, if you look closely at the output above, you see the line &lt;code&gt;A tibble&lt;/code&gt; followd by dimensions. We can learn the class of the returned object using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% group_by(region) %&amp;gt;% class()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;grouped_df&amp;quot; &amp;quot;tbl_df&amp;quot;     &amp;quot;tbl&amp;quot;        &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;tbl&lt;/code&gt;, pronounced tibble, is a special kind of data frame. The functions &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarize&lt;/code&gt; always return this type of data frame. The &lt;code&gt;group_by&lt;/code&gt; function returns a special kind of &lt;code&gt;tbl&lt;/code&gt;, the &lt;code&gt;grouped_df&lt;/code&gt;. We will say more about these later. For consistency, the &lt;strong&gt;dplyr&lt;/strong&gt; manipulation verbs (&lt;code&gt;select&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;mutate&lt;/code&gt;, and &lt;code&gt;arrange&lt;/code&gt;) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble. For example, in Chapter &lt;a href=&#34;#importing-data&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we will see that tidyverse functions used to import data create tibbles.&lt;/p&gt;
&lt;p&gt;Tibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.&lt;/p&gt;
&lt;div id=&#34;tibbles-display-better&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tibbles display better&lt;/h3&gt;
&lt;p&gt;The print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing &lt;code&gt;murders&lt;/code&gt; and the output of murders if we convert it to a tibble. We can do this using &lt;code&gt;as_tibble(murders)&lt;/code&gt;. If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsets-of-tibbles-are-tibbles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsets of tibbles are tibbles&lt;/h3&gt;
&lt;p&gt;If you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders[,4])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is not a data frame. With tibbles this does not happen:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(as_tibble(murders)[,4])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;tbl_df&amp;quot;     &amp;quot;tbl&amp;quot;        &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is useful in the tidyverse since functions require data frames as input.&lt;/p&gt;
&lt;p&gt;With tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor &lt;code&gt;$&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(as_tibble(murders)$population)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write &lt;code&gt;Population&lt;/code&gt; instead of &lt;code&gt;population&lt;/code&gt; this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$Population&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;returns a &lt;code&gt;NULL&lt;/code&gt; with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_tibble(murders)$Population&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Unknown or uninitialised column: `Population`.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tibbles-can-have-complex-entries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tibbles can have complex entries&lt;/h3&gt;
&lt;p&gt;While data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(id = c(1, 2, 3), func = c(mean, median, sd))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 2
##      id func  
##   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;
## 1     1 &amp;lt;fn&amp;gt;  
## 2     2 &amp;lt;fn&amp;gt;  
## 3     3 &amp;lt;fn&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tibbles-can-be-grouped&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tibbles can be grouped&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;group_by&lt;/code&gt; returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the &lt;code&gt;summarize&lt;/code&gt; function, are aware of the group information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-tibble-using-tibble-instead-of-data.frame&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create a tibble using &lt;code&gt;tibble&lt;/code&gt; instead of &lt;code&gt;data.frame&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;It is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the &lt;code&gt;tibble&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grades &amp;lt;- tibble(names = c(&amp;quot;John&amp;quot;, &amp;quot;Juan&amp;quot;, &amp;quot;Jean&amp;quot;, &amp;quot;Yao&amp;quot;),
                     exam_1 = c(95, 80, 90, 85),
                     exam_2 = c(90, 85, 85, 90))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that base R (without packages loaded) has a function with a very similar name, &lt;code&gt;data.frame&lt;/code&gt;, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default &lt;code&gt;data.frame&lt;/code&gt; coerces characters into factors without providing a warning or message:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grades &amp;lt;- data.frame(names = c(&amp;quot;John&amp;quot;, &amp;quot;Juan&amp;quot;, &amp;quot;Jean&amp;quot;, &amp;quot;Yao&amp;quot;),
                     exam_1 = c(95, 80, 90, 85),
                     exam_2 = c(90, 85, 85, 90))
class(grades$names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To avoid this, we use the rather cumbersome argument &lt;code&gt;stringsAsFactors&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grades &amp;lt;- data.frame(names = c(&amp;quot;John&amp;quot;, &amp;quot;Juan&amp;quot;, &amp;quot;Jean&amp;quot;, &amp;quot;Yao&amp;quot;),
                     exam_1 = c(95, 80, 90, 85),
                     exam_2 = c(90, 85, 85, 90),
                     stringsAsFactors = FALSE)
class(grades$names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To convert a regular data frame to a tibble, you can use the &lt;code&gt;as_tibble&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_tibble(grades) %&amp;gt;% class()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;tbl_df&amp;quot;     &amp;quot;tbl&amp;quot;        &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-dot-operator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The dot operator&lt;/h2&gt;
&lt;p&gt;One of the advantages of using the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 &amp;lt;- filter(murders, region == &amp;quot;South&amp;quot;)
tab_2 &amp;lt;- mutate(tab_1, rate = total / population * 10^5)
rates &amp;lt;- tab_2$rate
median(rates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.398069&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can avoid defining any new intermediate objects by instead typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, region == &amp;quot;South&amp;quot;) %&amp;gt;%
  mutate(rate = total / population * 10^5) %&amp;gt;%
  summarize(median = median(rate)) %&amp;gt;%
  pull(median)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.398069&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the &lt;code&gt;pull&lt;/code&gt; function was not available and we wanted to access &lt;code&gt;tab_2$rate&lt;/code&gt;? What data frame name would we use? The answer is the dot operator.&lt;/p&gt;
&lt;p&gt;For example to access the rate vector without the &lt;code&gt;pull&lt;/code&gt; function we could use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rates &amp;lt;-   filter(murders, region == &amp;quot;South&amp;quot;) %&amp;gt;%
  mutate(rate = total / population * 10^5) %&amp;gt;%
  .$rate
median(rates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.398069&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the next section, we will see other instances in which using the &lt;code&gt;.&lt;/code&gt; is useful.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;do&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;, tidyverse functions consistently return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The &lt;code&gt;quantile&lt;/code&gt; function is an example we described in Section &lt;a href=&#34;#summarize&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;. The &lt;code&gt;do&lt;/code&gt; function serves as a bridge between R functions such as &lt;code&gt;quantile&lt;/code&gt; and the tidyverse. The &lt;code&gt;do&lt;/code&gt; function understands grouped tibbles and always returns a data frame.&lt;/p&gt;
&lt;p&gt;In Section &lt;a href=&#34;#summarize&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;, we noted that if we attempt to use &lt;code&gt;quantile&lt;/code&gt; to obtain the min, median and max in one call, we will receive an error: &lt;code&gt;Error: expecting result of length one, got : 2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(heights)
heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  summarize(range = quantile(height, c(0, 0.5, 1)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;do&lt;/code&gt; function to fix this.&lt;/p&gt;
&lt;p&gt;First we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_summary &amp;lt;- function(dat){
  x &amp;lt;- quantile(dat$height, c(0, 0.5, 1))
  tibble(min = x[1], median = x[2], max = x[3])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now apply the function to the heights dataset to obtain the summaries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  my_summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 3
##     min median   max
##   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1    50   68.5  82.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this is not what we want. We want a summary for each sex and the code returned just one summary. This is because &lt;code&gt;my_summary&lt;/code&gt; is not part of the tidyverse and does not know how to handled grouped tibbles. &lt;code&gt;do&lt;/code&gt; makes this connection:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  do(my_summary(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 4
## # Groups:   sex [2]
##   sex      min median   max
##   &amp;lt;fct&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Female    51   65.0  79  
## 2 Male      50   69    82.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that here we need to use the dot operator. The tibble created by &lt;code&gt;group_by&lt;/code&gt; is piped to &lt;code&gt;do&lt;/code&gt;. Within the call to &lt;code&gt;do&lt;/code&gt;, the name of this tibble is &lt;code&gt;.&lt;/code&gt; and we want to send it to &lt;code&gt;my_summary&lt;/code&gt;. If you do not use the dot, then &lt;code&gt;my_summary&lt;/code&gt; has &lt;em&gt;no argument&lt;/em&gt; and returns an error telling us that &lt;code&gt;argument &#34;dat&#34;&lt;/code&gt; is missing. You can see the error by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  do(my_summary())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not use the parenthesis, then the function is not executed and instead &lt;code&gt;do&lt;/code&gt; tries to return the function. This gives an error because &lt;code&gt;do&lt;/code&gt; must always return a data frame. You can see the error by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  do(my_summary)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-purrr-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;strong&gt;purrr&lt;/strong&gt; package&lt;/h2&gt;
&lt;p&gt;In Section &lt;a href=&#34;#vectorization&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we learned about the &lt;code&gt;sapply&lt;/code&gt; function, which permitted us to apply the same function to each element of a vector. We constructed a function and used &lt;code&gt;sapply&lt;/code&gt; to compute the sum of the first &lt;code&gt;n&lt;/code&gt; integers for several values of &lt;code&gt;n&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_s_n &amp;lt;- function(n){
  x &amp;lt;- 1:n
  sum(x)
}
n &amp;lt;- 1:25
s_n &amp;lt;- sapply(n, compute_s_n)
s_n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]   1   3   6  10  15  21  28  36  45  55  66  78  91 105 120 136 153 171 190
## [20] 210 231 253 276 300 325&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The &lt;strong&gt;purrr&lt;/strong&gt; package includes functions similar to &lt;code&gt;sapply&lt;/code&gt; but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, &lt;code&gt;sapply&lt;/code&gt; can return several different object types; for example, we might expect a numeric result from a line of code, but &lt;code&gt;sapply&lt;/code&gt; might convert our result to character under some circumstances. &lt;strong&gt;purrr&lt;/strong&gt; functions will never do this: they will return objects of a specified type or return an error if this is not possible.&lt;/p&gt;
&lt;p&gt;The first &lt;strong&gt;purrr&lt;/strong&gt; function we will learn is &lt;code&gt;map&lt;/code&gt;, which works very similar to &lt;code&gt;sapply&lt;/code&gt; but always, without exception, returns a list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr) # or library(tidyverse)
n &amp;lt;- 1:25
s_n &amp;lt;- map(n, compute_s_n)
class(s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want a numeric vector, we can instead use &lt;code&gt;map_dbl&lt;/code&gt; which always returns a vector of numeric values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_n &amp;lt;- map_dbl(n, compute_s_n)
class(s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This produces the same results as the &lt;code&gt;sapply&lt;/code&gt; call shown above.&lt;/p&gt;
&lt;p&gt;A particularly useful &lt;strong&gt;purrr&lt;/strong&gt; function for interacting with the rest of the tidyverse is &lt;code&gt;map_df&lt;/code&gt;, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a &lt;code&gt;Argument 1 must have names&lt;/code&gt; error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_n &amp;lt;- map_df(n, compute_s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to change the function to make this work:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_s_n &amp;lt;- function(n){
  x &amp;lt;- 1:n
  tibble(sum = sum(x))
}
s_n &amp;lt;- map_df(n, compute_s_n)
head(s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 1
##     sum
##   &amp;lt;int&amp;gt;
## 1     1
## 2     3
## 3     6
## 4    10
## 5    15
## 6    21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;purrr&lt;/strong&gt; package provides much more functionality not covered here. For more details you can consult &lt;a href=&#34;https://jennybc.github.io/purrr-tutorial/&#34;&gt;this online resource&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidyverse-conditionals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidyverse conditionals&lt;/h2&gt;
&lt;p&gt;A typical data analysis will often involve one or more conditional operations. In Section &lt;a href=&#34;#conditionals&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we described the &lt;code&gt;ifelse&lt;/code&gt; function, which we will use extensively in this book. In this section we present two &lt;strong&gt;dplyr&lt;/strong&gt; functions that provide further functionality for performing conditional operations.&lt;/p&gt;
&lt;div id=&#34;case_when&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;case_when&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;case_when&lt;/code&gt; function is useful for vectorizing conditional statements. It is similar to &lt;code&gt;ifelse&lt;/code&gt; but can output any number of values, as opposed to just &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;. Here is an example splitting numbers into negative, positive, and 0:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(-2, -1, 0, 1, 2)
case_when(x &amp;lt; 0 ~ &amp;quot;Negative&amp;quot;,
          x &amp;gt; 0 ~ &amp;quot;Positive&amp;quot;,
          x == 0  ~ &amp;quot;Zero&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Negative&amp;quot; &amp;quot;Negative&amp;quot; &amp;quot;Zero&amp;quot;     &amp;quot;Positive&amp;quot; &amp;quot;Positive&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: &lt;em&gt;New England&lt;/em&gt;, &lt;em&gt;West Coast&lt;/em&gt;, &lt;em&gt;South&lt;/em&gt;, and &lt;em&gt;other&lt;/em&gt;. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign &lt;em&gt;other&lt;/em&gt;. Here is how we use &lt;code&gt;case_when&lt;/code&gt; to do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  mutate(group = case_when(
    abb %in% c(&amp;quot;ME&amp;quot;, &amp;quot;NH&amp;quot;, &amp;quot;VT&amp;quot;, &amp;quot;MA&amp;quot;, &amp;quot;RI&amp;quot;, &amp;quot;CT&amp;quot;) ~ &amp;quot;New England&amp;quot;,
    abb %in% c(&amp;quot;WA&amp;quot;, &amp;quot;OR&amp;quot;, &amp;quot;CA&amp;quot;) ~ &amp;quot;West Coast&amp;quot;,
    region == &amp;quot;South&amp;quot; ~ &amp;quot;South&amp;quot;,
    TRUE ~ &amp;quot;Other&amp;quot;)) %&amp;gt;%
  group_by(group) %&amp;gt;%
  summarize(rate = sum(total) / sum(population) * 10^5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 × 2
##   group        rate
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 New England  1.72
## 2 Other        2.71
## 3 South        3.63
## 4 West Coast   2.90&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;between&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;between&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;A common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector &lt;code&gt;x&lt;/code&gt; are between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; we can type&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;gt;= a &amp;amp; x &amp;lt;= b&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this can become cumbersome, especially within the tidyverse approach. The &lt;code&gt;between&lt;/code&gt; function performs the same operation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;between(x, a, b)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the &lt;code&gt;murders&lt;/code&gt; dataset. Which of the following is true?&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; is in tidy format and is stored in a tibble.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; is in tidy format and is stored in a data frame.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; is not in tidy format and is stored in a tibble.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; is not in tidy format and is stored in a data frame.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use &lt;code&gt;as_tibble&lt;/code&gt; to convert the &lt;code&gt;murders&lt;/code&gt; data table into a tibble and save it in an object called &lt;code&gt;murders_tibble&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;group_by&lt;/code&gt; function to convert &lt;code&gt;murders&lt;/code&gt; into a tibble that is grouped by region.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write tidyverse code that is equivalent to this code:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(mean(log(murders$population)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Write it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with &lt;code&gt;murders %&amp;gt;%&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use the &lt;code&gt;map_df&lt;/code&gt; to create a data frame with three columns named &lt;code&gt;n&lt;/code&gt;, &lt;code&gt;s_n&lt;/code&gt;, and &lt;code&gt;s_n_2&lt;/code&gt;. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; the row number.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-video&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lecture Video&lt;/h2&gt;
&lt;p&gt;All videos are in the SSC442 Mediaspace channel &lt;a href=&#34;https://mediaspace.msu.edu/channel/SSC442+-+Spring+2021+-+KIRKPATRICK/199607633/subscribe&#34;&gt;available here &lt;i class=&#34;fas fa-film&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;!---
# Videos

`
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/D6WqHA8TDWQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
`{=html}
--&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;I discovered the &lt;code&gt;emo::ji()&lt;/code&gt; function at 8:55am. My wife joked that I would find a way to use the poop emoji by 9:00am. It is now 8:59am. She was right.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;If you have not installed this package already, you must use &lt;code&gt;install.packages(&#34;tidyverse&#34;)&lt;/code&gt; prior to the &lt;code&gt;library()&lt;/code&gt; call you see below.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Syllabus</title>
      <link>https://ssc442.netlify.app/syllabus/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/syllabus/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/syllabus/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#office-hours&#34;&gt;Office Hours&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#contacting-me&#34;&gt;Contacting Me&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-this-course-and-can-should-you-take-it&#34;&gt;What is This Course and Can / Should You Take It?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-this-course-is-not&#34;&gt;What This Course is Not&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#success-in-this-course&#34;&gt;Success in this Course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#course-materials&#34;&gt;Course materials&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-and-rstudio&#34;&gt;R and RStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#online-help&#34;&gt;Online Help&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluations-and-grades&#34;&gt;Evaluations and Grades&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#class-participation&#34;&gt;Class Participation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#academic-honesty&#34;&gt;Academic honesty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grading&#34;&gt;Grading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resources&#34;&gt;Resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#accommodations&#34;&gt;Accommodations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mental-health-and-wellbeing&#34;&gt;Mental Health and Wellbeing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mandated-reporting&#34;&gt;Mandated Reporting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#miscellanea&#34;&gt;Miscellanea&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ta-office-hours&#34;&gt;TA Office Hours&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-office-hours&#34;&gt;Using Office Hours&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#letters-of-recommendation-references&#34;&gt;Letters of Recommendation / References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;div class=&#34;row&#34;&gt;
    &lt;div class=&#34;col-md-4&#34;&gt;
        &lt;h3&gt;Instructor&lt;/h3&gt;

        &lt;ul class=&#34;icon-list&#34;&gt;
            &lt;li&gt;&lt;i class=&#34;fas fa-user&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://www.benbushong.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; Prof. Ben Bushong&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;i class=&#34;fas fa-university&#34;&gt;&lt;/i&gt; 25A Marshall-Adams Hall&lt;/li&gt;
            &lt;li&gt;&lt;i class=&#34;fas fa-envelope&#34;&gt;&lt;/i&gt; &lt;a href=&#34;mailto:bbushong@msu.edu&#34;&gt; bbushong@msu.edu&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;i class=&#34;fab fa-twitter&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://twitter.com/benbushong&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; @benbushong&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
    &lt;/div&gt;

    &lt;div class=&#34;col-md-4&#34;&gt;
        &lt;h3&gt;Course details&lt;/h3&gt;

        &lt;ul class=&#34;icon-list&#34;&gt;
            &lt;li&gt;&lt;i class=&#34;far fa-calendar&#34;&gt;&lt;/i&gt; Tuesday and Thursday&lt;/li&gt;
            &lt;li&gt;&lt;i class=&#34;far fa-calendar-alt&#34;&gt;&lt;/i&gt; September – December, 2021&lt;/li&gt;
            &lt;li&gt;&lt;i class=&#34;far fa-clock&#34;&gt;&lt;/i&gt; 12:40pm - 2:00pm&lt;/li&gt;
            
            &lt;li&gt;&lt;i class=&#34;fab fa-slack&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://join.slack.com/t/ssc442/shared_invite/zt-v3n7r8hh-TjwMzpDLBSywvVYwzftqXA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
    &lt;/div&gt;

    &lt;div class=&#34;col-md-4 contact-policy&#34;&gt;
        &lt;h3&gt;Contacting me&lt;/h3&gt;

        &lt;p&gt;Please consider whether your question is short and concrete; if so, feel free to email me. If your question is deep, vague, interesting, or otherwise complex, please come to office hours or we can discuss in class. See syllabus for details.&lt;/p&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;div id=&#34;office-hours&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Office Hours&lt;/h3&gt;
&lt;p&gt;My office hours are 4:15 - 5:15 on Thursdays. Zoom link &lt;a href=&#34;https://msu.zoom.us/j/2711311082&#34;&gt;here&lt;/a&gt; (Passcode: GODUCKS).&lt;/p&gt;
&lt;p&gt;The TA’s office hours are 10:15 - 11:15 on Wednesdays. Zoom link &lt;a href=&#34;https://us04web.zoom.us/j/71454731119?pwd=WHFWNFJSR3U4WE0zQThIWS9RSlZyUT09&#34;&gt;here&lt;/a&gt; (Passcode: officehour)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;contacting-me&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Contacting Me&lt;/h3&gt;
&lt;p&gt;I have moved this up front in a (likely unsuccessful) attempt to minimize our collective headache.&lt;/p&gt;
&lt;p&gt;Email is a blessing and a curse. Instant communication is wonderful, but often email is the wrong medium to have a productive conversation about course material. Moreover, I get a &lt;strong&gt;lot&lt;/strong&gt; of emails. This means that I am frequently triaging emails into two piles: “my house is burning down” and “everything else”. Your email is unlikely to make the former pile. So, asking questions about course material is always best done in-class or in office hours. Students always roll their eyes when professors say things like that, but it’s true that if you have a question, it’s very likely someone else has the same question.&lt;/p&gt;
&lt;p&gt;That said, it benefits us both if any emails you send are clear and effective. There’s an (unfunny) joke in academia that professors (i) read an email until they find a question; (ii) respond to that question and; (iii) ignore the rest of the email. I won’t do this, but I think it is helpful to assume that the person on the receiving end of an email will operate this way.&lt;/p&gt;
&lt;p&gt;Some general tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Always&lt;/strong&gt; include &lt;code&gt;[SSC442]&lt;/code&gt; in your subject line (brackets included).&lt;/li&gt;
&lt;li&gt;Use a short but informative subject line. For example: &lt;code&gt;[SSC442] Final Project Grading&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Use your University-supplied email for University business. This helps me know who you are.&lt;/li&gt;
&lt;li&gt;One topic, one email. If you have multiple things to discuss, and you anticipate followup replies, it is best to split them into two emails so that the threads do not get cluttered.&lt;/li&gt;
&lt;li&gt;Ask direct questions. If you’re asking multiple questions in one email, use a bulleted list.&lt;/li&gt;
&lt;li&gt;Don’t ask questions that are answered by reading the syllabus. This drives me nuts.&lt;/li&gt;
&lt;li&gt;I’ve also found that students are overly polite in emails. I suppose it may be intimidating to email a professor, and you should try to match the style that the professor prefers, but I view email for a course as a casual form of communication. Said another way: get to the point. Students often send an entire paragraph introducing themselves, but if you use your University email address, and add the course name in the subject, I will already know who you are. Here’s an example of an excellent email:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Subject: [SSC442] Lab, Question 2, Typo&lt;/p&gt;
&lt;p&gt;Hi Prof. Bushong,&lt;/p&gt;
&lt;p&gt;There seems to be a typo in the Lab on Question 2. The problem says to use a column of data that doesn’t seem to exist. Can you correct this or which should we use?&lt;/p&gt;
&lt;p&gt;Thanks,
Student McStudentFace&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now on to your regularly scheduled syllabus.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-this-course-and-can-should-you-take-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is This Course and Can / Should You Take It?&lt;/h2&gt;
&lt;p&gt;Innovations in statistical learning have created many engineering breakthroughs. From real time voice recognition to automatic categorization (and in some cases production) of news stories, machine learning is transforming the way we live our lives. These techniques are, at their heart, novel ways to work with data, and therefore they should have implications for social science. This course explores the intersection of statistical learning (or machine learning) and social science and aims to answer two primary questions about these new techniques:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-roman&#34;&gt;
&lt;li&gt;&lt;p&gt;How does statistical learning work and what kinds of statistical guarantees can be made about the performance of statistical-learning algorithms?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How can statistical learning be used to answer questions that interest social science researchers, such as testing theories or improving social policy?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to address these questions, we will cover so-called “standard” techniques such as supervised and unsupervised learning, statistical learning theory and nonparametric and Bayesian approaches. If it were up to me, this course would be titled “Statistical Learning for Social Scientists”—I believe this provides a more appropriate guide to the content of this course. And while this class will cover these novel statistical methodologies in some detail, it is not a substitute for the appropriate class in Computer Science or Statistics. Nor is this a class that teaches specific skills for the job market. Rather, this class will teach you to think about data analytics broadly. We will spend a great deal of time learning how to interpret the output of statistical learning algorithms and approaches, and will also spend a great deal of time on better understanding the basic ideas in statistical learning. This, of course, comes at some cost in terms of time spent on learning computational and/or programming skills.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enrollment for credit in this course is simply not suitable for those unprepared in or uninterested in elementary statistical theory no matter the intensity of interest in machine learning or “Big Data”. Really.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You will be required to understand elementary mathematics in this course and should have at least some exposure to statistical theory. The class is front-loaded technically: early lectures are more mathematically oriented, while later lectures are more applied.&lt;/p&gt;
&lt;p&gt;The topics covered in this course are listed later in this document. I will assign readings sparingly from &lt;a href=&#34;https://www.statlearning.com/&#34;&gt;Introduction to Statistical Learning&lt;/a&gt;, henceforth referred to as ISL. This text is available for free online and, for those who like physical books, can be purchased for about $25. Importantly, the lectures deviate a fair bit from the reading, and thus you will rely on your course notes more than you might in other classes.&lt;/p&gt;
&lt;p&gt;If—after you have read this document and preferably after attending the first lecture—you have any questions about whether this course is appropriate for you, please come talk to me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-this-course-is-not&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What This Course is Not&lt;/h2&gt;
&lt;p&gt;The focus of this course is conceptual. The goal is to create a working understanding of when and how tools from computer science and statistics can be profitably applied to problems in social science. Though students will be required to apply some of these techniques themselves, this course is not…&lt;/p&gt;
&lt;p&gt;&lt;em&gt;…a replacement for EC420 or a course in causal inference.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As social scientists, we are most often concerned with causal inference in order to analyze and write policies. Statistical learning and the other methods we will discuss in this course are generally not well-suited to these problems, and while I’ll give a short overview of standard methods, this is only to build intuitions. Ultimately, this course has a different focus and you should still pursue standard methodological insights from your home departments.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;…a course on the computational aspects of the underlying methods.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are many important innovations that have made machine learning techniques computationally feasible. We will not discuss these, as there are computer science courses better equipped to cover them. When appropriate, we will discuss whether something &lt;strong&gt;is&lt;/strong&gt; computable, and we will even give rough approximations of the amount of time required (e.g. &lt;strong&gt;P&lt;/strong&gt; vs &lt;strong&gt;NP&lt;/strong&gt;). But we will not discuss how optimizers work or best practices in programming.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;…a primer on the nitty-gritty of how to use these tools or a way to pad your resume.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The mechanics of implementation, whether it be programming languages or learning to use APIs, will not be covered in any satisfying level of depth. Students will be expected to learn most of the programming skills on their own. Specifically, while there will be some material to remind you of basic &lt;code&gt;R&lt;/code&gt; commands, this is not a good course for people who are simply looking to learn the mechanics of programming. This course is designed to get you to use both traditional analytics and, eventually, machine learning tools. We will do some review of basic programming, and you will have the opportunity to explore topics that interest you through a final project, but ultimately this is a course that largely focuses on the theoretical and practical aspects of statistical learning as applied to social science and &lt;strong&gt;not&lt;/strong&gt; a class on programming.&lt;/p&gt;
&lt;p&gt;Perhaps most importantly, this course is an attempt to push undergraduate education toward the frontiers in social science. Accordingly, please allow some messiness. Some topics may be underdeveloped for a given person’s passions, but given the wide variety of technical skills and overall interests, this is a near certainty. Both the challenge and opportunity of this area comes from the fact that there is no fully developed, wholly unifying framework. Our collective struggle—me from teaching, you from learning—will ultimately bear fruit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;success-in-this-course&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Success in this Course&lt;/h2&gt;
&lt;p&gt;I &lt;em&gt;promise&lt;/em&gt;, you are equipped to succeed in this course.&lt;/p&gt;
&lt;p&gt;Learning &lt;code&gt;R&lt;/code&gt; can be difficult at first. Like learning a new language—Spanish, French, or Mandarin—it takes dedication and perseverance. Hadley Wickham (the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like) &lt;strong&gt;ggplot2&lt;/strong&gt;—&lt;a href=&#34;https://r-posts.com/advice-to-young-and-old-programmers-a-conversation-with-hadley-wickham/&#34;&gt;made this wise observation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Even experienced programmers (like me) find themselves bashing their heads against seemingly intractable errors.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; If you’re finding yourself bashing your head against a wall and not making progress, try the following. First, take a break. Sometimes you just need space to see an error. Next, talk to classmates. Finally, if you genuinely cannot see the solution, e-mail the TA. But, honestly, it’s probably just a typo.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/syllabus/hosrt_error_tweet.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/allison_horst/status/1213275783675822080&#34;&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/syllabus/gator_error.jpg&#34; alt=&#34;Alison Horst: Gator error&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;course-materials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Course materials&lt;/h2&gt;
&lt;p&gt;The course website can be found at &lt;a href=&#34;https://ssc442.netlify.app&#34;&gt;https://ssc442.netlify.app&lt;/a&gt; (but you know that. You’re on it right now.)&lt;/p&gt;
&lt;p&gt;All of the readings and software in this class are &lt;strong&gt;free&lt;/strong&gt;. There are free online version of all the texts including &lt;a href=&#34;https://www.statlearning.com/&#34;&gt;&lt;strong&gt;Introduction to Statistical Learning&lt;/strong&gt;&lt;/a&gt; and &lt;code&gt;R&lt;/code&gt; / RStudio are free. (Don’t pay for RStudio.) We will reference outside readings and there exist paper versions of some “books” but you won’t need to buy anything&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;r-and-rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R and RStudio&lt;/h3&gt;
&lt;p&gt;You will do all of your analysis with the open source (and free!) programming language &lt;a href=&#34;https://cran.r-project.org/&#34;&gt;&lt;code&gt;R&lt;/code&gt;&lt;/a&gt;. You will use &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt; as the main program to access R. &lt;strong&gt;I find it helpful to think of &lt;code&gt;R&lt;/code&gt; as an engine and RStudio as a car dashboard&lt;/strong&gt;—&lt;code&gt;R&lt;/code&gt; handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is free, but it can sometimes be a pain to install and configure. To make life easier, you can use the free &lt;a href=&#34;http://rstudio.cloud/&#34;&gt;RStudio.cloud&lt;/a&gt; service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with &lt;code&gt;R&lt;/code&gt;. We recommend this for those who may be switching between computers and are trying to get some work done. That said, while RStudio.cloud is convenient, it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis and graphics. You also can’t use your own custom fonts with RStudio.cloud.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; And, generally speaking, you should have (from the prerequisite course) sufficient experience to make your &lt;code&gt;R&lt;/code&gt; work. If not, over the course of the semester, you’ll probably want to get around to installing &lt;code&gt;R&lt;/code&gt;, RStudio, and other &lt;code&gt;R&lt;/code&gt; packages on your computer and wean yourself off of RStudio.cloud. If you plan on making a career out of data science, you should consider this a necessary step.&lt;/p&gt;
&lt;p&gt;You can &lt;a href=&#34;https://ssc442.netlify.app/resource/install/&#34;&gt;find instructions for installing &lt;code&gt;R&lt;/code&gt;, RStudio, and all the tidyverse packages here.&lt;/a&gt; And you may find some other goodies.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;online-help&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Online Help&lt;/h3&gt;
&lt;p&gt;Data science and statistical programming can be difficult. Computers are stupid: they do only what you ask, not what you intend. This means that little errors in your code can cause hours of headache, even if you’ve been doing this stuff for years!&lt;/p&gt;
&lt;p&gt;Fortunately there are tons of online resources to help you with this. Two of the most important are &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;StackOverflow&lt;/a&gt; (a Q&amp;amp;A site with hundreds of thousands of answers to all sorts of programming questions) and &lt;a href=&#34;https://community.rstudio.com/&#34;&gt;RStudio Community&lt;/a&gt; (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).&lt;/p&gt;
&lt;p&gt;If you use Twitter, you can try posting R-related questions and content with &lt;a href=&#34;https://twitter.com/search?q=%23rstats&#34;&gt;#rstats&lt;/a&gt;. The community there is exceptionally generous and helpful.&lt;/p&gt;
&lt;p&gt;Searching for help with &lt;code&gt;R&lt;/code&gt; on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Likewise, whenever using a specific package, try searching for that package name instead of the letter “r” (e.g. “ggplot scatterplot”). Good, concise searches are generally more effective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Help with Using R&lt;/strong&gt;: There are some excellent additional tutorials on R available through &lt;a href=&#34;https://rstudio.cloud/learn/primers&#34;&gt;Rstudio Clould Primers&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluations-and-grades&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluations and Grades&lt;/h2&gt;
&lt;p&gt;Your grade in this course will be based on attendance/participation, labs, weekly writings, and a final project.&lt;/p&gt;
&lt;p&gt;The general breakdown will be approximately 55% for labs, participation, and weekly writings, and 45% for projects (see below for specific details). The primary focus of the course is a final project; this requires two “mini-projects” to ensure you’re making satisfactory progress. Assignment of numeric grades will follow the standard, where ties (e.g., 91.5%) are rounded to favor the student. Evaluations (read: grades) are designed not to deter anyone from taking this course who might otherwise be interested, but will be taken seriously.&lt;/p&gt;
&lt;p&gt;Weekly writings are intended to be an easy way to get some points. Labs will be short homework assignments that require you to do something practical using a basic statistical language. Support will be provided for the &lt;code&gt;R&lt;/code&gt; language only. You must have access to computing resources and the ability to program basic statistical analyses. As mentioned above, this course will not teach you how to program or how to write code in a specific language. If you are unprepared to do implement basic statistical coding, please take (or retake) PLS202. I highly encourage seeking coding advice from those who instruct computer science courses – it’s their job and they are better at it than I am. I’ll try to provide a good service, but I’m really not an expert in computer science.&lt;/p&gt;
&lt;p&gt;More in-depth descriptions for all the assignments are on the &lt;a href=&#34;https://ssc442.netlify.app/assignment/&#34;&gt;assignments page&lt;/a&gt;. As the course progresses, the assignments themselves will be posted within that page.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To Recap:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;centered-table&#34;&gt;
&lt;table style=&#34;width:68%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;41%&#34; /&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;13%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Assignment&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Points&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Percent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Class Participation&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Weekly Writings (11 x 10),
drop lowest&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;20%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Labs (11 x 15), drop lowest&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;150&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;30%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mini project 1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mini project 2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;10%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Final project&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;130&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;26%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Total&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;500&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;—&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;centered-table&#34;&gt;
&lt;table style=&#34;width:50%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;13%&#34; /&gt;
&lt;col width=&#34;11%&#34; /&gt;
&lt;col width=&#34;13%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Grade&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Range&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Grade&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Range&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;4.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;92-100%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;72-76%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;87-91%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;67-72%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;3.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;82-87%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;62-67%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2.5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;77-81%&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;0.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;bad-66%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;class-participation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Class Participation&lt;/h3&gt;
&lt;p&gt;Participation can take many forms. The bare minimum can best be described as “showing your presence and having some engagement.” To encourage some form of participation, I will often pose questions to the class. I am not above bribery - your response to these extra credit questions will earn extra credit points, up to 5, for participation. Thus, you can easily pad your score by (1) meeting the minimum participation requirements such that I know you are present, and (2) earning extra credit by responding to in-class extra credit prompts. I will clearly state which questions are extra credit. When it comes to participation, wrong answers get the same credit as right answers. We are here to learn. If you knew everything already, you wouldn’t be in the class.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;academic-honesty&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Academic honesty&lt;/h3&gt;
&lt;p&gt;Violation of &lt;a href=&#34;http://asmsu.msu.edu/home/initiatives/spartan-code-of-honor/&#34;&gt;MSU’s Spartan Code of Honor&lt;/a&gt; will result in a grade of 0.0 in the course. Moreover, I am required by MSU policy to report suspected cases of academic dishonesty for possible disciplinary action.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grading&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Grading&lt;/h3&gt;
&lt;p&gt;All grades are considered final. Any request for a re-grade beyond simple point-tallying mistakes will require that the entire assignment be re-graded. Any points previously awarded may be changed in either direction in the re-grade.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;p&gt;Mental health concerns or stressful events may lead to diminished academic performance or reduce a student’s ability to participate in daily activities. Services are available to assist you with addressing these and other concerns you may be experiencing. You can learn more about the broad range of confidential mental health services available on campus via the Counseling &amp;amp; Psychiatric Services (CAPS) website at www.caps.msu.edu.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accommodations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Accommodations&lt;/h2&gt;
&lt;p&gt;This class is designed to be fairly accomodating without a student asking. However, if you need a special accommodation for a disability, religious observance, or have any other concerns about your ability to perform well in this course, please contact me immediately so that we can discuss the issue and make appropriate arrangements.&lt;/p&gt;
&lt;p&gt;Michigan State University is committed to providing equal opportunity for participation in all programs, services and activities. Requests for accommodations by persons with disabilities may be made by contacting the Resource Center for Persons with Disabilities at 517-884-RCPD or on the web at &lt;a href=&#34;rcpd.msu.edu&#34;&gt;here&lt;/a&gt;. Once your eligibility for an accommodation has been determined, you will be issued a verified individual services accommodation (“VISA”) form. Please present this form to me at the start of the term and/or two weeks prior to the accommodation date (test, project, etc). Requests received after this date will be honored whenever possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mental-health-and-wellbeing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mental Health and Wellbeing&lt;/h2&gt;
&lt;p&gt;Things for you might be especially hard right now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I’m fully committed to making sure that you learn everything you were hoping to learn from this class.&lt;/strong&gt; I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Given the challenges of the past two years, that flexibility and leniency is intensified.&lt;/p&gt;
&lt;p&gt;If you feel like you’re behind or not understanding everything, &lt;strong&gt;do not suffer in silence.&lt;/strong&gt; &lt;em&gt;Please&lt;/em&gt; contact me. I’m available at &lt;a href=&#34;mailto:bbushong@msu.edu&#34;&gt;e-mail&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mandated-reporting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mandated Reporting&lt;/h2&gt;
&lt;p&gt;Essays, journals, and other materials submitted for this class are generally considered confidential pursuant to the University’s student record policies. However, students should be aware that University employees, including instructors, may not be able to maintain confidentiality when it conflicts with their responsibility to report certain issues to protect the health and safety of MSU community members and others. As the instructor, I must report the following information to other University offices (including the Department of Police and Public Safety) if you share it with me:
• Suspected child abuse/neglect, even if this maltreatment happened when you were a child;
• Allegations of sexual assault, relationship violence, stalking, or sexual harassment; and
• Credible threats of harm to oneself or to others.
These reports may trigger contact from a campus official who will want to talk with you about the incident that you have shared. In almost all cases, it will be your decision whether you wish to speak with that individual. If you would like to talk about these events in a more confidential setting, you are encouraged to make an appointment with the MSU Counseling and Psychiatric Services.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowledgements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This syllabus and course structure was developed in tandem with Prof. Justin Kirkpatrick. All credit goes to Prof. Kirkpatrick; all errors are my own.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;miscellanea&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Miscellanea&lt;/h2&gt;
&lt;p&gt;D2L will be used sparingly for submission of weekly writings and assignments and distribution of grades.&lt;/p&gt;
&lt;div id=&#34;ta-office-hours&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;TA Office Hours&lt;/h3&gt;
&lt;p&gt;Our TA, Xueshi Wang, has generously offered to host (digital) office hours on Wednesdays between 10:10 and 11:10. Zoom link below. She can be contacted via email using wangxu36 @ msu.edu&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-office-hours&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using Office Hours&lt;/h3&gt;
&lt;p&gt;Please use my office hours. It would be remarkable if you didn’t need some assistance with the material, and I am here to help. One of the benefits of open office hours is to accommodate many students at once; if fellow students are “in my office”, please join in and feel very free to show up in groups. Office hours will move around a little bit throughout the semester to attempt to meet the needs of all students.&lt;/p&gt;
&lt;p&gt;In addition to drop-in office hours, I always have sign-up office hours for advising and other purposes. As a general rule, please first seek course-related help from the drop-in office hours. However, if my scheduled office hours are always infeasible for you, let me know, and then I may encourage you to make appointments with me. I ask that you schedule your studying so that you are prepared to ask questions during office hours – office hours are not a lecture and if you’re not prepared with questions we will end up awkwardly staring at each other for an hour until you leave.&lt;/p&gt;
&lt;p&gt;Some gentle requests regarding office hours and on contacting me. First, my office hours end sharply at the end, so don’t arrive 10 minutes before the scheduled end and expect a full session. Please arrive early if you have lengthy questions, or if you don’t want to risk not having time due to others’ questions. You are free to ask me some stuff by e-mail, (e.g. a typo or something on a handout), but please know e-mail sucks for answering many types of questions. “How do I do this lab?” or “What’s up with &lt;code&gt;R&lt;/code&gt;?” are short questions with long answers. Come to office hours.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;letters-of-recommendation-references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Letters of Recommendation / References&lt;/h3&gt;
&lt;p&gt;If you are applying for further study or another pursuit that requires letters of recommendation and you’d like me to recommend you, I will be glad to write a letter on your behalf if your final grade is a 4.0. Grades below a 4.0 may be handled on a case-by-case basis. In addition, you should have held at least three substantial conversations with me about the course material or other academic subjects over the course of the semester.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;By the end of the course, you will realize that 1) I make many many many errors; 2) that I frequently cannot remember a command or the correct syntax; and 3) that none of this matters too much in the big picture because I know the broad approaches I’m trying to take and I know how to Google stuff. Learn from my idiocy.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;If you’ve got money to burn, you can buy me a burrito.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;This bothers me way more than it should.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;So just don’t cheat or plagiarize. This is an easy problem to avoid.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Final project</title>
      <link>https://ssc442.netlify.app/assignment/final-project/</link>
      <pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/final-project/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#teams&#34;&gt;Teams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#suggested-outline&#34;&gt;Suggested outline&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#theory-and-background&#34;&gt;Theory and Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-and-analyses&#34;&gt;Data and Analyses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;requirements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;p&gt;Data analytics is inherently a hands-on endeavor. Thus, the final project for this class is hands-on. As per the overview page, the final project has the following elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For your final project in this class, you will analyze &lt;strong&gt;existing data&lt;/strong&gt; in some area of interest to you.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Aggregating data from multiple sources is encouraged, but is not required.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;You must visualize (at least) three &lt;strong&gt;interesting&lt;/strong&gt; features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.[^4]&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You will submit &lt;strong&gt;three things&lt;/strong&gt; via D2L:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A PDF of your report (see the outline below for details of what this needs to contain) rendered from your R Markdown. You might want to write the prose-heavy sections in a word processor like Word or Google Docs and copy/paste the text into your &lt;code&gt;R&lt;/code&gt; Markdown document, since RStudio doesn’t have a nice spell checker or grammar checker. This should have &lt;em&gt;no visible &lt;code&gt;R&lt;/code&gt; code, warnings, or messages in it&lt;/em&gt;. To do this, you must set &lt;code&gt;echo = FALSE&lt;/code&gt; in the code chunk options &lt;code&gt;knitr::opts_chunk$set(echo = FALSE, ...)&lt;/code&gt; at the beginning of your document template before you knit.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The same PDF as above, but with all the R code in it (set &lt;code&gt;echo = TRUE&lt;/code&gt; at the beginning of your document and reknit the file). Please label files in an obvious way.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A CSV file of your data; or a link to the data online if your code pulls from the internet. This must be a separate file titled “data.csv” or “data.txt” as applicable.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This project is due by &lt;strong&gt;11:59 PM on Tuesday, April 27th, 2021.&lt;/strong&gt; &lt;span style=&#34;color: #81056F; font-weight: bold&#34;&gt; No late work will be accepted. For real. MSU has grading deadlines and I’ve given you every second that can be spared.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is no final exam. This project is your final exam.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The project will &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Technical skills: Was the project easy? Does it showcase mastery of data analysis? (20%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visual design: Was the information smartly conveyed and usable? Was it beautiful? (25%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analytic design: Was the analysis appropriate? Was it sensible, given the dataset? (20%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Story: Did we learn something? (25%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Following instructions: Did you surpress &lt;code&gt;R&lt;/code&gt; code as asked? Did you submit a separate datafile and label it correctly? (10%)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;teams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Teams&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;My team sucks; how can I punish them for their lack of effort?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;On this front, we will be more supportive. While you have to put up with your team regardless of their quality, you can indicate that your team members are not carrying their fair share by issuing a &lt;strong&gt;strike&lt;/strong&gt;. This processs works as follows:
1. A team member systematically fails to exert effort on collaborative projects (for example, by not showing up for meetings or not communicating, or by simply leeching off others without contributing.)
2. Your frustration reaches a boiling point. You decide this has to stop. You decide to issue a &lt;strong&gt;strike&lt;/strong&gt;
3. You send an email with the following information:
- &lt;code&gt;Subject line:&lt;/code&gt; [SSC442] Strike against [Last name of Recipient]
- &lt;code&gt;Body:&lt;/code&gt; You do &lt;strong&gt;not&lt;/strong&gt; need to provide detailed reasoning. However, you must discuss the actions (plural) you took to remedy the situation before sending the strike email.&lt;/p&gt;
&lt;p&gt;A strike is a serious matter, and will reduce that team member’s grade on joint work by 10%. If any team-member gets strikes from all other members of his or her team, their grade will be reduced by 50%.&lt;/p&gt;
&lt;p&gt;Strikes are &lt;em&gt;anonymous&lt;/em&gt; so that you do not need to fear social retaliation. However, they are not anonymous to allow you to issue them without thoughtful consideration. Perhaps the other person has a serious issue that is preventing them from completing work (e.g., a relative passing away). Please be thoughtful in using this remedy and consider it a last resort.&lt;/p&gt;
&lt;!-- &gt; Do I really need to create a team GitHub repository? I don&#39;t like GitHub / programming/ work. --&gt;
&lt;!-- Yes, you need to become familiar with GitHub and you and your team will work in a central repository for mini-projects and your final project. --&gt;
&lt;!-- This is for two reasons. First, computer scientists spent a huge amount of time coming up with the solutions that are implemented in GitHub (and other flavors of `git`). Their efforts are largely dedicated toward solving a very concrete goal: how can two people edit the same thing at the same time without creating a ton of new issues. While you could use a paid variant of GitHub (e.g., you could all collaborate over the Microsoft Office suite as implemented by the 360 software that MSU provides), you&#39;d ultimately have the following issues: --&gt;
&lt;!-- 1. The software doesn&#39;t support some file types. --&gt;
&lt;!-- 2. The software doesn&#39;t autosave versions.[^1] If someone accidentally deletes something, you&#39;re in trouble. --&gt;
&lt;!-- 3. You have to learn an entirely new system every time you change classes / universities / jobs, because said institute doesn&#39;t buy the product you love.[^2] --&gt;
&lt;!-- [^1]: Some products, of course, solve this problem a little bit. For example, Dropbox allows users to share files with ease (of any file type) and saves a (coarse) version history. However, Dropbox does not allow multiple users to work on the same file, and has no way of merging edits together. --&gt;
&lt;!-- [^2]: This logic is also why we utilize only free software in this course. It sucks to get really good at, say, `SAS` (as I did many years ago) only to realize that the software costs about $10000 and many firms are unwilling to spent that. We will try our best to avoid giving you dead-end skills. --&gt;
&lt;blockquote&gt;
&lt;p&gt;I’m on a smaller-than-normal team. Does this mean that I have to do more work?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Your instructors are able to count and are aware the teams are imbalanced. Evaluations of final projects will take this into account. While your final product should reflect the best ability of your team, we do not anticipate that the uneven teams will lead to substantively different outputs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;suggested-outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Suggested outline&lt;/h2&gt;
&lt;p&gt;You must write and present your analysis as if presenting to a &lt;strong&gt;C-suite executive&lt;/strong&gt;. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;
- Concretely, this requires a written memo, which describes the data, analyses, and results. This must be clear and easy to understand for a non-expert in your field. Figures and tables do not apply to the page limit.&lt;/p&gt;
&lt;p&gt;Below is a very loose guide to the sort of content that we expect for the final project. Word limits are suggestions only. Note your final report will be approximately&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Describe the motivation for this analysis. Briefly describe the dataset, and explain why the analysis you’re undertaking matters for society. (Or matters for some decision-making. You should not feel constrained to asking only “big questions.” The best projects will be narrow-scope but well-defined.) (&lt;strong&gt;≈300 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theory-and-background&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Theory and Background&lt;/h3&gt;
&lt;p&gt;Provide in-depth background about the data of interest and about your analytics question. (&lt;strong&gt;≈300 words&lt;/strong&gt;)&lt;/p&gt;
&lt;div id=&#34;theory&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;“Theory”&lt;/h4&gt;
&lt;p&gt;Provide some theoretical guidance to the functional relationship you hope to explore. If you’re interested on how, say, height affects scoring in the NBA, write down a proposed function that might map height to scoring. Describe how you might look for this unknown relationship in the data.(&lt;strong&gt;≈300 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypotheses&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hypotheses&lt;/h4&gt;
&lt;p&gt;Make predictions. Declare what you think will happen. (Note, this may carry over from second project.) (&lt;strong&gt;≈250 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-and-analyses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data and Analyses&lt;/h3&gt;
&lt;div id=&#34;data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Data&lt;/h4&gt;
&lt;p&gt;Given your motivations, limits on feasibility, and hypotheses, describe the data you use. (&lt;strong&gt;≈100 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analyses&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Analyses&lt;/h4&gt;
&lt;p&gt;Generate the analyses relevant to your hypotheses and interests. Here you must include three figures and must describe what they contain in simple, easy to digest language. Why did you visualize these elements? Your analyses also must include brief discussion.&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;As many words as you need to fully describe your analysis and results&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;What caveats should we consider? Do you believe this is a truly causal relationship? Why does any of this matter to the decision-maker? (&lt;strong&gt;≈75 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Note that &lt;strong&gt;existing&lt;/strong&gt; is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged. But you cannot stand with a clipboard outside a store and count visitors (for instance).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This exercise provides you with an opportunity to identify your marketable skills and to practice them. I encourage those who will be looking for jobs soon to take this exercise seriously.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression II</title>
      <link>https://ssc442.netlify.app/content/07-content/</link>
      <pubDate>Thu, 04 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/07-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#supplemental-readings&#34;&gt;Supplemental Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-models-ii&#34;&gt;Linear Models II&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-moneyball&#34;&gt;Case study: Moneyball&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sabermetics&#34;&gt;Sabermetics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#baseball-basics&#34;&gt;Baseball basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#no-awards-for-bb&#34;&gt;No awards for BB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#base-on-balls-or-stolen-bases&#34;&gt;Base on balls or stolen bases?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regression-applied-to-baseball-statistics&#34;&gt;Regression applied to baseball statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confounding&#34;&gt;Confounding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#understanding-confounding-through-stratification&#34;&gt;Understanding confounding through stratification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multivariate-regression&#34;&gt;Multivariate regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lse&#34;&gt;Least squares estimates&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interpreting-linear-models&#34;&gt;Interpreting linear models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#least-squares-estimates-lse&#34;&gt;Least Squares Estimates (LSE)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-lm-function&#34;&gt;The &lt;code&gt;lm&lt;/code&gt; function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lse-are-random-variables&#34;&gt;LSE are random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predicted-values-are-random-variables&#34;&gt;Predicted values are random variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-regression-in-the-tidyverse&#34;&gt;Linear regression in the tidyverse&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-broom-package&#34;&gt;The broom package&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-moneyball-continued&#34;&gt;Case study: Moneyball (continued)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#adding-salary-and-position-information&#34;&gt;Adding salary and position information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#picking-nine-players&#34;&gt;Picking nine players&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-regression-fallacy&#34;&gt;The regression fallacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#measurement-error-models&#34;&gt;Measurement error models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;supplemental-readings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Supplemental Readings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; Chapter 3 in &lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf&#34;&gt;Introduction to Statistical Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How do we interpret linear regression outputs?&lt;/li&gt;
&lt;li&gt;How are the standard errors derived?&lt;/li&gt;
&lt;li&gt;When should we turn to linear regression versus alternative approaches?&lt;/li&gt;
&lt;li&gt;Why do we use linear regression so often in data analytics?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slides&lt;/h3&gt;
&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;slide-tabs&#34; role=&#34;tablist&#34;&gt;
&lt;li class=&#34;nav-item&#34;&gt;
&lt;a class=&#34;nav-link active&#34; id=&#34;introduction-tab&#34; data-toggle=&#34;tab&#34; href=&#34;#introduction&#34; role=&#34;tab&#34; aria-controls=&#34;introduction&#34; aria-selected=&#34;true&#34;&gt;Introduction&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;slide-tabs&#34; class=&#34;tab-content&#34;&gt;
&lt;div id=&#34;introduction&#34; class=&#34;tab-pane fade show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;introduction-tab&#34;&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;https://ssc442.netlify.app/slides/06-slides.html#Introduction&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models-ii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Linear Models II&lt;/h1&gt;
&lt;p&gt;Since Galton’s original development, regression has become one of the most widely used tools in data science. One reason has to do with the fact that regression permits us to find relationships between two variables taking into account the effects of other variables that affect both. This has been particularly popular in fields where randomized experiments are hard to run, such as economics and epidemiology.&lt;/p&gt;
&lt;p&gt;When we are not able to randomly assign each individual to a treatment or control group, confounding is particularly prevalent. For example, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in a jurisdiction. Fast food consumers are more likely to be smokers, drinkers, and have lower incomes. Therefore, a naive regression model may lead to an overestimate of the negative health effect of fast food. So how do we account for confounding in practice? In this lecture we learn how linear models can help with such situations and can be used to describe how one or more variables affect an outcome variable.&lt;/p&gt;
&lt;div id=&#34;case-study-moneyball&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: Moneyball&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Moneyball: The Art of Winning an Unfair Game&lt;/em&gt; is a book by Michael Lewis about the Oakland Athletics (A’s) baseball team and its general manager, the person tasked with building the team, Billy Beane.&lt;/p&gt;
&lt;p&gt;Traditionally, baseball teams use &lt;em&gt;scouts&lt;/em&gt; to help them decide what players to hire. These scouts evaluate players by observing them perform. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand. This in turn drives up their salaries.&lt;/p&gt;
&lt;p&gt;From 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to buy the best players and, during that time, they were one of the best teams. However, in 1995 the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball. He could no longer afford the most sought-after players. Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to scouts, as a method for finding low-cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a large role in this approach.&lt;/p&gt;
&lt;p&gt;As motivation for this lecture, we will pretend it is 2002 (holy shit I’m old) and try to build a baseball team with a limited budget, just like the A’s had to do. To appreciate what you are up against, note that in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s $39,679,746:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/mlb-2002-payroll-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;sabermetics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sabermetics&lt;/h3&gt;
&lt;p&gt;Statistics have been used in baseball since its beginnings. The dataset we will be using, included in the &lt;strong&gt;Lahman&lt;/strong&gt; library, goes back to the 19th century. For example, a summary statistics we will describe soon, the &lt;em&gt;batting average&lt;/em&gt;, has been used for decades to summarize a batter’s success. Other statistics&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; such as home runs (HR), runs batted in (RBI), and stolen bases (SB) are reported for each player in the game summaries included in the sports section of newspapers, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily decided on without much thought as to whether they actually predicted anything or were related to helping a team win.&lt;/p&gt;
&lt;p&gt;This changed with Bill James&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. In the late 1970s, this aspiring writer and baseball fan started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to predict what outcomes best predicted if a team would win &lt;em&gt;sabermetrics&lt;/em&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Currently, sabermetrics popularity is no longer limited to just baseball; other sports have started to use this approach as well.&lt;/p&gt;
&lt;p&gt;In this lecture, to simplify the exercise, we will focus on scoring runs and ignore the two other important aspects of the game: pitching and fielding. We will see how regression analysis can help develop strategies to build a competitive baseball team with a constrained budget. The approach can be divided into two separate data analyses. In the first, we determine which recorded player-specific statistics predict runs. In the second, we examine if players were undervalued based on what our first analysis predicts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;baseball-basics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Baseball basics&lt;/h3&gt;
&lt;p&gt;To see how regression will help us find undervalued players, we actually don’t need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem.&lt;/p&gt;
&lt;p&gt;The goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team’s &lt;em&gt;pitcher&lt;/em&gt; throws the ball and the batter tries to hit it. The PA ends with an binary outcome: the batter either makes an &lt;em&gt;out&lt;/em&gt; (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases). Each team gets nine tries, referred to as &lt;em&gt;innings&lt;/em&gt;, to score runs and each inning ends after three outs (three failures).&lt;/p&gt;
&lt;p&gt;Here is a video showing a success: &lt;a href=&#34;https://www.youtube.com/watch?v=HL-XjMCPfio&#34;&gt;https://www.youtube.com/watch?v=HL-XjMCPfio&lt;/a&gt;. And here is one showing a failure: &lt;a href=&#34;https://www.youtube.com/watch?v=NeloljCx-1g&#34;&gt;https://www.youtube.com/watch?v=NeloljCx-1g&lt;/a&gt;. In these videos, we see how luck is involved in the process. When at bat, the batter wants to hit the ball hard. If the batter hits it hard enough, it is a HR, the best possible outcome as the batter gets at least one automatic run. But sometimes, due to chance, the batter hits the ball very hard and a defender catches it, resulting in an out. In contrast, sometimes the batter hits the ball softly, but it lands just in the right place. The fact that there is chance involved hints at why probability models will be involved.&lt;/p&gt;
&lt;p&gt;Now there are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many &lt;em&gt;bases&lt;/em&gt; as possible. There are four bases with the fourth one called &lt;em&gt;home plate&lt;/em&gt;. Home plate is where batters start by trying to hit, so the bases form a cycle.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./06-content_files/Baseball_Diamond1.png&#34; /&gt;
(Courtesy of Cburnett&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. CC BY-SA 3.0 license&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.)
&lt;!--Source: [Wikipedia Commons](https://commons.wikimedia.org/wiki/File:Baseball_diamond_simplified.svg))--&gt;&lt;/p&gt;
&lt;p&gt;A batter who &lt;em&gt;goes around the bases&lt;/em&gt; and arrives home, scores a run.&lt;/p&gt;
&lt;p&gt;We are simplifying a bit, but there are five ways a batter can succeed, that is, not make an out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base.&lt;/li&gt;
&lt;li&gt;Single - Batter hits the ball and gets to first base.&lt;/li&gt;
&lt;li&gt;Double (2B) - Batter hits the ball and gets to second base.&lt;/li&gt;
&lt;li&gt;Triple (3B) - Batter hits the ball and gets to third base.&lt;/li&gt;
&lt;li&gt;Home Run (HR) - Batter hits the ball and goes all the way home and scores a run.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an example of a HR:
&lt;a href=&#34;https://www.youtube.com/watch?v=xYxSZJ9GZ-w&#34;&gt;https://www.youtube.com/watch?v=xYxSZJ9GZ-w&lt;/a&gt;.
If a batter gets to a base, the batter still has a chance of getting home and scoring a run if the next batter hits successfully. While the batter is &lt;em&gt;on base&lt;/em&gt;, the batter can also try to steal a base (SB). If a batter runs fast enough, the batter can try to go from one base to the next without the other team tagging the runner. [Here] is an example of a stolen base: &lt;a href=&#34;https://www.youtube.com/watch?v=JSE5kfxkzfk&#34;&gt;https://www.youtube.com/watch?v=JSE5kfxkzfk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All these events are kept track of during the season and are available to us through the &lt;strong&gt;Lahman&lt;/strong&gt; package. Now we will start discussing how data analysis can help us decide how to use these statistics to evaluate players.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;no-awards-for-bb&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;No awards for BB&lt;/h3&gt;
&lt;p&gt;Historically, the &lt;em&gt;batting average&lt;/em&gt; has been considered the most important offensive statistic. To define this average, we define a &lt;em&gt;hit&lt;/em&gt; (H) and an &lt;em&gt;at bat&lt;/em&gt; (AB). Singles, doubles, triples, and home runs are hits. The fifth way to be successful, BB, is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. Today this success rate ranges from 20% to 38%. We refer to the batting average in thousands so, for example, if your success rate is 28%, we call it &lt;em&gt;batting 280&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/./06-content_files/JumboTron.png&#34; /&gt;
(Picture courtesy of Keith Allison&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;. CC BY-SA 2.0 license&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;One of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. He proposed we use the &lt;em&gt;on base percentage&lt;/em&gt; (OBP) instead of batting average. He defined OBP as (H+BB)/(AB+BB) which is simply the proportion of plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic. In contrast, total stolen bases were considered important and an award&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data science to determine if it’s better to pay for players with high BB or SB?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;base-on-balls-or-stolen-bases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Base on balls or stolen bases?&lt;/h3&gt;
&lt;p&gt;One of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. We do keep track of the number of runs scored by a player. However, remember that if a player X bats right before someone who hits many HRs, batter X will score many runs. But these runs don’t necessarily happen if we hire player X but not his HR hitting teammate. However, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? We have data! Let’s examine some.&lt;/p&gt;
&lt;p&gt;Let’s start with an obvious one: HRs. Do teams that hit more home runs score more runs? We examine data from 1961 to 2001. The visualization of choice when exploring the relationship between two variables, such as HRs and wins, is a scatterplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Lahman)

Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(HR_per_game = HR / G, R_per_game = R / G) %&amp;gt;%
  ggplot(aes(HR_per_game, R_per_game)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/runs-vs-hrs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot shows a strong association: teams with more HRs tend to score more runs. Now let’s examine the relationship between stolen bases and runs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(SB_per_game = SB / G, R_per_game = R / G) %&amp;gt;%
  ggplot(aes(SB_per_game, R_per_game)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/runs-vs-sb-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here the relationship is not as clear. Finally, let’s examine the relationship between BB and runs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(BB_per_game = BB/G, R_per_game = R/G) %&amp;gt;%
  ggplot(aes(BB_per_game, R_per_game)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/runs-vs-bb-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here again we see a clear association. But does this mean that increasing a team’s BBs &lt;strong&gt;causes&lt;/strong&gt; an increase in runs? One of the most important lessons you learn in this book is that &lt;strong&gt;association is not causation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In fact, it looks like BBs and HRs are also associated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;% filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(HR_per_game = HR/G, BB_per_game = BB/G) %&amp;gt;%
  ggplot(aes(HR_per_game, BB_per_game)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/bb-vs-hrs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We know that HRs cause runs because, as the name “home run” implies, when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is &lt;em&gt;confounding&lt;/em&gt;, an important concept we will learn more about throughout this lecture.&lt;/p&gt;
&lt;p&gt;Linear regression will help us parse all this out and quantify the associations. This will then help us determine what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BBs, but keep the HRs fixed? Regression will help us answer questions like this one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-applied-to-baseball-statistics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression applied to baseball statistics&lt;/h3&gt;
&lt;p&gt;Can we use regression with these data? First, notice that the HR and Run data appear to be bivariate normal. We save the plot into the object &lt;code&gt;p&lt;/code&gt; as we will use it again later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Lahman)
p &amp;lt;- Teams %&amp;gt;% filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(HR_per_game = HR/G, R_per_game = R/G) %&amp;gt;%
  ggplot(aes(HR_per_game, R_per_game)) +
  geom_point(alpha = 0.5)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/hr-runs-bivariate-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The qq-plots confirm that the normal approximation is useful here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;% filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(z_HR = round((HR - mean(HR))/sd(HR)),
         R_per_game = R/G) %&amp;gt;%
  filter(z_HR %in% -2:3) %&amp;gt;%
  ggplot() +
  stat_qq(aes(sample=R_per_game)) +
  facet_wrap(~z_HR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/hr-by-runs-qq-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we are ready to use linear regression to predict the number of runs a team will score if we know how many home runs the team hits. All we need to do is compute the five summary statistics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary_stats &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(HR_per_game = HR/G, R_per_game = R/G) %&amp;gt;%
  summarize(avg_HR = mean(HR_per_game),
            s_HR = sd(HR_per_game),
            avg_R = mean(R_per_game),
            s_R = sd(R_per_game),
            r = cor(HR_per_game, R_per_game))
summary_stats&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      avg_HR      s_HR    avg_R       s_R         r
## 1 0.8547104 0.2429707 4.355262 0.5885791 0.7615597&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and use the formulas given above to create the regression lines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reg_line &amp;lt;- summary_stats %&amp;gt;% summarize(slope = r*s_R/s_HR,
                            intercept = avg_R - slope*avg_HR)

p + geom_abline(intercept = reg_line$intercept, slope = reg_line$slope)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/hr-versus-runs-regression-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Soon we will learn R functions, such as &lt;code&gt;lm&lt;/code&gt;, that make fitting regression lines much easier. Another example is the &lt;strong&gt;ggplot2&lt;/strong&gt; function &lt;code&gt;geom_smooth&lt;/code&gt; which computes and adds a regression line to plot along with confidence intervals, which we also learn about later. We use the argument &lt;code&gt;method = &#34;lm&#34;&lt;/code&gt; which stands for &lt;em&gt;linear model&lt;/em&gt;, the title of an upcoming section. So we can simplify the code above like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/hr-versus-runs-regression-easy-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the example above, the slope is 1.8448241. So this tells us that teams that hit 1 more HR per game than the average team, score 1.8448241 more runs per game than the average team. Given that the most common final score is a difference of a run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because we are working on a budget, we will need to find some other way to increase wins. So in the next section we move our attention to BB.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;confounding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confounding&lt;/h2&gt;
&lt;p&gt;Previously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(Lahman)
get_slope &amp;lt;- function(x, y) cor(x, y) * sd(y) / sd(x)

bb_slope &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(BB_per_game = BB/G, R_per_game = R/G) %&amp;gt;%
  summarize(slope = get_slope(BB_per_game, R_per_game))

bb_slope&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       slope
## 1 0.7353288&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score 1.5 more runs per game?&lt;/p&gt;
&lt;p&gt;We are again reminded that association is not causation. The data does provide strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game. But this does not mean that BB are the cause.&lt;/p&gt;
&lt;p&gt;Note that if we compute the regression line slope for singles we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;singles_slope &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %&amp;gt;%
  summarize(slope = get_slope(Singles_per_game, R_per_game))

singles_slope&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       slope
## 1 0.4494253&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is a lower value than what we obtain for BB.&lt;/p&gt;
&lt;p&gt;Also, notice that a single gets you to first base just like a BB. Those that know about baseball will tell you that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %&amp;gt;%
  summarize(cor(BB, HR), cor(Singles, HR), cor(BB, Singles))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   cor(BB, HR) cor(Singles, HR) cor(BB, Singles)
## 1   0.4039313       -0.1737435      -0.05603822&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BBs and a team with many HRs will also have more BBs. Although it may appear that BBs cause runs, it is actually the HRs that cause most of these runs. We say that BBs are &lt;em&gt;confounded&lt;/em&gt; with HRs. Nonetheless, could it be that BBs still help? To find out, we somehow have to adjust for the HR effect. Regression can help with this as well.&lt;/p&gt;
&lt;div id=&#34;understanding-confounding-through-stratification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Understanding confounding through stratification&lt;/h3&gt;
&lt;p&gt;A first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest ten. We filter out the strata with few points to avoid highly variable estimates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(HR_strata = round(HR/G, 1),
         BB_per_game = BB / G,
         R_per_game = R / G) %&amp;gt;%
  filter(HR_strata &amp;gt;= 0.4 &amp;amp; HR_strata &amp;lt;=1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then make a scatterplot for each strata:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  ggplot(aes(BB_per_game, R_per_game)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  facet_wrap( ~ HR_strata)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/runs-vs-bb-by-hr-strata-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Remember that the regression slope for predicting runs with BB was 0.7. Once we stratify by HR, these slopes are substantially reduced:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR_strata) %&amp;gt;%
  summarize(slope = get_slope(BB_per_game, R_per_game))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 2
##   HR_strata slope
##       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1       0.4 0.734
## 2       0.5 0.566
## 3       0.6 0.412
## 4       0.7 0.285
## 5       0.8 0.365
## 6       0.9 0.261
## 7       1   0.512
## 8       1.1 0.454
## 9       1.2 0.440&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The slopes are reduced, but they are not 0, which indicates that BBs are helpful for producing runs, just not as much as previously thought.
In fact, the values above are closer to the slope we obtained from singles, 0.45, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.&lt;/p&gt;
&lt;p&gt;Although our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BBs to get this plot:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/runs-vs-hr-by-bb-strata-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, the slopes do not change much from the original:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% group_by(BB_strata) %&amp;gt;%
   summarize(slope = get_slope(HR_per_game, R_per_game))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 × 2
##    BB_strata slope
##        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1       2.8  1.52
##  2       2.9  1.57
##  3       3    1.52
##  4       3.1  1.49
##  5       3.2  1.58
##  6       3.3  1.56
##  7       3.4  1.48
##  8       3.5  1.63
##  9       3.6  1.83
## 10       3.7  1.45
## 11       3.8  1.70
## 12       3.9  1.30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They are reduced a bit, which is consistent with the fact that BB do in fact cause some runs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hr_slope &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(HR_per_game = HR/G, R_per_game = R/G) %&amp;gt;%
  summarize(slope = get_slope(HR_per_game, R_per_game))

hr_slope&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      slope
## 1 1.844824&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Regardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multivariate regression&lt;/h3&gt;
&lt;p&gt;It is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1(x_2) x_1 + \beta_2(x_1) x_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with the slopes for &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; changing for different values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and vice versa. But is there an easier approach?&lt;/p&gt;
&lt;p&gt;If we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that &lt;span class=&#34;math inline&#34;&gt;\(\beta_1(x_2)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2(x_1)\)&lt;/span&gt; are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This model suggests that if the number of HR is fixed at &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, we observe a linear relationship between runs and BB with an intercept of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_2 x_2\)&lt;/span&gt;. Our exploratory data analysis suggested this. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 x_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this analysis, referred to as &lt;em&gt;multivariate regression&lt;/em&gt;, you will often hear people say that the BB slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is &lt;em&gt;adjusted&lt;/em&gt; for the HR effect. If the model is correct then confounding has been accounted for. But how do we estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; from the data? For this, we learn about linear models and least squares estimates.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Least squares estimates&lt;/h2&gt;
&lt;p&gt;We have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a &lt;em&gt;linear model&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We note that “linear” here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, &lt;span class=&#34;math inline&#34;&gt;\(3x - 4y + 5z\)&lt;/span&gt; is a linear combination of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. We can also add a constant so &lt;span class=&#34;math inline&#34;&gt;\(2 + 3x - 4y + 5z\)&lt;/span&gt; is also linear combination of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_1 x_1 + \beta_2 x_2\)&lt;/span&gt;, is a linear combination of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.
The simplest linear model is a constant &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;; the second simplest is a line &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_1 x\)&lt;/span&gt;. If we were to specify a linear model for Galton’s data, we would denote the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; observed father heights with &lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_n\)&lt;/span&gt;, then we model the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; son heights we are trying to predict with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \, i=1,\dots,N.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the father’s height, which is fixed (not random) due to the conditioning, and &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; is the random son’s height that we want to predict. We further assume that &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt; are independent from each other, have expected value 0 and the standard deviation, call it &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, does not depend on &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the above model, we know the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, but to have a useful model for prediction, we need &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;. We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We show how to do this in the next section.&lt;/p&gt;
&lt;p&gt;Note that if we further assume that the &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is normally distributed, then this model is exactly the same one we derived earlier by assuming bivariate normal data. A somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and that the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;s is not specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.&lt;/p&gt;
&lt;div id=&#34;interpreting-linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpreting linear models&lt;/h3&gt;
&lt;p&gt;One reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; for each inch we increase the father’s height &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Because not all sons with fathers of height &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; are of equal height, we need the term &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.&lt;/p&gt;
&lt;p&gt;Given how we wrote the model above, the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 (x_i - \bar{x}) + \varepsilon_i, \, i=1,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\bar{x} = 1/N \sum_{i=1}^N x_i\)&lt;/span&gt; the average of the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. In this case &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; represents the height when &lt;span class=&#34;math inline&#34;&gt;\(x_i = \bar{x}\)&lt;/span&gt;, which is the height of the son of an average father.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;least-squares-estimates-lse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Least Squares Estimates (LSE)&lt;/h3&gt;
&lt;p&gt;For linear models to be useful, we have to estimate the unknown &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;s. The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this lecture. For Galton’s data, we would write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
RSS = \sum_{i=1}^n \left\{  y_i - \left(\beta_0 + \beta_1 x_i \right)\right\}^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt;. Let’s demonstrate this with the previously defined dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HistData)
data(&amp;quot;GaltonFamilies&amp;quot;)
set.seed(1983)
galton_heights &amp;lt;- GaltonFamilies %&amp;gt;%
  filter(gender == &amp;quot;male&amp;quot;) %&amp;gt;%
  group_by(family) %&amp;gt;%
  sample_n(1) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(father, childHeight) %&amp;gt;%
  rename(son = childHeight)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s write a function that computes the RSS for any pair of values &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rss &amp;lt;- function(beta0, beta1, data){
  resid &amp;lt;- galton_heights$son - (beta0+beta1*galton_heights$father)
  return(sum(resid^2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So for any pair of values, we get an RSS. Here is a plot of the RSS as a function of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; when we keep the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; fixed at 25.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta1 = seq(0, 1, len=nrow(galton_heights))
results &amp;lt;- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %&amp;gt;% ggplot(aes(beta1, rss)) + geom_line() +
  geom_line(aes(beta1, rss))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/rss-versus-estimate-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see a clear minimum for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; at around 0.65. However, this minimum for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is for when &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 = 25\)&lt;/span&gt;, a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.&lt;/p&gt;
&lt;p&gt;Trial and error is not going to work in this case. We could search for a minimum within a fine grid of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; values, but this is unnecessarily time-consuming since we can use calculus: take the partial derivatives, set them to 0 and solve for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt;. Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will learn these next. To learn the mathematics behind this, you can consult a book on linear models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-lm-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;lm&lt;/code&gt; function&lt;/h3&gt;
&lt;p&gt;In R, we can obtain the least squares estimates using the &lt;code&gt;lm&lt;/code&gt; function. To fit the model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; the son’s height and &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; the father’s height, we can use this code to obtain the least squares estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- lm(son ~ father, data = galton_heights)
fit$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)      father 
##   37.287605    0.461392&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most common way we use &lt;code&gt;lm&lt;/code&gt; is by using the character &lt;code&gt;~&lt;/code&gt; to let &lt;code&gt;lm&lt;/code&gt; know which is the variable we are predicting (left of &lt;code&gt;~&lt;/code&gt;) and which we are using to predict (right of &lt;code&gt;~&lt;/code&gt;). The intercept is added automatically to the model that will be fit.&lt;/p&gt;
&lt;p&gt;The object &lt;code&gt;fit&lt;/code&gt; includes more information about the fit. We can use the function &lt;code&gt;summary&lt;/code&gt; to extract more of this information (not shown):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = son ~ father, data = galton_heights)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.3543 -1.5657 -0.0078  1.7263  9.4150 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 37.28761    4.98618   7.478 3.37e-12 ***
## father       0.46139    0.07211   6.398 1.36e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.45 on 177 degrees of freedom
## Multiple R-squared:  0.1878, Adjusted R-squared:  0.1833 
## F-statistic: 40.94 on 1 and 177 DF,  p-value: 1.36e-09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lse-are-random-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LSE are random variables&lt;/h3&gt;
&lt;p&gt;The LSE is derived from the data &lt;span class=&#34;math inline&#34;&gt;\(y_1,\dots,y_N\)&lt;/span&gt;, which are a realization of random variables &lt;span class=&#34;math inline&#34;&gt;\(Y_1, \dots, Y_N\)&lt;/span&gt;. This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size &lt;span class=&#34;math inline&#34;&gt;\(N=50\)&lt;/span&gt;, and compute the regression slope coefficient for each one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 1000
N &amp;lt;- 50
lse &amp;lt;- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %&amp;gt;%
    lm(son ~ father, data = .) %&amp;gt;%
    .$coef
})
lse &amp;lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the variability of the estimates by plotting their distributions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gridExtra&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/lse-distributions-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The reason these look normal is because the central limit theorem applies here as well: for large enough &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, the least squares estimates will be approximately normal with expected value &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, respectively. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the &lt;code&gt;lm&lt;/code&gt; function. Here it is for one of our simulated data sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; sample_n(galton_heights, N, replace = TRUE) %&amp;gt;%
  lm(son ~ father, data = .) %&amp;gt;%
  summary %&amp;gt;% .$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Estimate Std. Error  t value     Pr(&amp;gt;|t|)
## (Intercept) 19.2791952 11.6564590 1.653950 0.1046637693
## father       0.7198756  0.1693834 4.249977 0.0000979167&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see that the standard errors estimates reported by the &lt;code&gt;summary&lt;/code&gt; are close to the standard errors from the simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lse %&amp;gt;% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      se_0      se_1
## 1 8.83591 0.1278812&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;summary&lt;/code&gt; function also reports t-statistics (&lt;code&gt;t value&lt;/code&gt;) and p-values (&lt;code&gt;Pr(&amp;gt;|t|)&lt;/code&gt;). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 / \hat{\mbox{SE}}(\hat{\beta}_0 )\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 / \hat{\mbox{SE}}(\hat{\beta}_1 )\)&lt;/span&gt;, follow a t-distribution with &lt;span class=&#34;math inline&#34;&gt;\(N-p\)&lt;/span&gt; degrees of freedom, with &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; the number of parameters in our model. In the case of height &lt;span class=&#34;math inline&#34;&gt;\(p=2\)&lt;/span&gt;, the two p-values are testing the null hypothesis that &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1=0\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Remember that, as we described in Section &lt;a href=&#34;#t-dist&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; for large enough &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about &lt;strong&gt;broom&lt;/strong&gt;, an add-on package that makes this easy.&lt;/p&gt;
&lt;p&gt;Although we do not show examples in this book, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predicted-values-are-random-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predicted values are random variables&lt;/h3&gt;
&lt;p&gt;Once we fit our model, we can obtain prediction of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; by plugging in the estimates into the regression model. For example, if the father’s height is &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then our prediction &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; for the son’s height will be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 x\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When we plot &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; versus &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we see the regression line.&lt;/p&gt;
&lt;p&gt;Keep in mind that the prediction &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the &lt;strong&gt;ggplot2&lt;/strong&gt; layer &lt;code&gt;geom_smooth(method = &#34;lm&#34;)&lt;/code&gt; that we previously used plots &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; and surrounds it by confidence intervals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;% ggplot(aes(son, father)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/father-son-regression-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The R function &lt;code&gt;predict&lt;/code&gt; takes an &lt;code&gt;lm&lt;/code&gt; object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- galton_heights %&amp;gt;% lm(son ~ father, data = .)

y_hat &amp;lt;- predict(fit, se.fit = TRUE)

names(y_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;fit&amp;quot;            &amp;quot;se.fit&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;residual.scale&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;We have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing how stable they are across the years. Since we have to pick players based on their previous performances, we will prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BBs.&lt;/p&gt;
&lt;p&gt;1. Before we get started, we want to generate two tables. One for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics. Here is how we create the 2017 table. Keeping only players with more than 100 plate appearances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Lahman)
dat &amp;lt;- Batting %&amp;gt;% filter(yearID == 2002) %&amp;gt;%
  mutate(pa = AB + BB,
         singles = (H - X2B - X3B - HR) / pa, bb = BB / pa) %&amp;gt;%
  filter(pa &amp;gt;= 100) %&amp;gt;%
  select(playerID, singles, bb)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now compute a similar table but with rates computed over 1999-2001.&lt;/p&gt;
&lt;p&gt;2. In Section &lt;a href=&#34;#joins&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we learn about the &lt;code&gt;inner_join&lt;/code&gt;, which you can use to have the 2001 data and averages in the same table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- inner_join(dat, avg, by = &amp;quot;playerID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compute the correlation between 2002 and the previous seasons for singles and BB.&lt;/p&gt;
&lt;p&gt;3. Note that the correlation is higher for BB. To quickly get an idea of the uncertainty associated with this correlation estimate, we will fit a linear model and compute confidence intervals for the slope coefficient. However, first make scatterplots to confirm that fitting a linear model is appropriate.&lt;/p&gt;
&lt;p&gt;4. Now fit a linear model for each metric and use the &lt;code&gt;confint&lt;/code&gt; function to compare the estimates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-regression-in-the-tidyverse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear regression in the tidyverse&lt;/h2&gt;
&lt;p&gt;To see how we use the &lt;code&gt;lm&lt;/code&gt; function in a more complex analysis, let’s go back to the baseball example. In a previous example, we estimated regression lines to predict runs for BB in different HR strata. We first constructed a data frame similar to this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(HR = round(HR/G, 1),
         BB = BB/G,
         R = R/G) %&amp;gt;%
  select(HR, BB, R) %&amp;gt;%
  filter(HR &amp;gt;= 0.4 &amp;amp; HR&amp;lt;=1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we didn’t know the &lt;code&gt;lm&lt;/code&gt; function, to compute the regression line in each strata, we used the formula directly like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_slope &amp;lt;- function(x, y) cor(x, y) * sd(y) / sd(x)
dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  summarize(slope = get_slope(BB, R))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We argued that the slopes are similar and that the differences were perhaps due to random variation. To provide a more rigorous defense of the slopes being the same, which led to our multivariate model, we could compute confidence intervals for each slope. We have not learned the formula for this, but the &lt;code&gt;lm&lt;/code&gt; function provides enough information to construct them.&lt;/p&gt;
&lt;p&gt;First, note that if we try to use the &lt;code&gt;lm&lt;/code&gt; function to get the estimated slope like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  lm(R ~ BB, data = .) %&amp;gt;% .$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          BB 
##   2.1983658   0.6378804&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we don’t get the result we want. The &lt;code&gt;lm&lt;/code&gt; function ignores the &lt;code&gt;group_by&lt;/code&gt;. This is expected because &lt;code&gt;lm&lt;/code&gt; is not part of the &lt;strong&gt;tidyverse&lt;/strong&gt; and does not know how to handle the outcome of a grouped tibble.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;tidyverse&lt;/strong&gt; functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;, &lt;strong&gt;tidyverse&lt;/strong&gt; functions consistently return data frames, since this assures that the output of a function is accepted as the input of another.
But most R functions do not recognize grouped tibbles nor do they return data frames. The &lt;code&gt;lm&lt;/code&gt; function is an example. The &lt;code&gt;do&lt;/code&gt; functions serves as a bridge between R functions, such as &lt;code&gt;lm&lt;/code&gt;, and the &lt;strong&gt;tidyverse&lt;/strong&gt;. The &lt;code&gt;do&lt;/code&gt; function understands grouped tibbles and always returns a data frame.&lt;/p&gt;
&lt;p&gt;So, let’s try to use the &lt;code&gt;do&lt;/code&gt; function to fit a regression line to each HR strata:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(fit = lm(R ~ BB, data = .))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 2
## # Rowwise: 
##      HR fit   
##   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;
## 1   0.4 &amp;lt;lm&amp;gt;  
## 2   0.5 &amp;lt;lm&amp;gt;  
## 3   0.6 &amp;lt;lm&amp;gt;  
## 4   0.7 &amp;lt;lm&amp;gt;  
## 5   0.8 &amp;lt;lm&amp;gt;  
## 6   0.9 &amp;lt;lm&amp;gt;  
## 7   1   &amp;lt;lm&amp;gt;  
## 8   1.1 &amp;lt;lm&amp;gt;  
## 9   1.2 &amp;lt;lm&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we did in fact fit a regression line to each strata. The &lt;code&gt;do&lt;/code&gt; function will create a data frame with the first column being the strata value and a column named &lt;code&gt;fit&lt;/code&gt; (we chose the name, but it can be anything). The column will contain the result of the &lt;code&gt;lm&lt;/code&gt; call. Therefore, the returned tibble has a column with &lt;code&gt;lm&lt;/code&gt; objects, which is not very useful.&lt;/p&gt;
&lt;p&gt;Also, if we do not name a column (note above we named it &lt;code&gt;fit&lt;/code&gt;), then &lt;code&gt;do&lt;/code&gt; will return the actual output of &lt;code&gt;lm&lt;/code&gt;, not a data frame, and this will result in an error since &lt;code&gt;do&lt;/code&gt; is expecting a data frame as output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(lm(R ~ BB, data = .))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Error: Results 1, 2, 3, 4, 5, ... must be data frames, not lm&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For a useful data frame to be constructed, the output of the function must be a data frame too. We could build a function that returns only what we want in the form of a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_slope &amp;lt;- function(data){
  fit &amp;lt;- lm(R ~ BB, data = data)
  data.frame(slope = fit$coefficients[2],
             se = summary(fit)$coefficient[2,2])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then use &lt;code&gt;do&lt;/code&gt; &lt;strong&gt;without&lt;/strong&gt; naming the output, since we are already getting a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(get_slope(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 3
## # Groups:   HR [9]
##      HR slope     se
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1   0.4 0.734 0.208 
## 2   0.5 0.566 0.110 
## 3   0.6 0.412 0.0974
## 4   0.7 0.285 0.0705
## 5   0.8 0.365 0.0653
## 6   0.9 0.261 0.0751
## 7   1   0.512 0.0751
## 8   1.1 0.454 0.0855
## 9   1.2 0.440 0.0801&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we name the output, then we get something we do not want, a column containing data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(slope = get_slope(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 × 2
## # Rowwise: 
##      HR slope       
##   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;      
## 1   0.4 &amp;lt;df [1 × 2]&amp;gt;
## 2   0.5 &amp;lt;df [1 × 2]&amp;gt;
## 3   0.6 &amp;lt;df [1 × 2]&amp;gt;
## 4   0.7 &amp;lt;df [1 × 2]&amp;gt;
## 5   0.8 &amp;lt;df [1 × 2]&amp;gt;
## 6   0.9 &amp;lt;df [1 × 2]&amp;gt;
## 7   1   &amp;lt;df [1 × 2]&amp;gt;
## 8   1.1 &amp;lt;df [1 × 2]&amp;gt;
## 9   1.2 &amp;lt;df [1 × 2]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is not very useful, so let’s cover one last feature of &lt;code&gt;do&lt;/code&gt;. If the data frame being returned has more than one row, these will be concatenated appropriately. Here is an example in which we return both estimated parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_lse &amp;lt;- function(data){
  fit &amp;lt;- lm(R ~ BB, data = data)
  data.frame(term = names(fit$coefficients),
    slope = fit$coefficients,
    se = summary(fit)$coefficient[,2])
}

dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(get_lse(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18 × 4
## # Groups:   HR [9]
##       HR term        slope     se
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1   0.4 (Intercept) 1.36  0.631 
##  2   0.4 BB          0.734 0.208 
##  3   0.5 (Intercept) 2.01  0.344 
##  4   0.5 BB          0.566 0.110 
##  5   0.6 (Intercept) 2.53  0.305 
##  6   0.6 BB          0.412 0.0974
##  7   0.7 (Intercept) 3.21  0.225 
##  8   0.7 BB          0.285 0.0705
##  9   0.8 (Intercept) 3.07  0.213 
## 10   0.8 BB          0.365 0.0653
## 11   0.9 (Intercept) 3.54  0.251 
## 12   0.9 BB          0.261 0.0751
## 13   1   (Intercept) 2.88  0.256 
## 14   1   BB          0.512 0.0751
## 15   1.1 (Intercept) 3.21  0.300 
## 16   1.1 BB          0.454 0.0855
## 17   1.2 (Intercept) 3.40  0.291 
## 18   1.2 BB          0.440 0.0801&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you think this is all a bit too complicated, you are not alone. To simplify things, we introduce the &lt;strong&gt;broom&lt;/strong&gt; package which was designed to facilitate the use of model fitting functions, such as &lt;code&gt;lm&lt;/code&gt;, with the &lt;strong&gt;tidyverse&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;the-broom-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The broom package&lt;/h3&gt;
&lt;p&gt;Our original task was to provide an estimate and confidence interval for the slope estimates of each strata. The &lt;strong&gt;broom&lt;/strong&gt; package will make this quite easy.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;broom&lt;/strong&gt; package has three main functions, all of which extract information from the object returned by &lt;code&gt;lm&lt;/code&gt; and return it in a &lt;strong&gt;tidyverse&lt;/strong&gt; friendly data frame. These functions are &lt;code&gt;tidy&lt;/code&gt;, &lt;code&gt;glance&lt;/code&gt;, and &lt;code&gt;augment&lt;/code&gt;. The &lt;code&gt;tidy&lt;/code&gt; function returns estimates and related information as a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)
fit &amp;lt;- lm(R ~ BB, data = dat)
tidy(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)    2.20     0.113       19.4 1.12e-70
## 2 BB             0.638    0.0344      18.5 1.35e-65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can add other important summaries, such as confidence intervals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 × 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)    2.20     0.113       19.4 1.12e-70    1.98      2.42 
## 2 BB             0.638    0.0344      18.5 1.35e-65    0.570     0.705&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because the outcome is a data frame, we can immediately use it with &lt;code&gt;do&lt;/code&gt; to string together the commands that produce the table we are after. Because a data frame is returned, we can filter and select the rows and columns we want, which facilitates working with &lt;strong&gt;ggplot2&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %&amp;gt;%
  filter(term == &amp;quot;BB&amp;quot;) %&amp;gt;%
  select(HR, estimate, conf.low, conf.high) %&amp;gt;%
  ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/do-tidy-example-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we return to discussing our original task of determining if slopes changed. The plot we just made, using &lt;code&gt;do&lt;/code&gt; and &lt;code&gt;tidy&lt;/code&gt;, shows that the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slope does not change is safe.&lt;/p&gt;
&lt;p&gt;The other functions provided by &lt;strong&gt;broom&lt;/strong&gt;, &lt;code&gt;glance&lt;/code&gt;, and &lt;code&gt;augment&lt;/code&gt;, relate to model-specific and observation-specific outcomes, respectively. Here, we can see the model fit summaries &lt;code&gt;glance&lt;/code&gt; returns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glance(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 × 12
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
##       &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     0.266         0.265 0.454      343. 1.35e-65     1  -596. 1199. 1214.
## # … with 3 more variables: deviance &amp;lt;dbl&amp;gt;, df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can learn more about these summaries in any regression text book.&lt;/p&gt;
&lt;p&gt;We will see an example of &lt;code&gt;augment&lt;/code&gt; in the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-moneyball-continued&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: Moneyball (continued)&lt;/h2&gt;
&lt;p&gt;In trying to answer how well BBs predict runs, data exploration led us to a model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[R \mid BB = x_1, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, the data is approximately normal and conditional distributions were also normal. Thus, we are justified in using a linear model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; runs per game for team &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_{i,1}\)&lt;/span&gt; walks per game, and &lt;span class=&#34;math inline&#34;&gt;\(x_{i,2}\)&lt;/span&gt;. To use &lt;code&gt;lm&lt;/code&gt; here, we need to let the function know we have two predictor variables. So we use the &lt;code&gt;+&lt;/code&gt; symbol as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(BB = BB/G, HR = HR/G,  R = R/G) %&amp;gt;%
  lm(R ~ BB + HR, data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use &lt;code&gt;tidy&lt;/code&gt; to see a nice summary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 7
##   term        estimate std.error statistic   p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)    1.74     0.0824      21.2 7.62e- 83    1.58      1.91 
## 2 BB             0.387    0.0270      14.3 1.20e- 42    0.334     0.440
## 3 HR             1.56     0.0490      31.9 1.78e-155    1.47      1.66&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we fit the model with only one variable, the estimated slopes were 0.7353288 and 1.8448241 for BB and HR, respectively. Note that when fitting the multivariate model both go down, with the BB effect decreasing much more.&lt;/p&gt;
&lt;p&gt;Now we want to construct a metric to pick players, we need to consider singles, doubles, and triples as well. Can we build a model that predicts runs based on all these outcomes?&lt;/p&gt;
&lt;p&gt;We now are going to take somewhat of a “leap of faith” and assume that these five variables are jointly normal. This means that if we pick any one of them, and hold the other four fixed, the relationship with the outcome is linear and the slope does not depend on the four values held constant. If this is true, then a linear model for our data is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \beta_3 x_{i,3}+ \beta_4 x_{i,4} + \beta_5 x_{i,5} + \varepsilon_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\)&lt;/span&gt; representing BB, singles, doubles, triples, and HR respectively.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;lm&lt;/code&gt;, we can quickly find the LSE for the parameters using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(BB = BB / G,
         singles = (H - X2B - X3B - HR) / G,
         doubles = X2B / G,
         triples = X3B / G,
         HR = HR / G,
         R = R / G) %&amp;gt;%
  lm(R ~ BB + singles + doubles + triples + HR, data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the coefficients using &lt;code&gt;tidy&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs &amp;lt;- tidy(fit, conf.int = TRUE)

coefs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 7
##   term        estimate std.error statistic   p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)   -2.77     0.0862     -32.1 4.76e-157   -2.94     -2.60 
## 2 BB             0.371    0.0117      31.6 1.87e-153    0.348     0.394
## 3 singles        0.519    0.0127      40.8 8.67e-217    0.494     0.544
## 4 doubles        0.771    0.0226      34.1 8.44e-171    0.727     0.816
## 5 triples        1.24     0.0768      16.1 2.12e- 52    1.09      1.39 
## 6 HR             1.44     0.0243      59.3 0            1.40      1.49&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function &lt;code&gt;predict&lt;/code&gt;, then make a plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;%
  filter(yearID %in% 2002) %&amp;gt;%
  mutate(BB = BB/G,
         singles = (H-X2B-X3B-HR)/G,
         doubles = X2B/G,
         triples =X3B/G,
         HR=HR/G,
         R=R/G)  %&amp;gt;%
  mutate(R_hat = predict(fit, newdata = .)) %&amp;gt;%
  ggplot(aes(R_hat, R, label = teamID)) +
  geom_point() +
  geom_text(nudge_x=0.1, cex = 2) +
  geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/model-predicts-runs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our model does quite a good job as demonstrated by the fact that points from the observed versus predicted plot fall close to the identity line.&lt;/p&gt;
&lt;p&gt;So instead of using batting average, or just number of HR, as a measure of picking players, we can use our fitted model to form a metric that relates more directly to run production. Specifically, to define a metric for player A, we imagine a team made up of players just like player A and use our fitted regression model to predict how many runs this team would produce. The formula would look like this:
-2.7691857 +
0.3712147 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; BB +
0.5193923 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; singles +
0.7711444 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; doubles +
1.2399696 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; triples +
1.4433701 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; HR.&lt;/p&gt;
&lt;p&gt;To define a player-specific metric, we have a bit more work to do. A challenge here is that we derived the metric for teams, based on team-level summary statistics. For example, the HR value that is entered into the equation is HR per game for the entire team. If we compute the HR per game for a player, it will be much lower since the total is accumulated by 9 batters. Furthermore, if a player only plays part of the game and gets fewer opportunities than average, it is still considered a game played. For players, a rate that takes into account opportunities is the per-plate-appearance rate.&lt;/p&gt;
&lt;p&gt;To make the per-game team rate comparable to the per-plate-appearance player rate, we compute the average number of team plate appearances per game:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pa_per_game &amp;lt;- Batting %&amp;gt;% filter(yearID == 2002) %&amp;gt;%
  group_by(teamID) %&amp;gt;%
  summarize(pa_per_game = sum(AB+BB)/max(G)) %&amp;gt;%
  pull(pa_per_game) %&amp;gt;%
  mean&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We compute the per-plate-appearance rates for players available in 2002 on data from 1997-2001. To avoid small sample artifacts, we filter players with less than 200 plate appearances per year. Here is the entire calculation in one line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players &amp;lt;- Batting %&amp;gt;% filter(yearID %in% 1997:2001) %&amp;gt;%
  group_by(playerID) %&amp;gt;%
  mutate(PA = BB + AB) %&amp;gt;%
  summarize(G = sum(PA)/pa_per_game,
    BB = sum(BB)/G,
    singles = sum(H-X2B-X3B-HR)/G,
    doubles = sum(X2B)/G,
    triples = sum(X3B)/G,
    HR = sum(HR)/G,
    AVG = sum(H)/sum(AB),
    PA = sum(PA)) %&amp;gt;%
  filter(PA &amp;gt;= 1000) %&amp;gt;%
  select(-G) %&amp;gt;%
  mutate(R_hat = predict(fit, newdata = .))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The player-specific predicted runs computed here can be interpreted as the number of runs we predict a team will score if all batters are exactly like that player. The distribution shows that there is wide variability across players:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(R_hat, data = players, binwidth = 0.5, color = I(&amp;quot;black&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/r-hat-hist-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;adding-salary-and-position-information&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding salary and position information&lt;/h3&gt;
&lt;p&gt;To actually build the team, we will need to know their salaries as well as their defensive position. For this, we join the &lt;code&gt;players&lt;/code&gt; data frame we just created with the player information data frame included in some of the other Lahman data tables. We will learn more about the join function (and we will discuss this further in a later lecture).&lt;/p&gt;
&lt;p&gt;Start by adding the 2002 salary of each player:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players &amp;lt;- Salaries %&amp;gt;%
  filter(yearID == 2002) %&amp;gt;%
  select(playerID, salary) %&amp;gt;%
  right_join(players, by=&amp;quot;playerID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we add their defensive position. This is a somewhat complicated task because players play more than one position each year. The &lt;strong&gt;Lahman&lt;/strong&gt; package table &lt;code&gt;Appearances&lt;/code&gt; tells how many games each player played in each position, so we can pick the position that was most played using &lt;code&gt;which.max&lt;/code&gt; on each row. We use &lt;code&gt;apply&lt;/code&gt; to do this. However, because some players are traded, they appear more than once on the table, so we first sum their appearances across teams.
Here, we pick the one position the player most played using the &lt;code&gt;top_n&lt;/code&gt; function. To make sure we only pick one position, in the case of ties, we pick the first row of the resulting data frame. We also remove the &lt;code&gt;OF&lt;/code&gt; position which stands for outfielder, a generalization of three positions: left field (LF), center field (CF), and right field (RF). We also remove pitchers since they don’t bat in the league in which the A’s play.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;position_names &amp;lt;-
  paste0(&amp;quot;G_&amp;quot;, c(&amp;quot;p&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;1b&amp;quot;,&amp;quot;2b&amp;quot;,&amp;quot;3b&amp;quot;,&amp;quot;ss&amp;quot;,&amp;quot;lf&amp;quot;,&amp;quot;cf&amp;quot;,&amp;quot;rf&amp;quot;, &amp;quot;dh&amp;quot;))

tmp &amp;lt;- Appearances %&amp;gt;%
  filter(yearID == 2002) %&amp;gt;%
  group_by(playerID) %&amp;gt;%
  summarize_at(position_names, sum) %&amp;gt;%
  ungroup()

pos &amp;lt;- tmp %&amp;gt;%
  select(position_names) %&amp;gt;%
  apply(., 1, which.max)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Note: Using an external vector in selections is ambiguous.
## ℹ Use `all_of(position_names)` instead of `position_names` to silence this message.
## ℹ See &amp;lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&amp;gt;.
## This message is displayed once per session.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players &amp;lt;- tibble(playerID = tmp$playerID, POS = position_names[pos]) %&amp;gt;%
  mutate(POS = str_to_upper(str_remove(POS, &amp;quot;G_&amp;quot;))) %&amp;gt;%
  filter(POS != &amp;quot;P&amp;quot;) %&amp;gt;%
  right_join(players, by=&amp;quot;playerID&amp;quot;) %&amp;gt;%
  filter(!is.na(POS)  &amp;amp; !is.na(salary))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we add their first and last name:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players &amp;lt;- Master %&amp;gt;%
  select(playerID, nameFirst, nameLast, debut) %&amp;gt;%
  mutate(debut = as.Date(debut)) %&amp;gt;%
  right_join(players, by=&amp;quot;playerID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are a baseball fan, you will recognize the top 10 players:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players %&amp;gt;% select(nameFirst, nameLast, POS, salary, R_hat) %&amp;gt;%
  arrange(desc(R_hat)) %&amp;gt;% top_n(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Selecting by R_hat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    nameFirst nameLast POS   salary    R_hat
## 1      Barry    Bonds  LF 15000000 8.441480
## 2      Larry   Walker  RF 12666667 8.344316
## 3       Todd   Helton  1B  5000000 7.764649
## 4      Manny  Ramirez  LF 15462727 7.714582
## 5      Sammy     Sosa  RF 15000000 7.559582
## 6       Jeff  Bagwell  1B 11000000 7.405572
## 7       Mike   Piazza   C 10571429 7.343984
## 8      Jason   Giambi  1B 10428571 7.263690
## 9      Edgar Martinez  DH  7086668 7.259399
## 10       Jim    Thome  1B  8000000 7.231955&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;picking-nine-players&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Picking nine players&lt;/h3&gt;
&lt;p&gt;On average, players with a higher metric have higher salaries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players %&amp;gt;% ggplot(aes(salary, R_hat, color = POS)) +
  geom_point() +
  scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/predicted-runs-vs-salary-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;!--Notice the very high salaries for most players. We do see some low-cost players with very high metrics. These will be great for our team. Some of these are likely young players that have not yet been able to negotiate a salary and are unavailable.

Here we remake plot without players that debuted before 1998. We use the __lubridate__ function `year`, introduced in Section \@ref(lubridate).

```r
library(lubridate)
players %&gt;% filter(year(debut) &lt; 1998) %&gt;%
 ggplot(aes(salary, R_hat, color = POS)) +
  geom_point() +
  scale_x_log10()
```

&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/predicted-runs-vs-salary-no-rookies-1.png&#34; width=&#34;672&#34; /&gt;
--&gt;
&lt;p&gt;We can search for good deals by looking at players who produce many more runs than others with similar salaries. We can use this table to decide what players to pick and keep our total salary below the 40 million dollars Billy Beane had to work with. This can be done using what computer scientists call linear programming. This is not something we teach, but here are the position players selected with this approach:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameFirst
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
POS
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
salary
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
R_hat
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Todd
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Helton
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1B
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.764649
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mike
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Piazza
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10571429
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.343984
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Edgar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Martinez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
DH
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7086668
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.259399
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jim
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Edmonds
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CF
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7333333
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.552456
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jeff
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kent
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2B
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.391614
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Phil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nevin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3B
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2600000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.163936
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Matt
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Stairs
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RF
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.062372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Henry
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Rodriguez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
LF
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
300000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.938315
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
John
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Valentin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SS
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
550000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.273441
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that all these players have above average BB and most have above average HR rates, while the same is not true for singles. Here is a table with statistics standardized across players so that, for example, above average HR hitters have values above 0.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
BB
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
singles
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
doubles
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
triples
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
HR
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AVG
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
R_hat
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Helton
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9088340
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2147828
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.6489997
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3105275
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5221254
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.6704562
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5316660
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Piazza
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3281058
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4231217
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2037161
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.4181571
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.8253653
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.1990055
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0890701
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Martinez
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.1352215
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0051702
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2649044
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2242578
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8079817
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.2032836
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0000756
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Edmonds
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0706548
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5579104
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7912381
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1517126
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9730052
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8543566
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2562767
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kent
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2316321
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7322902
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0113988
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4483097
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7658693
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7871932
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0870488
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nevin
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3066863
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9051225
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4787634
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1908955
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1927055
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1048721
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8475017
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Stairs
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0996635
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5127562
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0460876
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1285395
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1209081
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5608456
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7406428
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Rodriguez
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2011513
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5963595
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3324557
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7823620
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3202734
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6723416
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6101181
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Valentin
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1802855
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9287069
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.7940379
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4348410
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0452462
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4717038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0894187
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-regression-fallacy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The regression fallacy&lt;/h2&gt;
&lt;p&gt;Wikipedia defines the &lt;em&gt;sophomore slump&lt;/em&gt; as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The &lt;em&gt;sophmore slump&lt;/em&gt; phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this Fox Sports article&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”.&lt;/p&gt;
&lt;p&gt;Does the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for batting average, we see that this observation holds true for the top performing ROYs:&lt;/p&gt;
&lt;!--The data is available in the Lahman library, but we have to do some work to create a table with the statistics for all the ROY. First we create a table with player ID, their names, and their most played position.--&gt;
&lt;!--
Now, we will create a table with only the ROY award winners and add their batting statistics. We filter out pitchers, since pitchers are not given awards for batting and we are going to focus on offense. Specifically, we will focus on batting average since it is the summary that most pundits talk about when discussing the sophomore slump:
--&gt;
&lt;!--
We also will keep only the rookie and sophomore seasons and remove players that did not play sophomore seasons:
--&gt;
&lt;!--
Finally, we will use the `spread` function to have one column for the rookie and sophomore years batting averages:
--&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameFirst
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rookie_year
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rookie
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sophomore
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Willie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
McCovey
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1959
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3541667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2384615
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ichiro
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Suzuki
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2001
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3497110
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3214838
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Al
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bumbry
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1973
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3370787
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2333333
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fred
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Lynn
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1975
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3314394
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3136095
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pujols
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2001
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3288136
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3135593
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In fact, the proportion of players that have a lower batting average their sophomore year is 0.6862745.&lt;/p&gt;
&lt;p&gt;So is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).&lt;/p&gt;
&lt;!--We perform similar operations to what we did above: --&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;playerID&amp;#39;. You can override using the `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same pattern arises when we look at the top performers: batting averages go down for most of the top performers.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameFirst
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2013
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2014
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miguel
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cabrera
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3477477
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3126023
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hanley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ramirez
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3453947
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2828508
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Michael
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cuddyer
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3312883
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3315789
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Scooter
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Gennett
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3239437
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2886364
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mauer
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3235955
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2769231
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But these are not rookies! Also, look at what happens to the worst performers of 2013:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameFirst
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2013
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2014
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Danny
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Espinosa
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1582278
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2192192
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Dan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Uggla
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1785714
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1489362
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jeff
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mathis
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1810345
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
B. J.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Upton
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1841432
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2080925
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adam
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Rosales
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1904762
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2621951
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Their batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/regression-fallacy-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The correlation is 0.460254 and
the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for any given player that had a 2013 batting average &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{Y - .255}{.032} = 0.46 \left( \frac{X - .261}{.023}\right) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; so it is expected that &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; will regress to the mean.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-error-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measurement error models&lt;/h2&gt;
&lt;p&gt;Up to now, all our linear regression examples have been applied to two or more random variables. We assume the pairs are bivariate normal and use this to motivate a linear model. This approach covers most real-life examples of linear regression. The other major application comes from measurement errors models. In these applications, it is common to have a non-random covariate, such as time, and randomness is introduced from measurement error rather than sampling or natural variability.&lt;/p&gt;
&lt;p&gt;To understand these models, imagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let’s simulate some data using the equations we know today and adding some measurement error. The &lt;strong&gt;dslabs&lt;/strong&gt; function &lt;code&gt;rfalling_object&lt;/code&gt; generates these simulations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
falling_object &amp;lt;- rfalling_object()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The assistants hand the data to Galileo and this is what he sees:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;falling_object %&amp;gt;%
  ggplot(aes(time, observed_distance)) +
  geom_point() +
  ylab(&amp;quot;Distance in meters&amp;quot;) +
  xlab(&amp;quot;Time in seconds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/gravity-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Galileo does not know the exact equation, but by looking at the plot above, he deduces that the position should follow a parabola, which we can write like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(x) = \beta_0 + \beta_1 x + \beta_2 x^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The data does not fall exactly on a parabola. Galileo knows this is due to measurement error. His helpers make mistakes when measuring the distance. To account for this, he models the data with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i, i=1,\dots,n \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; representing distance in meters, &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; representing time in seconds, and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; accounting for measurement error. The measurement error is assumed to be random, independent from each other, and having the same distribution for each &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. We also assume that there is no bias, which means the expected value &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}[\varepsilon] = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that this is a linear model because it is a linear combination of known quantities (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x^2\)&lt;/span&gt; are known) and unknown parameters (the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;s are unknown parameters to Galileo). Unlike our previous examples, here &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a fixed quantity; we are not conditioning.&lt;/p&gt;
&lt;p&gt;To pose a new physical theory and start making predictions about other falling objects, Galileo needs actual numbers, rather than unknown parameters. Using LSE seems like a reasonable approach. How do we find the LSE?&lt;/p&gt;
&lt;p&gt;LSE calculations do not require the errors to be approximately normal. The &lt;code&gt;lm&lt;/code&gt; function will find the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; s that will minimize the residual sum of squares:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- falling_object %&amp;gt;%
  mutate(time_sq = time^2) %&amp;gt;%
  lm(observed_distance~time+time_sq, data=.)
tidy(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)   56.1       0.592    94.9   2.23e-17
## 2 time          -0.786     0.845    -0.930 3.72e- 1
## 3 time_sq       -4.53      0.251   -18.1   1.58e- 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check if the estimated parabola fits the data. The &lt;strong&gt;broom&lt;/strong&gt; function &lt;code&gt;augment&lt;/code&gt; lets us do this easily:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;augment(fit) %&amp;gt;%
  ggplot() +
  geom_point(aes(time, observed_distance)) +
  geom_line(aes(time, .fitted), col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/07-content_files/figure-html/falling-object-fit-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thanks to my high school physics teacher, I know that the equation for the trajectory of a falling object is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d = h_0 + v_0 t -  0.5 \times 9.8 t^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(h_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_0\)&lt;/span&gt; the starting height and velocity, respectively. The data we simulated above followed this equation and added measurement error to simulate &lt;code&gt;n&lt;/code&gt; observations for dropping the ball &lt;span class=&#34;math inline&#34;&gt;\((v_0=0)\)&lt;/span&gt; from the tower of Pisa &lt;span class=&#34;math inline&#34;&gt;\((h_0=55.86)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These are consistent with the parameter estimates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 × 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)   56.1       0.592    94.9   2.23e-17    54.8      57.4 
## 2 time          -0.786     0.845    -0.930 3.72e- 1    -2.65      1.07
## 3 time_sq       -4.53      0.251   -18.1   1.58e- 9    -5.08     -3.98&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Tower of Pisa height is within the confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, the initial velocity 0 is in the confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; (note the p-value is larger than 0.05), and the acceleration constant is in a confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(-2 \times \beta_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\mbox{BB}}{\mbox{PA}} + \frac{\mbox{Singles} + 2 \mbox{Doubles} + 3 \mbox{Triples} + 4\mbox{HR}}{\mbox{AB}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;They called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Compute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For every year since 1961, compute the correlation between runs per game and OPS; then plot these correlations as a function of year.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note that we can rewrite OPS as a weighted average of BBs, singles, doubles, triples, and HRs. We know that the weights for doubles, triples, and HRs are 2, 3, and 4 times that of singles. But what about BB? What is the weight for BB relative to singles? Hint: the weight for BB relative to singles will be a function of AB and PA.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note that the weight for BB, &lt;span class=&#34;math inline&#34;&gt;\(\frac{\mbox{AB}}{\mbox{PA}}\)&lt;/span&gt;, will change from team to team. To see how variable it is, compute and plot this quantity for each team for each year since 1961. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;So now we know that the formula for OPS is proportional to &lt;span class=&#34;math inline&#34;&gt;\(0.91 \times \mbox{BB} + \mbox{singles} + 2 \times \mbox{doubles} + 3 \times \mbox{triples} + 4 \times \mbox{HR}\)&lt;/span&gt;. Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1961, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1961, compute the OPS, the predicted runs with the regression model and compute the correlation between the two as well as the correlation with runs per game.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://mlb.mlb.com/stats/league_leaders.jsp&#34; class=&#34;uri&#34;&gt;http://mlb.mlb.com/stats/league_leaders.jsp&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Bill_James&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Bill_James&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Sabermetrics&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Sabermetrics&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/User:Cburnett&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/User:Cburnett&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/3.0/deed.en&#34; class=&#34;uri&#34;&gt;https://creativecommons.org/licenses/by-sa/3.0/deed.en&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.flickr.com/people/27003603@N00&#34; class=&#34;uri&#34;&gt;https://www.flickr.com/people/27003603@N00&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/2.0&#34; class=&#34;uri&#34;&gt;https://creativecommons.org/licenses/by-sa/2.0&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.baseball-almanac.com/awards/lou_brock_award.shtml&#34; class=&#34;uri&#34;&gt;http://www.baseball-almanac.com/awards/lou_brock_award.shtml&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715&#34; class=&#34;uri&#34;&gt;http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715&lt;/a&gt;&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizations in Practice</title>
      <link>https://ssc442.netlify.app/content/04-content/</link>
      <pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/04-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#readings&#34;&gt;Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-visualization-principles&#34;&gt;Data visualization principles&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#encoding-data-using-visual-cues&#34;&gt;Encoding data using visual cues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#avoid-psuedo-3d-plots&#34;&gt;Avoid Psuedo-3D Plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#avoid-too-many-significant-digits&#34;&gt;Avoid too many significant digits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#know-your-audience&#34;&gt;Know your audience&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#know-when-to-include-0&#34;&gt;Know when to include 0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#do-not-distort-quantities&#34;&gt;Do not distort quantities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#order-categories-by-a-meaningful-value&#34;&gt;Order categories by a meaningful value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#show-the-data&#34;&gt;Show the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#faceting&#34;&gt;Faceting&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#use-common-axes-with-facets&#34;&gt;Use common axes with facets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes&#34;&gt;Align plots vertically to see horizontal changes and horizontally to see vertical changes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#facet-grids&#34;&gt;Facet grids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visual-cues-to-be-compared-should-be-adjacent-continued&#34;&gt;Visual cues to be compared should be adjacent, continued&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-color&#34;&gt;Use color&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#think-of-the-color-blind&#34;&gt;Think of the color blind&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#using-a-discrete-color-palette&#34;&gt;Using a discrete color palette&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-a-continuous-color-palette&#34;&gt;Using a continuous color palette&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gridextra-and-grid.arrange&#34;&gt;gridExtra and &lt;code&gt;grid.arrange&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;readings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Readings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Why do we create visualizations? What types of data are best suited for visuals?&lt;/li&gt;
&lt;li&gt;How do we best visualize the variability in our data?&lt;/li&gt;
&lt;li&gt;What makes a visual compelling?&lt;/li&gt;
&lt;li&gt;What are the worst visuals? Which of these are most frequently used? Why?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As with last week’s content, the technical aspects of this lecture will be explored in greater detail in the Thursday practical lecture. Today, we will focus on some principles.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The greatest value of a picture is when it forces us to notice what we never expected to see.” – John Tukey&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;data-visualization-principles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data visualization principles&lt;/h1&gt;
&lt;p&gt;We have already provided some rules to follow as we created plots for our examples. Here, we aim to provide some general principles we can use as a guide for effective data visualization. Much of this section is based on a talk by Karl Broman&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; titled “Creating Effective Figures and Tables”&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; and includes some of the figures which were made with code that Karl makes available on his GitHub repository&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, as well as class notes from Peter Aldhous’ Introduction to Data Visualization course&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Following Karl’s approach, we show some examples of plot styles we should avoid, explain how to improve them, and use these as motivation for a list of principles. We compare and contrast plots that follow these principles to those that don’t.&lt;/p&gt;
&lt;p&gt;The principles are mostly based on research related to how humans detect patterns and make visual comparisons. The preferred approaches are those that best fit the way our brains process visual information. When deciding on a visualization approach, it is also important to keep our goal in mind. We may be comparing a viewable number of quantities, describing distributions for categories or numeric values, comparing the data from two groups, or describing the relationship between two variables. As a final note, we want to emphasize that for a data scientist it is important to adapt and optimize graphs to the audience. For example, an exploratory plot made for ourselves will be different than a chart intended to communicate a finding to a general audience.&lt;/p&gt;
&lt;p&gt;As with the discussion above, we will be using these libraries—note the addition of &lt;strong&gt;gridExtra&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
library(gridExtra)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;encoding-data-using-visual-cues&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Encoding data using visual cues&lt;/h3&gt;
&lt;p&gt;We start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue.&lt;/p&gt;
&lt;p&gt;To illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/piechart-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. The donut chart is an example of a plot that uses only area:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/donutchart-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In fact, the &lt;code&gt;pie&lt;/code&gt; R function help file states that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Browser
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2000
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2015
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Opera
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Safari
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Firefox
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Chrome
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
27
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/two-barplots-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.&lt;/p&gt;
&lt;p&gt;If for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/excel-barplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;avoid-psuedo-3d-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Avoid Psuedo-3D Plots&lt;/h3&gt;
&lt;p&gt;The figure below, taken from the scientific literature&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;,
shows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/fig8b.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Image courtesy of Karl Broman)&lt;/p&gt;
&lt;p&gt;Humans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##First read data
url &amp;lt;- &amp;quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig8dat.csv&amp;quot;
dat &amp;lt;- read.csv(url)

##Now make alternative plot
dat %&amp;gt;% gather(drug, survival, -log.dose) %&amp;gt;%
  mutate(drug = gsub(&amp;quot;Drug.&amp;quot;,&amp;quot;&amp;quot;,drug)) %&amp;gt;%
  ggplot(aes(log.dose, survival, color = drug)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/colors-for-different-lines-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how much easier it is to determine the survival values.&lt;/p&gt;
&lt;p&gt;Pseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/fig1e.png&#34; /&gt;
&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/fig2d.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Images courtesy of Karl Broman)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;avoid-too-many-significant-digits&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Avoid too many significant digits&lt;/h3&gt;
&lt;p&gt;By default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
state
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Measles
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Pertussis
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Polio
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1940
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.8826320
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.3397861
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8266512
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13.9124205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.7467350
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.9742639
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1960
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.1386471
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2640419
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9767889
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1980
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3743467
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0515466
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
state
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Measles
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Pertussis
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Polio
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1940
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1960
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1980
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Useful ways to change the number of significant digits or to round numbers are &lt;code&gt;signif&lt;/code&gt; and &lt;code&gt;round&lt;/code&gt;. You can define the number of significant digits globally by setting options like this: &lt;code&gt;options(digits = 3)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Another principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
state
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
disease
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1940
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1950
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1960
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1970
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1980
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Measles
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pertussis
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Polio
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;know-your-audience&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Know your audience&lt;/h3&gt;
&lt;p&gt;Graphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot.&lt;/p&gt;
&lt;p&gt;As a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;know-when-to-include-0&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Know when to include 0&lt;/h3&gt;
&lt;p&gt;When using barplots, it is misinformative not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example used by Peter Aldhous in this lecture: &lt;a href=&#34;http://paldhous.github.io/ucb/2016/dataviz/week2.html&#34;&gt;http://paldhous.github.io/ucb/2016/dataviz/week2.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/class2_8.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: Fox News, via Media Matters&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;From the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/barplot-from-zero-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is another example, described in detail in a Flowing Data blog post:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/Bush-cuts.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: Fox News, via Flowing Data&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;This plot makes a 13% increase look like a five fold change. Here is the appropriate plot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/barplot-from-zero-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/venezuela-election.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source:
Venezolana de Televisión via Pakistan Today&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; and Diego Mariano.)&lt;/p&gt;
&lt;p&gt;Here is the appropriate plot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/barplot-from-zero-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/points-plot-not-from-zero-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do-not-distort-quantities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Do not distort quantities&lt;/h3&gt;
&lt;p&gt;During President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/state-of-the-union.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: The 2011 State of the Union Address&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Judging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gdp &amp;lt;- c(14.6, 5.7, 5.3, 3.3, 2.5)
gdp_data &amp;lt;- data.frame(Country = rep(c(&amp;quot;United States&amp;quot;, &amp;quot;China&amp;quot;, &amp;quot;Japan&amp;quot;, &amp;quot;Germany&amp;quot;, &amp;quot;France&amp;quot;),2),
           y = factor(rep(c(&amp;quot;Radius&amp;quot;,&amp;quot;Area&amp;quot;),each=5), levels = c(&amp;quot;Radius&amp;quot;, &amp;quot;Area&amp;quot;)),
           GDP= c(gdp^2/min(gdp^2), gdp/min(gdp))) %&amp;gt;%
   mutate(Country = reorder(Country, GDP))
gdp_data %&amp;gt;%
  ggplot(aes(Country, y, size = GDP)) +
  geom_point(show.legend = FALSE, color = &amp;quot;blue&amp;quot;) +
  scale_size(range = c(2,25)) +
  coord_flip() +
  ylab(&amp;quot;&amp;quot;) + xlab(&amp;quot;&amp;quot;) # identical to labs(y = &amp;quot;&amp;quot;, x = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/area-not-radius-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not surprisingly, &lt;strong&gt;ggplot2&lt;/strong&gt; defaults to using area rather than
radius. Of course, in this case, we really should not be using area at all since we can use position and length:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gdp_data %&amp;gt;%
  filter(y == &amp;quot;Area&amp;quot;) %&amp;gt;%
  ggplot(aes(Country, GDP)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, width = 0.5) +
  labs(y = &amp;quot;GDP in trillions of US dollars&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/barplot-better-than-area-1.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;order-categories-by-a-meaningful-value&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Order categories by a meaningful value&lt;/h3&gt;
&lt;p&gt;When one of the axes is used to show categories, as is done in barplots, the default &lt;strong&gt;ggplot2&lt;/strong&gt; behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.&lt;/p&gt;
&lt;p&gt;We previously learned how to use the &lt;code&gt;reorder&lt;/code&gt; function, which helps us achieve this goal.
To appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(murders)
p1 &amp;lt;- murders %&amp;gt;% mutate(murder_rate = total / population * 100000) %&amp;gt;%
  ggplot(aes(x = state, y = murder_rate)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 8))  +
  xlab(&amp;quot;&amp;quot;)

p2 &amp;lt;- murders %&amp;gt;% mutate(murder_rate = total / population * 100000) %&amp;gt;%
  mutate(state = reorder(state, murder_rate)) %&amp;gt;% # here&amp;#39;s the magic!
  ggplot(aes(x = state, y = murder_rate)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 8))  +
  xlab(&amp;quot;&amp;quot;)

grid.arrange(p1, p2, ncol = 2) # we&amp;#39;ll cover this later&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/do-not-order-alphabetically-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can make the second plot like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(murders)
murders %&amp;gt;% mutate(murder_rate = total / population * 100000) %&amp;gt;%
  mutate(state = reorder(state, murder_rate)) %&amp;gt;%
  ggplot(aes(state, murder_rate)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 6)) +
  xlab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;reorder&lt;/code&gt; function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/reorder-boxplot-example-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first orders the regions alphabetically, while the second orders them by the group’s median.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;show-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Show the data&lt;/h2&gt;
&lt;p&gt;We have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups.&lt;/p&gt;
&lt;p&gt;To motivate our first principle, “show the data”, we go back to our artificial example of describing heights to a person who is unaware of some basic facts about the population of interest (and is otherwise unsophisticated). This time let’s assume that this person is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors.&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; The plot looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/show-data-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.&lt;/p&gt;
&lt;p&gt;This brings us to our first principle: show the data. This simple &lt;strong&gt;ggplot2&lt;/strong&gt; code already generates a more informative plot than the barplot by simply showing all the data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(sex, height)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/show-data-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.&lt;/p&gt;
&lt;p&gt;The first is to add &lt;em&gt;jitter&lt;/em&gt;, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using &lt;em&gt;alpha blending&lt;/em&gt;: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(sex, height)) +
  geom_jitter(width = 0.1, alpha = 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/show-points-with-jitter-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;faceting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Faceting&lt;/h2&gt;
&lt;p&gt;Looking at the previous plot, it’s easy to tell that males tend to be taller than females. Before, we showed how we can plot two distributions over each other using an aesthetic mapping. Something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(x = height, fill = sex)) +
  geom_histogram(alpha = .5, show.legend = TRUE) +
  labs(fill = &amp;#39;Sex&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
Sometimes, putting the plots on top of each other, even with a well-chosen alpha, does not clearly communicate the differences in the distribution. When we want to compare side-by-side, we will often use &lt;strong&gt;facets&lt;/strong&gt;. Facets are a bit like supercharged aesthetic mapping because they let us separate plots based on categorical variables, but instead of putting them together, we can have side-by-side plots.&lt;/p&gt;
&lt;p&gt;Two functions in &lt;code&gt;ggplot&lt;/code&gt; give facets: &lt;code&gt;facet_wrap&lt;/code&gt; and &lt;code&gt;facet_grid&lt;/code&gt;. We’ll use &lt;code&gt;facet_grid&lt;/code&gt; as this is a little more powerful.&lt;/p&gt;
&lt;p&gt;Facets are added as an additional layer like this: &lt;code&gt;+ facet_grid(. ~ sex)&lt;/code&gt;. Inside the function, we have a “formula” that is written without quotes (which is unusual for R). Since &lt;code&gt;facet_grid&lt;/code&gt; takes a “formula”, all we have to do to facet is decide how we want to lay out our plots. If we want each of the faceting groups to lie along the vertical axis, we put the variable on which we want to facet before the “~”, and after the “~” we simply put a period. If we want the groups to lie along the horizontal axis, we put the variable after the “~” and the period before. In the example, we’ll separate the histogram by drawing them side by side along the horizontal axis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(x = height)) +
  geom_histogram(binwidth = 1, color=&amp;quot;black&amp;quot;) +
  facet_grid(.~sex)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;
This would be the result if we took the females, plotted the histogram, then took the males, made another histogram, and then put them side by side. But we do it in one command by adding &lt;code&gt;+facet_grid(...)&lt;/code&gt;&lt;/p&gt;
&lt;div id=&#34;use-common-axes-with-facets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use common axes with facets&lt;/h3&gt;
&lt;p&gt;Since we have plots side-by-side, they can have different scales along the x-axis (or along the y-axis if we were stacking with &lt;code&gt;sex ~ .&lt;/code&gt;). We want to be careful here - if we don’t have matching scales on these axes, then it’ll be really hard to visually see differences in the distribution.&lt;/p&gt;
&lt;p&gt;As an example of what not to do, and to show that we can use the &lt;code&gt;scales&lt;/code&gt; argument in &lt;code&gt;facet_grid&lt;/code&gt;, we can allow the x-axis to freely scale between the plots. This makes it hard to tell that males are, on average, taller because the average male height, despite being larger than the average female height (70 vs. 65 or so) &lt;em&gt;falls in the same location within the plot box&lt;/em&gt;. Note that 80 is the extreme edge for the left plot, but not in the right plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1, color=&amp;quot;black&amp;quot;) +
  facet_grid(. ~ sex, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/common-axes-histograms-wrong-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Align plots vertically to see horizontal changes and horizontally to see vertical changes&lt;/h3&gt;
&lt;p&gt;In these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1, color=&amp;quot;black&amp;quot;) +
  facet_grid(. ~ sex)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/common-axes-histograms-right-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2 &amp;lt;- heights %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1, color=&amp;quot;black&amp;quot;) +
  facet_grid(sex~.)
p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot makes it much easier to notice that men’s heights are, on average, higher.&lt;/p&gt;
&lt;p&gt;The sample size of females is smaller than of males – that is, we have more males in the data. Try &lt;code&gt;table(heights$sex)&lt;/code&gt; to see this. It’s also clear from the above plot because the height of the bars on the y-axis (&lt;code&gt;count&lt;/code&gt;) are smaller for females. If we are interested in the distribution within our sample, this is useful. If we’re interested in the distribution of females vs. the distribution of males, we might want to re-scale the y-axis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2 &amp;lt;- heights %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1, color=&amp;quot;black&amp;quot;) +
  facet_grid(sex~., scales = &amp;#39;free_y&amp;#39;)
p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We still have &lt;code&gt;count&lt;/code&gt; on the y-axis, so we didn’t switch to density (though it would look the same). Instead, we rescaled the y-axis, which gives us a different perspective but still contains the count information.&lt;/p&gt;
&lt;p&gt;If we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our &lt;em&gt;show the data&lt;/em&gt; principle, we then overlay all the data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;- heights %&amp;gt;%
  ggplot(aes(sex, height)) +
  geom_boxplot(coef=3) +
  geom_jitter(width = 0.1, alpha = 0.2) +
  ylab(&amp;quot;Height in inches&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now contrast and compare these three plots, based on exactly the same data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/show-the-data-comparison-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;facet-grids&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Facet grids&lt;/h3&gt;
&lt;p&gt;As the name implies, &lt;code&gt;facet_grid&lt;/code&gt; can make more than just side-by-plots. If we specify variables on boths sides of the “~”, we get a grid of plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder::gapminder %&amp;gt;%
  filter(year %in% c(1952,1972, 1992, 2002)) %&amp;gt;%
  filter(continent != &amp;#39;Oceania&amp;#39;) %&amp;gt;%
  ggplot(aes(x = lifeExp)) +
  geom_density() +
  facet_grid(continent ~ year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This makes it easy to read the life expectancy distribution over time (left-to-right) and across continents (up-and-down). It makes it easy to see that Africa has spread it’s life expectancy distribution (some improved, some didn’t), while Europe has become more clustered at the top end over time. Faceting in a grid is very helpful when you have a time dimension.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visual-cues-to-be-compared-should-be-adjacent-continued&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visual cues to be compared should be adjacent, continued&lt;/h3&gt;
&lt;p&gt;For each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.&lt;/p&gt;
&lt;p&gt;Note that there are two &lt;code&gt;gapminder&lt;/code&gt; datasets, one in &lt;code&gt;dslabs&lt;/code&gt; and one in the &lt;code&gt;gapminder&lt;/code&gt; package. The &lt;code&gt;dslabs&lt;/code&gt; version has more data, so I will switch to that here by using &lt;code&gt;dslabs::gapminder&lt;/code&gt; as our data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dslabs::gapminder %&amp;gt;%
  filter(year %in% c(1970, 2010) &amp;amp; !is.na(gdp)) %&amp;gt;%
  mutate(dollars_per_day = gdp/population/365) %&amp;gt;%
  mutate(labels = paste(year, continent)) %&amp;gt;%  # creating text labels
  ggplot(aes(x = labels, y = dollars_per_day)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.25)) +
  scale_y_continuous(trans = &amp;quot;log2&amp;quot;) +
  ylab(&amp;quot;Income in dollars per day&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/boxplots-not-adjacent-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The default in &lt;strong&gt;ggplot2&lt;/strong&gt; is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dslabs::gapminder %&amp;gt;%
  filter(year %in% c(1970, 2010) &amp;amp; !is.na(gdp)) %&amp;gt;%
  mutate(dollars_per_day = gdp/population/365) %&amp;gt;%
  mutate(labels = paste(continent, year)) %&amp;gt;%
  ggplot(aes(labels, dollars_per_day)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .25)) +
  scale_y_continuous(trans = &amp;quot;log2&amp;quot;) +
  ylab(&amp;quot;Income in dollars per day&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/boxplot-adjacent-comps-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-color&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use color&lt;/h3&gt;
&lt;p&gt;The comparison becomes even easier to make if we use color to denote the two things we want to compare. Now we do not have to make the labels column and can just use &lt;code&gt;continent&lt;/code&gt; on the x-axis:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/boxplot-adjacent-comps-with-color-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;think-of-the-color-blind&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Think of the color blind&lt;/h2&gt;
&lt;p&gt;About 10% of the population is color blind. Unfortunately, the default colors used in &lt;strong&gt;ggplot2&lt;/strong&gt; are not optimal for this group. However, &lt;strong&gt;ggplot2&lt;/strong&gt; does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: &lt;a href=&#34;http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette&#34;&gt;http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_blind_friendly_cols &amp;lt;-
  c(&amp;quot;#999999&amp;quot;, &amp;quot;#E69F00&amp;quot;, &amp;quot;#56B4E9&amp;quot;, &amp;quot;#009E73&amp;quot;,
    &amp;quot;#F0E442&amp;quot;, &amp;quot;#0072B2&amp;quot;, &amp;quot;#D55E00&amp;quot;, &amp;quot;#CC79A7&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the colors
&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/color-blind-friendly-colors-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;http://www.pnas.org/content/pnas/early/2017/01/24/1617948114.full.pdf&#34;&gt;Seafood Prices Reveal Impacts of a Major Ecological Disturbance&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/Colorblind_example.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are several resources that can help you select colors, for example this one: &lt;a href=&#34;http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/&#34;&gt;http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;using-a-discrete-color-palette&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using a discrete color palette&lt;/h3&gt;
&lt;p&gt;If you’re simply trying to differentiate between groups by using color, there are many ways of changing your color palette in &lt;code&gt;ggplot&lt;/code&gt;. Most use &lt;code&gt;scale_fill_discrete&lt;/code&gt; or &lt;code&gt;scale_color_discrete&lt;/code&gt; (depending on the aesthetic for which you’re setting the color).&lt;/p&gt;
&lt;p&gt;The easiest way of getting good-looking (e.g. non-default) colors is the &lt;code&gt;scale_fill_viridis_d&lt;/code&gt; function, which “inherits” (takes the place of and has the properties of) &lt;code&gt;scale_fill_discrete&lt;/code&gt;. Viridis has four color palettes and each is designed to be used to maximize the differentiation between colors.&lt;/p&gt;
&lt;p&gt;We will subset our &lt;code&gt;dslabs::gapminder&lt;/code&gt; dataset to five different years and take a look at what Viridis colors can do across those five:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp = dslabs::gapminder %&amp;gt;%
filter(year == 1990 | year == 1995 | year==2000 |  year == 2005 | year==2010 ) %&amp;gt;%
ggplot(aes(x = continent, y = gdp/population, fill = as.factor(year)))  + coord_flip()

gp + geom_boxplot()  + labs(title = &amp;#39;Default&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The default uses five different colors plucked seemingly at random. They are actually drawn from a palette of default ggplot colors.&lt;/p&gt;
&lt;p&gt;Let’s try Viridis&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp = dslabs::gapminder %&amp;gt;%
filter(year == 1990 | year == 1995 | year==2000 |  year == 2005 | year==2010 ) %&amp;gt;%
ggplot(aes(x = continent, y = gdp/population, fill = as.factor(year)))  + coord_flip() + labs(fill = &amp;#39;Year&amp;#39;)

viridis_a = gp + geom_boxplot()  + labs(title = &amp;#39;Viridis A&amp;#39;) + scale_fill_viridis_d(option = &amp;#39;A&amp;#39;)
viridis_b = gp + geom_boxplot()  + labs(title = &amp;#39;Viridis B&amp;#39;) + scale_fill_viridis_d(option = &amp;#39;B&amp;#39;)
viridis_c = gp + geom_boxplot()  + labs(title = &amp;#39;Viridis C&amp;#39;) + scale_fill_viridis_d(option = &amp;#39;C&amp;#39;)
viridis_d = gp + geom_boxplot()  + labs(title = &amp;#39;Viridis D&amp;#39;) + scale_fill_viridis_d(option = &amp;#39;D&amp;#39;)

grid.arrange(viridis_a, viridis_b, viridis_c, viridis_d)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Viridis uses a better palette of colors that, though distinct, have some cohesiveness to them.&lt;/p&gt;
&lt;p&gt;We can also use a custom palette, like the colorblind palette from before. If the palette has more entries than we have (N) distinct categories, R reverts to the default.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gp = dslabs::gapminder %&amp;gt;%
filter(year == 1990 | year == 1995 | year==2000 |  year == 2005 | year==2010 ) %&amp;gt;%
ggplot(aes(x = continent, y = gdp/population, fill = as.factor(year)))  + coord_flip() + labs(fill = &amp;#39;Year&amp;#39;)

custom_a = gp + geom_boxplot()  + labs(title = &amp;#39;Viridis A&amp;#39;) + scale_fill_discrete(type = color_blind_friendly_cols)
custom_b = gp + geom_boxplot()  + labs(title = &amp;#39;Viridis A&amp;#39;) + scale_fill_discrete(type = color_blind_friendly_cols[1:3])

grid.arrange(custom_a, custom_b)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the lower plot, we only give it a length-3 vector of colors, and it needs 5, so it returns to default.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-a-continuous-color-palette&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using a continuous color palette&lt;/h3&gt;
&lt;p&gt;We may often want to use the color to indicate a numeric value instead of simply using it to delineate groupings. When this is the case, the &lt;code&gt;fill&lt;/code&gt; or &lt;code&gt;color&lt;/code&gt; aesthetic is set to a continuous value. For instance, if one were to plot election results by precinct, we may represent precincts with heavy Republican support as dark red, swing districts as purple or white, and Democratic districts as blue. The intensity of red/blue indicates how heavily slanted votes in that precinct were in the election. This is known as a &lt;em&gt;color ramp&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Lets plot one country’s GDP by year, but have the color indicate the life expectancy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dslabs::gapminder %&amp;gt;%
  filter(country==&amp;#39;Romania&amp;#39; &amp;amp; year&amp;gt;1980) %&amp;gt;%
  ggplot(aes(x = year, y = gdp/population, color = life_expectancy)) +
  scale_fill_continuous() +
  geom_point(size = 5) +
  labs(x = &amp;#39;Year&amp;#39;, y = &amp;#39;GDP Per Capita&amp;#39;, fill = &amp;#39;Life Expectancy&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that GDP per capita went up, then down in 1989 (fall of the Soviet Union), then up after that. The color ramp tells us that life expectancy reached 75 years near the end, and it certainly improved in the post-2000 era.&lt;/p&gt;
&lt;p&gt;We can set some of the points on the ramp manually - here, the ramp starts at dark blue and ends at light blue, but what if we wanted to start at red, and at blue, and cross white in the middle? Easy! We use &lt;code&gt;scale_color_gradient2&lt;/code&gt; and specify the colors for low, mid, and high, and specify the midpoint at 72.5 years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dslabs::gapminder %&amp;gt;%
  filter(country==&amp;#39;Romania&amp;#39; &amp;amp; year&amp;gt;1980) %&amp;gt;%
  ggplot(aes(x = year, y = gdp/population, color = life_expectancy)) +
  scale_color_gradient2(low = &amp;#39;red&amp;#39;, mid = &amp;#39;white&amp;#39;, high = &amp;#39;blue&amp;#39;, midpoint = 72.5) +
  geom_point(size = 5) +
  labs(x = &amp;#39;Year&amp;#39;, y = &amp;#39;GDP Per Capita&amp;#39;, fill = &amp;#39;Life Expectancy&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The midpoint specification is extra useful when there is a threshold (like 50% of the vote) that indicates a different qualitative outcome.&lt;/p&gt;
&lt;p&gt;The gradient2 method does not always work with the colorblind discrete palette - the colors interpolated may be in the range in which colorblindness tends to be a problem:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dslabs::gapminder %&amp;gt;%
  filter(country==&amp;#39;Romania&amp;#39; &amp;amp; year&amp;gt;1980) %&amp;gt;%
  ggplot(aes(x = year, y = gdp/population, color = life_expectancy)) +
  scale_color_gradient2(low = color_blind_friendly_cols[3], mid = color_blind_friendly_cols[4], high = color_blind_friendly_cols[5], midpoint = 72.5) +
  geom_point(size = 5) +
  labs(x = &amp;#39;Year&amp;#39;, y = &amp;#39;GDP Per Capita&amp;#39;, fill = &amp;#39;Life Expectancy&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/04-content_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;gridextra-and-grid.arrange&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;gridExtra and &lt;code&gt;grid.arrange&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;gridExtra&lt;/code&gt; package has been used a few times in this lesson to combine plots using the &lt;code&gt;grid.arrange&lt;/code&gt; function. The use is pretty intuitive - you save your plots as objects &lt;code&gt;plot1 &amp;lt;- ggplot(data, aes(x = var1))&lt;/code&gt; and &lt;code&gt;plot2 &amp;lt;- ggplot(data, aes(x = var2))&lt;/code&gt;, and then use &lt;code&gt;grid.arrange(plot1, plot2)&lt;/code&gt; to combine. The function will align as best it can, and there are more advanced &lt;a href=&#34;https://cran.r-project.org/web/packages/gridExtra/vignettes/arrangeGrob.html&#34;&gt;grob-based functions&lt;/a&gt; that can adjust and align axes between plots, but we won’t get into them. If we want to set the layout, we can specify &lt;code&gt;nrow&lt;/code&gt; and &lt;code&gt;ncol&lt;/code&gt; to set the rows and columns.&lt;/p&gt;
&lt;p&gt;The very-useful &lt;code&gt;patchwork&lt;/code&gt; package is quickly replacing &lt;code&gt;grid.arrange&lt;/code&gt; and provides more flexibility.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://kbroman.org/&#34; class=&#34;uri&#34;&gt;http://kbroman.org/&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf&#34; class=&#34;uri&#34;&gt;https://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kbroman/Talk_Graphs&#34; class=&#34;uri&#34;&gt;https://github.com/kbroman/Talk_Graphs&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://paldhous.github.io/ucb/2016/dataviz/index.html&#34; class=&#34;uri&#34;&gt;http://paldhous.github.io/ucb/2016/dataviz/index.html&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://projecteuclid.org/download/pdf_1/euclid.ss/1177010488&#34; class=&#34;uri&#34;&gt;https://projecteuclid.org/download/pdf_1/euclid.ss/1177010488&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507&#34; class=&#34;uri&#34;&gt;http://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/&#34; class=&#34;uri&#34;&gt;http://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote&#34; class=&#34;uri&#34;&gt;https://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kl2g40GoRxg&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=kl2g40GoRxg&lt;/a&gt;&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;If you’re unfamiliar, standard errors are defined later in the course—do not confuse them with the standard deviation of the data.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Probability and Statistics</title>
      <link>https://ssc442.netlify.app/content/05-content/</link>
      <pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/05-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#supplemental-readings&#34;&gt;Supplemental Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discrete-probability&#34;&gt;Discrete probability&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#relative-frequency&#34;&gt;Relative frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#notation&#34;&gt;Notation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probability-distributions&#34;&gt;Probability distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monte-carlo-simulations-for-categorical-data&#34;&gt;Monte Carlo simulations for categorical data&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#setting-the-random-seed&#34;&gt;Setting the random seed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#with-and-without-replacement&#34;&gt;With and without replacement&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#independence&#34;&gt;Independence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-probabilities&#34;&gt;Conditional probabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#addition-and-multiplication-rules&#34;&gt;Addition and multiplication rules&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#multiplication-rule&#34;&gt;Multiplication rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiplication-rule-under-independence&#34;&gt;Multiplication rule under independence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#addition-rule&#34;&gt;Addition rule&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#combinations-and-permutations&#34;&gt;Combinations and permutations&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#monte-carlo-example&#34;&gt;Monte Carlo example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examples&#34;&gt;Examples&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#monty-hall-problem&#34;&gt;Monty Hall problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#birthday-problem&#34;&gt;Birthday problem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#infinity-in-practice&#34;&gt;Infinity in practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#theoretical-continuous-distributions&#34;&gt;Theoretical continuous distributions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#theoretical-distributions-as-approximations&#34;&gt;Theoretical distributions as approximations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-probability-density&#34;&gt;The probability density&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monte-carlo-simulations-for-continuous-variables&#34;&gt;Monte Carlo simulations for continuous variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-distributions&#34;&gt;Continuous distributions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-variables&#34;&gt;Random variables&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#definition-of-random-variables&#34;&gt;Definition of Random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sampling-models&#34;&gt;Sampling models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-probability-distribution-of-a-random-variable&#34;&gt;The probability distribution of a random variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distributions-versus-probability-distributions&#34;&gt;Distributions versus probability distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#notation-for-random-variables&#34;&gt;Notation for random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-expected-value-and-standard-error&#34;&gt;The expected value and standard error&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#population-sd-versus-the-sample-sd&#34;&gt;Population SD versus the sample SD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#central-limit-theorem&#34;&gt;Central Limit Theorem&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#how-large-is-large-in-the-central-limit-theorem&#34;&gt;How large is large in the Central Limit Theorem?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#statistical-properties-of-averages&#34;&gt;Statistical properties of averages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#law-of-large-numbers&#34;&gt;Law of large numbers&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#misinterpreting-law-of-averages&#34;&gt;Misinterpreting law of averages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;supplemental-readings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Supplemental Readings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://hbr.org/2016/11/why-its-so-hard-for-us-to-visualize-uncertainty&#34;&gt;Why It’s So Hard for Us to Visualize Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fab fa-youtube&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=0L1tGo-DvD0&#34;&gt;Amanda Cox’s keynote address at the 2017 OpenVis Conf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://eagereyes.org/blog/2017/communicating-uncertainty-when-lives-are-on-the-line&#34;&gt;Communicating Uncertainty When Lives Are on the Line&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://flowingdata.com/2016/11/15/showing-uncertainty-during-the-live-election-forecast/&#34;&gt;Showing uncertainty during the live election forecast&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://flowingdata.com/2017/06/27/trolling-the-uncertainty-dial/&#34;&gt;Trolling the uncertainty dial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Why is uncertainty inherently a major part of data analytics?&lt;/li&gt;
&lt;li&gt;How have past attempts to visualize uncertainty failed?&lt;/li&gt;
&lt;li&gt;What is the right way to visualize election uncertainty?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As with last week, today’s lecture will ask you to work with &lt;code&gt;R&lt;/code&gt; during the lecture.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discrete-probability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discrete probability&lt;/h1&gt;
&lt;p&gt;We start by covering some basic principles related to categorical data. The subset of probability is referred to as &lt;em&gt;discrete probability&lt;/em&gt;. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications. Discrete probability is more useful in card games and therefore we use these as examples.&lt;/p&gt;
&lt;div id=&#34;relative-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Relative frequency&lt;/h3&gt;
&lt;p&gt;The word probability is used in everyday language. Answering questions about probability is often hard, if not impossible. Here we discuss a mathematical definition of &lt;em&gt;probability&lt;/em&gt; that does permit us to give precise answers to certain questions.&lt;/p&gt;
&lt;p&gt;For example, if I have 2 red beads and 3 blue beads inside an urn&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; (most probability books use this archaic term, so we do too) and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is .4 for red and .6 for blue.&lt;/p&gt;
&lt;p&gt;A more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Notation&lt;/h3&gt;
&lt;p&gt;We use the notation &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(A)\)&lt;/span&gt; to denote the probability of event &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; happening. We use the very general term &lt;em&gt;event&lt;/em&gt; to refer to things that can happen when something occurs by chance. In our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”.&lt;/p&gt;
&lt;p&gt;In data science applications, we will often deal with continuous variables. These events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: &lt;span class=&#34;math inline&#34;&gt;\(X \geq 6\)&lt;/span&gt;. We will see more of these examples later. Here we focus on categorical data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;probability-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Probability distributions&lt;/h3&gt;
&lt;p&gt;If we know the relative frequency of the different categories, defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution.&lt;/p&gt;
&lt;p&gt;If we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided, and 2% Green Party, these proportions define the probability for each group. The probability distribution is:&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Pr(picking a Republican)&lt;/td&gt;
&lt;td&gt;=&lt;/td&gt;
&lt;td&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Pr(picking a Democrat)&lt;/td&gt;
&lt;td&gt;=&lt;/td&gt;
&lt;td&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Pr(picking an undecided)&lt;/td&gt;
&lt;td&gt;=&lt;/td&gt;
&lt;td&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Pr(picking a Green)&lt;/td&gt;
&lt;td&gt;=&lt;/td&gt;
&lt;td&gt;0.02&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;monte-carlo-simulations-for-categorical-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo simulations for categorical data&lt;/h2&gt;
&lt;p&gt;Computers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.&lt;/p&gt;
&lt;p&gt;An example is the &lt;code&gt;sample&lt;/code&gt; function in R. We demonstrate its use in the code below. First, we use the function &lt;code&gt;rep&lt;/code&gt; to generate the urn:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beads &amp;lt;- rep(c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), times = c(2,3))
beads&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;red&amp;quot;  &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then use &lt;code&gt;sample&lt;/code&gt; to pick a bead at random:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;red&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This line of code produces one random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat forever. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to repeating forever. &lt;strong&gt;This is an example of a &lt;em&gt;Monte Carlo&lt;/em&gt; simulation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Much of what mathematical and theoretical statisticians study, which we do not cover in this class, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this lecture, we provide a practical approach to deciding what is “large enough”.&lt;/p&gt;
&lt;p&gt;To perform our first Monte Carlo simulation, we use the &lt;code&gt;replicate&lt;/code&gt; function, which permits us to repeat the same task any number of times. Here, we repeat the random event &lt;span class=&#34;math inline&#34;&gt;\(B =\)&lt;/span&gt; 10,000 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
events &amp;lt;- replicate(B, sample(beads, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use &lt;code&gt;table&lt;/code&gt; to see the distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- table(events)
tab&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## events
## blue  red 
## 6113 3887&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and &lt;code&gt;prop.table&lt;/code&gt; gives us the proportions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.table(tab)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## events
##   blue    red 
## 0.6113 0.3887&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory, not covered here, tells us that as &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; gets larger, the estimates get closer to 3/5=.6 and 2/5=.4.&lt;/p&gt;
&lt;p&gt;Although this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.&lt;/p&gt;
&lt;div id=&#34;setting-the-random-seed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting the random seed&lt;/h3&gt;
&lt;p&gt;Before we continue, we will briefly explain the following important line of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1986)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Throughout this class, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the class may show a different result than what you obtain when you try to code as shown in the class. This is actually fine since the results are random and change from time to time. However, if you want to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number. Above we set it to 1986. We want to avoid using the same seed everytime. A popular way to pick the seed is the year - month - day. For example, we picked 1986 on December 20, 2018: &lt;span class=&#34;math inline&#34;&gt;\(2018 - 12 - 20 = 1986\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;You can learn more about setting the seed by looking at the documentation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?set.seed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;with-and-without-replacement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;With and without replacement&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;sample&lt;/code&gt; has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs &lt;em&gt;without replacement&lt;/em&gt;: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;red&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;red&amp;quot;  &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;blue&amp;quot; &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot; &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Error in sample.int(length(x), size, replace, prob) :   cannot take a sample larger than the population when &#39;replace = FALSE&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;However, the &lt;code&gt;sample&lt;/code&gt; function can be used directly, without the use of &lt;code&gt;replicate&lt;/code&gt;, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample &lt;em&gt;with replacement&lt;/em&gt;: return the bead back to the urn after selecting it.
We can tell &lt;code&gt;sample&lt;/code&gt; to do this by changing the &lt;code&gt;replace&lt;/code&gt; argument, which defaults to &lt;code&gt;FALSE&lt;/code&gt;, to &lt;code&gt;replace = TRUE&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;events &amp;lt;- sample(beads, B, replace = TRUE)
prop.table(table(events))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## events
##   blue    red 
## 0.6017 0.3983&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not surprisingly, we get results very similar to those previously obtained with &lt;code&gt;replicate&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;independence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Independence&lt;/h2&gt;
&lt;p&gt;We say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws.&lt;/p&gt;
&lt;p&gt;Many examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, &lt;span class=&#34;math inline&#34;&gt;\(\dots\)&lt;/span&gt;, Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore &lt;strong&gt;not independent&lt;/strong&gt;: the first outcome affected the next one.&lt;/p&gt;
&lt;p&gt;To see an extreme case of non-independent events, consider our example of drawing five beads at random &lt;strong&gt;without&lt;/strong&gt; replacement:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sample(beads, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x[2:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;red&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;would you still guess blue? Of course not. Now you know that the probability of red is 1 since the only bead left is red. The events are not independent, so the probabilities change.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-probabilities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional probabilities&lt;/h2&gt;
&lt;p&gt;When events are not independent, &lt;em&gt;conditional probabilities&lt;/em&gt; are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(\mbox{Card 2 is a king} \mid \mbox{Card 1 is a king}) = 3/51
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We use the &lt;span class=&#34;math inline&#34;&gt;\(\mid\)&lt;/span&gt; as shorthand for “given that” or “conditional on”.&lt;/p&gt;
&lt;p&gt;When two events, say &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, are independent, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mid B) = \mbox{Pr}(A)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the mathematical way of saying: the fact that &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; happened does not affect the probability of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; happening. In fact, this can be considered the mathematical definition of independence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;addition-and-multiplication-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Addition and multiplication rules&lt;/h2&gt;
&lt;div id=&#34;multiplication-rule&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiplication rule&lt;/h3&gt;
&lt;p&gt;If we want to know the probability of two events, say &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, occurring, we can use the multiplication rule:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mbox{ and } B) = \mbox{Pr}(A)\mbox{Pr}(B \mid A)
\]&lt;/span&gt;
Let’s use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose).&lt;/p&gt;
&lt;p&gt;So, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being an Ace and multiply by the probability of drawing a face card or a 10 given that the first was an Ace: &lt;span class=&#34;math inline&#34;&gt;\(1/13 \times 16/51 \approx 0.025\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The multiplication rule also applies to more than two events. We can use induction to expand for more events:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mbox{ and } B \mbox{ and } C) = \mbox{Pr}(A)\mbox{Pr}(B \mid A)\mbox{Pr}(C \mid A \mbox{ and } B)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiplication-rule-under-independence&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiplication rule under independence&lt;/h3&gt;
&lt;p&gt;When we have independent events, then the multiplication rule becomes simpler:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mbox{ and } B \mbox{ and } C) = \mbox{Pr}(A)\mbox{Pr}(B)\mbox{Pr}(C)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But we have to be very careful before using this since assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence.&lt;/p&gt;
&lt;p&gt;As an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an “expert” to testify that 1/10 men have beards and 1/5 have mustaches, so using the multiplication rule we conclude that only &lt;span class=&#34;math inline&#34;&gt;\(1/10 \times 1/5\)&lt;/span&gt; or 0.02 have both.&lt;/p&gt;
&lt;p&gt;But to multiply like this we need to assume independence! Say the conditional probability of a man having a mustache conditional on him having a beard is .95. So the correct calculation probability is much higher: &lt;span class=&#34;math inline&#34;&gt;\(1/10 \times 95/100 = 0.095\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The multiplication rule also gives us a general formula for computing conditional probabilities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(B \mid A) = \frac{\mbox{Pr}(A \mbox{ and } B)}{ \mbox{Pr}(A)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To illustrate how we use these formulas and concepts in practice, we will use several examples related to card games.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;addition-rule&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Addition rule&lt;/h3&gt;
&lt;p&gt;The addition rule tells us that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mbox{ or } B) = \mbox{Pr}(A) + \mbox{Pr}(B) - \mbox{Pr}(A \mbox{ and } B)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice so we need to substract one instance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/figure-html/venn-diagram-addition-rule-1.png&#34; width=&#34;35%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;combinations-and-permutations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Combinations and permutations&lt;/h2&gt;
&lt;p&gt;In our very first example, we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue.&lt;/p&gt;
&lt;p&gt;For more complicated cases, the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush” in poker? In a discrete probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers.&lt;/p&gt;
&lt;p&gt;First, let’s construct a deck of cards. For this, we will use the &lt;code&gt;expand.grid&lt;/code&gt; and &lt;code&gt;paste&lt;/code&gt; functions. We use &lt;code&gt;paste&lt;/code&gt; to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;number &amp;lt;- &amp;quot;Three&amp;quot;
suit &amp;lt;- &amp;quot;Hearts&amp;quot;
paste(number, suit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Three Hearts&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;paste&lt;/code&gt; also works on pairs of vectors performing the operation element-wise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste(letters[1:5], as.character(1:5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a 1&amp;quot; &amp;quot;b 2&amp;quot; &amp;quot;c 3&amp;quot; &amp;quot;d 4&amp;quot; &amp;quot;e 5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;expand.grid&lt;/code&gt; gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey, and plaid shirts, all your combinations are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expand.grid(pants = c(&amp;quot;blue&amp;quot;, &amp;quot;black&amp;quot;), shirt = c(&amp;quot;white&amp;quot;, &amp;quot;grey&amp;quot;, &amp;quot;plaid&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   pants shirt
## 1  blue white
## 2 black white
## 3  blue  grey
## 4 black  grey
## 5  blue plaid
## 6 black plaid&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is how we generate a deck of cards:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suits &amp;lt;- c(&amp;quot;Diamonds&amp;quot;, &amp;quot;Clubs&amp;quot;, &amp;quot;Hearts&amp;quot;, &amp;quot;Spades&amp;quot;)
numbers &amp;lt;- c(&amp;quot;Ace&amp;quot;, &amp;quot;Deuce&amp;quot;, &amp;quot;Three&amp;quot;, &amp;quot;Four&amp;quot;, &amp;quot;Five&amp;quot;, &amp;quot;Six&amp;quot;, &amp;quot;Seven&amp;quot;,
             &amp;quot;Eight&amp;quot;, &amp;quot;Nine&amp;quot;, &amp;quot;Ten&amp;quot;, &amp;quot;Jack&amp;quot;, &amp;quot;Queen&amp;quot;, &amp;quot;King&amp;quot;)
deck &amp;lt;- expand.grid(number=numbers, suit=suits)
deck &amp;lt;- paste(deck$number, deck$suit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the deck constructed, we can double check that the probability of a King in the first card is 1/13 by computing the proportion of possible outcomes that satisfy our condition:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kings &amp;lt;- paste(&amp;quot;King&amp;quot;, suits)
mean(deck %in% kings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.07692308&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, how about the conditional probability of the second card being a King given that the first was a King? Earlier, we deduced that if one King is already out of the deck and there are 51 left, then this probability is 3/51. Let’s confirm by listing out all possible outcomes.&lt;/p&gt;
&lt;p&gt;To do this, we can use the &lt;code&gt;permutations&lt;/code&gt; function from the &lt;strong&gt;gtools&lt;/strong&gt; package. For any list of size &lt;code&gt;n&lt;/code&gt;, this function computes all the different combinations we can get when we select &lt;code&gt;r&lt;/code&gt; items. Here are all the ways we can choose two numbers from a list consisting of &lt;code&gt;1,2,3&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gtools)
permutations(3, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    2
## [2,]    1    3
## [3,]    2    1
## [4,]    2    3
## [5,]    3    1
## [6,]    3    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.&lt;/p&gt;
&lt;p&gt;Optionally, we can add a vector. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_phone_numbers &amp;lt;- permutations(10, 7, v = 0:9)
n &amp;lt;- nrow(all_phone_numbers)
index &amp;lt;- sample(n, 5)
all_phone_numbers[index,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]    1    3    8    0    6    7    5
## [2,]    2    9    1    6    4    8    0
## [3,]    5    1    6    0    9    8    2
## [4,]    7    4    6    0    2    8    1
## [5,]    4    6    5    9    2    8    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of using the numbers 1 through 10, the default, it uses what we provided through &lt;code&gt;v&lt;/code&gt;: the digits 0 through 9.&lt;/p&gt;
&lt;p&gt;To compute all possible ways we can choose two cards when the order matters, we type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hands &amp;lt;- permutations(52, 2, v = deck)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a matrix with two columns and 2652 rows. With a matrix we can get the first and second cards like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;first_card &amp;lt;- hands[,1]
second_card &amp;lt;- hands[,2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the cases for which the first hand was a King can be computed like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kings &amp;lt;- paste(&amp;quot;King&amp;quot;, suits)
sum(first_card %in% kings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 204&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get the conditional probability, we compute what fraction of these have a King in the second card:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(first_card%in%kings &amp;amp; second_card%in%kings) / sum(first_card%in%kings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05882353&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is exactly 3/51, as we had already deduced. Notice that the code above is equivalent to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(first_card%in%kings &amp;amp; second_card%in%kings) / mean(first_card%in%kings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05882353&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which uses &lt;code&gt;mean&lt;/code&gt; instead of &lt;code&gt;sum&lt;/code&gt; and is an R version of:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\mbox{Pr}(A \mbox{ and } B)}{ \mbox{Pr}(A)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a &lt;em&gt;Natural 21&lt;/em&gt; and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the &lt;em&gt;combinations&lt;/em&gt;, not the permutations, since the order does not matter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;combinations(3,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    2
## [2,]    1    3
## [3,]    2    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the second line, the outcome does not include (2,1) because (1,2) already was enumerated. The same applies to (3,1) and (3,2).&lt;/p&gt;
&lt;p&gt;So to compute the probability of a &lt;em&gt;Natural 21&lt;/em&gt; in Blackjack, we can do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aces &amp;lt;- paste(&amp;quot;Ace&amp;quot;, suits)

facecard &amp;lt;- c(&amp;quot;King&amp;quot;, &amp;quot;Queen&amp;quot;, &amp;quot;Jack&amp;quot;, &amp;quot;Ten&amp;quot;)
facecard &amp;lt;- expand.grid(number = facecard, suit = suits)
facecard &amp;lt;- paste(facecard$number, facecard$suit)

hands &amp;lt;- combinations(52, 2, v = deck)
mean(hands[,1] %in% aces &amp;amp; hands[,2] %in% facecard)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04826546&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the last line, we assume the Ace comes first. This is only because we know the way &lt;code&gt;combination&lt;/code&gt; enumerates possibilities and it will list this case first. But to be safe, we could have written this and produced the same answer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean((hands[,1] %in% aces &amp;amp; hands[,2] %in% facecard) |
       (hands[,2] %in% aces &amp;amp; hands[,1] %in% facecard))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04826546&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;monte-carlo-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monte Carlo example&lt;/h3&gt;
&lt;p&gt;Instead of using &lt;code&gt;combinations&lt;/code&gt; to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw two cards without replacements:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hand &amp;lt;- sample(deck, 2)
hand&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Queen Clubs&amp;quot;  &amp;quot;Seven Spades&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say &lt;em&gt;face card&lt;/em&gt;. Now we need to check both possibilities:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(hands[1] %in% aces &amp;amp; hands[2] %in% facecard) |
  (hands[2] %in% aces &amp;amp; hands[1] %in% facecard)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21.&lt;/p&gt;
&lt;p&gt;Let’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blackjack &amp;lt;- function(){
   hand &amp;lt;- sample(deck, 2)
  (hand[1] %in% aces &amp;amp; hand[2] %in% facecard) |
    (hand[2] %in% aces &amp;amp; hand[1] %in% facecard)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we do have to check both possibilities: Ace first or Ace second because we are not using the &lt;code&gt;combinations&lt;/code&gt; function. The function returns &lt;code&gt;TRUE&lt;/code&gt; if we get a 21 and &lt;code&gt;FALSE&lt;/code&gt; otherwise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blackjack()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can play this game, say, 10,000 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
results &amp;lt;- replicate(B, blackjack())
mean(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0475&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;In this section, we describe two discrete probability popular examples: the Monty Hall problem and the birthday problem. We use R to help illustrate the mathematical concepts.&lt;/p&gt;
&lt;div id=&#34;monty-hall-problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monty Hall problem&lt;/h3&gt;
&lt;p&gt;In the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and show the contestant there was no prize behind that door. Then he would ask “Do you want to switch doors?” What would you do?&lt;/p&gt;
&lt;p&gt;We can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; or read one on Wikipedia&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.&lt;/p&gt;
&lt;p&gt;Let’s start with the stick strategy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
monty_hall &amp;lt;- function(strategy){
  doors &amp;lt;- as.character(1:3)
  prize &amp;lt;- sample(c(&amp;quot;car&amp;quot;, &amp;quot;goat&amp;quot;, &amp;quot;goat&amp;quot;))
  prize_door &amp;lt;- doors[prize == &amp;quot;car&amp;quot;]
  my_pick  &amp;lt;- sample(doors, 1)
  show &amp;lt;- sample(doors[!doors %in% c(my_pick, prize_door)],1)
  stick &amp;lt;- my_pick
  stick == prize_door
  switch &amp;lt;- doors[!doors%in%c(my_pick, show)]
  choice &amp;lt;- ifelse(strategy == &amp;quot;stick&amp;quot;, stick, switch)
  choice == prize_door
}
stick &amp;lt;- replicate(B, monty_hall(&amp;quot;stick&amp;quot;))
mean(stick)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3416&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;switch &amp;lt;- replicate(B, monty_hall(&amp;quot;switch&amp;quot;))
mean(switch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6682&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we write the code, we note that the lines starting with &lt;code&gt;my_pick&lt;/code&gt; and &lt;code&gt;show&lt;/code&gt; have no influence on the last logical operation when we stick to our original choice anyway. From this we should realize that the chance is 1 in 3, what we began with. When we switch,
the Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by showing that we are removing a door, &lt;code&gt;show&lt;/code&gt;, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;birthday-problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Birthday problem&lt;/h3&gt;
&lt;p&gt;Suppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.&lt;/p&gt;
&lt;p&gt;First, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 50
bdays &amp;lt;- sample(1:365, n, replace = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check if in this particular set of 50 people we have at least two with the same birthday, we can use the function &lt;code&gt;duplicated&lt;/code&gt;, which returns &lt;code&gt;TRUE&lt;/code&gt; whenever an element of a vector is a duplicate. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;duplicated(c(1,2,3,1,4,3,5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second time 1 and 3 appear, we get a &lt;code&gt;TRUE&lt;/code&gt;. So to check if two birthdays were the same, we simply use the &lt;code&gt;any&lt;/code&gt; and &lt;code&gt;duplicated&lt;/code&gt; functions like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;any(duplicated(bdays))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we see that it did happen. At least two people had the same birthday.&lt;/p&gt;
&lt;p&gt;To estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 50 birthdays over and over:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
same_birthday &amp;lt;- function(n){
  bdays &amp;lt;- sample(1:365, n, replace=TRUE)
  any(duplicated(bdays))
}
results &amp;lt;- replicate(B, same_birthday(50))
mean(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9691&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Were you expecting the probability to be this high?&lt;/p&gt;
&lt;p&gt;People tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.&lt;/p&gt;
&lt;p&gt;Say we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?&lt;/p&gt;
&lt;p&gt;Let’s create a look-up table. We can quickly create a function to compute this for any group size:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_prob &amp;lt;- function(n, B=10000){
  results &amp;lt;- replicate(B, same_birthday(n))
  mean(results)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the function &lt;code&gt;sapply&lt;/code&gt;, we can perform element-wise operations on any function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- seq(1,60)
prob &amp;lt;- sapply(n, compute_prob)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now make a plot of the estimated probabilities of two people having the same birthday in a group of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
prob &amp;lt;- sapply(n, compute_prob)
qplot(n, prob)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/figure-html/birthday-problem-mc-probabilities-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments.&lt;/p&gt;
&lt;p&gt;To make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule.&lt;/p&gt;
&lt;p&gt;Let’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
1 \times \frac{364}{365}\times\frac{363}{365} \dots \frac{365-n + 1}{365}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can write a function that does this for any number:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exact_prob &amp;lt;- function(n){
  prob_unique &amp;lt;- seq(365,365-n+1)/365
  1 - prod( prob_unique)
}
eprob &amp;lt;- sapply(n, exact_prob)
qplot(n, prob) + geom_line(aes(n, eprob), col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/figure-html/birthday-problem-exact-probabilities-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;infinity-in-practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Infinity in practice&lt;/h2&gt;
&lt;p&gt;The theory described here requires repeating experiments over and over forever. In practice we can’t do this.
In the examples above, we used &lt;span class=&#34;math inline&#34;&gt;\(B=10,000\)&lt;/span&gt; Monte Carlo experiments and it turned out that this provided accurate estimates. The larger this number, the more accurate the estimate becomes until the approximaton is so good that your computer can’t tell the difference. But in more complex calculations, 10,000 may not be nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is, so we won’t know if our Monte Carlo estimate is accurate. We know that the larger &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, the better the approximation. But how big do we need it to be? This is actually a challenging question and answering it often requires advanced theoretical statistics training.&lt;/p&gt;
&lt;p&gt;One practical approach we will describe here is to check for the stability of the estimate. The following is an example with the birthday problem for a group of 25 people.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10^seq(1, 5, len = 100)
compute_prob &amp;lt;- function(B, n=25){
  same_day &amp;lt;- replicate(B, same_birthday(n))
  mean(same_day)
}
prob &amp;lt;- sapply(B, compute_prob)
qplot(log10(B), prob, geom = &amp;quot;line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/figure-html/monte-carlo-convergence-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this plot, we can see that the values start to stabilize (that is, they vary less than .01) around 1000. Note that the exact probability, which we know in this case, is 0.5686997.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the probability that the ball will not be cyan?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling &lt;strong&gt;without&lt;/strong&gt; replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling &lt;strong&gt;with&lt;/strong&gt; replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two events &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are independent if &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(A \mbox{ and } B) = \mbox{Pr}(A) P(B)\)&lt;/span&gt;. Under which situation are the draws independent?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;You don’t replace the draw.&lt;/li&gt;
&lt;li&gt;You replace the draw.&lt;/li&gt;
&lt;li&gt;Neither&lt;/li&gt;
&lt;li&gt;Both&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you roll a 6-sided die six times, what is the probability of not seeing a 6?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win &lt;strong&gt;at least&lt;/strong&gt; one game?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a Monte Carlo simulation to confirm your answer to the previous problem. Use &lt;code&gt;B &amp;lt;- 10000&lt;/code&gt; simulations. Hint: use the following code to generate the results of the first four games:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;celtic_wins &amp;lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Celtics must win one of these 4 games.&lt;/p&gt;
&lt;ol start=&#34;10&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Confirm the results of the previous question with a Monte Carlo simulation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two teams, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, are playing a seven game series. Team &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is better than team &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and has a &lt;span class=&#34;math inline&#34;&gt;\(p&amp;gt;0.5\)&lt;/span&gt; chance of winning each game. Given a value &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, the probability of winning the series for the underdog team &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; can be computed with the following function based on a Monte Carlo simulation:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prob_win &amp;lt;- function(p){
  B &amp;lt;- 10000
  result &amp;lt;- replicate(B, {
    b_win &amp;lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))
    sum(b_win)&amp;gt;=4
  })
  mean(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the function &lt;code&gt;sapply&lt;/code&gt; to compute the probability, call it &lt;code&gt;Pr&lt;/code&gt;, of winning for &lt;code&gt;p &amp;lt;- seq(0.5, 0.95, 0.025)&lt;/code&gt;. Then plot the result.&lt;/p&gt;
&lt;ol start=&#34;13&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Repeat the exercise above, but now keep the probability fixed at &lt;code&gt;p &amp;lt;- 0.75&lt;/code&gt; and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, &lt;code&gt;N &amp;lt;- seq(1, 25, 2)&lt;/code&gt;. Hint: use this function:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prob_win &amp;lt;- function(N, p=0.75){
  B &amp;lt;- 10000
  result &amp;lt;- replicate(B, {
    b_win &amp;lt;- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))
    sum(b_win)&amp;gt;=(N+1)/2
  })
  mean(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;In previous lectures, we explained why when summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome. For example, if we measure every single person in a very large population of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; with extremely high precision, since no two people are exactly the same height, we need to assign the proportion &lt;span class=&#34;math inline&#34;&gt;\(1/n\)&lt;/span&gt; to each observed value and attain no useful summary at all. Similarly, when defining probability distributions, it is not useful to assign a very small probability to every single height.&lt;/p&gt;
&lt;p&gt;Just as when using distributions to summarize numeric data, it is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the &lt;em&gt;cumulative distribution function&lt;/em&gt; (CDF).&lt;/p&gt;
&lt;p&gt;We described empirical cumulative distribution function (eCDF) as a basic summary of a list of numeric values. As an example, we earlier defined the height distribution for adult male students. Here, we define the vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to contain these heights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
data(heights)
x &amp;lt;- heights %&amp;gt;% filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;% pull(height)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We defined the empirical distribution function as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;F &amp;lt;- function(a) mean(x&amp;lt;=a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which, for any value &lt;code&gt;a&lt;/code&gt;, gives the proportion of values in the list &lt;code&gt;x&lt;/code&gt; that are smaller or equal than &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Keep in mind that we have not yet introduced probability in the context of CDFs. Let’s do this by asking the following: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 inches. Using the CDF we obtain an answer by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - F(70)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3768473&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once a CDF is defined, we can use this to compute the probability of any subset. For instance, the probability of a student being between height &lt;code&gt;a&lt;/code&gt; and height &lt;code&gt;b&lt;/code&gt; is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;F(b)-F(a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we can compute the probability for any possible event this way, the cumulative probability function defines the probability distribution for picking a height at random from our vector of heights &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theoretical-continuous-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Theoretical continuous distributions&lt;/h2&gt;
&lt;p&gt;The normal distribution is a useful approximation to many naturally occurring distributions, including that of height. The cumulative distribution for the normal distribution is defined by a mathematical formula which in &lt;code&gt;R&lt;/code&gt; can be obtained with the function &lt;code&gt;pnorm&lt;/code&gt;. We say that a random quantity is normally distributed with average &lt;code&gt;m&lt;/code&gt; and standard deviation &lt;code&gt;s&lt;/code&gt; if its probability distribution is defined by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;F(a) = pnorm(a, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is useful because if we are willing to use the normal approximation for, say, height, we don’t need the entire dataset to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- mean(x)
s &amp;lt;- sd(x)
1 - pnorm(70.5, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.371369&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;theoretical-distributions-as-approximations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Theoretical distributions as approximations&lt;/h3&gt;
&lt;p&gt;The normal distribution is derived mathematically: we do not need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/figure-html/plot-of-height-frequencies-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787, which is 177 centimeters. The probability assigned to this height is 0.0012315 or 1 in 812. The probability for 70 inches is much higher at 0.1059113, but does it really make sense to think of the probability of being exactly 70 inches as being different than 69.6850393700787? Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch.&lt;/p&gt;
&lt;p&gt;With continuous distributions, the probability of a singular value is not even defined. For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, we define probabilities for intervals. We thus could ask what is the probability that someone is between 69.5 and 70.5.&lt;/p&gt;
&lt;p&gt;In cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. For example, the normal distribution is useful for approximating the proportion of students reporting values in intervals like the following three:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 68.5) - mean(x &amp;lt;= 67.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.114532&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 69.5) - mean(x &amp;lt;= 68.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1194581&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 70.5) - mean(x &amp;lt;= 69.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1219212&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how close we get with the normal approximation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(68.5, m, s) - pnorm(67.5, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1031077&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(69.5, m, s) - pnorm(68.5, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1097121&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(70.5, m, s) - pnorm(69.5, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1081743&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, the approximation is not as useful for other intervals. For instance, notice how the approximation breaks down when we try to estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 70.9) - mean(x&amp;lt;=70.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02216749&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(70.9, m, s) - pnorm(70.1, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08359562&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, we call this situation &lt;em&gt;discretization&lt;/em&gt;. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-probability-density&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The probability density&lt;/h3&gt;
&lt;p&gt;For categorical distributions, we can define the probability of a category. For example, a roll of a die, let’s call it &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, can be 1,2,3,4,5 or 6. The probability of 4 is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(X=4) = 1/6
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The CDF can then easily be defined:
&lt;span class=&#34;math display&#34;&gt;\[
F(4) = \mbox{Pr}(X\leq 4) =  \mbox{Pr}(X = 4) +  \mbox{Pr}(X = 3) +  \mbox{Pr}(X = 2) +  \mbox{Pr}(X = 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Although for continuous distributions the probability of a single value &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(X=x)\)&lt;/span&gt; is not defined, there is a theoretical definition that has a similar interpretation. The probability density at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is defined as the function &lt;span class=&#34;math inline&#34;&gt;\(f(a)\)&lt;/span&gt; such that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
F(a) = \mbox{Pr}(X\leq a) = \int_{-\infty}^a f(x)\, dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For those that know calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don’t know calculus, you can think of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; as a curve for which the area under that curve up to the value &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, gives you the probability &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(X\leq a)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - pnorm(76, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03206008&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which mathematically is the grey area below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/figure-html/intergrals-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The curve you see is the probability density for the normal distribution. In R, we get this using the function &lt;code&gt;dnorm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Although it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;monte-carlo-simulations-for-continuous-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo simulations for continuous variables&lt;/h2&gt;
&lt;p&gt;R provides functions to generate normally distributed outcomes. Specifically, the &lt;code&gt;rnorm&lt;/code&gt; function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produces random numbers. Here is an example of how we could generate data that looks like our reported heights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- length(x)
m &amp;lt;- mean(x)
s &amp;lt;- sd(x)
simulated_heights &amp;lt;- rnorm(n, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not surprisingly, the distribution looks normal:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/figure-html/simulated-heights-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations.&lt;/p&gt;
&lt;p&gt;If, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer in a group of 800 males? The following Monte Carlo simulation helps us answer that question:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
tallest &amp;lt;- replicate(B, {
  simulated_data &amp;lt;- rnorm(800, m, s)
  max(simulated_data)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having a seven footer is quite rare:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(tallest &amp;gt;= 7*12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0172&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the resulting distribution:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/figure-html/simulated-tallest-height-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that it does not look normal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous distributions&lt;/h2&gt;
&lt;p&gt;The normal distribution is not the only useful theoretical distribution. Other continuous distributions that we may encounter are the student-t, Chi-square, exponential, gamma, beta, and beta-binomial. &lt;code&gt;R&lt;/code&gt; provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. &lt;code&gt;R&lt;/code&gt; uses a convention that lets us remember the names, namely using the letters &lt;code&gt;d&lt;/code&gt;, &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;p&lt;/code&gt;, and &lt;code&gt;r&lt;/code&gt; in front of a shorthand for the distribution. We have already seen the functions &lt;code&gt;dnorm&lt;/code&gt;, &lt;code&gt;pnorm&lt;/code&gt;, and &lt;code&gt;rnorm&lt;/code&gt; for the normal distribution. The functions &lt;code&gt;qnorm&lt;/code&gt; gives us the quantiles. We can therefore draw a distribution like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(-4, 4, length.out = 100)
qplot(x, f, geom = &amp;quot;line&amp;quot;, data = data.frame(x, f = dnorm(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the student-t, described later as we move toward hypothesis testing, the shorthand &lt;code&gt;t&lt;/code&gt; is used so the functions are &lt;code&gt;dt&lt;/code&gt; for the density, &lt;code&gt;qt&lt;/code&gt; for the quantiles, &lt;code&gt;pt&lt;/code&gt; for the cumulative distribution function, and &lt;code&gt;rt&lt;/code&gt; for Monte Carlo simulation.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Notice that the answer to the question does not change when you change units. This makes sense since the answer to the question should not be affected by what units we use. In fact, if you look closely, you notice that 61 and 64 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To see the math that explains why the answers to questions 3, 4, and 5 are the same, suppose we have a random variable with average &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and standard error &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. Suppose we ask the probability of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; being smaller or equal to &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;. Remember that, by definition, &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\((a - m)/s\)&lt;/span&gt; standard deviations &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; away from the average &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;. The probability is:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(X \leq a)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we subtract &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; to both sides and then divide both sides by &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(\frac{X-m}{s} \leq \frac{a-m}{s} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq \frac{a-m}{s} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, no matter the units, the probability of &lt;span class=&#34;math inline&#34;&gt;\(X\leq a\)&lt;/span&gt; is the same as the probability of a standard normal variable being less than &lt;span class=&#34;math inline&#34;&gt;\((a - m)/s\)&lt;/span&gt;. If &lt;code&gt;mu&lt;/code&gt; is the average and &lt;code&gt;sigma&lt;/code&gt; the standard error, which of the following R code would give us the right answer in every situation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;mean(X&amp;lt;=a)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pnorm((a - m)/s)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pnorm((a - m)/s, m, s)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pnorm(a)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use &lt;code&gt;qnorm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with &lt;code&gt;B=1000&lt;/code&gt; generating 10,000 IQ scores and keeping the highest. Make a histogram.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Random variables&lt;/h1&gt;
&lt;p&gt;In data science, we often deal with data that is affected by chance in some way: the data comes from a random sample, the data is affected by measurement error, or the data measures some outcome that is random in nature. Being able to quantify the uncertainty introduced by randomness is one of the most important jobs of a data analyst. Statistical inference offers a framework, as well as several practical tools, for doing this. The first step is to learn how to mathematically describe random variables.&lt;/p&gt;
&lt;p&gt;In this section, we introduce random variables and their properties starting with their application to games of chance. We then describe some of the events surrounding the financial crisis of 2007-2008&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; using probability theory. This financial crisis was in part caused by underestimating the risk of certain securities&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; sold by financial institutions. Specifically, the risks of mortgage-backed securities (MBS) and collateralized debt obligations (CDO) were grossly underestimated. These assets were sold at prices that assumed most homeowners would make their monthly payments, and the probability of this not occurring was calculated as being low. A combination of factors resulted in many more defaults than were expected, which led to a price crash of these securities. As a consequence, banks lost so much money that they needed government bailouts to avoid closing down completely.&lt;/p&gt;
&lt;div id=&#34;definition-of-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definition of Random variables&lt;/h2&gt;
&lt;p&gt;Random variables are numeric outcomes resulting from random processes. We can easily generate random variables using some of the simple examples we have shown. For example, define &lt;code&gt;X&lt;/code&gt; to be 1 if a bead is blue and red otherwise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beads &amp;lt;- rep( c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), times = c(2,3))
X &amp;lt;- ifelse(sample(beads, 1) == &amp;quot;blue&amp;quot;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;X&lt;/code&gt; is a random variable: every time we select a new bead the outcome changes randomly. See below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ifelse(sample(beads, 1) == &amp;quot;blue&amp;quot;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ifelse(sample(beads, 1) == &amp;quot;blue&amp;quot;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ifelse(sample(beads, 1) == &amp;quot;blue&amp;quot;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sometimes it’s 1 and sometimes it’s 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sampling models&lt;/h2&gt;
&lt;p&gt;Many data generation procedures, those that produce the data we study, can be modeled quite well as draws from an urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 code for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing the outcome for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can also be modeled by draws from an urn given the way individuals are assigned into groups: when getting assigned, you draw your group at random. Sampling models are therefore ubiquitous in data science. Casino games offer a plethora of examples of real-world situations in which sampling models are used to answer specific questions. We will therefore start with such examples.&lt;/p&gt;
&lt;p&gt;Suppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play and that the only game you can play on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will pass on installing roulette wheels.&lt;/p&gt;
&lt;p&gt;We are going to define a random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; that will represent the casino’s total winnings. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color &amp;lt;- rep(c(&amp;quot;Black&amp;quot;, &amp;quot;Red&amp;quot;, &amp;quot;Green&amp;quot;), c(18, 18, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 1,000 outcomes from 1,000 people playing are independent draws from this urn. If red comes up, the gambler wins and the casino loses a dollar, so we draw a -1. Otherwise, the casino wins a dollar and we draw a 1. To construct our random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, we can use this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
X &amp;lt;- sample(ifelse(color == &amp;quot;Red&amp;quot;, -1, 1),  n, replace = TRUE)
X[1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] -1  1  1 -1 -1 -1  1  1  1  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining &lt;code&gt;color&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We call this a &lt;strong&gt;sampling model&lt;/strong&gt; since we are modeling the random behavior of roulette with the sampling of draws from an urn. The total winnings &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is simply the sum of these 1,000 independent draws:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
S &amp;lt;- sum(X)
S&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-probability-distribution-of-a-random-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The probability distribution of a random variable&lt;/h2&gt;
&lt;p&gt;If you run the code above, you see that &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; changes every time. This is, of course, because &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is a &lt;strong&gt;random variable&lt;/strong&gt;. The probability distribution of a random variable tells us the probability of the observed value falling at any given interval. So, for example, if we want to know the probability that we lose money, we are asking the probability that &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is in the interval &lt;span class=&#34;math inline&#34;&gt;\(S&amp;lt;0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that if we can define a cumulative distribution function &lt;span class=&#34;math inline&#34;&gt;\(F(a) = \mbox{Pr}(S\leq a)\)&lt;/span&gt;, then we will be able to answer any question related to the probability of events defined by our random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, including the event &lt;span class=&#34;math inline&#34;&gt;\(S&amp;lt;0\)&lt;/span&gt;. We call this &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; the random variable’s &lt;em&gt;distribution function&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can estimate the distribution function for the random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people play roulette, over and over, specifically &lt;span class=&#34;math inline&#34;&gt;\(B = 10,000\)&lt;/span&gt; times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
B &amp;lt;- 10000
roulette_winnings &amp;lt;- function(n){
  X &amp;lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
  sum(X)
}
S &amp;lt;- replicate(B, roulette_winnings(n))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can ask the following: in our simulations, how often did we get sums less than or equal to &lt;code&gt;a&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(S &amp;lt;= a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will be a very good approximation of &lt;span class=&#34;math inline&#34;&gt;\(F(a)\)&lt;/span&gt; and we can easily answer the casino’s question: how likely is it that we will lose money? We can see it is quite low:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(S&amp;lt;0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0456&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the distribution of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; by creating a histogram showing the probability &lt;span class=&#34;math inline&#34;&gt;\(F(b)-F(a)\)&lt;/span&gt; for several intervals &lt;span class=&#34;math inline&#34;&gt;\((a,b]\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/05-content_files/figure-html/normal-approximates-distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the distribution appears to be approximately normal. A qq-plot will confirm that the normal approximation is close to a perfect approximation for this distribution. If, in fact, the distribution is normal, then all we need to define the distribution is the average and the standard deviation. Because we have the original values from which the distribution is created, we can easily compute these with &lt;code&gt;mean(S)&lt;/code&gt; and &lt;code&gt;sd(S)&lt;/code&gt;. The blue curve you see added to the histogram above is a normal density with this average and standard deviation.&lt;/p&gt;
&lt;p&gt;This average and this standard deviation have special names. They are referred to as the &lt;em&gt;expected value&lt;/em&gt; and &lt;em&gt;standard error&lt;/em&gt; of the random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. We will say more about these in the next section.&lt;/p&gt;
&lt;p&gt;Statistical theory provides a way to derive the distribution of random variables defined as independent random draws from an urn. Specifically, in our example above, we can show that &lt;span class=&#34;math inline&#34;&gt;\((S+n)/2\)&lt;/span&gt; follows a binomial distribution. We therefore do not need to run for Monte Carlo simulations to know the probability distribution of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. We did this for illustrative purposes.&lt;/p&gt;
&lt;p&gt;We can use the function &lt;code&gt;dbinom&lt;/code&gt; and &lt;code&gt;pbinom&lt;/code&gt; to compute the probabilities exactly. For example, to compute &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(S &amp;lt; 0)\)&lt;/span&gt; we note that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{Pr}(S &amp;lt; 0) = \mbox{Pr}((S+n)/2 &amp;lt; (0+n)/2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we can use the &lt;code&gt;pbinom&lt;/code&gt; to compute &lt;span class=&#34;math display&#34;&gt;\[\mbox{Pr}(S \leq 0)\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
pbinom(n/2, size = n, prob = 10/19)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05109794&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because this is a discrete probability function, to get &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(S &amp;lt; 0)\)&lt;/span&gt; rather than &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(S \leq 0)\)&lt;/span&gt;, we write:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pbinom(n/2-1, size = n, prob = 10/19)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04479591&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the details of the binomial distribution, you can consult any basic probability book or even Wikipedia&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we do not cover these details. Instead, we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums and averages of draws from any urn: the Central Limit Theorem (CLT).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distributions-versus-probability-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Distributions versus probability distributions&lt;/h2&gt;
&lt;p&gt;Before we continue, let’s make an important distinction and connection between the distribution of a list of numbers and a probability distribution. In the visualization lectures, we described how any list of numbers &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n\)&lt;/span&gt; has a distribution. The definition is quite straightforward. We define &lt;span class=&#34;math inline&#34;&gt;\(F(a)\)&lt;/span&gt; as the function that tells us what proportion of the list is less than or equal to &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;. Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. These are defined with a straightforward operation of the vector containing the list of numbers &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- sum(x)/length(x)
s &amp;lt;- sqrt(sum((x - m)^2) / length(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; has a distribution function. To define this, we do not need a list of numbers. It is a theoretical concept. In this case, we define the distribution as the &lt;span class=&#34;math inline&#34;&gt;\(F(a)\)&lt;/span&gt; that answers the question: what is the probability that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is less than or equal to &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;? There is no list of numbers.&lt;/p&gt;
&lt;p&gt;However, if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. In this case, the distribution of that list is the probability distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and the average and standard deviation of that list are the expected value and standard error of the random variable.&lt;/p&gt;
&lt;p&gt;Another way to think about it that does not involve an urn is to run a Monte Carlo simulation and generate a very large list of outcomes of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. These outcomes are a list of numbers. The distribution of this list will be a very good approximation of the probability distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The longer the list, the better the approximation. The average and standard deviation of this list will approximate the expected value and standard error of the random variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notation-for-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notation for random variables&lt;/h2&gt;
&lt;p&gt;In statistical textbooks, upper case letters are used to denote random variables and we follow this convention here. Lower case letters are used for observed values. You will see some notation that includes both. For example, you will see events defined as &lt;span class=&#34;math inline&#34;&gt;\(X \leq x\)&lt;/span&gt;. Here &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a random variable, making it a random event, and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is an arbitrary value and not random. So, for example, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; might represent the number on a die roll and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; will represent an actual value we see 1, 2, 3, 4, 5, or 6. So in this case, the probability of &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt; is 1/6 regardless of the observed value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. This notation is a bit strange because, when we ask questions about probability, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is not an observed quantity. Instead, it’s a random quantity that we will see in the future. We can talk about what we expect it to be, what values are probable, but not what it is. But once we have data, we do see a realization of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. So data scientists talk of what could have been after we see what actually happened.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-expected-value-and-standard-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The expected value and standard error&lt;/h2&gt;
&lt;p&gt;We have described sampling models for draws. We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion which we will need to understand how polls work.&lt;/p&gt;
&lt;p&gt;The first important concept to learn is the &lt;em&gt;expected value&lt;/em&gt;.
In statistics books, it is common to use letter &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}\)&lt;/span&gt; like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{E}[X]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to denote the expected value of the random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take.&lt;/p&gt;
&lt;p&gt;Theoretical statistics provides techniques that facilitate the calculation of expected values in different circumstances. For example, a useful formula tells us that the &lt;em&gt;expected value of a random variable defined by one draw is the average of the numbers in the urn&lt;/em&gt;. In the urn used to model betting on red in roulette, we have 20 one dollars and 18 negative one dollars. The expected value is thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[X] = (20 + -18)/38
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is about 5 cents. It is a bit counterintuitive to say that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; varies around 0.05, when the only values it takes is 1 and -1. One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10^6
x &amp;lt;- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19))
mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, if the urn has two possible outcomes, say &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, with proportions &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt; respectively, the average is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{E}[X] = ap + b(1-p)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To see this, notice that if there are &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; beads in the urn, then we have &lt;span class=&#34;math inline&#34;&gt;\(np\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;s and &lt;span class=&#34;math inline&#34;&gt;\(n(1-p)\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;s and because the average is the sum, &lt;span class=&#34;math inline&#34;&gt;\(n\times a \times p + n\times b \times (1-p)\)&lt;/span&gt;, divided by the total &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, we get that the average is &lt;span class=&#34;math inline&#34;&gt;\(ap + b(1-p)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. The first useful fact is that the &lt;em&gt;expected value of the sum of the draws&lt;/em&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{}\mbox{number of draws } \times \mbox{ average of the numbers in the urn}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So if 1,000 people play roulette, the casino expects to win, on average, about 1,000 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; $0.05 = $50. But this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question. The &lt;em&gt;standard error&lt;/em&gt; (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{SE}[X]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to denote the standard error of a random variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If our draws are independent&lt;/strong&gt;, then the &lt;em&gt;standard error of the sum&lt;/em&gt; is given by the equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{\mbox{number of draws }} \times \mbox{ standard deviation of the numbers in the urn}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; with proportions &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((1-p)\)&lt;/span&gt;, respectively, the standard deviation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mid b - a \mid \sqrt{p(1-p)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So in our roulette example, the standard deviation of the values inside the urn is: &lt;span class=&#34;math inline&#34;&gt;\(\mid 1 - (-1) \mid \sqrt{10/19 \times 9/19}\)&lt;/span&gt; or:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 * sqrt(90)/19&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.998614&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we either get 1 or -1, with 1 slightly favored over -1.&lt;/p&gt;
&lt;p&gt;Using the formula above, the sum of 1,000 people playing has standard error of about $32:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
sqrt(n) * 2 * sqrt(90)/19&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.57895&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet. But we still haven’t answered the question: how likely is it to lose money? Here the CLT will help.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced note&lt;/strong&gt;: Before continuing we should point out that exact probability calculations for the casino winnings can be performed with the binomial distribution. However, here we focus on the CLT, which can be generally applied to sums of random variables in a way that the binomial distribution can’t.&lt;/p&gt;
&lt;div id=&#34;population-sd-versus-the-sample-sd&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Population SD versus the sample SD&lt;/h3&gt;
&lt;p&gt;The standard deviation of a list &lt;code&gt;x&lt;/code&gt; (below we use heights as an example) is defined as the square root of the average of the squared differences:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
x &amp;lt;- heights$height
m &amp;lt;- mean(x)
s &amp;lt;- sqrt(mean((x-m)^2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using mathematical notation we write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu = \frac{1}{n} \sum_{i=1}^n x_i \\
\sigma =  \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, be aware that the &lt;code&gt;sd&lt;/code&gt; function returns a slightly different result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;identical(s, sd(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s-sd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.001942661&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is because the &lt;code&gt;sd&lt;/code&gt; function R does not return the &lt;code&gt;sd&lt;/code&gt; of the list, but rather uses a formula that estimates standard deviations of a population from a random sample &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_N\)&lt;/span&gt; which, for reasons not discussed here, divide the sum of squares by the &lt;span class=&#34;math inline&#34;&gt;\(N-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\bar{X} = \frac{1}{N} \sum_{i=1}^N X_i, \,\,\,\,
s =  \sqrt{\frac{1}{N-1} \sum_{i=1}^N (X_i - \bar{X})^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You can see that this is the case by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- length(x)
s-sd(x)*sqrt((n-1) / n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For all the theory discussed here, you need to compute the actual standard deviation as defined:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(mean((x-m)^2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So be careful when using the &lt;code&gt;sd&lt;/code&gt; function in R. However, keep in mind that throughout the book we sometimes use the &lt;code&gt;sd&lt;/code&gt; function when we really want the actual SD. This is because when the list size is big, these two are practically equivalent since &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{(N-1)/N} \approx 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;central-limit-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Central Limit Theorem&lt;/h2&gt;
&lt;p&gt;The Central Limit Theorem (CLT) tells us that when the number of draws, also called the &lt;em&gt;sample size&lt;/em&gt;, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.&lt;/p&gt;
&lt;p&gt;Previously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error.&lt;/p&gt;
&lt;p&gt;We previously ran this Monte Carlo simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
B &amp;lt;- 10000
roulette_winnings &amp;lt;- function(n){
  X &amp;lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
  sum(X)
}
S &amp;lt;- replicate(B, roulette_winnings(n))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Central Limit Theorem (CLT) tells us that the sum &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is approximated by a normal distribution.
Using the formulas above, we know that the expected value and standard error are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n * (20-18)/38&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 52.63158&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(n) * 2 * sqrt(90)/19&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.57895&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The theoretical values above match those obtained with the Monte Carlo simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(S)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 52.2242&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(S)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.65508&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu &amp;lt;- n * (20-18)/38
se &amp;lt;-  sqrt(n) * 2 * sqrt(90)/19
pnorm(0, mu, se)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04779035&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is also in very good agreement with our Monte Carlo result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(S &amp;lt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0458&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;how-large-is-large-in-the-central-limit-theorem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How large is large in the Central Limit Theorem?&lt;/h3&gt;
&lt;p&gt;The CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need much larger sample sizes.&lt;/p&gt;
&lt;p&gt;By way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate.&lt;/p&gt;
&lt;p&gt;You can examine the properties of the Poisson distribution using &lt;code&gt;dpois&lt;/code&gt; and &lt;code&gt;ppois&lt;/code&gt;. You can generate random variables following this distribution with &lt;code&gt;rpois&lt;/code&gt;. However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-properties-of-averages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical properties of averages&lt;/h2&gt;
&lt;p&gt;There are several useful mathematical results that we used above and often employ when working with data. We list them below.&lt;/p&gt;
&lt;p&gt;1. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[X_1+X_2+\dots+X_n] =  \mbox{E}[X_1] + \mbox{E}[X_2]+\dots+\mbox{E}[X_n]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are independent draws from the urn, then they all have the same expected value. Let’s call it &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[X_1+X_2+\dots+X_n]=  n\mu
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is another way of writing the result we show above for the sum of draws.&lt;/p&gt;
&lt;p&gt;2. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[aX] =  a\times\mbox{E}[X]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; again:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[(X_1+X_2+\dots+X_n) / n]=   \mbox{E}[X_1+X_2+\dots+X_n] / n = n\mu/n = \mu
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;3. The square of the standard error of the sum of &lt;strong&gt;independent&lt;/strong&gt; random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SE}[X_1+X_2+\dots+X_n] = \sqrt{\mbox{SE}[X_1]^2 + \mbox{SE}[X_2]^2+\dots+\mbox{SE}[X_n]^2  }
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The square of the standard error is referred to as the &lt;em&gt;variance&lt;/em&gt; in statistical textbooks. Note that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.&lt;/p&gt;
&lt;p&gt;4. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation:
&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SE}[aX] =  a \times \mbox{SE}[X]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To see why this is intuitive, again think of units.&lt;/p&gt;
&lt;p&gt;A consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; (the number of draws), call it &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mbox{SE}[(X_1+X_2+\dots+X_n) / n] &amp;amp;=   \mbox{SE}[X_1+X_2+\dots+X_n]/n \\
&amp;amp;= \sqrt{\mbox{SE}[X_1]^2+\mbox{SE}[X_2]^2+\dots+\mbox{SE}[X_n]^2}/n \\
&amp;amp;= \sqrt{\sigma^2+\sigma^2+\dots+\sigma^2}/n\\
&amp;amp;= \sqrt{n\sigma^2}/n\\
&amp;amp;= \sigma / \sqrt{n}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;5. If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a normally distributed random variable, then if &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; are non-random constants, &lt;span class=&#34;math inline&#34;&gt;\(aX + b\)&lt;/span&gt; is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, then shifting the center by &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that statistical textbooks use the Greek letters &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to denote the expected value and standard error, respectively. This is because &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the Greek letter for &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;, the first letter of &lt;em&gt;mean&lt;/em&gt;, which is another term used for expected value. Similarly, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is the Greek letter for &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, the first letter of standard error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;law-of-large-numbers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Law of large numbers&lt;/h2&gt;
&lt;p&gt;An important implication of the final result is that the standard error of the average becomes smaller and smaller as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; grows larger. When &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages.&lt;/p&gt;
&lt;div id=&#34;misinterpreting-law-of-averages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Misinterpreting law of averages&lt;/h3&gt;
&lt;p&gt;The law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. These events are independent so the chance of a coin landing heads is 50% regardless of the previous 5. This is also the case for the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses.&lt;/p&gt;
&lt;p&gt;Another funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Urn_problem&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Urn_problem&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem&#34; class=&#34;uri&#34;&gt;https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Monty_Hall_problem&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Monty_Hall_problem&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Security_(finance)&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/w/index.php?title=Security_(finance)&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Binomial_distribution&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/w/index.php?title=Binomial_distribution&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Poisson_distribution&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/w/index.php?title=Poisson_distribution&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Welcome Back to R</title>
      <link>https://ssc442.netlify.app/content/01-content/</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/content/01-content/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#readings&#34;&gt;Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-brief-introduction-to-ssc442&#34;&gt;A Brief Introduction to SSC442&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#about-me&#34;&gt;About Me&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#about-you&#34;&gt;About You&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#this-course&#34;&gt;This Course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-about-this-course&#34;&gt;More About This Course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#and-finally&#34;&gt;And finally…&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-data-analytics&#34;&gt;What is “Data Analytics”?&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#starting-point-for-this-course&#34;&gt;Starting point for this course&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#statistical-learning&#34;&gt;Statistical Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-pros-and-cons-of-correlation&#34;&gt;The Pros and Cons of Correlation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-case-study-in-prediction&#34;&gt;A Case Study in Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-recent-examples-of-prediction&#34;&gt;More Recent Examples of Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#an-aside-nomenclature&#34;&gt;An Aside: Nomenclature&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learning-from-data&#34;&gt;Learning from Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-basics&#34;&gt;&lt;code&gt;R&lt;/code&gt; basics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-us-homicides-by-firearm&#34;&gt;Case study: US homicides by firearm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-very-basics&#34;&gt;The (very) basics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objects&#34;&gt;Objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-workspace&#34;&gt;The workspace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functions&#34;&gt;Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-prebuilt-objects&#34;&gt;Other prebuilt objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variable-names&#34;&gt;Variable names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#saving-your-workspace&#34;&gt;Saving your workspace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivating-scripts&#34;&gt;Motivating scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#commenting-your-code&#34;&gt;Commenting your code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-types&#34;&gt;Data types&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-frames&#34;&gt;Data frames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examining-an-object&#34;&gt;Examining an object&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-accessor&#34;&gt;The accessor: &lt;code&gt;$&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors-numerics-characters-and-logical&#34;&gt;Vectors: numerics, characters, and logical&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#factors&#34;&gt;Factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lists&#34;&gt;Lists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrices&#34;&gt;Matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors&#34;&gt;Vectors&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#creating-vectors&#34;&gt;Creating vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#names&#34;&gt;Names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sequences&#34;&gt;Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subsetting&#34;&gt;Subsetting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coercion&#34;&gt;Coercion&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#not-availables-na&#34;&gt;Not availables (NA)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sorting&#34;&gt;Sorting&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sort&#34;&gt;&lt;code&gt;sort&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#order&#34;&gt;&lt;code&gt;order&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#max-and-which.max&#34;&gt;&lt;code&gt;max&lt;/code&gt; and &lt;code&gt;which.max&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rank&#34;&gt;&lt;code&gt;rank&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#beware-of-recycling&#34;&gt;Beware of recycling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vector-arithmetics&#34;&gt;Vector arithmetics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rescaling-a-vector&#34;&gt;Rescaling a vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-vectors&#34;&gt;Two vectors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#indexing&#34;&gt;Indexing&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#subsetting-with-logicals&#34;&gt;Subsetting with logicals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logical-operators&#34;&gt;Logical operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#which&#34;&gt;&lt;code&gt;which&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#match&#34;&gt;&lt;code&gt;match&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#in&#34;&gt;&lt;code&gt;%in%&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rmarkdown&#34;&gt;Rmarkdown&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Readings&lt;/h2&gt;
&lt;p&gt;As noted in the syllabus, your readings will be assigned each week in this area. For this initial week, please read the course content. &lt;strong&gt;Read closely&lt;/strong&gt; the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://ssc442.netlify.app/syllabus/&#34;&gt;syllabus&lt;/a&gt;, &lt;a href=&#34;https://ssc442.netlify.app/content/&#34;&gt;content&lt;/a&gt;, &lt;a href=&#34;https://ssc442.netlify.app/example/&#34;&gt;examples&lt;/a&gt;, and &lt;a href=&#34;https://ssc442.netlify.app/lab/&#34;&gt;labs&lt;/a&gt; pages for this class.&lt;/li&gt;
&lt;li&gt;This page. Yes, the whole thing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Things to stress from syllabus:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E-mail isn’t the ideal solution for technical problems&lt;/li&gt;
&lt;li&gt;No appointments necessary for regularly scheduled office hours; or by appointment.&lt;/li&gt;
&lt;li&gt;TA office hours are great as well. Our TA has experience in this course.&lt;/li&gt;
&lt;li&gt;Can only reschedule exams (with good reason) if you tell me &lt;strong&gt;before&lt;/strong&gt; the exam that you have a conflict.
&lt;ul&gt;
&lt;li&gt;Notify me immediately if you need accommodations because of RCPD or religious convictions; If you approach me at the last minute, I may not be able to help.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Despite my apparent hard-assness, I’m here to help. I am not in the business of giving bad grades for no reason, and I genuinely want you to learn a lot and enjoy the course.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-brief-introduction-to-ssc442&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Brief Introduction to SSC442&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;I keep saying that the sexy job in the next 10 years will be statisticians. And I’m not kidding.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;hal-varian-chief-economist-google&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hal Varian, Chief Economist, Google&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;about-me&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;About Me&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Me:&lt;/strong&gt; My primary area of expertise is behavioral economics (also known as psychology and economics). While my research occasionally touches the topics in the course, I mostly utilize things in the course as tools. In this way, we are likely the same.&lt;/p&gt;
&lt;p&gt;This class is totally, unapologetically a work in progress. The material is a mish-mash of stuff from courses offered at Caltech, Stanford, Harvard, and Duke…so, yeah, it will be challenging. Hopefully, you’ll find it fun!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-you&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;About You&lt;/h3&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;em&gt;New phone who dis?&lt;/em&gt; Please email me bbushong@msu.edu your&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;name (with pronunciation guide)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;major&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;desired graduation year and semester&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;interest in this course on a 10-point scale (1: not at all interested; 10: helllllll yeah)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;You &lt;strong&gt;must&lt;/strong&gt; spend 5 minutes emailing me a little bit about your interests before the next class.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;this-course&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;This Course&lt;/h3&gt;
&lt;p&gt;The syllabus is posted on the course website. I’ll walk through highlights now, but read it later – it’s long.
- But eventually, please read it. It is “required.”&lt;/p&gt;
&lt;p&gt;Syllabus highlights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Grade is composed of weekly writings, labs, and projects.
&lt;ul&gt;
&lt;li&gt;Weekly writings: 22%&lt;/li&gt;
&lt;li&gt;Participation: 4%&lt;/li&gt;
&lt;li&gt;Labs: 29%&lt;/li&gt;
&lt;li&gt;Projects: 45%&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;This structure is designed to give ~55% “for free”. Success on the projects will require real work.&lt;/li&gt;
&lt;li&gt;Labs consist of a practical implementation of something we’ve covered in the course (e.g., code your own Recommender System).&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;grading&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Grading&lt;/h4&gt;
&lt;p&gt;Grading: &lt;strong&gt;come to class.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If&lt;/strong&gt; you complete all assignments and attend all class dates, I suspect you will do very well. Given the way the syllabus is structured, I conjecture that the following is a loose guide to grades:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;4.0&lt;/code&gt; Turned in all assignments with good effort, worked hard on the projects and was proud of final product.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;3.5&lt;/code&gt; Turned in all assignments with good effort, worked a bit on the projects and was indifferent to final product.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;3.0&lt;/code&gt; Turned in all assignments with some effort, worked a bit on the projects and was shy about final product.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;lt; 3.0&lt;/code&gt; Very little effort, or did not turn in all assignments, worked very little on the projects and was embarassed by final product.&lt;/p&gt;
&lt;p&gt;…of course, failing to turn in assignments can lead to a grade dramatically &lt;strong&gt;lower&lt;/strong&gt; than just a 3.0.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;more-about-this-course&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;More About This Course&lt;/h3&gt;
&lt;p&gt;There are sort of three texts for this course and sort of zero.&lt;/p&gt;
&lt;p&gt;The “main text” is free and available online. The secondary text is substantially more difficult, but also free online. The third text costs about $25. Assigned readings can be found on the course website under “Content”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please please please please please:&lt;/strong&gt; Ask questions during class.
- Most ideas will be new.
- Sometimes (often?) the material itself will be confusing or interesting—or both!
- Teaching is incredibly challenging right now.
- &lt;strong&gt;Note:&lt;/strong&gt; If I find that attendance is terrible, I may have to start incorporating attendance into participation.&lt;/p&gt;
&lt;p&gt;Return of the Please: If there is some topic that you really want to learn about, ask. If you are uncomfortable asking in front of the whole group, please see me during office hours.&lt;/p&gt;
&lt;p&gt;Because this is a new course:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some of the lectures will be way too long or too short.&lt;/li&gt;
&lt;li&gt;Some (most?) of the lectures won’t make sense.&lt;/li&gt;
&lt;li&gt;Some of the time I’ll forget what I intended to say and awkwardly stare at you for a few moments (sorry).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Comment &lt;strong&gt;throughout&lt;/strong&gt; the course, not just at the end.&lt;/p&gt;
&lt;p&gt;The material will improve with time and feedback.&lt;/p&gt;
&lt;p&gt;I encourage measured feedback and thoughtful responses to questions. If I call on you and you don’t know immediately, don’t freak out. If you don’t know, it’s totally okay to say you don’t know.&lt;/p&gt;
&lt;div id=&#34;super-big-important-explanation-of-the-course&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;SUPER BIG IMPORTANT EXPLANATION OF THE COURSE&lt;/h4&gt;
&lt;p&gt;We teach using “math”. Don’t be afraid. The math won’t hurt you. I fundamentally believe that true knowledge of how we learn from data depends on a basic understanding of the underlying mathematics.&lt;/p&gt;
&lt;p&gt;-The good news is that you’ll face no black boxes. In this class you’ll &lt;strong&gt;actually learn&lt;/strong&gt; how things work. (Probably. Hopefully?)
-More good news: the level of required math is reasonably low. High-school algebra or equivalent should be fine.
-The bad news is that (at times) the course is notation-heavy. This class will require an active mind.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;and-finally&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;And finally…&lt;/h3&gt;
&lt;p&gt;I cannot address field-specific questions in areas outside economics to any satisfying degree. I’m good at knowing what I don’t know and have a very small ego, which means that I’m much less likely to blow smoke up your ass than other professors. So I won’t pretend I know everything. Of course, this implies that I can’t help with certain types of questions.&lt;/p&gt;
&lt;p&gt;This course should be applicable broadly, but many of the examples will lean on my personal expertise (sorry).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;p&gt;For future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do you remember anything about &lt;code&gt;R&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;What are the different data types in &lt;code&gt;R&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;How do you index specific elements of a vector? Why might you want to do that?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-data-analytics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is “Data Analytics”?&lt;/h1&gt;
&lt;p&gt;How do &lt;strong&gt;you&lt;/strong&gt; define “data analytics”? (Not a rhetorical question!)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This course will avoid this nomenclature. It is confusing and imprecise. But you signed up (suckers) and I owe an explanation of what this course will cover.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some “data analytics” topics we will cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear regression (&lt;em&gt;il classico&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;Models of classification or discrete choice.&lt;/li&gt;
&lt;li&gt;Analysis of ``wide’’ data.&lt;/li&gt;
&lt;li&gt;Decision trees and other non-linear models.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;starting-point-for-this-course&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Starting point for this course&lt;/h2&gt;
&lt;p&gt;Better utilizing existing data can improve our predictive power whilst providing interpretable outputs for considering new policies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING:&lt;/strong&gt; Causation is tough and we will spend the entire course warning you to avoid making causal claims!&lt;/p&gt;
&lt;div id=&#34;statistical-learning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Statistical Learning&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A Brief History&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose you are a researcher and you want to teach a computer to recognize images of a tree.&lt;/p&gt;
&lt;p&gt;Note: this is an ``easy” problem. If you show pictures to a 3-year-old, that child will probably be able to tell you if there is a tree in the picture.&lt;/p&gt;
&lt;p&gt;Computer scientists spent about 20 years on this problem because they thought about the problem like nerds and tried to write down a series of rules.&lt;/p&gt;
&lt;p&gt;Rules are difficult to form, and simply writing rules misses the key insight: the data can tell you something.&lt;/p&gt;
&lt;div id=&#34;social-science-approaches-to-statistical-learning&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Social Science Approaches to Statistical Learning&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;A Brief History&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose you are a researcher and you want to know whether prisons reduce crime.&lt;/p&gt;
&lt;p&gt;from ``A Call for a Moratorium on Prison Building’’ (1976)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Between 1955 and 1975, fifteen states increased the collective capacity of their adult prison systems by 56% (from, on average, 63,100 to 98,649).&lt;/li&gt;
&lt;li&gt;Fifteen other states increased capacity by less than 4% (from 49,575 to 51,440).&lt;/li&gt;
&lt;li&gt;In “heavy-construction” states the crime rate increased by 167%; in “low-construction” states the crime rate increased by 145%.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Prison Capacity&lt;/th&gt;
&lt;th&gt;Crime Rate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;High construction&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\uparrow\)&lt;/span&gt;~56%&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\uparrow\)&lt;/span&gt;~167%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Low construction&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\uparrow\)&lt;/span&gt;~4%&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(\uparrow\)&lt;/span&gt;~145%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pros-and-cons-of-correlation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Pros and Cons of Correlation&lt;/h3&gt;
&lt;p&gt;Pros:
- Nature gives you correlations for free.
- In principle, everyone can agree on the facts.&lt;/p&gt;
&lt;p&gt;Cons:
- Correlations are not very helpful.
- They show what has happened, but not why.
- For many things, we care about why.&lt;/p&gt;
&lt;div id=&#34;why-a-correlation-exists-between-x-and-y&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Why a Correlation Exists Between X and Y&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow Y\)&lt;/span&gt;
X causes Y (causality)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X \leftarrow Y\)&lt;/span&gt;
Y causes X (reverse causality)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Z \rightarrow X\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(Z \rightarrow Y\)&lt;/span&gt;
Z causes X and Y (common cause)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow Y\)&lt;/span&gt;; &lt;span class=&#34;math inline&#34;&gt;\(Y \rightarrow X\)&lt;/span&gt;
X causes Y and Y causes X (simultaneous equations)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;uniting-social-science-and-computer-science&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Uniting Social Science and Computer Science&lt;/h4&gt;
&lt;p&gt;We will start in this course by examining situations where we do &lt;strong&gt;not&lt;/strong&gt; care about why something has happened, but instead we care about our ability to predict its occurrence from existing data.&lt;/p&gt;
&lt;p&gt;(But of course keep in back of mind that if you are making policy, you must care about why something happened).&lt;/p&gt;
&lt;p&gt;We will also borrow a few other ideas from CS:
- Anything is data
+ Satellite data
+ Unstructured text or audio
+ Facial expressions or vocal intonations
- Subtle improvements on existing techniques
- An eye towards practical implementability over ``cleanliness”&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-case-study-in-prediction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Case Study in Prediction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; a firm wishes to predict user behavior based on previous purchases or interactions.&lt;/p&gt;
&lt;p&gt;Small margins &lt;span class=&#34;math inline&#34;&gt;\(\rightarrow\)&lt;/span&gt; huge payoffs. &lt;span class=&#34;math inline&#34;&gt;\(10\% \rightarrow\)&lt;/span&gt; $1 million.&lt;/p&gt;
&lt;p&gt;Not obvious to me why this was worth so much for Netflix (that’s an interesting research question). However, it’s quite obvious why this is true in financial markets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;more-recent-examples-of-prediction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;More Recent Examples of Prediction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Identify the risk factors for prostate cancer.&lt;/li&gt;
&lt;li&gt;Classify a tissue sample into one of several cancer classes, based on a gene expression profile.&lt;/li&gt;
&lt;li&gt;Classify a recorded phoneme based on a log-periodogram.&lt;/li&gt;
&lt;li&gt;Predict whether someone will have a heart attack on the basis of demographic, diet and clinical measurements.&lt;/li&gt;
&lt;li&gt;Customize an email spam detection system.&lt;/li&gt;
&lt;li&gt;Identify a hand-drawn object.&lt;/li&gt;
&lt;li&gt;Determine which oscillations of stellar luminosity are likely due to exoplanets.&lt;/li&gt;
&lt;li&gt;Establish the relationship between salary and demographic variables in population survey data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;an-aside-nomenclature&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;An Aside: Nomenclature&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; arose as a subfield of Artificial Intelligence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Statistical learning&lt;/strong&gt; arose as a subfield of Statistics.&lt;/p&gt;
&lt;p&gt;There is much overlap; however, a few points of distinction:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Machine learning has a greater emphasis on large scale applications and prediction accuracy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Statistical learning emphasizes models and their interpretability, and precision and uncertainty.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;But the distinction has become more and more blurred, and there is a great deal of “cross-fertilization”.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Obviously true:&lt;/strong&gt; machine learning has the upper hand in marketing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learning-from-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Learning from Data&lt;/h3&gt;
&lt;p&gt;The following are the basic requirements for statistical learning:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A pattern exists.&lt;/li&gt;
&lt;li&gt;This pattern is not easily expressed in a closed mathematical form.&lt;/li&gt;
&lt;li&gt;You have data.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;ALERT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The course content below should be considered a prerequisite for success. For those concerned about basics of &lt;code&gt;R&lt;/code&gt;, you absolutely must read this content and attempt the coding exercises. If you struggle to follow the content, please contact the professor or TA.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;r-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;code&gt;R&lt;/code&gt; basics&lt;/h1&gt;
&lt;p&gt;In this class, we will be using &lt;code&gt;R&lt;/code&gt; software environment for all our analyses. You will learn &lt;code&gt;R&lt;/code&gt; and data analysis techniques simultaneously. To follow along you will therefore need access to &lt;code&gt;R&lt;/code&gt;. We also recommend the use of an &lt;em&gt;integrated development environment&lt;/em&gt; (IDE), such as RStudio, to save your work.
Note that it is common for a course or workshop to offer access to an &lt;code&gt;R&lt;/code&gt; environment and an IDE through your web browser, as done by RStudio cloud&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. If you have access to such a resource, you don’t need to install &lt;code&gt;R&lt;/code&gt; and RStudio. However, if you intend on becoming a practicing data analyst, we highly recommend installing these tools on your computer&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. This is not hard.&lt;/p&gt;
&lt;p&gt;Both &lt;code&gt;R&lt;/code&gt; and RStudio are free and available online.&lt;/p&gt;
&lt;div id=&#34;case-study-us-homicides-by-firearm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: US homicides by firearm&lt;/h2&gt;
&lt;p&gt;Imagine you live in Europe (if only!) and are offered a job in a US company with many locations in every state. It is a great job, but headlines such as &lt;strong&gt;US Gun Homicide Rate Higher Than Other Developed Countries&lt;/strong&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; have you worried. Fox News runs a scary looking graphic, and charts like the one below only add to that concern:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/01-content_files/figure-html/murder-rate-example-1-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;!--(Source:
[Ma’ayan Rosenzweigh/ABC News](https://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/), Data from UNODC Homicide Statistics) --&gt;
&lt;p&gt;Or even worse, this version from &lt;a href=&#34;https://everytownresearch.org&#34;&gt;everytown.org&lt;/a&gt;:
&lt;img src=&#34;https://ssc442.netlify.app/content/01-content_files/figure-html/murder-rate-example-2-1.png&#34; width=&#34;70%&#34; /&gt;
&lt;!--(Source  [everytown.org](https://everytownresearch.org))--&gt;&lt;/p&gt;
&lt;p&gt;But then you remember that (1) this is a hypothetical exercise; (2) you’ll take literally any job at this point; and (3) Geographic diversity matters – the United States is a large and diverse country with 50 very different states (plus the District of Columbia and some lovely territories).&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = &amp;quot;none&amp;quot;` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/content/01-content_files/figure-html/us-murders-by-state-map-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;California, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial.&lt;/p&gt;
&lt;p&gt;This is a relatively simple and straightforward problem in social science: you have options of where to live, and want to determine the safety of the various states. Your “research” is clearly policy-relevant: you will eventually have to live somewhere. We will begin to tackle the problem by examining data related to gun homicides in the US during 2010 using &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced &lt;code&gt;R&lt;/code&gt; skills. Ideally, this is a refresher. However, we are aware that your preparation in previously courses varies greatly from student to student. Moreover, we want you to be aware that the usefulness of some of these early building blocks may not be immediately obvious. Later in the class you will appreciate having these skills. Mastery will be rewarded both in this class and (of course) in life.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-very-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The (very) basics&lt;/h2&gt;
&lt;p&gt;Before we get started with the motivating dataset, we need to cover the very basics of &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;objects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Objects&lt;/h3&gt;
&lt;p&gt;Suppose a relatively math unsavvy student asks us for help solving several quadratic equations of the form &lt;span class=&#34;math inline&#34;&gt;\(ax^2+bx+c = 0\)&lt;/span&gt;. You—a savvy student—recall that the quadratic formula gives us the solutions:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{-b - \sqrt{b^2 - 4ac}}{2a}\,\, \mbox{ and } \frac{-b + \sqrt{b^2 - 4ac}}{2a}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which of course depend on the values of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. That is, the quadratic equation represents a &lt;em&gt;function&lt;/em&gt; with three &lt;em&gt;arguments&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;One advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve &lt;span class=&#34;math inline&#34;&gt;\(x^2 + x -1 = 0\)&lt;/span&gt;, then we define:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 1
b &amp;lt;- 1
c &amp;lt;- -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which stores the values for later use. We use &lt;code&gt;&amp;lt;-&lt;/code&gt; to assign values to the variables.&lt;/p&gt;
&lt;p&gt;We can also assign values using &lt;code&gt;=&lt;/code&gt; instead of &lt;code&gt;&amp;lt;-&lt;/code&gt;, but we recommend against using &lt;code&gt;=&lt;/code&gt; to avoid confusion.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Copy and paste the code above into your console to define the three variables. Note that &lt;code&gt;R&lt;/code&gt; does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Throughout these written notes, you’ll have the most success if you continue to copy code into your own console.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To see the value stored in a variable, we simply ask &lt;code&gt;R&lt;/code&gt; to evaluate &lt;code&gt;a&lt;/code&gt; and it shows the stored value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A more explicit way to ask &lt;code&gt;R&lt;/code&gt; to show us the value stored in &lt;code&gt;a&lt;/code&gt; is using &lt;code&gt;print&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the term &lt;em&gt;object&lt;/em&gt; to describe stuff that is stored in &lt;code&gt;R&lt;/code&gt;. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-workspace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The workspace&lt;/h3&gt;
&lt;p&gt;As we define objects in the console, we are actually changing the &lt;em&gt;workspace&lt;/em&gt;. You can see all the variables saved in your workspace by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ls()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot;       &amp;quot;b&amp;quot;       &amp;quot;c&amp;quot;       &amp;quot;dat&amp;quot;     &amp;quot;filter&amp;quot;  &amp;quot;murders&amp;quot; &amp;quot;select&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that one of &lt;em&gt;my&lt;/em&gt; variables listed above comes from generating the graphs above). In RStudio, the &lt;em&gt;Environment&lt;/em&gt; tab shows the values:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/rstudio-environment.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We should see &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt;. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type &lt;code&gt;x&lt;/code&gt; you will receive the following message: &lt;code&gt;Error: object &#39;x&#39; not found&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.618034&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1.618034&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;p&gt;Once you define variables, the data analysis process can usually be described as a series of &lt;em&gt;functions&lt;/em&gt; applied to the data. &lt;code&gt;R&lt;/code&gt; includes several zillion predefined functions and most of the analysis pipelines we construct make extensive use of the built-in functions. But &lt;code&gt;R&lt;/code&gt;’s power comes from its scalability. We have access to (nearly) infinite functions via &lt;code&gt;install.packages&lt;/code&gt; and &lt;code&gt;library&lt;/code&gt;. As we go through the course, we will carefully note new functions we bring to each problem. For now, though, we will stick to the basics.&lt;/p&gt;
&lt;p&gt;Note that you’ve used a function already: you used the function &lt;code&gt;sqrt&lt;/code&gt; to solve the quadratic equation above. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.&lt;/p&gt;
&lt;p&gt;In general, we need to use parentheses to evaluate a function. If you type &lt;code&gt;ls&lt;/code&gt;, the function is not evaluated and instead &lt;code&gt;R&lt;/code&gt; shows you the code that defines the function. If you type &lt;code&gt;ls()&lt;/code&gt; the function is evaluated and, as seen above, we see objects in the workspace.&lt;/p&gt;
&lt;p&gt;Unlike &lt;code&gt;ls&lt;/code&gt;, most functions require one or more &lt;em&gt;arguments&lt;/em&gt;. Below is an example of how we assign an object to the argument of the function &lt;code&gt;log&lt;/code&gt;. Remember that we earlier defined &lt;code&gt;a&lt;/code&gt; to be 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.079442&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find out what the function expects and what it does by reviewing the very useful manuals included in &lt;code&gt;R&lt;/code&gt;. You can get help by using the &lt;code&gt;help&lt;/code&gt; function like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(&amp;quot;log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For most functions, we can also use this shorthand:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?log&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The help page will show you what arguments the function is expecting. For example, &lt;code&gt;log&lt;/code&gt; needs &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;base&lt;/code&gt; to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with &lt;code&gt;=&lt;/code&gt;. Defining these is optional.&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; For example, the base of the function &lt;code&gt;log&lt;/code&gt; defaults to &lt;code&gt;base = exp(1)&lt;/code&gt;—that is, &lt;code&gt;log&lt;/code&gt; evaluates the natural log by default.&lt;/p&gt;
&lt;p&gt;If you want a quick look at the arguments without opening the help system, you can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;args(log)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (x, base = exp(1)) 
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can change the default values by simply assigning another object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(8, base = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we have not been specifying the argument &lt;code&gt;x&lt;/code&gt; as such:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(x = 8, base = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code works, but we can save ourselves some typing: if no argument name is used, &lt;code&gt;R&lt;/code&gt; assumes you are entering arguments in the order shown in the help file or by &lt;code&gt;args&lt;/code&gt;. So by not using the names, it assumes the arguments are &lt;code&gt;x&lt;/code&gt; followed by &lt;code&gt;base&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(8,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If using the arguments’ names, then we can include them in whatever order we want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(base = 2, x = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To specify arguments, we must use &lt;code&gt;=&lt;/code&gt;, and cannot use &lt;code&gt;&amp;lt;-&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 ^ 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the arithmetic operators by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(&amp;quot;+&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?&amp;quot;+&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the relational operators by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(&amp;quot;&amp;gt;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?&amp;quot;&amp;gt;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-prebuilt-objects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other prebuilt objects&lt;/h3&gt;
&lt;p&gt;There are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; will show you Mauna Loa atmospheric &lt;span class=&#34;math inline&#34;&gt;\(CO^2\)&lt;/span&gt; concentration data.&lt;/p&gt;
&lt;p&gt;Other prebuilt objects are mathematical quantities, such as the constant &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.141593&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Inf+1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Inf&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;variable-names&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variable names&lt;/h3&gt;
&lt;p&gt;We have used the letters &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt; as variable names, but variable names can be almost anything. Some basic rules in &lt;code&gt;R&lt;/code&gt; are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in &lt;code&gt;R&lt;/code&gt;. For example, don’t name one of your variables &lt;code&gt;install.packages&lt;/code&gt; by typing something like &lt;code&gt;install.packages &amp;lt;- 2&lt;/code&gt;. Usually, &lt;code&gt;R&lt;/code&gt; is smart enough to prevent you from doing such nonsense, but it’s important to develop good habits.&lt;/p&gt;
&lt;p&gt;A nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solution_1 &amp;lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a)
solution_2 &amp;lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more advice, we highly recommend studying (Hadley Wickham’s style guide)[&lt;a href=&#34;http://adv-r.had.co.nz/Style.html&#34; class=&#34;uri&#34;&gt;http://adv-r.had.co.nz/Style.html&lt;/a&gt;].&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;saving-your-workspace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Saving your workspace&lt;/h3&gt;
&lt;p&gt;Values remain in the workspace until you end your session or erase them with the function &lt;code&gt;rm&lt;/code&gt;. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.&lt;/p&gt;
&lt;p&gt;We actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function &lt;code&gt;save&lt;/code&gt; or &lt;code&gt;save.image&lt;/code&gt;. To load, use the function &lt;code&gt;load&lt;/code&gt;. When saving a workspace, we recommend the suffix &lt;code&gt;rda&lt;/code&gt; or &lt;code&gt;RData&lt;/code&gt;. In RStudio, you can also do this by navigating to the &lt;em&gt;Session&lt;/em&gt; tab and choosing &lt;em&gt;Save Workspace as&lt;/em&gt;. You can later load it using the &lt;em&gt;Load Workspace&lt;/em&gt; options in the same tab.
You can read the help pages on &lt;code&gt;save&lt;/code&gt;, &lt;code&gt;save.image&lt;/code&gt;, and &lt;code&gt;load&lt;/code&gt; to learn more.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivating-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Motivating scripts&lt;/h3&gt;
&lt;p&gt;To solve another equation such as &lt;span class=&#34;math inline&#34;&gt;\(3x^2 + 2x -1\)&lt;/span&gt;, we can copy and paste the code above and then redefine the variables and recompute the solution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 3
b &amp;lt;- 2
c &amp;lt;- -1
(-b + sqrt(b^2 - 4*a*c)) / (2*a)
(-b - sqrt(b^2 - 4*a*c)) / (2*a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;commenting-your-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Commenting your code&lt;/h3&gt;
&lt;p&gt;If a line of &lt;code&gt;R&lt;/code&gt; code starts with the symbol &lt;code&gt;#&lt;/code&gt;, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Code to compute solution to quadratic equation of the form ax^2 + bx + c
## define the variables
a &amp;lt;- 3
b &amp;lt;- 2
c &amp;lt;- -1

## now compute the solution
(-b + sqrt(b^2 - 4*a*c)) / (2*a)
(-b - sqrt(b^2 - 4*a*c)) / (2*a)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What is the sum of the first 100 positive integers? The formula for the sum of integers &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n(n+1)/2\)&lt;/span&gt;. Define &lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt; and then use &lt;code&gt;R&lt;/code&gt; to compute the sum of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt; using the formula. What is the sum?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the same formula to compute the sum of the integers from 1 through 1,000.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Look at the result of typing the following code into R:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
x &amp;lt;- seq(1, n)
sum(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on the result, what do you think the functions &lt;code&gt;seq&lt;/code&gt; and &lt;code&gt;sum&lt;/code&gt; do? You can use &lt;code&gt;help&lt;/code&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;sum&lt;/code&gt; creates a list of numbers and &lt;code&gt;seq&lt;/code&gt; adds them up.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;seq&lt;/code&gt; creates a list of numbers and &lt;code&gt;sum&lt;/code&gt; adds them up.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;seq&lt;/code&gt; creates a random list and &lt;code&gt;sum&lt;/code&gt; computes the sum of 1 through 1,000.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sum&lt;/code&gt; always returns the same number.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type &lt;code&gt;sqrt(4)&lt;/code&gt;, we evaluate the &lt;code&gt;sqrt&lt;/code&gt; function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which of the following will always return the numeric value stored in &lt;code&gt;x&lt;/code&gt;? You can try out examples and use the help system if you want.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;log(10^x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;log10(x^10)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;log(exp(x))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;exp(log(x, base = 2))&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data types&lt;/h2&gt;
&lt;p&gt;Variables in &lt;code&gt;R&lt;/code&gt; can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function &lt;code&gt;class&lt;/code&gt; helps us determine what type of object we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 2
class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To work efficiently in R, it is important to learn the different types of variables and what we can do with these.&lt;/p&gt;
&lt;div id=&#34;data-frames&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data frames&lt;/h3&gt;
&lt;p&gt;Up to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in &lt;code&gt;R&lt;/code&gt; is in a &lt;em&gt;data frame&lt;/em&gt;. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.&lt;/p&gt;
&lt;p&gt;A large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the &lt;strong&gt;dslabs&lt;/strong&gt; library and loading the &lt;code&gt;murders&lt;/code&gt; dataset using the &lt;code&gt;data&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see that this is in fact a data frame, we type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;examining-an-object&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Examining an object&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;str&lt;/code&gt; is useful for finding out more about the structure of an object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    51 obs. of  5 variables:
## $ state : chr &amp;quot;Alabama&amp;quot; &amp;quot;Alaska&amp;quot; &amp;quot;Arizona&amp;quot; &amp;quot;Arkansas&amp;quot; ...
## $ abb : chr &amp;quot;AL&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;AR&amp;quot; ...
## $ region : Factor w/ 4 levels &amp;quot;Northeast&amp;quot;,&amp;quot;South&amp;quot;,..: 2 4 4 2 4 4 1 2 2 2 ...
## $ population: num 4779736 710231 6392017 2915918 37253956 ...
## $ total : num 135 19 232 93 1257 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function &lt;code&gt;head&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total
## 1    Alabama  AL  South    4779736   135
## 2     Alaska  AK   West     710231    19
## 3    Arizona  AZ   West    6392017   232
## 4   Arkansas  AR  South    2915918    93
## 5 California  CA   West   37253956  1257
## 6   Colorado  CO   West    5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this dataset, each state is considered an observation and five variables are reported for each state.&lt;/p&gt;
&lt;p&gt;Before we go any further in answering our original question about different states, let’s learn more about the components of this object.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-accessor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The accessor: &lt;code&gt;$&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;For our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator &lt;code&gt;$&lt;/code&gt; in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$population&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097   897934
##  [9]   601723 19687653  9920000  1360301  1567582 12830632  6483802  3046355
## [17]  2853118  4339367  4533372  1328361  5773552  6547629  9883640  5303925
## [25]  2967297  5988927   989415  1826341  2700551  1316470  8791894  2059179
## [33] 19378102  9535483   672591 11536504  3751351  3831074 12702379  1052567
## [41]  4625364   814180  6346105 25145561  2763885   625741  8001024  6724540
## [49]  1852994  5686986   563626&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But how did we know to use &lt;code&gt;population&lt;/code&gt;? Previously, by applying the function &lt;code&gt;str&lt;/code&gt; to the object &lt;code&gt;murders&lt;/code&gt;, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;state&amp;quot;      &amp;quot;abb&amp;quot;        &amp;quot;region&amp;quot;     &amp;quot;population&amp;quot; &amp;quot;total&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to know that the order of the entries in &lt;code&gt;murders$population&lt;/code&gt; preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: &lt;code&gt;R&lt;/code&gt; comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing &lt;code&gt;murders$p&lt;/code&gt; then hitting the &lt;kbd&gt;tab&lt;/kbd&gt; key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors-numerics-characters-and-logical&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vectors: numerics, characters, and logical&lt;/h3&gt;
&lt;p&gt;The object &lt;code&gt;murders$population&lt;/code&gt; is not one number but several. We call these types of objects &lt;em&gt;vectors&lt;/em&gt;. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function &lt;code&gt;length&lt;/code&gt; tells you how many entries are in the vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pop &amp;lt;- murders$population
length(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This particular vector is &lt;em&gt;numeric&lt;/em&gt; since population sizes are numbers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a numeric vector, every entry must be a number.&lt;/p&gt;
&lt;p&gt;To store character strings, vectors can also be of class &lt;em&gt;character&lt;/em&gt;. For example, the state names are characters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders$state)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with numeric vectors, all entries in a character vector need to be a character.&lt;/p&gt;
&lt;p&gt;Another important type of vectors are &lt;em&gt;logical vectors&lt;/em&gt;. These must be either &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- 3 == 2
z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;logical&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the &lt;code&gt;==&lt;/code&gt; is a relational operator asking if 3 is equal to 2. In &lt;code&gt;R&lt;/code&gt;, if you just use one &lt;code&gt;=&lt;/code&gt;, you actually assign a variable, but if you use two &lt;code&gt;==&lt;/code&gt; you test for equality. Yet another reason to avoid assigning via &lt;code&gt;=&lt;/code&gt;… it can get confusing and typos can really mess things up.&lt;/p&gt;
&lt;p&gt;You can see the other &lt;em&gt;relational operators&lt;/em&gt; by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?Comparison&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In future sections, you will see how useful relational operators can be.&lt;/p&gt;
&lt;p&gt;We discuss more important features of vectors after the next set of exercises.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Mathematically, the values in &lt;code&gt;pop&lt;/code&gt; are integers and there is an integer class in &lt;code&gt;R&lt;/code&gt;. However, by default, numbers are assigned class numeric even when they are round integers. For example, &lt;code&gt;class(1)&lt;/code&gt; returns numeric. You can turn them into class integer with the &lt;code&gt;as.integer()&lt;/code&gt; function or by adding an &lt;code&gt;L&lt;/code&gt; like this: &lt;code&gt;1L&lt;/code&gt;. Note the class by typing: &lt;code&gt;class(1L)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factors&lt;/h3&gt;
&lt;p&gt;In the &lt;code&gt;murders&lt;/code&gt; dataset, we might expect the region to also be a character vector. However, it is not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders$region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a &lt;em&gt;factor&lt;/em&gt;. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the &lt;code&gt;levels&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(murders$region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Northeast&amp;quot;     &amp;quot;South&amp;quot;         &amp;quot;North Central&amp;quot; &amp;quot;West&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the background, &lt;code&gt;R&lt;/code&gt; stores these &lt;em&gt;levels&lt;/em&gt; as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. It is also useful for computational reasons we’ll explore later.&lt;/p&gt;
&lt;p&gt;Note that the levels have an order that is different from the order of appearance in the factor object. The default in &lt;code&gt;R&lt;/code&gt; is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the &lt;code&gt;levels&lt;/code&gt; argument when creating the factor with the &lt;code&gt;factor&lt;/code&gt; function. For example, in the murders dataset regions are ordered from east to west. The function &lt;code&gt;reorder&lt;/code&gt; lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.&lt;/p&gt;
&lt;p&gt;Suppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the &lt;code&gt;reorder&lt;/code&gt; and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;region &amp;lt;- murders$region
value &amp;lt;- murders$total
region &amp;lt;- reorder(region, value, FUN = sum)
levels(region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Northeast&amp;quot;     &amp;quot;North Central&amp;quot; &amp;quot;West&amp;quot;          &amp;quot;South&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new order is in agreement with the fact that the Northeast has the least murders and the South has the most.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lists&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lists&lt;/h3&gt;
&lt;p&gt;Data frames are a special case of &lt;em&gt;lists&lt;/em&gt;. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $name
## [1] &amp;quot;John Doe&amp;quot;
## 
## $student_id
## [1] 1234
## 
## $grades
## [1] 95 82 91 97 93
## 
## $final_grade
## [1] &amp;quot;A&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(record)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with data frames, you can extract the components of a list with the accessor &lt;code&gt;$&lt;/code&gt;. In fact, data frames are a type of list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record$student_id&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use double square brackets (&lt;code&gt;[[&lt;/code&gt;) like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record[[&amp;quot;student_id&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should get used to the fact that in &lt;code&gt;R&lt;/code&gt; there are often several ways to do the same thing. such as accessing entries.&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You might also encounter lists without variable names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;John Doe&amp;quot;
## 
## [[2]]
## [1] 1234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If a list does not have names, you cannot extract the elements with &lt;code&gt;$&lt;/code&gt;, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record2[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;John Doe&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We won’t be using lists until later, but you might encounter one in your own exploration of &lt;code&gt;R&lt;/code&gt;. For this reason, we show you some basics here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrices&lt;/h3&gt;
&lt;p&gt;Matrices are another type of object that are common in &lt;code&gt;R&lt;/code&gt;. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.&lt;/p&gt;
&lt;p&gt;Yet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this class, but much of what happens in the background when you perform a data analysis involves matrices. We describe them briefly here since some of the functions we will learn return matrices.&lt;/p&gt;
&lt;p&gt;We can define a matrix using the &lt;code&gt;matrix&lt;/code&gt; function. We need to specify the number of rows and columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat &amp;lt;- matrix(1:12, 4, 3)
mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    5    9
## [2,]    2    6   10
## [3,]    3    7   11
## [4,]    4    8   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can access specific entries in a matrix using square brackets (&lt;code&gt;[&lt;/code&gt;). If you want the second row, third column, you use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[2, 3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want the entire second row, you leave the column spot empty:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[2, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  2  6 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that this returns a vector, not a matrix.&lt;/p&gt;
&lt;p&gt;Similarly, if you want the entire third column, you leave the row spot empty:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[, 3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  9 10 11 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is also a vector, not a matrix.&lt;/p&gt;
&lt;p&gt;You can access more than one column or more than one row if you like. This will give you a new matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[, 2:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    5    9
## [2,]    6   10
## [3,]    7   11
## [4,]    8   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can subset both rows and columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[1:2, 2:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    5    9
## [2,]    6   10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can convert matrices into data frames using the function &lt;code&gt;as.data.frame&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.data.frame(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   V1 V2 V3
## 1  1  5  9
## 2  2  6 10
## 3  3  7 11
## 4  4  8 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use single square brackets (&lt;code&gt;[&lt;/code&gt;) to access rows and columns of a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;murders&amp;quot;)
murders[25, 1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Mississippi&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders[2:3, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     state abb region population total
## 2  Alaska  AK   West     710231    19
## 3 Arizona  AZ   West    6392017   232&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the US murders dataset.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the function &lt;code&gt;str&lt;/code&gt; to examine the structure of the &lt;code&gt;murders&lt;/code&gt; object. Which of the following best describes the variables represented in this data frame?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The 51 states.&lt;/li&gt;
&lt;li&gt;The murder rates for all 50 states and DC.&lt;/li&gt;
&lt;li&gt;The state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;str&lt;/code&gt; shows no relevant information.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What are the column names used by the data frame for these five variables?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the accessor &lt;code&gt;$&lt;/code&gt; to extract the state abbreviations and assign them to the object &lt;code&gt;a&lt;/code&gt;. What is the class of this object?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the square brackets to extract the state abbreviations and assign them to the object &lt;code&gt;b&lt;/code&gt;. Use the &lt;code&gt;identical&lt;/code&gt; function to determine if &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are the same.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We saw that the &lt;code&gt;region&lt;/code&gt; column stores a factor. You can corroborate this by typing:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders$region)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With one line of code, use the function &lt;code&gt;levels&lt;/code&gt; and &lt;code&gt;length&lt;/code&gt; to determine the number of regions defined by this dataset.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The function &lt;code&gt;table&lt;/code&gt; takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vectors&lt;/h2&gt;
&lt;p&gt;In R, the most basic objects available to store data are &lt;em&gt;vectors&lt;/em&gt;. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.&lt;/p&gt;
&lt;div id=&#34;creating-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating vectors&lt;/h3&gt;
&lt;p&gt;We can create vectors using the function &lt;code&gt;c&lt;/code&gt;, which stands for &lt;em&gt;concatenate&lt;/em&gt;. We use &lt;code&gt;c&lt;/code&gt; to concatenate entries in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(380, 124, 818)
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 380 124 818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country &amp;lt;- c(&amp;quot;italy&amp;quot;, &amp;quot;canada&amp;quot;, &amp;quot;egypt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt; you can also use single quotes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country &amp;lt;- c(&amp;#39;italy&amp;#39;, &amp;#39;canada&amp;#39;, &amp;#39;egypt&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But be careful not to confuse the single quote ’ with the &lt;em&gt;back quote&lt;/em&gt;, which shares a keyboard key with &lt;kbd&gt;~&lt;/kbd&gt;.&lt;/p&gt;
&lt;p&gt;By now you should know that if you type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country &amp;lt;- c(italy, canada, egypt)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;you receive an error because the variables &lt;code&gt;italy&lt;/code&gt;, &lt;code&gt;canada&lt;/code&gt;, and &lt;code&gt;egypt&lt;/code&gt; are not defined. If we do not use the quotes, &lt;code&gt;R&lt;/code&gt; looks for variables with those names and returns an error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;names&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Names&lt;/h3&gt;
&lt;p&gt;Sometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(italy = 380, canada = 124, egypt = 818)
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada  egypt 
##    380    124    818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The object &lt;code&gt;codes&lt;/code&gt; continues to be a numeric vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(codes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but with names:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(codes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;italy&amp;quot;  &amp;quot;canada&amp;quot; &amp;quot;egypt&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the use of strings without quotes looks confusing, know that you can use the quotes as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(&amp;quot;italy&amp;quot; = 380, &amp;quot;canada&amp;quot; = 124, &amp;quot;egypt&amp;quot; = 818)
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada  egypt 
##    380    124    818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no difference between this function call and the previous one. This is one of the many ways in which &lt;code&gt;R&lt;/code&gt; is quirky compared to other languages.&lt;/p&gt;
&lt;p&gt;We can also assign names using the &lt;code&gt;names&lt;/code&gt; functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(380, 124, 818)
country &amp;lt;- c(&amp;quot;italy&amp;quot;,&amp;quot;canada&amp;quot;,&amp;quot;egypt&amp;quot;)
names(codes) &amp;lt;- country
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada  egypt 
##    380    124    818&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sequences&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sequences&lt;/h3&gt;
&lt;p&gt;Another useful function for creating vectors generates sequences:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 10, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 3 5 7 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want consecutive integers, we can use the following shorthand:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we use these functions, &lt;code&gt;R&lt;/code&gt; produces integers, not numerics, because they are typically used to index something:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, if we create a sequence including non-integers, the class changes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(seq(1, 10, 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;subsetting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting&lt;/h3&gt;
&lt;p&gt;We use square brackets to access specific elements of a vector. For the vector &lt;code&gt;codes&lt;/code&gt; we defined above, we can access the second element using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## canada 
##    124&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can get more than one entry by using a multi-entry vector as an index:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[c(1,3)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## italy egypt 
##   380   818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sequences defined above are particularly useful if we want to access, say, the first two elements:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada 
##    380    124&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the elements have names, we can also access the entries using these names. Below are two examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[&amp;quot;canada&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## canada 
##    124&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[c(&amp;quot;egypt&amp;quot;,&amp;quot;italy&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## egypt italy 
##   818   380&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;coercion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coercion&lt;/h2&gt;
&lt;p&gt;In general, &lt;em&gt;coercion&lt;/em&gt; is an attempt by &lt;code&gt;R&lt;/code&gt; to be flexible with data types. When an entry does not match the expected, some of the prebuilt &lt;code&gt;R&lt;/code&gt; functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand &lt;em&gt;coercion&lt;/em&gt; can drive programmers crazy when attempting to code in &lt;code&gt;R&lt;/code&gt; since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.&lt;/p&gt;
&lt;p&gt;We said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(1, &amp;quot;canada&amp;quot;, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we don’t get one, not even a warning! What happened? Look at &lt;code&gt;x&lt;/code&gt; and its class:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1&amp;quot;      &amp;quot;canada&amp;quot; &amp;quot;3&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R &lt;em&gt;coerced&lt;/em&gt; the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings &lt;code&gt;&#34;1&#34;&lt;/code&gt; and “&lt;code&gt;3&lt;/code&gt;”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;R also offers functions to change from one type to another. For example, you can turn numbers into characters with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:5
y &amp;lt;- as.character(x)
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; &amp;quot;5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can turn it back with &lt;code&gt;as.numeric&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is actually quite useful since datasets that include numbers as character strings are common.&lt;/p&gt;
&lt;div id=&#34;not-availables-na&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Not availables (NA)&lt;/h3&gt;
&lt;p&gt;This “topic” seems to be wholly unappreciated and it has been our experience that students often panic when encountering an &lt;code&gt;NA&lt;/code&gt;. This often happens when a function tries to coerce one type to another and encounters an impossible case. In such circumstances, &lt;code&gt;R&lt;/code&gt; usually gives us a warning and turns the entry into a special value called an &lt;code&gt;NA&lt;/code&gt; (for “not available”). For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(&amp;quot;1&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;3&amp;quot;)
as.numeric(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1 NA  3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R does not have any guesses for what number you want when you type &lt;code&gt;b&lt;/code&gt;, so it does not try.&lt;/p&gt;
&lt;p&gt;While coercion is a common case leading to &lt;code&gt;NA&lt;/code&gt;s, you’ll see them in nearly every real-world dataset. Most often, you will encounter the &lt;code&gt;NA&lt;/code&gt;s as a stand-in for missing data. Again, this a common problem in real-world datasets and you need to be aware that it will come up.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sorting&lt;/h2&gt;
&lt;p&gt;Now that we have mastered some basic &lt;code&gt;R&lt;/code&gt; knowledge (ha!), let’s try to gain some insights into the safety of different states in the context of gun murders.&lt;/p&gt;
&lt;div id=&#34;sort&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;sort&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Say we want to rank the states from least to most gun murders. The function &lt;code&gt;sort&lt;/code&gt; sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
sort(murders$total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]    2    4    5    5    7    8   11   12   12   16   19   21   22   27   32
## [16]   36   38   53   63   65   67   84   93   93   97   97   99  111  116  118
## [31]  120  135  142  207  219  232  246  250  286  293  310  321  351  364  376
## [46]  413  457  517  669  805 1257&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;order&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;order&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;order&lt;/code&gt; is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(31, 4, 15, 92, 65)
sort(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  4 15 31 65 92&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rather than sort the input vector, the function &lt;code&gt;order&lt;/code&gt; returns the index that sorts input vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- order(x)
x[index]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  4 15 31 65 92&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the same output as that returned by &lt;code&gt;sort(x)&lt;/code&gt;. If we look at this index, we see why it works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31  4 15 92 65&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;order(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2 3 1 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second entry of &lt;code&gt;x&lt;/code&gt; is the smallest, so &lt;code&gt;order(x)&lt;/code&gt; starts with &lt;code&gt;2&lt;/code&gt;. The next smallest is the third entry, so the second entry is &lt;code&gt;3&lt;/code&gt; and so on.&lt;/p&gt;
&lt;p&gt;How does this help us order the states by murders? First, remember that the entries of vectors you access with &lt;code&gt;$&lt;/code&gt; follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$state[1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Alabama&amp;quot;    &amp;quot;Alaska&amp;quot;     &amp;quot;Arizona&amp;quot;    &amp;quot;Arkansas&amp;quot;   &amp;quot;California&amp;quot;
## [6] &amp;quot;Colorado&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$abb[1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;AL&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;AR&amp;quot; &amp;quot;CA&amp;quot; &amp;quot;CO&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- order(murders$total)
murders$abb[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;VT&amp;quot; &amp;quot;ND&amp;quot; &amp;quot;NH&amp;quot; &amp;quot;WY&amp;quot; &amp;quot;HI&amp;quot; &amp;quot;SD&amp;quot; &amp;quot;ME&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;MT&amp;quot; &amp;quot;RI&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;IA&amp;quot; &amp;quot;UT&amp;quot; &amp;quot;WV&amp;quot; &amp;quot;NE&amp;quot;
## [16] &amp;quot;OR&amp;quot; &amp;quot;DE&amp;quot; &amp;quot;MN&amp;quot; &amp;quot;KS&amp;quot; &amp;quot;CO&amp;quot; &amp;quot;NM&amp;quot; &amp;quot;NV&amp;quot; &amp;quot;AR&amp;quot; &amp;quot;WA&amp;quot; &amp;quot;CT&amp;quot; &amp;quot;WI&amp;quot; &amp;quot;DC&amp;quot; &amp;quot;OK&amp;quot; &amp;quot;KY&amp;quot; &amp;quot;MA&amp;quot;
## [31] &amp;quot;MS&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;SC&amp;quot; &amp;quot;TN&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;NJ&amp;quot; &amp;quot;VA&amp;quot; &amp;quot;NC&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;OH&amp;quot; &amp;quot;MO&amp;quot; &amp;quot;LA&amp;quot; &amp;quot;IL&amp;quot; &amp;quot;GA&amp;quot;
## [46] &amp;quot;MI&amp;quot; &amp;quot;PA&amp;quot; &amp;quot;NY&amp;quot; &amp;quot;FL&amp;quot; &amp;quot;TX&amp;quot; &amp;quot;CA&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the above, California had the most murders.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;max-and-which.max&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;max&lt;/code&gt; and &lt;code&gt;which.max&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If we are only interested in the entry with the largest value, we can use &lt;code&gt;max&lt;/code&gt; for the value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(murders$total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1257&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and &lt;code&gt;which.max&lt;/code&gt; for the index of the largest value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;i_max &amp;lt;- which.max(murders$total)
murders$state[i_max]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;California&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the minimum, we can use &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;which.min&lt;/code&gt; in the same way.&lt;/p&gt;
&lt;p&gt;Does this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: &lt;code&gt;rank&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rank&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;rank&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Although not as frequently used as &lt;code&gt;order&lt;/code&gt; and &lt;code&gt;sort&lt;/code&gt;, the function &lt;code&gt;rank&lt;/code&gt; is also related to order and can be useful.
For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(31, 4, 15, 92, 65)
rank(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 1 2 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To summarize, let’s look at the results of the three functions we have introduced:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
original
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sort
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rank
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;beware-of-recycling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Beware of recycling&lt;/h3&gt;
&lt;p&gt;Another common source of unnoticed errors in &lt;code&gt;R&lt;/code&gt; is the use of &lt;em&gt;recycling&lt;/em&gt;. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(1,2,3)
y &amp;lt;- c(10, 20, 30, 40, 50, 60, 70)
x+y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in x + y: longer object length is not a multiple of shorter object
## length&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11 22 33 41 52 63 71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do get a warning, but no error. For the output, &lt;code&gt;R&lt;/code&gt; has recycled the numbers in &lt;code&gt;x&lt;/code&gt;. Notice the last digit of numbers in the output.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For these exercises we will use the US murders dataset. Make sure you load it prior to starting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;$&lt;/code&gt; operator to access the population size data and store it as the object &lt;code&gt;pop&lt;/code&gt;. Then use the &lt;code&gt;sort&lt;/code&gt; function to redefine &lt;code&gt;pop&lt;/code&gt; so that it is sorted. Finally, use the &lt;code&gt;[&lt;/code&gt; operator to report the smallest population size.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use &lt;code&gt;order&lt;/code&gt; instead of &lt;code&gt;sort&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can actually perform the same operation as in the previous exercise using the function &lt;code&gt;which.min&lt;/code&gt;. Write one line of code that does this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable &lt;code&gt;states&lt;/code&gt; to be the state names from the &lt;code&gt;murders&lt;/code&gt; data frame. Report the name of the state with the smallest population.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can create a data frame using the &lt;code&gt;data.frame&lt;/code&gt; function. Here is a quick example:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp &amp;lt;- c(35, 88, 42, 84, 81, 30)
city &amp;lt;- c(&amp;quot;Beijing&amp;quot;, &amp;quot;Lagos&amp;quot;, &amp;quot;Paris&amp;quot;, &amp;quot;Rio de Janeiro&amp;quot;,
          &amp;quot;San Juan&amp;quot;, &amp;quot;Toronto&amp;quot;)
city_temps &amp;lt;- data.frame(name = city, temperature = temp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the &lt;code&gt;rank&lt;/code&gt; function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called &lt;code&gt;ranks&lt;/code&gt;, then create a data frame with the state name and its rank. Call the data frame &lt;code&gt;my_df&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Repeat the previous exercise, but this time order &lt;code&gt;my_df&lt;/code&gt; so that the states are ordered from least populous to most populous. Hint: create an object &lt;code&gt;ind&lt;/code&gt; that stores the indexes needed to order the population values. Then use the bracket operator &lt;code&gt;[&lt;/code&gt; to re-order each column in the data frame.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;na_example&lt;/code&gt; vector represents a series of counts. You can quickly examine the object using:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;na_example&amp;quot;)
str(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, when we compute the average with the function &lt;code&gt;mean&lt;/code&gt;, we obtain an &lt;code&gt;NA&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;is.na&lt;/code&gt; function returns a logical vector that tells us which entries are &lt;code&gt;NA&lt;/code&gt;. Assign this logical vector to an object called &lt;code&gt;ind&lt;/code&gt; and determine how many &lt;code&gt;NA&lt;/code&gt;s does &lt;code&gt;na_example&lt;/code&gt; have.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Now compute the average again, but only for the entries that are not &lt;code&gt;NA&lt;/code&gt;. Hint: remember the &lt;code&gt;!&lt;/code&gt; operator.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vector-arithmetics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vector arithmetics&lt;/h2&gt;
&lt;p&gt;California had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)
murders$state[which.max(murders$population)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;California&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;with over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of &lt;code&gt;R&lt;/code&gt; come in handy.&lt;/p&gt;
&lt;div id=&#34;rescaling-a-vector&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rescaling a vector&lt;/h3&gt;
&lt;p&gt;In R, arithmetic operations on vectors occur &lt;em&gt;element-wise&lt;/em&gt;. For a quick example, suppose we have height in inches:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inches &amp;lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and want to convert to centimeters. Notice what happens when we multiply &lt;code&gt;inches&lt;/code&gt; by 2.54:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inches * 2.54&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inches - 69&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0 -7 -3  1  1  4 -2  4 -2  1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;two-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two vectors&lt;/h3&gt;
&lt;p&gt;If we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{pmatrix}
a\\
b\\
c\\
d
\end{pmatrix}
+
\begin{pmatrix}
e\\
f\\
g\\
h
\end{pmatrix}
=
\begin{pmatrix}
a +e\\
b + f\\
c + g\\
d + h
\end{pmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The same holds for other mathematical operations, such as &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This implies that to compute the murder rates we can simply type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murder_rate &amp;lt;- murders$total / murders$population * 100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$abb[order(murder_rate)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;VT&amp;quot; &amp;quot;NH&amp;quot; &amp;quot;HI&amp;quot; &amp;quot;ND&amp;quot; &amp;quot;IA&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;UT&amp;quot; &amp;quot;ME&amp;quot; &amp;quot;WY&amp;quot; &amp;quot;OR&amp;quot; &amp;quot;SD&amp;quot; &amp;quot;MN&amp;quot; &amp;quot;MT&amp;quot; &amp;quot;CO&amp;quot; &amp;quot;WA&amp;quot;
## [16] &amp;quot;WV&amp;quot; &amp;quot;RI&amp;quot; &amp;quot;WI&amp;quot; &amp;quot;NE&amp;quot; &amp;quot;MA&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;KS&amp;quot; &amp;quot;NY&amp;quot; &amp;quot;KY&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;OH&amp;quot; &amp;quot;CT&amp;quot; &amp;quot;NJ&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;IL&amp;quot;
## [31] &amp;quot;OK&amp;quot; &amp;quot;NC&amp;quot; &amp;quot;NV&amp;quot; &amp;quot;VA&amp;quot; &amp;quot;AR&amp;quot; &amp;quot;TX&amp;quot; &amp;quot;NM&amp;quot; &amp;quot;CA&amp;quot; &amp;quot;FL&amp;quot; &amp;quot;TN&amp;quot; &amp;quot;PA&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;GA&amp;quot; &amp;quot;MS&amp;quot; &amp;quot;MI&amp;quot;
## [46] &amp;quot;DE&amp;quot; &amp;quot;SC&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;MO&amp;quot; &amp;quot;LA&amp;quot; &amp;quot;DC&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Previously we created this data frame:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp &amp;lt;- c(35, 88, 42, 84, 81, 30)
city &amp;lt;- c(&amp;quot;Beijing&amp;quot;, &amp;quot;Lagos&amp;quot;, &amp;quot;Paris&amp;quot;, &amp;quot;Rio de Janeiro&amp;quot;,
          &amp;quot;San Juan&amp;quot;, &amp;quot;Toronto&amp;quot;)
city_temps &amp;lt;- data.frame(name = city, temperature = temp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is &lt;span class=&#34;math inline&#34;&gt;\(C = \frac{5}{9} \times (F - 32)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write code to compute the following sum &lt;span class=&#34;math inline&#34;&gt;\(1+1/2^2 + 1/3^2 + \dots 1/100^2\)&lt;/span&gt;? &lt;em&gt;Hint:&lt;/em&gt; thanks to Euler, we know it should be close to &lt;span class=&#34;math inline&#34;&gt;\(\pi^2/6\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the per 100,000 murder rate for each state and store it in the object &lt;code&gt;murder_rate&lt;/code&gt;. Then compute the average murder rate for the US using the function &lt;code&gt;mean&lt;/code&gt;. What is the average?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;indexing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Indexing&lt;/h2&gt;
&lt;p&gt;Indexing is a boring name for an important tool. &lt;code&gt;R&lt;/code&gt; provides a powerful and convenient way of referencing specific elements of vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;subsetting-with-logicals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting with logicals&lt;/h3&gt;
&lt;p&gt;We have now calculated the murder rate using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murder_rate &amp;lt;- murders$total / murders$population * 100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Imagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of &lt;code&gt;R&lt;/code&gt; is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- murder_rate &amp;lt; 0.71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we instead want to know if a value is less or equal, we can use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- murder_rate &amp;lt;= 0.71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we get back a logical vector with &lt;code&gt;TRUE&lt;/code&gt; for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$state[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hawaii&amp;quot;        &amp;quot;Iowa&amp;quot;          &amp;quot;New Hampshire&amp;quot; &amp;quot;North Dakota&amp;quot; 
## [5] &amp;quot;Vermont&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to count how many are TRUE, the function &lt;code&gt;sum&lt;/code&gt; returns the sum of the entries of a vector and logical vectors get &lt;em&gt;coerced&lt;/em&gt; to numeric with &lt;code&gt;TRUE&lt;/code&gt; coded as 1 and &lt;code&gt;FALSE&lt;/code&gt; as 0. Thus we can count the states using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(ind)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logical-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logical operators&lt;/h3&gt;
&lt;p&gt;Suppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator &lt;em&gt;and&lt;/em&gt;, which in &lt;code&gt;R&lt;/code&gt; is represented with &lt;code&gt;&amp;amp;&lt;/code&gt;. This operation results in &lt;code&gt;TRUE&lt;/code&gt; only when both logicals are &lt;code&gt;TRUE&lt;/code&gt;. To see this, consider this example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TRUE &amp;amp; TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TRUE &amp;amp; FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FALSE &amp;amp; FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our example, we can form two logicals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;west &amp;lt;- murders$region == &amp;quot;West&amp;quot;
safe &amp;lt;- murder_rate &amp;lt;= 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we can use the &lt;code&gt;&amp;amp;&lt;/code&gt; to get a vector of logicals that tells us which states satisfy both conditions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- safe &amp;amp; west
murders$state[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hawaii&amp;quot;  &amp;quot;Idaho&amp;quot;   &amp;quot;Oregon&amp;quot;  &amp;quot;Utah&amp;quot;    &amp;quot;Wyoming&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;which&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;which&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Suppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function &lt;code&gt;which&lt;/code&gt; tells us which entries of a logical vector are TRUE. So we can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- which(murders$state == &amp;quot;California&amp;quot;)
murder_rate[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.374138&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;match&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;match&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function &lt;code&gt;match&lt;/code&gt;. This function tells us which indexes of a second vector match each of the entries of a first vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- match(c(&amp;quot;New York&amp;quot;, &amp;quot;Florida&amp;quot;, &amp;quot;Texas&amp;quot;), murders$state)
ind&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 33 10 44&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can look at the murder rates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murder_rate[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.667960 3.398069 3.201360&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;in&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;%in%&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function &lt;code&gt;%in%&lt;/code&gt;. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(&amp;quot;Boston&amp;quot;, &amp;quot;Dakota&amp;quot;, &amp;quot;Washington&amp;quot;) %in% murders$state&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we will be using &lt;code&gt;%in%&lt;/code&gt; often throughout the book.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: There is a connection between &lt;code&gt;match&lt;/code&gt; and &lt;code&gt;%in%&lt;/code&gt; through &lt;code&gt;which&lt;/code&gt;. To see this, notice that the following two lines produce the same index (although in different order):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;match(c(&amp;quot;New York&amp;quot;, &amp;quot;Florida&amp;quot;, &amp;quot;Texas&amp;quot;), murders$state)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 33 10 44&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;which(murders$state%in%c(&amp;quot;New York&amp;quot;, &amp;quot;Florida&amp;quot;, &amp;quot;Texas&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10 33 44&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;rmarkdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rmarkdown&lt;/h2&gt;
&lt;p&gt;If you’re new to Rmarkdown, a previous instructor (Prof. Kirkpatrick) has provided &lt;a href=&#34;https://mediaspace.msu.edu/media/Spring2021_R_Part3/1_yigvqy1i&#34;&gt;a short video on how to use it &lt;i class=&#34;fas fa-film&#34;&gt;&lt;/i&gt;&lt;/a&gt;. This video is for his EC420 course, but the principles are universal.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start by loading the library and data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Compute the per 100,000 murder rate for each state and store it in an object called &lt;code&gt;murder_rate&lt;/code&gt;. Then use logical operators to create a logical vector named &lt;code&gt;low&lt;/code&gt; that tells us which entries of &lt;code&gt;murder_rate&lt;/code&gt; are lower than 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the results from the previous exercise and the function &lt;code&gt;which&lt;/code&gt; to determine the indices of &lt;code&gt;murder_rate&lt;/code&gt; associated with values lower than 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the results from the previous exercise to report the names of the states with murder rates lower than 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector &lt;code&gt;low&lt;/code&gt; and the logical operator &lt;code&gt;&amp;amp;&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of &lt;code&gt;murders$abb&lt;/code&gt; that match the three abbreviations, then use the &lt;code&gt;[&lt;/code&gt; operator to extract the states.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;%in%&lt;/code&gt; operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Extend the code you used in exercise 7 to report the one entry that is &lt;strong&gt;not&lt;/strong&gt; an actual abbreviation. Hint: use the &lt;code&gt;!&lt;/code&gt; operator, which turns &lt;code&gt;FALSE&lt;/code&gt; into &lt;code&gt;TRUE&lt;/code&gt; and vice versa, then &lt;code&gt;which&lt;/code&gt; to obtain an index.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://rstudio.cloud&#34; class=&#34;uri&#34;&gt;https://rstudio.cloud&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://rafalab.github.io/dsbook/installing-r-rstudio.html&#34; class=&#34;uri&#34;&gt;https://rafalab.github.io/dsbook/installing-r-rstudio.html&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/&#34; class=&#34;uri&#34;&gt;http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;I’m especially partial to Puerto Rico.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;This is, without a doubt, my least favorite aspect of &lt;code&gt;R&lt;/code&gt;. I’d even venture to call it stupid. The logic behind this pesky &lt;code&gt;&amp;lt;-&lt;/code&gt; is a total mystery to me, but there &lt;em&gt;is&lt;/em&gt; logic to avoiding &lt;code&gt;=&lt;/code&gt;. But, you do you.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;This equals sign is the reasons we assign values with &lt;code&gt;&amp;lt;-&lt;/code&gt;; then when arguments of a function are assigned values, we don’t end up with multiple equals signs. But… who cares.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Whether you view this as a feature or a bug is a good indicator whether you’ll enjoy working with &lt;code&gt;R&lt;/code&gt;.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Schedule</title>
      <link>https://ssc442.netlify.app/schedule/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/schedule/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/schedule/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Below is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon. Accordingly, you should visit this page frequently throughout the term.&lt;/p&gt;
&lt;p&gt;As mentioned in the syllabus, the course is structured by topics. Each week introduces a new topic. Within each week, there are three elements of the course—these are described below.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;The class is structured with three distinct bits. First, the Tuesday lecture will give an overview of the topic for the week. Next, the Thursday lecture will have a shorter, practical lecture and an activity which is designed to give you hands-on experience and a greater understanding of the broader material. Finally, you will complete weekly writings (short) and labs (also short; requiring coding in &lt;code&gt;R&lt;/code&gt;). Out of class, you will complete readings and can watch supplemental videos on the week’s topic. You are not required to view supplemental recorded videos unless specifically noted.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/content/&#34;&gt;&lt;strong&gt;Content&lt;/strong&gt;&lt;/a&gt; (&lt;i class=&#34;fas fa-book-reader&#34;&gt;&lt;/i&gt;): This page contains the readings and recorded lectures for the topic. These pages should be read completely. Lectures are &lt;em&gt;not&lt;/em&gt; an exact replication of the written content; on the contrary, the lectures are intended to keep you focused on the high-level ideas, while the readings are broader and more comprehensive. Accordingly, lectures are shorter than the (often quite lengthy) written content.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/&#34;&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt; (&lt;i class=&#34;fas fa-laptop-code&#34;&gt;&lt;/i&gt;): This page the material that we will discuss in Thursday classes. In addition to teaching specific content, there are many more &lt;code&gt;R&lt;/code&gt; code examples. These are intended as a useful reference to various functions that you will need when working on (nearly) weekly labs and your group project.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/&#34;&gt;&lt;strong&gt;Assignments&lt;/strong&gt;&lt;/a&gt; (&lt;i class=&#34;fas fa-pencil-ruler&#34;&gt;&lt;/i&gt;): This page contains the instructions for the weekly lab (1–3 brief tasks) and for the two mini projects + final project. &lt;strong&gt;Labs are due by 11:59 PM (Eastern).&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;office-hours-ta-to-be-determined.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Office Hours (TA): To Be Determined.&lt;/h3&gt;
&lt;p&gt;The teaching assistant for this course (TBD) will host office each week to help promote additional understanding. I highly encourage you to utilize this resource, especially if you struggle with basic &lt;code&gt;R&lt;/code&gt; programming.&lt;/p&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;p&gt;&lt;strong&gt;tl;dr&lt;/strong&gt;: You should follow this general process (in order) each week:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do everything on the content (&lt;i class=&#34;fas fa-book-reader&#34;&gt;&lt;/i&gt;) page before Tuesday&lt;/li&gt;
&lt;li&gt;Come to the lecture on Tuesday.&lt;/li&gt;
&lt;li&gt;While “in class” on Thursday, work through the example (&lt;i class=&#34;fas fa-laptop-code&#34;&gt;&lt;/i&gt;) page&lt;/li&gt;
&lt;li&gt;Complete the lab (&lt;i class=&#34;fas fa-pencil-ruler&#34;&gt;&lt;/i&gt;) and the weekly writing (assigned in class) before the next Tuesday.&lt;/li&gt;
&lt;li&gt;As needed, attend the lab hours hosted by the TA.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;table class=&#34;table schedule&#34; style=&#34;max-width:100%&#34;&gt;
    &lt;tbody&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;&lt;span class=&#34;fake-header-table&#34;&gt;Programming Foundations&lt;/span&gt;&lt;/td&gt;

            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Content&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Example&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Assignment&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 0 (30 August)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;(Re-) Introduction to R&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/01-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/01-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/01-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 1 (6 September)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Programming Basics, the tidyverse, and Visualization&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/02-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/02-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/02-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 2 (13 September)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Visualization II&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/03-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/03-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/03-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 3 (20 September)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Visualization III&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/04-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/04-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/04-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;&lt;span class=&#34;fake-header-table&#34;&gt;Data Analysis Foundations&lt;/span&gt;&lt;/td&gt;

            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Content&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Example&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Assignment&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 4 (27 September)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Probability and Statistics in R&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/05-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/05-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/05-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;8 October&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;&lt;i class=&#34;fas fa-star&#34;&gt;&lt;/i&gt; &lt;strong&gt;Project 1 Due&lt;/strong&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/project1/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 5 (4 October)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Linear Regression I&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/06-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/06-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/06-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 6 (11 October)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Linear Regression II&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/07-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/07-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/07-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 7 (18 October)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Linear Regression III&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/08-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/08-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/08-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;&lt;span class=&#34;fake-header-table&#34;&gt;Applications of Data Analysis&lt;/span&gt;&lt;/td&gt;

            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Content&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Example&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Assignment&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 8 (25 October)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Bias vs Variance&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/09-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/09-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/08-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 9 (1 November)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Nonlinear Regression&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/10-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/10-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/10-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;7 November&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;&lt;i class=&#34;fas fa-star&#34;&gt;&lt;/i&gt; &lt;strong&gt;Project 2 Due&lt;/strong&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 10 (8 November)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Classification&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/11-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/11-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/11-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 11 (15 November)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Wrangling Data&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a
                    href=&#34;https://ssc442.netlify.app/content/12-content/&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/12-example/&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/12-assignment/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;&lt;span class=&#34;fake-header-table&#34;&gt;Further Extensions&lt;/span&gt;&lt;/td&gt;

            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Content&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Example&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Assignment&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 12 (22 November)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Geospatial Data in R&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 12 (29 November)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Text as Data&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;Week 14 (6 December)&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;Advanced Topics and Analyses&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;&lt;span class=&#34;fake-header-table&#34;&gt;Conclusions&lt;/span&gt;&lt;/td&gt;

            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Content&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Example&lt;/td&gt;
            &lt;td style=&#34;width:8%;text-align:center&#34; class=&#34;mid-table-header&#34;&gt;Assignment&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td align=&#34;right&#34; style=&#34;width:18%;text-align:right&#34;&gt;14 December, 11:59 PM Eastern&lt;/td&gt;
            &lt;td style=&#34;width:50%;text-align:left&#34;&gt;&lt;i class=&#34;fas fa-star&#34;&gt;&lt;/i&gt; &lt;strong&gt;Final Project Due&lt;/strong&gt;&lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-book-reader fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;
                &lt;font color=&#34;f1f1f1&#34;&gt;
                    &lt;i class=&#34;fas fa-laptop-code fa-lg&#34;&gt;&lt;/i&gt;&lt;/font&gt;
            &lt;/td&gt;
            &lt;td align=&#34;center&#34; style=&#34;width:10%;text-align:center&#34;&gt;&lt;a href=&#34;https://ssc442.netlify.app/assignment/final-project/&#34;&gt;
                    &lt;i class=&#34;fas fa-pencil-ruler fa-lg&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/td&gt;
        &lt;/tr&gt;

    &lt;/tbody&gt;

&lt;/table&gt;
&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
