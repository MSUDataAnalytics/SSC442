<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Assignments and Evaluations | Data Analytics</title>
    <link>https://ssc442.netlify.app/assignment/</link>
      <atom:link href="https://ssc442.netlify.app/assignment/index.xml" rel="self" type="application/rss+xml" />
    <description>Assignments and Evaluations</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <image>
      <url>https://ssc442.netlify.app/img/social-image.png</url>
      <title>Assignments and Evaluations</title>
      <link>https://ssc442.netlify.app/assignment/</link>
    </image>
    
    <item>
      <title>Advanced Model Building</title>
      <link>https://ssc442.netlify.app/assignment/08-assignment/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/08-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#backstory-and-set-up&#34;&gt;Backstory and Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-cleaning&#34;&gt;Data Cleaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-models&#34;&gt;Linear Models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#assesing-model-accuracy&#34;&gt;Assesing Model Accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-complexity&#34;&gt;Model Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#test-train-split&#34;&gt;Test-Train Split&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-flexibility-to-linear-models&#34;&gt;Adding Flexibility to Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;READ THIS CAREFULLY&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The content below describes both Lab 7 and Lab 8. Lab 7 is Exercise 1; Lab 8 is Exercise 2. Also, you may find some other tasks in the text…&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, October 24th for Lab 8; turn in Lab 9 by 11:59 PM Eastern Time on Sunday, October 31st (a very spooky due date!)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backstory-and-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Backstory and Set Up&lt;/h2&gt;
&lt;p&gt;You still work for Zillow as a junior analyst (sorry). But you’re hunting a promotion. Your job is to present some more advanced predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.&lt;/p&gt;
&lt;p&gt;As always, let’s load the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Ames &amp;lt;- read.table(&amp;quot;https://ssc442.netlify.app/projects/data/ames.csv&amp;quot;,
                 header = TRUE,
                 sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-cleaning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Cleaning&lt;/h2&gt;
&lt;p&gt;Oh, the Ames data yet again. It’s given us lots of trouble. Many of you have found a few variables (columns) that should be avoided. The main problem is that some columns have only one value in them, or they have only &lt;code&gt;NA&lt;/code&gt; and one value, so once &lt;code&gt;lm(...)&lt;/code&gt; drops the &lt;code&gt;NA&lt;/code&gt; rows, they are left with only one value. Linear regression by OLS does not like variables that don’t vary! So, let’s be systematic about figuring out which columns in our data are to be avoided.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;skimr&lt;/code&gt; package is very helpful for seeing what our data contains. Install it, and then use &lt;code&gt;skim(Ames)&lt;/code&gt; directly in the console (we’re just looking at data at the moment). Take a look at the “complete rate” column - this tells us the fraction of observations in that column that are &lt;code&gt;NA&lt;/code&gt;. If it’s very small (see &lt;code&gt;Alley&lt;/code&gt;), then that variable will be problematic. The “n_unique” column tells us if there are few or many different values - a “1” in “n_unique” is definitely going to be a problem!&lt;/p&gt;
&lt;p&gt;You can make a note of those columns that have extremely low “complete rates” and drop them to start off. There are about 5-6 of them that will cause an error if we include them in a regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Models&lt;/h2&gt;
&lt;p&gt;When exploring linear models in other classes, we often emphasize asymptotic results under distributional assumptions. That is, we make assumptions about the model in order to derive properties of large samples. This general approach is useful for creating and performing hypothesis tests. Frequently, when developing a linear regression model, part of the goal is to &lt;strong&gt;explain&lt;/strong&gt; a relationship. However, this isn’t always the case. And it’s often not a valid approach, as we discussed in this week’s content.&lt;/p&gt;
&lt;p&gt;So, we will ignore much of what we have learned in other classes (sorry, EC420) and instead simply use regression as a tool to &lt;strong&gt;predict&lt;/strong&gt;. Instead of a model which supposedly explains relationships, we seek a model which minimizes &lt;strong&gt;errors&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To discuss linear models in the context of prediction, we return to the &lt;code&gt;Ames&lt;/code&gt; data. Accordingly, you should utilize some of the early code from Lab 2 to hasten your progress in this lab.&lt;/p&gt;
&lt;div id=&#34;assesing-model-accuracy&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Assesing Model Accuracy&lt;/h3&gt;
&lt;p&gt;There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}(\hat{f}, \text{Data}) = \sqrt{\frac{1}{n}\displaystyle\sum_{i = 1}^{n}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For a linear model , the estimate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;, is given by the fitted regression line.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{y}({\bf{x}_i}) = \hat{f}({\bf{x}_i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can write an &lt;code&gt;R&lt;/code&gt; function that will be useful for performing this calculation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-complexity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Complexity&lt;/h3&gt;
&lt;p&gt;Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We write a simple &lt;code&gt;R&lt;/code&gt; function to extract this information from a model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_complexity = function(model) {
  length(coef(model)) - 1
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When deciding how complex of a model to use, we can utilize two techniques: &lt;em&gt;forward selection&lt;/em&gt; or &lt;em&gt;backward selection&lt;/em&gt;. Forward selection means that we start with the simplest model (with a single predictor) and then add one at a time until we decide to stop. Backward selection means that we start with the most complex model (with every available predictor) and then remove one at a time until we decide to stop. There are many criteria for “when to stop”. Below, we’ll try to give you some intuition on the model-building process.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 1&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Load the &lt;code&gt;Ames&lt;/code&gt; data. Using &lt;code&gt;skimr::skim&lt;/code&gt;, find the variables that have a complete rate of below 60% and drop them. 60% isn’t a magic number by any means, the “right” number is entirely dependent on your data. It is always standard practice to document the fields you have dropped from the data, so make sure you state which variables have been dropped.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take a look at &lt;code&gt;Utilities&lt;/code&gt;. Use the &lt;code&gt;table&lt;/code&gt; function to see a tabulation of the values of &lt;code&gt;Utilities&lt;/code&gt;. Do you see why this field is not likely to be useful to us, or even problematic?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using &lt;strong&gt;forward selection&lt;/strong&gt; (that is, select one variable, then select another) create a series of models up to complexity length 15. You may use any variable within the dataset, including categorical variables.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a chart plotting the model complexity as the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;-axis variable and RMSE as the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis variable. Describe any patterns you see. Do you think you should use the full-size model? Why or why not? What criterion are you using to make this statement?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;test-train-split&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Test-Train Split&lt;/h3&gt;
&lt;p&gt;There is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts: it is essentially cheating. As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down—or, in very specific cases where a new predictor is completely uncorrelated with the target, stay the same. This might seem to suggest that in order to predict well, we should use the largest possible model. However, in reality we have fit to a specific dataset, but as soon as we see new data, a large model may (in fact) predict poorly. This is called &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The most common approach to overfitting is to take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the &lt;strong&gt;training&lt;/strong&gt; data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the &lt;strong&gt;test&lt;/strong&gt; data. Test data should &lt;em&gt;never&lt;/em&gt; be used to train a model—its pupose is to evaluate the fitted model once you’ve settled on something.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here we use the &lt;code&gt;sample()&lt;/code&gt; function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the &lt;code&gt;set.seed()&lt;/code&gt; function to allow use to reproduce the same random split each time we perform this analysis. Sometimes we don’t want to do this; if we want to run lots of independent splits, then we do not need to set the initial seed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(9)
num_obs = nrow(Ames)

train_index = sample(num_obs, size = trunc(0.50 * num_obs))
train_data = Ames[train_index, ]
test_data = Ames[-train_index, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will look at two measures that assess how well a model is predicting: &lt;strong&gt;train RMSE&lt;/strong&gt; and &lt;strong&gt;test RMSE&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_\text{Train} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Tr}}}\sum_{i \in \text{Train}}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Tr}\)&lt;/span&gt; is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_{\text{Test}} = \text{RMSE}(\hat{f}, \text{Test Data}) = \sqrt{\frac{1}{n_{\text{Te}}}\sum_{i \in \text{Test}} \left ( y_i - \hat{f}(\bf{x}_i) \right ) ^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Te}\)&lt;/span&gt; is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict &lt;strong&gt;in general&lt;/strong&gt;, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.&lt;/p&gt;
&lt;p&gt;We will start with the simplest possible linear model, that is, a model with no predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(SalePrice ~ 1, data = train_data)
get_complexity(fit_0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
sqrt(mean((train_data$SalePrice - predict(fit_0, train_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80875.98&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
sqrt(mean((test_data$SalePrice - predict(fit_0, test_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77928.62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
rmse(actual = train_data$SalePrice, predicted = predict(fit_0, train_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80875.98&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
rmse(actual = test_data$SalePrice, predicted = predict(fit_0, test_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77928.62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = train_data, response = &amp;quot;SalePrice&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 80875.98&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = test_data, response = &amp;quot;SalePrice&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 77928.62&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Apply this basic function with different arguments. Do you understand how we’ve nested functions within functions?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Define a total of five models using the first five models you fit in Exercise 1. Define these as &lt;code&gt;fit_1&lt;/code&gt; through &lt;code&gt;fit_5&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-flexibility-to-linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding Flexibility to Linear Models&lt;/h3&gt;
&lt;p&gt;Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting. To better understand the relationship between train RMSE, test RMSE, and model complexity, we’ll explore the results from Exercise 1.&lt;/p&gt;
&lt;p&gt;Hopefully, you tried the in-line excercise above. If so, we can create a list of the models fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then obtain train RMSE, test RMSE, and model complexity for each. In doing so, we’ll introduce a handy function from &lt;code&gt;R&lt;/code&gt; called &lt;code&gt;sapply()&lt;/code&gt;. You can likely intuit what it does by looking at the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_rmse = sapply(model_list, get_rmse, data = train_data, response = &amp;quot;SalePrice&amp;quot;)
test_rmse = sapply(model_list, get_rmse, data = test_data, response = &amp;quot;SalePrice&amp;quot;)
model_complexity = sapply(model_list, get_complexity)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Run &lt;code&gt;?sapply()&lt;/code&gt; to understand what are valid arguments to the function.&lt;/p&gt;
&lt;p&gt;Once you’ve done this, you’ll notice the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is the same as the apply command above

test_rmse = c(get_rmse(fit_1, test_data, &amp;quot;SalePrice&amp;quot;),
              get_rmse(fit_2, test_data, &amp;quot;SalePrice&amp;quot;),
              get_rmse(fit_3, test_data, &amp;quot;SalePrice&amp;quot;),
              get_rmse(fit_4, test_data, &amp;quot;SalePrice&amp;quot;),
              get_rmse(fit_5, test_data, &amp;quot;SalePrice&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot the results. If you execute the code below, you’ll see the train RMSE in blue, while the test RMSE is given in orange.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model_complexity, train_rmse, type = &amp;quot;b&amp;quot;,
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02,
              max(c(train_rmse, test_rmse)) + 0.02),
     col = &amp;quot;dodgerblue&amp;quot;,
     xlab = &amp;quot;Model Size&amp;quot;,
     ylab = &amp;quot;RMSE&amp;quot;)
lines(model_complexity, test_rmse, type = &amp;quot;b&amp;quot;, col = &amp;quot;darkorange&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also summarize the results as a table. &lt;code&gt;fit_1&lt;/code&gt; is the least flexible, and &lt;code&gt;fit_5&lt;/code&gt; is the most flexible. If we were to do this (see the exercise below) we would see that Train RMSE decreases as flexibility increases forever. However, this may not be the case for the Test RMSE.&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;12%&#34; /&gt;
&lt;col width=&#34;26%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Train RMSE&lt;/th&gt;
&lt;th&gt;Test RMSE&lt;/th&gt;
&lt;th&gt;Predictors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;RMSE&lt;span class=&#34;math inline&#34;&gt;\(_{\text{train}}\)&lt;/span&gt; for model 1&lt;/td&gt;
&lt;td&gt;RMSE&lt;span class=&#34;math inline&#34;&gt;\(_{\text{test}}\)&lt;/span&gt; for model 1&lt;/td&gt;
&lt;td&gt;put predictors here&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;…&lt;/td&gt;
&lt;td&gt;…&lt;/td&gt;
&lt;td&gt;….&lt;/td&gt;
&lt;td&gt;…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;RMSE&lt;span class=&#34;math inline&#34;&gt;\(_{\text{train}}\)&lt;/span&gt; for model 5&lt;/td&gt;
&lt;td&gt;RMSE&lt;span class=&#34;math inline&#34;&gt;\(_{\text{train}}\)&lt;/span&gt; for model 5&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; predictors&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To summarize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Underfitting models:&lt;/strong&gt; In general &lt;em&gt;High&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overfitting models:&lt;/strong&gt; In general &lt;em&gt;Low&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; Then a model is underfitting if there exists a more complex model with lower Test RMSE.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(AKA Lab 8)&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Make a table exactly like the table above for the 15 models you fit in Exercise 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;This question should be the most time-consuming question.&lt;/strong&gt; Using any method you choose and any number of regressors, predict &lt;code&gt;SalePrice&lt;/code&gt;. Calculate the Train and Test RMSE. Your goal is to have a lower Test RMSE than others in the class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In a short paragraph, describe the resulting model. Discuss how you arrived at this model, what interactions you’re using (if any) and how confident you are that your prediction will perform well, relative to other people in the class.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Difficult; extra credit:&lt;/strong&gt; Visualize your final model in a sensible way and provide a two-paragraph interpretation.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;A final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that &lt;strong&gt;predicted&lt;/strong&gt; well, and paid no attention to a model for &lt;strong&gt;explaination&lt;/strong&gt;. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Note that sometimes the terms &lt;em&gt;evaluation set&lt;/em&gt; and &lt;em&gt;test set&lt;/em&gt; are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. We often see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later. Also, we might intuitively expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get (un)lucky and this will not be true.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The labels of under and overfitting are &lt;em&gt;relative&lt;/em&gt; to the best model we see. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Model Building</title>
      <link>https://ssc442.netlify.app/assignment/07-assignment/</link>
      <pubDate>Tue, 12 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/07-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#backstory-and-set-up&#34;&gt;Backstory and Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-a-model&#34;&gt;Building a Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, October 17th.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This week’s lab will extend last week’s lab. The introduction is a verbatim repeat of that lab.&lt;/p&gt;
&lt;div id=&#34;backstory-and-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Backstory and Set Up&lt;/h2&gt;
&lt;p&gt;You have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.&lt;/p&gt;
&lt;p&gt;First, let’s load the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ameslist &amp;lt;- read.table(&amp;quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv&amp;quot;,
                 header = TRUE,
                 sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building a Model&lt;/h2&gt;
&lt;p&gt;We’re now ready to start playing with a model. We will start by using the &lt;code&gt;lm()&lt;/code&gt; function to fit a simple linear regression
model, with &lt;code&gt;SalePrice&lt;/code&gt; as the response and lstat as the predictor.&lt;/p&gt;
&lt;p&gt;Recall that the basic &lt;code&gt;lm()&lt;/code&gt; syntax is &lt;code&gt;lm(y∼x,data)&lt;/code&gt;, where &lt;code&gt;y&lt;/code&gt; is the &lt;strong&gt;response&lt;/strong&gt;, &lt;code&gt;x&lt;/code&gt; is the &lt;strong&gt;predictor&lt;/strong&gt;, and &lt;code&gt;data&lt;/code&gt; is the data set in which these two variables are kept. Let’s quickly run this with two variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit = lm(SalePrice ~ GrLivArea)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This yields:
&lt;code&gt;Error in eval(expr, envir, enclos) : Object &#34;SalePrice&#34; not found&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This command causes an error because &lt;code&gt;R&lt;/code&gt; does not know where to find the variables. We can fix this by attaching the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attach(Ames)
lm.fit = lm(SalePrice ~ GrLivArea)
# Alternatively...
lm.fit = lm(SalePrice ~ GrLivArea, data=Ames)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next line tells &lt;code&gt;R&lt;/code&gt; that the variables are in the object known as &lt;code&gt;Ames&lt;/code&gt;. If you haven’t created this object yet (as in the previous lab) you’ll get an error at this stage. But once we attach &lt;code&gt;Ames&lt;/code&gt;, the first line works fine because &lt;code&gt;R&lt;/code&gt; now recognizes the variables. Alternatively, we could specify this within the &lt;code&gt;lm()&lt;/code&gt; call using &lt;code&gt;data = Ames&lt;/code&gt;. We’ve presented this way because it may be new to you; choose whichever you find most reasonable.&lt;/p&gt;
&lt;p&gt;If we type &lt;code&gt;lm.fit&lt;/code&gt;, some basic information about the model is output. For more detailed information, we use &lt;code&gt;summary(lm.fit)&lt;/code&gt;. This gives us p-values and standard errors for the coefficients, as well as the &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; statistic and &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-statistic for the entire model.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Utilizing these functions hels us see some interesting results. Note that we built (nearly) the simplest possible model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{SalePrice} = \beta_0 + \beta_1*(\text{GrLivArea}) + \epsilon.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But even on its own, this model is instructive. It suggest that an increase in overall living area of 1 ft &lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt; is correlated with an expected increase in sales price of $107. (Note that we &lt;strong&gt;cannot&lt;/strong&gt; make causal claims!)&lt;/p&gt;
&lt;p&gt;Saving the model as we did above is useful because we can explore other pieces of information it stores. Specifically, we can use the &lt;code&gt;names()&lt;/code&gt; function in order to find out what else is stored in &lt;code&gt;lm.fit&lt;/code&gt;. Although we can extract these quan- tities by name—e.g. &lt;code&gt;lm.fit$coefficients&lt;/code&gt;—it is safer to use the extractor functions like &lt;code&gt;coef()&lt;/code&gt; to access them. We can also use a handy tool like &lt;code&gt;plot()&lt;/code&gt; applied directly to &lt;code&gt;lm.fit&lt;/code&gt; to see some interesting data that is automatically stored by the model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Use &lt;code&gt;plot()&lt;/code&gt; to explore the model above. Do you suspect that some outliers have a large influence on the data? We will explore this point specifically in the future.&lt;/p&gt;
&lt;p&gt;We can now go crazy adding variables to our model. It’s as simple as appending them to the previous code—though you should be careful executing this, as it will overwrite your previous output:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.fit = lm(SalePrice ~ GrLivArea + LotArea)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Does controlling for &lt;code&gt;LotArea&lt;/code&gt; change the &lt;em&gt;qualitative&lt;/em&gt; conclusions from the previous regression? What about the &lt;em&gt;quantitative&lt;/em&gt; results? Does the direction of the change in the quantitative results make sense to you?&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;lm()&lt;/code&gt; function in a &lt;strong&gt;simple&lt;/strong&gt; linear regression (e.g., with only one predictor) with &lt;code&gt;SalePrice&lt;/code&gt; as the response to determine the value of a garage.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;lm()&lt;/code&gt; function to perform a multiple linear regression with &lt;code&gt;SalePrice&lt;/code&gt; as the response and all other variables from your &lt;code&gt;Ames&lt;/code&gt; data as the predictors. Use the &lt;code&gt;summary()&lt;/code&gt; function to print the results. Comment on the output. For instance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is there a relationship between the predictors and the response?&lt;/li&gt;
&lt;li&gt;Which predictors appear to have a statistically significant relationship to the response? (Hint: look for stars)&lt;/li&gt;
&lt;li&gt;What does the coefficient for the year variable suggest?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;:&lt;/code&gt; symbols to fit a linear regression model with &lt;em&gt;one&lt;/em&gt; well-chosen interaction effects. Why did you do this?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try a few (e.g., two) different transformations of the variables, such as &lt;span class=&#34;math inline&#34;&gt;\(ln(x)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\sqrt x\)&lt;/span&gt;. Do any of these make sense to include in a model of &lt;code&gt;SalePrice&lt;/code&gt;? Comment on your findings.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(Bonus; very very challenging)&lt;/strong&gt; How might we build a model to estimate the elasticity of demand from this dataset?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;When we use the simple regression model with a single input, the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-stat includes the intercept term. Otherwise, it does not. See Lecture 5 for more detail.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Correlations and Simple Models</title>
      <link>https://ssc442.netlify.app/assignment/06-assignment/</link>
      <pubDate>Tue, 05 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/06-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#backstory-and-set-up&#34;&gt;Backstory and Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-exploration-and-processing&#34;&gt;Data Exploration and Processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, October 10th.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backstory-and-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Backstory and Set Up&lt;/h2&gt;
&lt;p&gt;You have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.&lt;/p&gt;
&lt;p&gt;First, let’s load the data. The code below ought to work, but if it doesn’t you can access the data at the link below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ameslist &amp;lt;- read.table(&amp;quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv&amp;quot;,
                 header = TRUE,
                 sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we proceed, let’s note a few things about the (simple) code above. First, we have specified &lt;code&gt;header = TRUE&lt;/code&gt; because—you guessed it—the original dataset has headers. Although simple, this is an incredibly important step because it allows &lt;code&gt;R&lt;/code&gt; to do some smart &lt;code&gt;R&lt;/code&gt; things. Specifically, once the headers are in, the variables are formatted as &lt;code&gt;int&lt;/code&gt; and &lt;code&gt;factor&lt;/code&gt; where appropriate. It is absolutely vital that we format the data correctly; otherwise, many &lt;code&gt;R&lt;/code&gt; commands will whine at us.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Run the above, but instead specifying &lt;code&gt;header = FALSE&lt;/code&gt;. What data type are the various columns? Now try ommitting the line altogether. What is the default behavior of the &lt;code&gt;read.table&lt;/code&gt; function?&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;data-exploration-and-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Exploration and Processing&lt;/h3&gt;
&lt;p&gt;We are not going to tell you anything about this data. This is intended to replicate a real-world experience that you will all encounter in the (possibly near) future: someone hands you data and you’re expected to make sense of it. Fortunately for us, this data is (somewhat) self-contained. We’ll first check the variable names to try to divine some information. Recall, we have a handy little function for that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(ameslist)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, when doing data exploration, we will sometimes choose to not save our output. This is a judgement call; here we’ve chosen to merely inspect the variables rather than diving in.&lt;/p&gt;
&lt;p&gt;Inspection yields some obvious truths. For example:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Explanation&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;ID&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Unique identifier for each row&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;LotArea&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Size of lot (&lt;strong&gt;units unknown&lt;/strong&gt;)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;SalePrice&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Sale price of house ($)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;…but we face some not-so-obvious things as well. For example:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Explanation&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Type&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;LotShape&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;? Something about the lot&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;factor&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;MSSubClass&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;? No clue at all&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;int&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;Condition1&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;? Seems like street info&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;&lt;code&gt;factor&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It will be difficult to learn anything about the data that is of type &lt;code&gt;int&lt;/code&gt; without outside documentation. However, we can learn something more about the &lt;code&gt;factor&lt;/code&gt;-type variables. In order to understand these a little better, we need to review some of the values that each take on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Go through the variables in the dataset and make a note about your interpretation for each. Many will be obvious, but some require additional thought.&lt;/p&gt;
&lt;p&gt;We now turn to another central issue—and one that explains our nomenclature choice thus far: the data object is of type &lt;code&gt;list&lt;/code&gt;. To verify this for yourself, check:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;typeof(ameslist)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This isn’t ideal—for some visualization packages, for instance, we need data frames and not lists. We’ll make a mental note of this as something to potentially clean up if we desire.&lt;/p&gt;
&lt;p&gt;Although there are some variables that would be difficult to clean, there are a few that we can address with relative ease. Consider, for instance, the variable &lt;code&gt;GarageType&lt;/code&gt;. This might not be that important, but, remember, the weather in Ames, IA is pretty crummy—a detached garage might be a dealbreaker for some would-be homebuyers. Let’s inspect the values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; unique(ameslist$GarageType)
[1] Attchd  Detchd  BuiltIn CarPort &amp;lt;NA&amp;gt; Basment 2Types&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this, we could make an informed decision and create a new variable. Let’s create &lt;code&gt;OutdoorGarage&lt;/code&gt; to indicate, say, homes that have any type of garage that requires the homeowner to walk outdoors after parking their car. (For those who aren’t familiar with different garage types, a car port is not insulated and is therefore considered outdoors. A detached garage presumably requires that the person walks outside after parking. The three other types are inside the main structure, and &lt;code&gt;2Types&lt;/code&gt; we can assume includes at least one attached garage of some sort). This is going to require a bit more coding and we will have to think through each step carefully.&lt;/p&gt;
&lt;p&gt;First, let’s create a new object that has indicator variables (that is, a variable whose values are either zero or one) for each of the &lt;code&gt;GarageType&lt;/code&gt; values. As with everything in &lt;code&gt;R&lt;/code&gt;, there’s a handy function to do this for us:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GarageTemp = model.matrix( ~ GarageType - 1, data=ameslist )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have two separate objects living in our computer’s memory: &lt;code&gt;ameslist&lt;/code&gt; and &lt;code&gt;GarageTemp&lt;/code&gt;—so named to indicate that it is a temporary object.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; We now need to stitch it back onto our original data; we’ll use a simple concatenation and write over our old list with the new one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ameslist &amp;lt;- cbind(ameslist, GarageTemp)
&amp;gt; Error in data.frame(..., check.names = FALSE) :
  arguments imply differing number of rows: 1460, 1379&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Huh. What’s going on?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Figure out what’s going on above. Fix this code so that you have a working version.&lt;/p&gt;
&lt;p&gt;Now that we’ve got that working (ha!) we can generate a new variable for our outdoor garage. We’ll use a somewhat gross version below because it is &lt;em&gt;verbose&lt;/em&gt;; that said, this can be easily accomplished using logical indexing for those who like that approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ameslist$GarageOutside &amp;lt;- ifelse(ameslist$GarageTypeDetchd == 1 | ameslist$GarageTypeCarPort == 1, 1, 0)
unique(ameslist$GarageOutside)
[1]  0  1 NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This seems to have worked. The command above &lt;code&gt;ifelse()&lt;/code&gt; does what it says: &lt;code&gt;if&lt;/code&gt; some condition is met (here, either of two variables equals one) then it returns a one; &lt;code&gt;else&lt;/code&gt; it returns a zero. Such functions are very handy, though as mentioned above, there are other ways of doing this. Also note, that while fixed the issue with &lt;code&gt;NA&lt;/code&gt; above, we’ve got new issues: we definitely don’t want &lt;code&gt;NA&lt;/code&gt; outputted from this operation. Accordingly, we’re going to need to deal with it somehow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Utilizing a similar approach to what you did above, fix this so that the only outputs are zero and one.&lt;/p&gt;
&lt;p&gt;Generally speaking, this is a persistent issue, and you will spend an extraordinary amount of time dealing with missing data or data that does not encode a variable exactly as you want it. This is expecially true if you deal with real-world data: you will need to learn how to handle &lt;code&gt;NA&lt;/code&gt;s. There are a number of fixes (as always, Google is your friend) and anything that works is good. But you should spend some time thinking about this and learning at least one approach.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Prune the data to all of the variables that are &lt;code&gt;type = int&lt;/code&gt; about which you have some reasonable intuition for what they mean. This &lt;strong&gt;must&lt;/strong&gt; include the variable &lt;code&gt;SalePrice&lt;/code&gt;. Save this new dataset as &lt;code&gt;Ames&lt;/code&gt;. Produce documentation for this object in the form of a .txt file. This must describe each of the preserved variables, the values it can take (e.g., can it be negative?) and your interpretation of the variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Produce a &lt;em&gt;scatterplot matrix&lt;/em&gt; which includes 12 of the variables that are &lt;code&gt;type = int&lt;/code&gt; in the data set. Choose those that you believe are likely to be correlated with &lt;code&gt;SalePrice&lt;/code&gt;.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute a matrix of correlations between these variables using the function &lt;code&gt;cor()&lt;/code&gt;. Does this match your prior beliefs? Briefly discuss the correlation between the miscellaneous variables and &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Produce a scatterplot between &lt;code&gt;SalePrice&lt;/code&gt; and &lt;code&gt;GrLivArea&lt;/code&gt;. Run a linear model using &lt;code&gt;lm()&lt;/code&gt; to explore the relationship. Finally, use the &lt;code&gt;abline()&lt;/code&gt; function to plot the relationship that you’ve found in the simple linear regression.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the largest outlier that is above the regression line? Produce the other information about this house.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;(Bonus)&lt;/strong&gt; Create a visualization that shows the rise of air conditioning over time in homes in Ames.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Of course, you could find out the defaults of the function by simply using the handy &lt;code&gt;?&lt;/code&gt; command. Don’t forget about this tool!&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;It’s not exactly true that these objects are in memory. They are… sort of. But how &lt;code&gt;R&lt;/code&gt; handles memory is complicated and silly and blah blah who cares. It’s basically in memory.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;If you are not familiar with this type of visualization, consult the book (&lt;em&gt;Introduction to Statistical Learning&lt;/em&gt;), Chapters 2 and 3. Google it; it’s free.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Statistical Models</title>
      <link>https://ssc442.netlify.app/assignment/05-assignment/</link>
      <pubDate>Tue, 28 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/05-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://ssc442.netlify.app/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#models&#34;&gt;Statistical models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#poll-aggregators&#34;&gt;Poll aggregators&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#poll-data&#34;&gt;Poll data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pollster-bias&#34;&gt;Pollster bias&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-driven-model&#34;&gt;Data-driven models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, October 3rd.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For this exercise you will need to ensure that you’ve carefully read this week’s content and example. We will build on both. The exercises (which you will turn in as this week’s lab) are at the bottom. Note that this week’s lab is much more theoretical than any other week in this class. This is to ensure that you have the foundations necessary to build rich statistical models and apply them to real-world data.&lt;/p&gt;
&lt;div id=&#34;models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Statistical models&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;“All models are wrong, but some are useful.” –George E. P. Box&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. Four years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer. Political commentator Joe Scarborough said during his show&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Anybody that thinks that this race is anything but a toss-up right now is such an ideologue … they’re jokes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To which Nate Silver responded via Twitter:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you think it’s a toss-up, let’s bet. If Obama wins, you donate $1,000 to the American Red Cross. If Romney wins, I do. Deal?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. In contrast, most other forecasters were almost certain she would win. She lost. But 71% is still more than 50%, so was Mr. Silver wrong? And what does probability mean in this context anyway? Are dice being tossed somewhere?&lt;/p&gt;
&lt;p&gt;In this lab we will demonstrate how &lt;em&gt;poll aggregators&lt;/em&gt;, such as FiveThirtyEight, collected and combined data reported by different experts to produce improved predictions. We will introduce ideas behind the &lt;em&gt;statistical models&lt;/em&gt;, also known as &lt;em&gt;probability models&lt;/em&gt;, that were used by poll aggregators to improve election forecasts beyond the power of individual polls. First, we’ll motivate the models, building on the statistical inference concepts we learned in this week’s content and example. We start with relatively simple models, realizing that the actual data science exercise of forecasting elections involves rather complex ones. We will introduce such modeks towards the end of this section of the course.&lt;/p&gt;
&lt;div id=&#34;poll-aggregators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poll aggregators&lt;/h2&gt;
&lt;p&gt;A few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
d &amp;lt;- 0.039
Ns &amp;lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p &amp;lt;- (d + 1) / 2

polls &amp;lt;- map_df(Ns, function(N) {
  x &amp;lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
  x_hat &amp;lt;- mean(x)
  se_hat &amp;lt;- sqrt(x_hat * (1 - x_hat) / N)
  list(estimate = 2 * x_hat - 1,
    low = 2*(x_hat - 1.96*se_hat) - 1,
    high = 2*(x_hat + 1.96*se_hat) - 1,
    sample_size = N)
}) %&amp;gt;% mutate(poll = seq_along(Ns))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/simulated-polls-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.&lt;/p&gt;
&lt;p&gt;Poll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.&lt;/p&gt;
&lt;p&gt;Although as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(polls$sample_size)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11269&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;participants. Basically, we construct an estimate of the spread, let’s call it &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, with a weighted average in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_hat &amp;lt;- polls %&amp;gt;%
  summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %&amp;gt;%
  pull(avg)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have an estimate of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.0184545.&lt;/p&gt;
&lt;p&gt;Thus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/confidence-coverage-2008-election-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits.&lt;/p&gt;
&lt;p&gt;Since the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. The day before the election the &lt;em&gt;New York Times&lt;/em&gt; reported&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; the following probabilities for Hillary Clinton winning the presidency:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
NYT
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
538
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
HuffPost
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
PW
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
PEC
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
DK
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Cook
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Roth
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Win Prob
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
85%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
71%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
98%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
89%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&amp;gt;99%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
92%
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Lean Dem
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Lean Dem
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!--(Source: [New York Times](https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html))--&gt;
&lt;p&gt;For example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled &lt;em&gt;Trump Is Just A Normal Polling Error Behind Clinton&lt;/em&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.
By understanding statistical models and how these forecasters use them, we will start to understand how this happened.&lt;/p&gt;
&lt;p&gt;Although not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. FiveThirtyEight predicted a 3.6% advantage for Clinton&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her an 81.4% chance. Their prediction was summarized with a chart like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/fivethirtyeight-densities-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model.
&lt;!--(Source: [FiveThirtyEight](https://projects.fivethirtyeight.com/2016-election-forecast/))--&gt;&lt;/p&gt;
&lt;p&gt;We introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. To understand the “81.4% chance” statement we need to describe Bayesian statistics, which we do in Sections &lt;a href=&#34;#bayesian-statistics&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;#bayesian-approach&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;poll-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poll data&lt;/h3&gt;
&lt;p&gt;We use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of the &lt;strong&gt;dslabs&lt;/strong&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(polls_us_election_2016)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls &amp;lt;- polls_us_election_2016 %&amp;gt;%
  filter(state == &amp;quot;U.S.&amp;quot; &amp;amp; enddate &amp;gt;= &amp;quot;2016-10-31&amp;quot; &amp;amp;
           (grade %in% c(&amp;quot;A+&amp;quot;,&amp;quot;A&amp;quot;,&amp;quot;A-&amp;quot;,&amp;quot;B+&amp;quot;) | is.na(grade)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We add a spread estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls &amp;lt;- polls %&amp;gt;%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this example, we will assume that there are only two parties and call &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; the proportion voting for Clinton and &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt; the proportion voting for Trump. We are interested in the spread &lt;span class=&#34;math inline&#34;&gt;\(2p-1\)&lt;/span&gt;. Let’s call the spread &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; (for difference).&lt;/p&gt;
&lt;p&gt;We have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the standard error is &lt;span class=&#34;math inline&#34;&gt;\(2\sqrt{p (1 - p) / N}\)&lt;/span&gt;. Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d_hat &amp;lt;- polls %&amp;gt;%
  summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %&amp;gt;%
  pull(d_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the standard error is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_hat &amp;lt;- (d_hat+1)/2
moe &amp;lt;- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize))
moe&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.006623178&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?&lt;/p&gt;
&lt;p&gt;A histogram of the reported spreads shows a problem:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls %&amp;gt;%
  ggplot(aes(spread)) +
  geom_histogram(color=&amp;quot;black&amp;quot;, binwidth = .01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/polls-2016-spread-histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data does not appear to be normally distributed and the standard error appears to be larger than 0.0066232. The theory is not quite working here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pollster-bias&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pollster bias&lt;/h3&gt;
&lt;p&gt;Notice that various pollsters are involved and some are taking several polls a week:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls %&amp;gt;% group_by(pollster) %&amp;gt;% summarize(n())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 × 2
##    pollster                                                   `n()`
##    &amp;lt;fct&amp;gt;                                                      &amp;lt;int&amp;gt;
##  1 ABC News/Washington Post                                       7
##  2 Angus Reid Global                                              1
##  3 CBS News/New York Times                                        2
##  4 Fox News/Anderson Robbins Research/Shaw &amp;amp; Company Research     2
##  5 IBD/TIPP                                                       8
##  6 Insights West                                                  1
##  7 Ipsos                                                          6
##  8 Marist College                                                 1
##  9 Monmouth University                                            1
## 10 Morning Consult                                                1
## 11 NBC News/Wall Street Journal                                   1
## 12 RKM Research and Communications, Inc.                          1
## 13 Selzer &amp;amp; Company                                               1
## 14 The Times-Picayune/Lucid                                       8
## 15 USC Dornsife/LA Times                                          8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s visualize the data for the pollsters that are regularly polling:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/pollster-bias-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls %&amp;gt;% group_by(pollster) %&amp;gt;%
  filter(n() &amp;gt;= 6) %&amp;gt;%
  summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 × 2
##   pollster                     se
##   &amp;lt;fct&amp;gt;                     &amp;lt;dbl&amp;gt;
## 1 ABC News/Washington Post 0.0265
## 2 IBD/TIPP                 0.0333
## 3 Ipsos                    0.0225
## 4 The Times-Picayune/Lucid 0.0196
## 5 USC Dornsife/LA Times    0.0183&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences &lt;em&gt;across the polls&lt;/em&gt;. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We also call them &lt;em&gt;pollster bias&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In the following section, rather than use the urn model theory, we are instead going to develop a data-driven model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-driven-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data-driven models&lt;/h2&gt;
&lt;p&gt;For each pollster, let’s collect their last reported result before the election:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one_poll_per_pollster &amp;lt;- polls %&amp;gt;% group_by(pollster) %&amp;gt;%
  filter(enddate == max(enddate)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a histogram of the data for these 15 pollsters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(spread, data = one_poll_per_pollster, binwidth = 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/05-assignment_files/figure-html/pollster-bias-histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly.&lt;/p&gt;
&lt;p&gt;The new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We &lt;em&gt;assume&lt;/em&gt; that the expected value of our urn is the actual spread &lt;span class=&#34;math inline&#34;&gt;\(d=2p-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Because instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p(1-p)}\)&lt;/span&gt;. Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is used to represent this parameter.&lt;/p&gt;
&lt;p&gt;In summary, we have two unknown parameters: the expected value &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our task is to estimate &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Because we model the observed values &lt;span class=&#34;math inline&#34;&gt;\(X_1,\dots X_N\)&lt;/span&gt; as a random sample from the urn, the CLT might still work in this situation because it is an average of independent random variables. For a large enough sample size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, the probability distribution of the sample average &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is approximately normal with expected value &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and standard error &lt;span class=&#34;math inline&#34;&gt;\(\sigma/\sqrt{N}\)&lt;/span&gt;. If we are willing to consider &lt;span class=&#34;math inline&#34;&gt;\(N=15\)&lt;/span&gt; large enough, we can use this to construct confidence intervals.&lt;/p&gt;
&lt;p&gt;A problem is that we don’t know &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. But theory tells us that we can estimate the urn model &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; with the &lt;em&gt;sample standard deviation&lt;/em&gt; defined as
&lt;span class=&#34;math inline&#34;&gt;\(s = \sqrt{ \sum_{i=1}^N (X_i - \bar{X})^2 / (N-1)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Unlike for the population standard deviation definition, we now divide by &lt;span class=&#34;math inline&#34;&gt;\(N-1\)&lt;/span&gt;. This makes &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; a better estimate of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;sd&lt;/code&gt; function in R computes the sample standard deviation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(one_poll_per_pollster$spread)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02419369&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to form a new confidence interval based on our new data-driven model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- one_poll_per_pollster %&amp;gt;%
  summarize(avg = mean(spread),
            se = sd(spread) / sqrt(length(spread))) %&amp;gt;%
  mutate(start = avg - 1.96 * se,
         end = avg + 1.96 * se)
round(results * 100, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   avg  se start end
## 1 2.9 0.6   1.7 4.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Note that using dollar signs &lt;code&gt;$ $&lt;/code&gt; to enclose some text is how you make the fancy math you see below. If you installed &lt;code&gt;tinytex&lt;/code&gt; or some other Latex distribution in order to render your PDFs, you should be equipped to insert mathematics directly into your .Rmd file. It only works in the text – inside the code chunks, the dollar sign is still the accessor.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;In this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(polls_us_election_2016)
polls &amp;lt;- polls_us_election_2016 %&amp;gt;%
  filter(pollster %in% c(&amp;quot;Rasmussen Reports/Pulse Opinion Research&amp;quot;,
                         &amp;quot;The Times-Picayune/Lucid&amp;quot;) &amp;amp;
           enddate &amp;gt;= &amp;quot;2016-10-15&amp;quot; &amp;amp;
           state == &amp;quot;U.S.&amp;quot;) %&amp;gt;%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to answer the question: is there a poll bias? First, make a plot showing the spreads for each poll.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We will model the observed data &lt;span class=&#34;math inline&#34;&gt;\(Y_{i,j}\)&lt;/span&gt; in the following way:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{i,j} = d + b_i + \varepsilon_{i,j}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(i=1,2\)&lt;/span&gt; indexing the two pollsters, &lt;span class=&#34;math inline&#34;&gt;\(b_i\)&lt;/span&gt; the bias for pollster &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_ij\)&lt;/span&gt; poll to poll chance variability. We assume the &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; are independent from each other, have expected value &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; regardless of &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Which of the following best represents our question?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Is &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i,j}\)&lt;/span&gt; = 0?&lt;/li&gt;
&lt;li&gt;How close are the &lt;span class=&#34;math inline&#34;&gt;\(Y_{i,j}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;?&lt;/li&gt;
&lt;li&gt;Is &lt;span class=&#34;math inline&#34;&gt;\(b_1 \neq b_2\)&lt;/span&gt;?&lt;/li&gt;
&lt;li&gt;Are &lt;span class=&#34;math inline&#34;&gt;\(b_1 = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b_2 = 0\)&lt;/span&gt; ?&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Suppose we define &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_1\)&lt;/span&gt; as the average of poll results from the first poll, &lt;span class=&#34;math inline&#34;&gt;\(Y_{1,1},\dots,Y_{1,N_1}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt; the number of polls conducted by the first pollster:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls %&amp;gt;%
  filter(pollster==&amp;quot;Rasmussen Reports/Pulse Opinion Research&amp;quot;) %&amp;gt;%
  summarize(N_1 = n())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_1\)&lt;/span&gt;?&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What is the standard error of &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_1\)&lt;/span&gt;? (It may be helpful to compute the expected value and standard error of &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2\)&lt;/span&gt; as well.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Suppose we define &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2\)&lt;/span&gt; as the average of poll results from the first poll, &lt;span class=&#34;math inline&#34;&gt;\(Y_{2,1},\dots,Y_{2,N_2}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt; the number of polls conducted by the first pollster. What is the expected value &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2\)&lt;/span&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What does the CLT tell us about the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2 - \bar{Y}_1\)&lt;/span&gt;?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Nothing because this is not the average of a sample.&lt;/li&gt;
&lt;li&gt;Because the &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij}\)&lt;/span&gt; are approximately normal, so are the averages.&lt;/li&gt;
&lt;li&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{Y}_1\)&lt;/span&gt; are sample averages, so if we assume &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt; are large enough, each is approximately normal. The difference of normals is also normal.&lt;/li&gt;
&lt;li&gt;The data are not 0 or 1, so CLT does not apply.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Construct a random variable that has expected value &lt;span class=&#34;math inline&#34;&gt;\(b_2 - b_1\)&lt;/span&gt;, the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_2\)&lt;/span&gt; (the variances of the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; above), but we can plug the sample standard deviations. &lt;strong&gt;Compute those now&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The statistic formed by dividing our estimate of &lt;span class=&#34;math inline&#34;&gt;\(b_2-b_1\)&lt;/span&gt; by its estimated standard error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\bar{Y}_2 - \bar{Y}_1}{\sqrt{s_2^2/N_2 + s_1^2/N_1}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is called the t-statistic. Now you should be able to answer the question: is &lt;span class=&#34;math inline&#34;&gt;\(b_2 - b_1\)&lt;/span&gt; different from 0?&lt;/p&gt;
&lt;p&gt;Notice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can actually construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect?&lt;/p&gt;
&lt;p&gt;For this exercise, create a new table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;polls &amp;lt;- polls_us_election_2016 %&amp;gt;%
  filter(enddate &amp;gt;= &amp;quot;2016-10-15&amp;quot; &amp;amp;
           state == &amp;quot;U.S.&amp;quot;) %&amp;gt;%
  group_by(pollster) %&amp;gt;%
  filter(n() &amp;gt;= 5) %&amp;gt;%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=TbKkjm-gheY&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=TbKkjm-gheY&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html&#34; class=&#34;uri&#34;&gt;https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/&#34; class=&#34;uri&#34;&gt;https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://projects.fivethirtyeight.com/2016-election-forecast/&#34; class=&#34;uri&#34;&gt;https://projects.fivethirtyeight.com/2016-election-forecast/&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Large(ish) Data</title>
      <link>https://ssc442.netlify.app/assignment/04-assignment/</link>
      <pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/04-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started&#34;&gt;Getting started&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bonus-exercise&#34;&gt;Bonus Exercise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#turning-everything-in&#34;&gt;Turning everything in&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#postscript-how-we-got-this-unemployment-data&#34;&gt;Postscript: how we got this unemployment data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, September 26th&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Getting started&lt;/h1&gt;
&lt;p&gt;For this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, &lt;a href=&#34;#postscript-how-we-got-this-unemployment-data&#34;&gt;we describe how we built this dataset down below&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/03-lab/data/unemployment.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;unemployment.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;To help you&lt;/strong&gt;, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/03-lab/03-lab.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; &lt;code&gt;03-lab.Rmd&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the end, to help you master file organization, we suggest that the structure of your project directory should look something like this:&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;your-project-name\
  03-lab.Rmd
  your-project-name.Rproj
  data\
    unemployment.csv&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/example/03-example/&#34;&gt;The example for this week&lt;/a&gt; will be &lt;strong&gt;&lt;em&gt;incredibly&lt;/em&gt;&lt;/strong&gt; helpful for this exercise. Reference it.&lt;/p&gt;
&lt;p&gt;For this week, you need to start making your plots look nice. For full credit, you will have to label axes, label the plot, and experiment with themes. Experiment with adding a &lt;code&gt;labs()&lt;/code&gt; layer or changing colors. Or, if you’re super brave, try modifying a theme and its elements. Default plots will not receive full credit.&lt;/p&gt;
&lt;p&gt;You’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type &lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;alt&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; on Windows, or &lt;kbd&gt;⌘&lt;/kbd&gt; + &lt;kbd&gt;⌥&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; on macOS.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use data from the US Bureau of Labor Statistics (BLS) to show the trends in employment rate for all 50 states between 2006 and 2016. What stories does this plot tell? Which states struggled to recover from the 2008–09 recession?&lt;/p&gt;
&lt;p&gt;Some hints/tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You won’t need to filter out any missing rows because the data here is complete—there are no state-year combinations with missing unemployment data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll be plotting 51 facets. You can filter out DC if you want to have a better grid (like 5 × 10), or you can try using &lt;code&gt;facet_geo()&lt;/code&gt; from the &lt;strong&gt;geofacet&lt;/strong&gt; package to lay out the plots like a map of the US (try this!).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot the &lt;code&gt;date&lt;/code&gt; column along the x-axis, &lt;em&gt;not&lt;/em&gt; the &lt;code&gt;year&lt;/code&gt; column. If you plot by year, you’ll get weird looking lines (try it for fun?), since these observations are monthly. If you really want to plot by year only, you’ll need to create a different data frame where you group by year and state and calculate the average unemployment rate for each year/state combination (i.e. &lt;code&gt;group_by(year, state) %&amp;gt;% summarize(avg_unemployment = mean(unemployment))&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try mapping other aesthetics onto the graph too. You’ll notice there are columns for region and division—play with those as colors, for instance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;This plot might be big, so make sure you adjust &lt;code&gt;fig.width&lt;/code&gt; and &lt;code&gt;fig.height&lt;/code&gt; in the chunk options so that it’s visible when you knit it. You might also want to used &lt;code&gt;ggsave()&lt;/code&gt; to save it with extra large dimensions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;EXERCISE 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Use data from the BLS to create a slopegraph that compares the unemployment rate in January 2006 with the unemployment rate in January 2009, either for all 50 states at once (good luck with that!) or for a specific region or division. Make sure the plot doesn’t look too busy or crowded in the end.&lt;/p&gt;
&lt;p&gt;What story does this plot tell? Which states in the US (or in the specific region you selected) were the most/least affected the Great Recession?&lt;/p&gt;
&lt;p&gt;Some hints/tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You should use &lt;code&gt;filter()&lt;/code&gt; to only select rows where the year is 2006 or 2009 (i.e. &lt;code&gt;filter(year %in% c(2006, 2009)&lt;/code&gt;) and to select rows where the month is January (&lt;code&gt;filter(month == 1)&lt;/code&gt; or &lt;code&gt;filter(month_name == &#34;January&#34;)&lt;/code&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In order for the year to be plotted as separate categories on the x-axis, it needs to be a factor, so use &lt;code&gt;mutate(year = factor(year))&lt;/code&gt; to convert it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To make ggplot draw lines between the 2006 and 2009 categories, you need to include &lt;code&gt;group = state&lt;/code&gt; in the aesthetics.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bonus-exercise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bonus Exercise&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This is entirely optional but might be fun.&lt;/strong&gt; Then again, it might not be fun. I don’t know.&lt;/p&gt;
&lt;p&gt;For extra fun times, if you feel like it, create a bump chart showing something from the unemployment data (perhaps the top 10 states or bottom 10 states in unemployment?) Adapt the code in the &lt;a href=&#34;https://ssc442.netlify.app/example/03-example/&#34;&gt;example for today’s session&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you do this, plotting 51 lines is going to be a huge mess. But filtering the data is also a bad idea, because states could drop in and out of the top/bottom 10 over time, and we don’t want to get rid of them. Instead, you can zoom in on a specific range of data in your plot with &lt;code&gt;coord_cartesian(ylim = c(1, 10))&lt;/code&gt;, for instance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;turning-everything-in&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Turning everything in&lt;/h2&gt;
&lt;p&gt;When you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. Upload the PDF file to D2L.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;postscript-how-we-got-this-unemployment-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Postscript: how we got this unemployment data&lt;/h2&gt;
&lt;p&gt;For the curious, &lt;a href=&#34;https://ssc442.netlify.app/projects/get_bls_data.R&#34;&gt;here’s the code we used&lt;/a&gt; to download the unemployment data from the BLS.&lt;/p&gt;
&lt;p&gt;And to pull the curtain back and show how much googling is involved in data visualization (and data analysis and programming in general), here was my process for getting this data:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;We thought “We want to have students show variation in something domestic over time” and then we googled “us data by state”. Nothing really came up (since it was an exceedingly vague search in the first place), but some results mentioned unemployment rates, so we figured that could be cool.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We googled “unemployment statistics by state over time” and found that the BLS keeps statistics on this. We clicked on the &lt;a href=&#34;https://www.bls.gov/data/&#34;&gt;“Data Tools” link in their main navigation bar&lt;/a&gt;, clicked on “Unemployment”, and then clicked on the “Multi-screen data search” button for the Local Area Unemployment Statistics (LAUS).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We walked through the multiple screens and got excited that we’d be able to download all unemployment stats for all states for a ton of years, &lt;em&gt;but then&lt;/em&gt; the final page had links to 51 individual Excel files, which was dumb.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;So we went back to Google and searched for “download bls data r” and found a few different packages people have written to do this. The first one we clicked on was &lt;a href=&#34;https://github.com/keberwein/blscrapeR&#34;&gt;&lt;code&gt;blscrapeR&lt;/code&gt; at GitHub&lt;/a&gt;, and it looked like it had been updated recently, so we went with it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We followed the examples in the &lt;code&gt;blscrapeR&lt;/code&gt; package and downloaded data for every state.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another day in the life of doing modern data science. This is an example of something you will be able to do by the end of this class. we had no idea people had written &lt;code&gt;R&lt;/code&gt; packages to access BLS data, but there are (at least) 3 packages out there. After a few minutes of tinkering, we got it working and it is relatively straightforward.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applying ggplot2 to Real Data</title>
      <link>https://ssc442.netlify.app/assignment/03-assignment/</link>
      <pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/03-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-markdown&#34;&gt;R Markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-prep&#34;&gt;Data Prep&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-help&#34;&gt;Getting help&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#turning-everything-in&#34;&gt;Turning everything in&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R Markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM Eastern Time on Sunday, September 19th.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;As always, we will first have to load &lt;code&gt;ggplot2&lt;/code&gt;. To do this, we will load the tidyverse by running this code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;The New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided &lt;a href=&#34;https://www1.nyc.gov/assets/buildings/html/essential-active-construction.html&#34;&gt;an interactive map here&lt;/a&gt; where you can see the different projects. There’s also a link there to download the complete dataset.&lt;/p&gt;
&lt;p&gt;For this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).&lt;/p&gt;
&lt;p&gt;As you hopefully figured out by now, you’ll be doing all your &lt;code&gt;R&lt;/code&gt; work in &lt;code&gt;R Markdown&lt;/code&gt;. You can use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud), but this is optional. If you decide to do so, either create a new project for this exercise only, or make a project for all your work in this class.&lt;/p&gt;
&lt;p&gt;You’ll need to download one CSV file and put it somewhere on your computer or upload it to RStudio.cloud—preferably in a folder named &lt;code&gt;data&lt;/code&gt; in your project folder. You can download the data from &lt;a href=&#34;https://www1.nyc.gov/assets/buildings/html/essential-active-construction.html&#34;&gt;the DOB’s map&lt;/a&gt;, or use this link to get it directly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/EssentialConstruction.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;EssentialConstruction.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;r-markdown&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Markdown&lt;/h3&gt;
&lt;p&gt;Writing regular text with &lt;code&gt;R Markdown&lt;/code&gt; follows the rules of Markdown. You can make lists; different-size headers, etc. This should be relatively straightfoward. We talked about a few Markdown features like &lt;strong&gt;bold&lt;/strong&gt; and &lt;em&gt;italics&lt;/em&gt; in class. See &lt;a href=&#34;https://ssc442kirkpatrick.netlify.app/resource/&#34;&gt;this resource for more formatting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You’ll also need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type &lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;alt&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; on Windows, or &lt;kbd&gt;⌘&lt;/kbd&gt; + &lt;kbd&gt;⌥&lt;/kbd&gt; + &lt;kbd&gt;i&lt;/kbd&gt; on macOS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/assignments/insert-chunk-button.png&#34; width=&#34;19%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-prep&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Prep&lt;/h3&gt;
&lt;p&gt;Once you download the &lt;code&gt;EssentialConstruction.csv&lt;/code&gt; file and save it in your project folder, you can open it and start cleaning. I’ll help with that. I’ll give you a .Rmd that will get you started on the Exercises below. Download this and use it in place of your lab assignment template.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/projects/02-lab/02-lab.Rmd&#34;&gt;&lt;i class=&#34;fab fa-r-project&#34;&gt;&lt;/i&gt; &lt;code&gt;02-lab.Rmd&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NOTE: You must change the title to Lab Assignment 02&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise 1 of 1: Essential pandemic construction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using the Lab 02 template above, do the following:&lt;/p&gt;
&lt;p&gt;A. Show the count or proportion of approved projects by borough using a bar chart.&lt;/p&gt;
&lt;p&gt;B. Show the count or proportion of approved projects by category using a lollipop chart. Not sure of what a lollipop chart is? Google &lt;code&gt;R ggplot lollipop&lt;/code&gt;. A huge portion of knowing how to code is knowing how to google, find examples, and figure out where to put your variables from your data!&lt;/p&gt;
&lt;p&gt;C. Show the proportion of approved projects by borough and category &lt;em&gt;simultaneously&lt;/em&gt; using a heatmap.&lt;/p&gt;
&lt;p&gt;You don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a &lt;code&gt;labs()&lt;/code&gt; layer or changing fill colors with &lt;code&gt;scale_fill_manual()&lt;/code&gt; or with palettes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bonus&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Overlay the data from Part 1 above onto a map of NYC. For double bonus, color the boroughs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-help&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting help&lt;/h2&gt;
&lt;p&gt;Use the SSC442 Slack if you get stuck (click the Slack logo at the top right of this website header).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;turning-everything-in&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Turning everything in&lt;/h2&gt;
&lt;p&gt;When you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. Upload the PDF file to D2L.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Programming Basics in R</title>
      <link>https://ssc442.netlify.app/assignment/01-assignment/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/01-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#programming-basics&#34;&gt;Programming basics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#conditionals&#34;&gt;Conditional expressions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#defining-functions&#34;&gt;Defining functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#namespaces&#34;&gt;Namespaces&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#for-loops&#34;&gt;For-loops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectorization&#34;&gt;Vectorization and functionals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM on Sunday, September 12th.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As you read through this assignment, practice with each of the examples (copy-paste them into an empty &lt;code&gt;R&lt;/code&gt; script and run them). At the bottom of this page you will find the questions that comprise the assignment. These questions apply and expand on the topics and &lt;code&gt;R&lt;/code&gt; functions in the assignment. Many assignments will have this same structure: some instruction preceeding specific exercises.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ajkirkpatrick/FS20/Spring2021/Rmarkdown_templates/SSC442_Lab_Assignment_Template.Rmd&#34;&gt;Right-click to download the .Rmd template for labs &lt;i class=&#34;fas fa-file-download&#34;&gt;&lt;/i&gt;&lt;/a&gt;. Please save the template into the labs folder in the SSC442 folder on your local hard drive. If you don’t have a nice file structure setup for the course, please make one now. &lt;em&gt;It will save you from headaches in the future&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;programming-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Programming basics&lt;/h1&gt;
&lt;p&gt;We teach &lt;code&gt;R&lt;/code&gt; because it greatly facilitates data analysis. By coding in &lt;code&gt;R&lt;/code&gt;, we can efficiently perform exploratory data analysis, build data analysis pipelines, and prepare data visualization to communicate results. However, &lt;code&gt;R&lt;/code&gt; is not just a data analysis environment but a programming language. Advanced &lt;code&gt;R&lt;/code&gt; programmers can develop complex packages and even improve &lt;code&gt;R&lt;/code&gt; itself. But we do not cover advanced programming in this course. Nonetheless, in this section, we introduce three key programming concepts: conditional expressions, for-loops, and functions. These are not just key building blocks for advanced programming, but are sometimes useful during data analysis. We also note that there are several functions that are widely used to program in &lt;code&gt;R&lt;/code&gt; but that we will not cover directly in this course. These include &lt;code&gt;split&lt;/code&gt;, &lt;code&gt;cut&lt;/code&gt;, &lt;code&gt;do.call&lt;/code&gt;, and &lt;code&gt;Reduce&lt;/code&gt;, as well as the &lt;strong&gt;data.table&lt;/strong&gt; package. These are worth learning if you plan to become an expert &lt;code&gt;R&lt;/code&gt; programmer.&lt;/p&gt;
&lt;div id=&#34;conditionals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional expressions&lt;/h2&gt;
&lt;p&gt;Conditional expressions are one of the basic features of programming. They are used for what is called &lt;em&gt;flow control&lt;/em&gt;. The most common conditional expression is the if-else statement. In &lt;code&gt;R&lt;/code&gt;, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally, and you will need them once you start writing your own functions and packages.&lt;/p&gt;
&lt;p&gt;Here is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of &lt;code&gt;a&lt;/code&gt; unless &lt;code&gt;a&lt;/code&gt; is 0:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 0

if(a!=0){
  print(1/a)
} else{
  print(&amp;quot;No reciprocal for 0.&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;No reciprocal for 0.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at one more example using the US murders data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
murder_rate &amp;lt;- murders$total / murders$population*100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The &lt;code&gt;if&lt;/code&gt; statement protects us from the case in which no state satisfies the condition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- which.min(murder_rate)

if(murder_rate[ind] &amp;lt; 0.5){
  print(murders$state[ind])
} else{
  print(&amp;quot;No state has murder rate that low&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Vermont&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we try it again with a rate of 0.25, we get a different answer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(murder_rate[ind] &amp;lt; 0.25){
  print(murders$state[ind])
} else{
  print(&amp;quot;No state has a murder rate that low.&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;No state has a murder rate that low.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A related function that is very useful is &lt;code&gt;ifelse&lt;/code&gt;. This function takes three arguments: a logical and two possible answers. If the logical is &lt;code&gt;TRUE&lt;/code&gt;, the value in the second argument is returned and if &lt;code&gt;FALSE&lt;/code&gt;, the value in the third argument is returned. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 0
ifelse(a &amp;gt; 0, 1/a, NA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is &lt;code&gt;TRUE&lt;/code&gt;, or elements from the vector provided in the third argument, if the entry is &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- c(0, 1, 2, -4, 5)
result &amp;lt;- ifelse(a &amp;gt; 0, 1/a, NA)&lt;/code&gt;&lt;/pre&gt;
This table helps us see what happened:
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
a
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
is_a_positive
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
answer1
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
answer2
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
result
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
Inf
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.50
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.25
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.20
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here is an example of how this function can be readily used to replace all the missing values in a vector with zeros:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(na_example)
no_nas &amp;lt;- ifelse(is.na(na_example), 0, na_example)
sum(is.na(no_nas))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two other useful functions are &lt;code&gt;any&lt;/code&gt; and &lt;code&gt;all&lt;/code&gt;. The &lt;code&gt;any&lt;/code&gt; function takes a vector of logicals and returns &lt;code&gt;TRUE&lt;/code&gt; if any of the entries is &lt;code&gt;TRUE&lt;/code&gt;. The &lt;code&gt;all&lt;/code&gt; function takes a vector of logicals and returns &lt;code&gt;TRUE&lt;/code&gt; if all of the entries are &lt;code&gt;TRUE&lt;/code&gt;. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- c(TRUE, TRUE, FALSE)
any(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;defining-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Defining functions&lt;/h2&gt;
&lt;p&gt;As you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector &lt;code&gt;x&lt;/code&gt; using the &lt;code&gt;sum&lt;/code&gt; and &lt;code&gt;length&lt;/code&gt; functions: &lt;code&gt;sum(x)/length(x)&lt;/code&gt;. Because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the &lt;code&gt;mean&lt;/code&gt; function and it is included in base &lt;code&gt;R&lt;/code&gt;. However, you will encounter situations in which the function does not already exist, so &lt;code&gt;R&lt;/code&gt; permits you to write your own. A simple version of a function that computes the average can be defined like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg &amp;lt;- function(x){
  s &amp;lt;- sum(x)
  n &amp;lt;- length(x)
  s/n
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now &lt;code&gt;avg&lt;/code&gt; is a function that computes the mean:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:100
identical(mean(x), avg(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that variables defined inside a function are not saved in the workspace. So while we use &lt;code&gt;s&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt; when we call &lt;code&gt;avg&lt;/code&gt;, the values are created and changed only during the call. Here is an illustrative example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- 3
avg(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how &lt;code&gt;s&lt;/code&gt; is still 3 after we call &lt;code&gt;avg&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In general, functions are objects, so we assign them to variable names with &lt;code&gt;&amp;lt;-&lt;/code&gt;. The function &lt;code&gt;function&lt;/code&gt; tells &lt;code&gt;R&lt;/code&gt; you are about to define a function. The general form of a function definition looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_function &amp;lt;- function(VARIABLE_NAME){
  perform operations on VARIABLE_NAME and calculate VALUE
  VALUE
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg &amp;lt;- function(x, arithmetic = TRUE){
  n &amp;lt;- length(x)
  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will learn more about how to create functions through experience as we face more complex tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;namespaces&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Namespaces&lt;/h2&gt;
&lt;p&gt;Once you start becoming more of an &lt;code&gt;R&lt;/code&gt; expert user, you will likely need to load several add-on packages for some of your analysis. Once you start doing this, it is likely that two packages use the same name for two different functions. And often these functions do completely different things. In fact, you have already encountered this because both &lt;strong&gt;dplyr&lt;/strong&gt; and the R-base &lt;strong&gt;stats&lt;/strong&gt; package define a &lt;code&gt;filter&lt;/code&gt; function. There are five other examples in &lt;strong&gt;dplyr&lt;/strong&gt;. We know this because when we first load &lt;strong&gt;dplyr&lt;/strong&gt; we see the following message:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what does &lt;code&gt;R&lt;/code&gt; do when we type &lt;code&gt;filter&lt;/code&gt;? Does it use the &lt;strong&gt;dplyr&lt;/strong&gt; function or the &lt;strong&gt;stats&lt;/strong&gt; function? From our previous work we know it uses the &lt;strong&gt;dplyr&lt;/strong&gt; one. But what if we want to use the &lt;strong&gt;stats&lt;/strong&gt; version?&lt;/p&gt;
&lt;p&gt;These functions live in different &lt;em&gt;namespaces&lt;/em&gt;. &lt;code&gt;R&lt;/code&gt; will follow a certain order when searching for a function in these &lt;em&gt;namespaces&lt;/em&gt;. You can see the order by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;search()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first entry in this list is the global environment which includes all the objects you define.&lt;/p&gt;
&lt;p&gt;So what if we want to use the &lt;strong&gt;stats&lt;/strong&gt; &lt;code&gt;filter&lt;/code&gt; instead of the &lt;strong&gt;dplyr&lt;/strong&gt; filter but &lt;strong&gt;dplyr&lt;/strong&gt; appears first in the search list? You can force the use of a specific namespace by using double colons (&lt;code&gt;::&lt;/code&gt;) like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats::filter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want to be absolutely sure that we use the &lt;strong&gt;dplyr&lt;/strong&gt; &lt;code&gt;filter&lt;/code&gt;, we can use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dplyr::filter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also note that if we want to use a function in a package without loading the entire package, we can use the double colon as well.&lt;/p&gt;
&lt;p&gt;For more on this more advanced topic we recommend the &lt;code&gt;R&lt;/code&gt; packages book&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;for-loops&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;For-loops&lt;/h2&gt;
&lt;p&gt;If we had to write this section in a single sentence, it would be: Don’t use for-loops. Looping is intuitive, but &lt;code&gt;R&lt;/code&gt; is designed to provide more computationally efficient solutions. For-loops should be considered a quick-and-dirty way to get an answer. But, hey, you live your own life. Below we provide a brief overview to for-looping.&lt;/p&gt;
&lt;p&gt;The formula for the sum of the series &lt;span class=&#34;math inline&#34;&gt;\(1+2+\dots+n\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n(n+1)/2\)&lt;/span&gt;. What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_s_n &amp;lt;- function(n){
  x &amp;lt;- 1:n
  sum(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How can we compute &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; for various values of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, say &lt;span class=&#34;math inline&#34;&gt;\(n=1,\dots,25\)&lt;/span&gt;? Do we write 25 lines of code calling &lt;code&gt;compute_s_n&lt;/code&gt;? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over, and the only thing that is changing is the value of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. For-loops let us define the range that our variable takes (in our example &lt;span class=&#34;math inline&#34;&gt;\(n=1,\dots,10\)&lt;/span&gt;), then change the value and evaluate expression as you &lt;em&gt;loop&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Perhaps the simplest example of a for-loop is this useless piece of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:5){
  print(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1
## [1] 2
## [1] 3
## [1] 4
## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the for-loop we would write for our &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- 25
s_n &amp;lt;- vector(length = m) # create an empty vector
for(n in 1:m){
  s_n[n] &amp;lt;- compute_s_n(n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In each iteration &lt;span class=&#34;math inline&#34;&gt;\(n=1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n=2\)&lt;/span&gt;, etc…, we compute &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; and store it in the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;th entry of &lt;code&gt;s_n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now we can create a plot to search for a pattern:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1:m
plot(n, s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/01-assignment_files/figure-html/sum-of-consecutive-squares-1.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you noticed that it appears to be a quadratic, you are on the right track because the formula is &lt;span class=&#34;math inline&#34;&gt;\(n(n+1)/2\)&lt;/span&gt;.
&lt;!--
which we can confirm with a table:


```r
head(data.frame(s_n = s_n, formula = n*(n+1)/2))
```

```
##   s_n formula
## 1   1       1
## 2   3       3
## 3   6       6
## 4  10      10
## 5  15      15
## 6  21      21
```

We can also overlay the two results by using the function `lines` to draw a line over the previously plotted points:


```r
plot(n, s_n)
lines(n, n*(n+1)/2)
```

&lt;img src=&#34;https://ssc442.netlify.app/assignment/01-assignment_files/figure-html/s_n-v-n-1.png&#34; width=&#34;672&#34; /&gt;

--&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vectorization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vectorization and functionals&lt;/h2&gt;
&lt;p&gt;Although for-loops are an important concept to understand, in &lt;code&gt;R&lt;/code&gt; we rarely use them. As you learn more &lt;code&gt;R&lt;/code&gt;, you will realize that &lt;em&gt;vectorization&lt;/em&gt; is preferred over for-loops since it results in shorter and clearer code. (It’s also vastly more efficient computationally, which can matter as your data grows.) A &lt;em&gt;vectorized&lt;/em&gt; function is a function that will apply the same operation on each of the vectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:10
sqrt(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- 1:10
x*y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]   1   4   9  16  25  36  49  64  81 100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make this calculation, there is no need for for-loops. However, not all functions work this way. For instance, the function we just wrote, &lt;code&gt;compute_s_n&lt;/code&gt;, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of &lt;code&gt;n&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1:25
compute_s_n(n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Functionals&lt;/em&gt; are functions that help us apply the same function to each entry in a vector, matrix, data frame, or list. Here we cover the functional that operates on numeric, logical, and character vectors: &lt;code&gt;sapply&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;sapply&lt;/code&gt; permits us to perform element-wise operations on any function. Here is how it works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:10
sapply(x, sqrt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427
##  [9] 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each element of &lt;code&gt;x&lt;/code&gt; is passed on to the function &lt;code&gt;sqrt&lt;/code&gt; and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original &lt;code&gt;x&lt;/code&gt;. This implies that the for-loop above can be written as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1:25
s_n &amp;lt;- sapply(n, compute_s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Other functionals are &lt;code&gt;apply&lt;/code&gt;, &lt;code&gt;lapply&lt;/code&gt;, &lt;code&gt;tapply&lt;/code&gt;, &lt;code&gt;mapply&lt;/code&gt;, &lt;code&gt;vapply&lt;/code&gt;, and &lt;code&gt;replicate&lt;/code&gt;. We mostly use &lt;code&gt;sapply&lt;/code&gt;, &lt;code&gt;apply&lt;/code&gt;, and &lt;code&gt;replicate&lt;/code&gt; in this book, but we recommend familiarizing yourselves with the others as they can be very useful.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;This is your first weekly lab assignment. Each lab assignment will need to be done in Rmarkdown using &lt;a href=&#34;https://raw.githubusercontent.com/ajkirkpatrick/FS20/Spring2021/Rmarkdown_templates/SSC442_Lab_Assignment_Template.Rmd&#34;&gt;the lab template&lt;/a&gt;, just right-click and Save As…&lt;strong&gt;Start a new folder on your drive for this course, and inside that a new folder for lab assignments, and inside that a new folder for Lab No. 1&lt;/strong&gt;. Rmarkdown will place some intermediate files in that folder, so leaving .Rmd files on your desktop will make things messy, fast.&lt;/p&gt;
&lt;p&gt;Once you’ve saved the file, open it up in Rstudio.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Change the title to “Lab 1”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Put your name on it&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Leave the date alone. That little &lt;code&gt;`r Sys.time(...)`&lt;/code&gt; will ask R to return the date (with M-D-Y formatting), which Rmarkdown will put in as if you had typed in the actual date.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When you type &lt;code&gt;## 1. Text of...&lt;/code&gt;, Markdown will recognize “1. Text of” as a header and will &lt;em&gt;automatically&lt;/em&gt; make it big.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;So please copy the number and text of the question you are answering here.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next will be the &lt;code&gt;```{r q1}&lt;/code&gt; text that will be in gray. &lt;strong&gt;R will recognize this as code and will treat it as such&lt;/strong&gt;. Anything run in that block will have an output.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If you want to see what the code will do, copy the code and paste it into the gray area. Then, click the green right arrow in the top-right corner &lt;em&gt;of the gray code chunk&lt;/em&gt;. It should show you the results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the results (plus your understanding of the code) to answer the question&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With each completed question, clidk the “Knit” button up above the script window. Rmarkdown will create a .pdf for you of your work (as long as it doesn’t hit any R errors). Knit often to make sure you haven’t hit an error!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;\newpage&lt;/code&gt; line is a Latex command (the program that makes the typesetting look nice). It will start a new pdf page.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the next page, copy question #2 to a new header using &lt;code&gt;##&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Once done, render one last .pdf and turn it in on D2L!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;ol start=&#34;0&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In your first code chunk, load the package library &lt;code&gt;tidyverse&lt;/code&gt;, which you will need for Question 8. Always load all your package libraries at the top, in the first code chunk!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What will this conditional expression return and why?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(1,2,-3,4)

if(all(x&amp;gt;0)){
  print(&amp;quot;All Postives&amp;quot;)
} else{
  print(&amp;quot;Not all positives&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which of the following expressions is always &lt;code&gt;FALSE&lt;/code&gt; when at least one entry of a logical vector &lt;code&gt;x&lt;/code&gt; is TRUE?&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;all(x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;any(x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;any(!x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;all(!x)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The function &lt;code&gt;nchar&lt;/code&gt; tells you how many characters long a character vector is. Write a line of code that assigns to the object &lt;code&gt;new_names&lt;/code&gt; the state abbreviation when the state name is longer than 8 characters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a function &lt;code&gt;sum_n&lt;/code&gt; that for any given value, say &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, computes the sum of the integers from 1 to n (inclusive). Use the function to determine the sum of integers from 1 to 5,000.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a function &lt;code&gt;altman_plot&lt;/code&gt; that takes two arguments, &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, and plots the difference against the sum. Use it to make an altman plot of &lt;code&gt;x &amp;lt;- c(5,7,9)&lt;/code&gt; and &lt;code&gt;y &amp;lt;- c(10,11,12)&lt;/code&gt;. When your function creates the plot, it will output automatically in your Rmarkdown knitted .pdf.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;After running the code below, what is the value of &lt;code&gt;x&lt;/code&gt; and why?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 3
my_func &amp;lt;- function(y){
  x &amp;lt;- 5
  y+5
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write a function &lt;code&gt;compute_s_n&lt;/code&gt; that for any given &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; computes the sum &lt;span class=&#34;math inline&#34;&gt;\(S_n = 1^2 + 2^2 + 3^2 + \dots n^2\)&lt;/span&gt;. Report the value of the sum when &lt;span class=&#34;math inline&#34;&gt;\(n=10\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Define an empty numerical vector &lt;code&gt;s_n&lt;/code&gt; of size 25 using &lt;code&gt;s_n &amp;lt;- vector(&#34;numeric&#34;, 25)&lt;/code&gt; and store in the results of &lt;span class=&#34;math inline&#34;&gt;\(S_1, S_2, \dots S_{25}\)&lt;/span&gt; using a for-loop.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat exercise 8, but this time use &lt;code&gt;sapply&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat exercise 8, but this time use &lt;code&gt;map_dbl&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; versus &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. Use points defined by &lt;span class=&#34;math inline&#34;&gt;\(n=1,\dots,25\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Confirm that the formula for this sum is &lt;span class=&#34;math inline&#34;&gt;\(S_n= n(n+1)(2n+1)/6\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://r-pkgs.had.co.nz/namespace.html&#34; class=&#34;uri&#34;&gt;http://r-pkgs.had.co.nz/namespace.html&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Basics of ggplot</title>
      <link>https://ssc442.netlify.app/assignment/02-assignment/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/02-assignment/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#using-ggplot2&#34;&gt;Using ggplot2&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-use-ggplot2-the-too-fast-and-wholly-unclear-recipe&#34;&gt;How to use &lt;code&gt;ggplot2&lt;/code&gt; – the too-fast and wholly unclear recipe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mappings-link-data-to-things-you-see&#34;&gt;Mappings Link Data to Things You See&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-recipe&#34;&gt;The Recipe&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mapping-aesthetics-vs-setting-them&#34;&gt;Mapping Aesthetics vs Setting them&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You must turn in a PDF document of your &lt;code&gt;R markdown&lt;/code&gt; code. Submit this to D2L by 11:59 PM on Tuesday, September 14th.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Our primary tool for data visualization in the course will be &lt;code&gt;ggplot&lt;/code&gt;. Technically, we’re using &lt;code&gt;ggplot2&lt;/code&gt;; the o.g. version lacked some of the modern features of its big brother. &lt;code&gt;ggplot2&lt;/code&gt; implements the grammar of graphics, a coherent and relatively straightforward system for describing and building graphs. With &lt;code&gt;ggplot2&lt;/code&gt;, you can do more faster by learning one system and applying it in many places. Other languages provide more specific tools, but require you to learn a different tool for each application. In this class, we’ll dig into a single package for our visuals.&lt;/p&gt;
&lt;div id=&#34;using-ggplot2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using ggplot2&lt;/h2&gt;
&lt;p&gt;In order to get our hands dirty, we will first have to load &lt;code&gt;ggplot2&lt;/code&gt;. To do this, and to access the datasets, help pages, and functions that we will use in this assignment, we will load the so-called tidyverse by running this code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you run this code and get an error message “there is no package called ‘tidyverse’”, you’ll need to first install it, then run library() once again. To install packages in &lt;code&gt;R&lt;/code&gt;, we utilize the simple function install.packages(). In this case, we would write:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we’re up and running, we’re ready to dive into some basic exercises. &lt;code&gt;ggplot2&lt;/code&gt; works by specifying the connections between the variables in the data and the colors, points, and shapes you see on the screen. These logical connections are called &lt;em&gt;aesthetic mappings&lt;/em&gt; or simply &lt;em&gt;aesthetics&lt;/em&gt;.&lt;/p&gt;
&lt;div id=&#34;how-to-use-ggplot2-the-too-fast-and-wholly-unclear-recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to use &lt;code&gt;ggplot2&lt;/code&gt; – the too-fast and wholly unclear recipe&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;data =&lt;/code&gt;: Define what your data is. For instance, below we’ll use the mpg data frame found in ggplot2 (by using &lt;code&gt;ggplot2::mpg&lt;/code&gt;). As a reminder, a data frame is a rectangular collection of variables (in the columns) and observations (in the rows). This structure of data is often called a “table” but we’ll try to use terms slightly more precisely. The &lt;code&gt;mpg&lt;/code&gt; data frame contains observations collected by the US Environmental Protection Agency on 38 different models of car.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;mapping = aes(...)&lt;/code&gt;: How to map the variables in the data to aesthetics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Axes, size of points, intensities of colors, which colors, shape of points, lines/points&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Then say what type of plot you want:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;boxplot, scatterplot, histogram, …&lt;/li&gt;
&lt;li&gt;these are called ‘geoms’ in ggplot’s grammar, such as &lt;code&gt;geom_point()&lt;/code&gt; giving scatter plots&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;library(ggplot2)
... + geom_point() # Produces scatterplots
... + geom_bar() # Bar plots
.... + geom_boxplot() # boxplots
... #&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You link these steps by &lt;em&gt;literally&lt;/em&gt; adding them together with &lt;code&gt;+&lt;/code&gt; as we’ll see.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; What other types of plots are there? Try to find several more &lt;code&gt;geom_&lt;/code&gt; functions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;mappings-link-data-to-things-you-see&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mappings Link Data to Things You See&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gapminder)
library(ggplot2)
gapminder::gapminder&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,704 × 6
##    country     continent  year lifeExp      pop gdpPercap
##    &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 Afghanistan Asia       1952    28.8  8425333      779.
##  2 Afghanistan Asia       1957    30.3  9240934      821.
##  3 Afghanistan Asia       1962    32.0 10267083      853.
##  4 Afghanistan Asia       1967    34.0 11537966      836.
##  5 Afghanistan Asia       1972    36.1 13079460      740.
##  6 Afghanistan Asia       1977    38.4 14880372      786.
##  7 Afghanistan Asia       1982    39.9 12881816      978.
##  8 Afghanistan Asia       1987    40.8 13867957      852.
##  9 Afghanistan Asia       1992    41.7 16317921      649.
## 10 Afghanistan Asia       1997    41.8 22227415      635.
## # … with 1,694 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/assignment/02-assignment_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above we’ve loaded a different dataset and have started to explore a particular relationship. Before putting in this code yourself, try to intuit what &lt;em&gt;might&lt;/em&gt; be going on.&lt;/p&gt;
&lt;p&gt;Any ideas?&lt;/p&gt;
&lt;p&gt;Here’s a breakdown of everything that happens after the &lt;code&gt;p&amp;lt;- ggplot()&lt;/code&gt; call:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data = gapminder&lt;/code&gt; tells ggplot to use gapminder dataset, so if variable names are mentioned, they should be looked up in gapminder&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mapping = aes(...)&lt;/code&gt; shows that the mapping is a function call. There is a deeper logic to this that I will disucss below, but it’s easiest to simply accept that this is how you write it. Put another way, the &lt;code&gt;mapping = aes(...)&lt;/code&gt; argument &lt;em&gt;links variables&lt;/em&gt; to &lt;em&gt;things you will see&lt;/em&gt; on the plot.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;aes(x = gdpPercap, y = lifeExp)&lt;/code&gt; maps the GDP data onto &lt;code&gt;x&lt;/code&gt;, which is a known aesthetic (the x-coordinate) and life expectancy data onto &lt;code&gt;y&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; are predefined names that are used by &lt;code&gt;ggplot&lt;/code&gt; and friends&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise 1:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let’s return to the &lt;code&gt;mpg&lt;/code&gt; data. Among the variables in &lt;code&gt;mpg&lt;/code&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;displ&lt;/code&gt;, a car’s engine size, in litres.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;hwy&lt;/code&gt;, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generate a scatterplot between these two variables. Does it capture the intuitive relationship you expected? What happens if you make a scatterplot of &lt;code&gt;class&lt;/code&gt; vs &lt;code&gt;drv&lt;/code&gt;? Why is the plot not useful?&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It turns out there’s a reason for doing all of this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The greatest value of a picture is when it forces us to notice what we never expected to see.”” — John Tukey&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the plot you made above, one group of points seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars?&lt;/p&gt;
&lt;p&gt;Let’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The &lt;code&gt;class&lt;/code&gt; variable of the &lt;code&gt;mpg&lt;/code&gt; dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular).&lt;/p&gt;
&lt;p&gt;You can add a third variable, like &lt;code&gt;class&lt;/code&gt;, to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its aesthetic properties. Since we already use the word “&lt;strong&gt;value&lt;/strong&gt;” to describe data, let’s use the word “&lt;strong&gt;level&lt;/strong&gt;” to describe aesthetic properties. Thus, we are interested in exploring &lt;code&gt;class&lt;/code&gt; as a level.&lt;/p&gt;
&lt;p&gt;You can convey information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. To map an aesthetic to a variable, associate the name of the aesthetic to the name of the variable inside &lt;code&gt;aes()&lt;/code&gt;. &lt;code&gt;ggplot2&lt;/code&gt; will automatically assign a unique level of the aesthetic (here a unique color) to each unique value of the variable, a process known as scaling. &lt;code&gt;ggplot2&lt;/code&gt; will also add a legend that explains which levels correspond to which values.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise 2:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Using your previous scatterplot of &lt;code&gt;displ&lt;/code&gt; and &lt;code&gt;hwy&lt;/code&gt;, map the colors of your points to the class variable to reveal the class of each car. What conclusions can we make?&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let’s explore our previously saved &lt;code&gt;p&lt;/code&gt; in greater detail. As with Exercise 1, we’ll add a &lt;em&gt;layer&lt;/em&gt;. This says how some data gets turned into concrete visual aspects.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p + geom_point()
p + geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Both of the above geom’s use the same mapping, where the x-axis represents &lt;code&gt;gdpPercap&lt;/code&gt; and the y-axis represents &lt;code&gt;lifeExp&lt;/code&gt;. You can find this yourself with some ease. But the first one maps the data to individual points, the other one maps it to a smooth line with error ranges.&lt;/p&gt;
&lt;p&gt;We get a message that tells us that &lt;code&gt;geom_smooth()&lt;/code&gt; is using the method = ‘gam’, so presumably we can use other methods. Let’s see if we can figure out which other methods there are.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;?geom_smooth
p + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ...)
p + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ..., color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may start to see why &lt;code&gt;ggplot2&lt;/code&gt;’s way of breaking up tasks is quite powerful: the geometric objects can all reuse the &lt;em&gt;same&lt;/em&gt; mapping of data to aesthetics, yet the results are quite different. And if we want later geoms to use different mappings, then we can override them – but it isn’t necessary.&lt;/p&gt;
&lt;p&gt;Consider the output we’ve explored thus far. One potential issue lurking in the data is that most of it is bunched to the left. If we instead used a logarithmic scale, we should be able to spread the data out better.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p + geom_point() + geom_smooth(method = &amp;quot;lm&amp;quot;) + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Describe what the &lt;code&gt;scale_x_log10()&lt;/code&gt; does. Why is it a more evenly distributed cloud of points now? (2-3 sentences.)&lt;/p&gt;
&lt;p&gt;Nice. We’re starting to get somewhere. But, you might notice that the x-axis now has scientific notation. Let’s change that.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(scales)
p + geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  scale_x_log10(labels = scales::dollar)
p + geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  scale_x_log10(labels = scales::...)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; What does the &lt;code&gt;dollar()&lt;/code&gt; call do? How can you find other ways of relabeling the scales when using &lt;code&gt;scale_x_log10()&lt;/code&gt;?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;?dollar()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-recipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Recipe&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Tell the &lt;code&gt;ggplot()&lt;/code&gt; function what our data is.&lt;/li&gt;
&lt;li&gt;Tell &lt;code&gt;ggplot()&lt;/code&gt; &lt;em&gt;what&lt;/em&gt; relationships we want to see. For convenience we will put the results of the first two steps in an object called &lt;code&gt;p&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Tell &lt;code&gt;ggplot&lt;/code&gt; &lt;em&gt;how&lt;/em&gt; we want to see the relationships in our data.&lt;/li&gt;
&lt;li&gt;Layer on geoms as needed, by adding them on the &lt;code&gt;p&lt;/code&gt; object one at a time.&lt;/li&gt;
&lt;li&gt;Use some additional functions to adjust scales, labels, tickmarks, titles.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;e.g. &lt;code&gt;scale_&lt;/code&gt;, &lt;code&gt;labs()&lt;/code&gt;, and &lt;code&gt;guides()&lt;/code&gt; functions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you start to run more &lt;code&gt;R&lt;/code&gt; code, you’re likely to run into problems. Don’t worry — it happens to everyone. I have been writing code in numerous languages for years, and every day I still write code that doesn’t work. Sadly, &lt;code&gt;R&lt;/code&gt; is particularly persnickity, and its error messages are often opaque.&lt;/p&gt;
&lt;p&gt;Start by carefully comparing the code that you’re running to the code in these notes. &lt;code&gt;R&lt;/code&gt; is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every ” is paired with another “. Sometimes you’ll run the code and nothing happens. Check the left-hand of your console: if it’s a +, it means that R doesn’t think you’ve typed a complete expression and it’s waiting for you to finish it. In this case, it’s usually easy to start from scratch again by pressing ESCAPE to abort processing the current command.&lt;/p&gt;
&lt;p&gt;One common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start.&lt;/p&gt;
&lt;div id=&#34;mapping-aesthetics-vs-setting-them&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mapping Aesthetics vs Setting them&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp, color = &amp;#39;yellow&amp;#39;))
p + geom_point() + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is interesting (or annoying): the points are not yellow. How can we tell ggplot to draw yellow points?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp, ...))
p + geom_point(...) + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; describe in your words what is going on.
One way to avoid such mistakes is to read arguments inside &lt;code&gt;aes(&amp;lt;property&amp;gt; = &amp;lt;variable&amp;gt;)&lt;/code&gt;as &lt;em&gt;the property &lt;property&gt; in the graph is determined by the data in &lt;variable&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Write the above sentence for the original call &lt;code&gt;aes(x = gdpPercap, y = lifeExp, color = &#39;yellow&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Aesthetics convey information about a variable in the dataset, whereas setting the color of all points to yellow conveys no information about the dataset - it changes the appearance of the plot in a way that is independent of the underlying data.&lt;/p&gt;
&lt;p&gt;Remember: &lt;code&gt;color = &#39;yellow&#39;&lt;/code&gt; and &lt;code&gt;aes(color = &#39;yellow&#39;)&lt;/code&gt; are very different, and the second makes usually no sense, as &lt;code&gt;&#39;yellow&#39;&lt;/code&gt; is treated as &lt;em&gt;data&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point() + geom_smooth(color = &amp;quot;orange&amp;quot;, se = FALSE, size = 8, method = &amp;quot;lm&amp;quot;) + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Write down what all those arguments in &lt;code&gt;geom_smooth(...)&lt;/code&gt; do.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p + geom_point(alpha = 0.3) +
  geom_smooth(method = &amp;quot;gam&amp;quot;) +
  scale_x_log10(labels = scales::dollar) +
  labs(x = &amp;quot;GDP Per Capita&amp;quot;, y = &amp;quot;Life Expectancy in Years&amp;quot;,
       title = &amp;quot;Economic Growth and Life Expectancy&amp;quot;,
       subtitle = &amp;quot;Data Points are country-years&amp;quot;,
       caption = &amp;quot;Source: Gapminder&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coloring by continent:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(scales)
p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp, color = continent, fill = continent))
p + geom_point()
p + geom_point() + scale_x_log10(labels = dollar)
p + geom_point() + scale_x_log10(labels = dollar) + geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; What does &lt;code&gt;fill = continent&lt;/code&gt; do? What do you think about the match of colors between lines and error bands?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(mapping = aes(color = continent)) + geom_smooth() + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; Notice how the above code leads to a single smooth line, not one per continent. Why?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try it:&lt;/strong&gt; What is bad about the following example, assuming the graph is the one we want? Think about why you should set aesthetics at the top level rather than at the individual geometry level if that’s your intent.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;p &amp;lt;- ggplot(data = gapminder,
            mapping = aes(x = gdpPercap, y = lifeExp))
p + geom_point(mapping = aes(color = continent)) +
  geom_smooth(mapping = aes(color = continent, fill = continent)) +
  scale_x_log10() +
  geom_smooth(mapping = aes(color = continent), method = &amp;quot;gam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;Exercise 3:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Generate two new plots with &lt;code&gt;data = gapminder&lt;/code&gt; (note: you’ll need to install the package by the same name if you have not already). Label the axes and the header with clear, easy to understand language. In a few sentences, describe what you’ve visualized and why.&lt;/p&gt;
&lt;p&gt;Note that this is your first foray into &lt;code&gt;ggplot2&lt;/code&gt;; accordingly, you should ry to make sure that you do not bite off more than you can chew. We will improve and refine our abilities as we progress through the semester.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Project 2</title>
      <link>https://ssc442.netlify.app/assignment/project2/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/project2/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#part-1-instructions&#34;&gt;Part 1: Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-2-hypotheses&#34;&gt;Part 2: Hypotheses&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-cleaning-code&#34;&gt;Data cleaning code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-to-possibly-use-in-your-plot&#34;&gt;Data to possibly use in your plot&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#country-totals-over-time&#34;&gt;Country totals over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cumulative-country-totals-over-time&#34;&gt;Cumulative country totals over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continent-totals-over-time&#34;&gt;Continent totals over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cumulative-continent-totals-over-time&#34;&gt;Cumulative continent totals over time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-about-other-data&#34;&gt;What about other data?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;As with Project 1, each member of the group should submit an &lt;strong&gt;identical&lt;/strong&gt; copy of the project to D2L (for ease of evaluation and to ensure communication across the group). You must write your group number all group members’ names across the top. Upload the components (see below) to the Project 2 assignment on D2L by 11:59pm on November 7, 2021.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The United States has resettled more than 600,000 refugees from 60 different countries since 2006.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://ssc442.netlify.app/img/assignments/refugees_welcome.jpg&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this project, you will use &lt;strong&gt;R, ggplot&lt;/strong&gt; and some form of graphics editor to explore where these refugees have come from.&lt;/p&gt;
&lt;div id=&#34;part-1-instructions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 1: Instructions&lt;/h2&gt;
&lt;p&gt;Here’s what you need to do:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; the Department of Homeland Security’s annual count of people granted refugee status between 2006-2015:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/refugee_status.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; DHS refugees, 2006-2015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Save this somewhere on your computer (you might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text). This data was originally &lt;a href=&#34;https://www.kaggle.com/dhs/refugee-report&#34;&gt;uploaded by the Department of Homeland Security to Kaggle&lt;/a&gt;, and is provided with a public domain license.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Clean&lt;/strong&gt; the data using the code we’ve given you below. As always, this code is presented without guarantee. You may need to deal with a few issues, depending on your computer’s setup.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Summarize&lt;/strong&gt; the data somehow. There is data for 60 countries over 10 years, so you’ll probably need to aggregate or reshape the data somehow (unless you do a 60-country sparkline). I’ve included some examples down below.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Create&lt;/strong&gt; an appropriate time-based visualization based on the data. I’ve shown a few different ways to summarize the data so that it’s plottable down below. Don’t just calculate overall averages or totals per country—the visualization needs to deal with change over time, and it needs to illustrate something non-obvious and insightful in the data. &lt;em&gt;Do as much polishing and refining in R&lt;/em&gt;—make adjustments to the colors, scales, labels, grid lines, and even fonts, etc. You may have more than one visualization, but only one is required. If you have more than one, they must be visually consistent (same appearance, coordinated colors, etc.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Refine and polish&lt;/strong&gt; the saved image, adding annotations, changing colors, and otherwise enhancing it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Design and create&lt;/strong&gt; an infographic “poster”. Your poster should look like a polished image that you might see on a newspaper website like &lt;a href=&#34;https://www.nytimes.com/interactive/2020/04/11/business/economy/coronavirus-us-economy-spending.html&#34;&gt;the NYT&lt;/a&gt;. Your infographic “poster” should include an eye-catching title, your plot, the caption describing the plot, and 2-4 short paragraphs succinctly describing the insights you are sharing about the data. You can (and should consider) integrating other images like national flags or arrows to convey some semantic meaning. *You may do the layout of the infographic “poster” in any software you choose - Publisher (do people still use that?), Adobe Illustrator, etc. Again, the idea is to have a polished plot with an interesting insight from the data, a polished layout to make it attractive, and a polished 2-4 paragraphs that sets up the plot and elaborates on your insight.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Upload&lt;/strong&gt; the following outputs to D2L:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your code (.Rmd) that generates the graphic.&lt;/li&gt;
&lt;li&gt;Your final poster, saved as a PDF.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this assignment, we are less concerned with the code (that’s why we gave most of it to you), and more concerned with the &lt;em&gt;design&lt;/em&gt;. Choose good colors based on palettes. Choose good, clean fonts. Use the heck out of &lt;code&gt;theme()&lt;/code&gt;. Add informative design elements. Make it look beautiful. Refer to &lt;a href=&#34;https://ssc442.netlify.app/resource/design/&#34;&gt;the design resources here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please seek out help when you need it!&lt;/strong&gt; You know enough R (and have enough examples of code from class and your readings) to be able to do this. &lt;em&gt;You can do this,&lt;/em&gt; and you’ll feel like a budding dataviz witch/wizard when you’re done.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-hypotheses&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 2: Hypotheses&lt;/h2&gt;
&lt;p&gt;For this part of the assignment, you need to provide five hypotheses about the relationship between variables in a dataset. You can (and should) consider making hypotheses about the dataset that you plan to use for your final project. However, this is not a requirement. All that is required is that you provide five hypotheses about some data. Your write-up should have an enumerated list of questions (e.g., “1. Are there more murders in states that have high unemployment.”). You will receive 2 points for each hypothesis.&lt;/p&gt;
&lt;div id=&#34;evaluation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;I will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 1&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Technical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (15 points)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Professionalism of visuals: Does the visualizations look like something you might see on TV or in the newspaper? (15 points)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Poster clarity: Does your poster clearly convey some point? (10 points)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Part 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each hypothesis is worth 2 points. (This is intended to be some free points for all; 10 points)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-cleaning-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data cleaning code&lt;/h2&gt;
&lt;p&gt;The data isn’t perfectly clean and tidy, but it’s real world data, so this is normal. Because the emphasis for this assignment is on design, not code, we’ve provided code to help you clean up the data.&lt;/p&gt;
&lt;p&gt;These are the main issues with the data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There are non-numeric values in the data, like &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;X&lt;/code&gt;, and &lt;code&gt;D&lt;/code&gt;. The data isn’t very well documented; we’re assuming &lt;code&gt;-&lt;/code&gt; indicates a missing value, but we’re not sure what &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;D&lt;/code&gt; mean, so for this assignment, we’ll just assume they’re also missing.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The data generally includes rows for dozens of countries, but there are also rows for some continents, “unknown,” “other,” and a total row. Because &lt;a href=&#34;https://twitter.com/africasacountry&#34;&gt;Africa is not a country&lt;/a&gt;, and neither are the other continents, we want to exclude all non-countries.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Maintaining consistent country names across different datasets is &lt;em&gt;literally&lt;/em&gt; the woooooooorst. Countries have different formal official names and datasets are never consistent in how they use those names.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; It’s such a tricky problem that social scientists have spent their careers just figuring out how to properly name and code countries. Really.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; There are international standards for country codes, though, like &lt;a href=&#34;https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3&#34;&gt;ISO 3166-1 alpha 3&lt;/a&gt; (my favorite), also known as ISO3. It’s not perfect—it omits microstates (some Polynesian countries) and gray area states (Palestine, Kosovo)—but it’s an international standard, so it has that going for it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To ensure that country names are consistent in this data, we use the &lt;strong&gt;countrycode&lt;/strong&gt; package (install it if you don’t have it), which is amazing. The &lt;code&gt;countrycode()&lt;/code&gt; function will take a country name in a given coding scheme and convert it to a different coding scheme using this syntax:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  countrycode(variable, &amp;quot;current-coding-scheme&amp;quot;, &amp;quot;new-coding-scheme&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It also does a farily good job at guessing and parsing inconsistent country names (e.g. it will recognize “Congo, Democratic Republic”, even though it should technically be “Democratic Republic of the Congo”). Here, we use &lt;code&gt;countrycode()&lt;/code&gt; to convert the inconsistent country names into ISO3 codes. We then create a cleaner version of the &lt;code&gt;origin_country&lt;/code&gt; column by converting the ISO3 codes back into country names. Note that the function chokes on North Korea initially, since it’s included as “Korea, North”—we use the &lt;code&gt;custom_match&lt;/code&gt; argument to help the function out.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The data isn’t tidy—there are individual columns for each year. &lt;code&gt;gather()&lt;/code&gt; takes every column and changes it to a row. We exclude the country, region, continent, and ISO3 code from the change-into-rows transformation with &lt;code&gt;-origin_country, -iso3, -origin_region, -origin_continent&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Currently, the year is being treated as a number, but it’s helpful to also treat it as an actual date. We create a new variable named &lt;code&gt;year_date&lt;/code&gt; that converts the raw number (e.g. 2009) into a date. The date needs to have at least a month, day, and year, so we actually convert it to January 1, 2009 with &lt;code&gt;ymd(paste0(year, &#34;-01-01&#34;))&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)    # For ggplot, dplyr, and friends
library(countrycode)  # For dealing with country names, abbreviations, and codes
library(lubridate)    # For dealing with dates
library(WDI)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_raw &amp;lt;- read_csv(&amp;quot;data/refugee_status.csv&amp;quot;, na = c(&amp;quot;-&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;D&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;non_countries &amp;lt;- c(&amp;quot;Africa&amp;quot;, &amp;quot;Asia&amp;quot;, &amp;quot;Europe&amp;quot;, &amp;quot;North America&amp;quot;, &amp;quot;Oceania&amp;quot;,
                   &amp;quot;South America&amp;quot;, &amp;quot;Unknown&amp;quot;, &amp;quot;Other&amp;quot;, &amp;quot;Total&amp;quot;)

refugees_clean &amp;lt;- refugees_raw %&amp;gt;%
  # Make this column name easier to work with
  rename(origin_country = `Continent/Country of Nationality`) %&amp;gt;%
  # Get rid of non-countries
  filter(!(origin_country %in% non_countries)) %&amp;gt;%
  # Convert country names to ISO3 codes
  mutate(iso3 = countrycode(origin_country, &amp;quot;country.name&amp;quot;, &amp;quot;iso3c&amp;quot;,
                            custom_match = c(&amp;quot;Korea, North&amp;quot; = &amp;quot;PRK&amp;quot;))) %&amp;gt;%
  # Convert ISO3 codes to country names, regions, and continents
  mutate(origin_country = countrycode(iso3, &amp;quot;iso3c&amp;quot;, &amp;quot;country.name&amp;quot;),
         origin_region = countrycode(iso3, &amp;quot;iso3c&amp;quot;, &amp;quot;region&amp;quot;),
         origin_continent = countrycode(iso3, &amp;quot;iso3c&amp;quot;, &amp;quot;continent&amp;quot;)) %&amp;gt;%
  # Make this data tidy
  gather(year, number, -origin_country, -iso3, -origin_region, -origin_continent) %&amp;gt;%
  # Make sure the year column is numeric + make an actual date column for years
  mutate(year = as.numeric(year),
         year_date = ymd(paste0(year, &amp;quot;-01-01&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-to-possibly-use-in-your-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data to possibly use in your plot&lt;/h2&gt;
&lt;p&gt;Here are some possible summaries of the data you might use…&lt;/p&gt;
&lt;div id=&#34;country-totals-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Country totals over time&lt;/h3&gt;
&lt;p&gt;This is just the &lt;code&gt;refugees_clean&lt;/code&gt; data frame I gave you. You’ll want to filter it and select specific countries, though—you won’t really be able to plot 60 countries all at once unless you use sparklines.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 7
##   origin_country iso3  origin_region         origin_continent  year number year_date 
##   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;date&amp;gt;    
## 1 Afghanistan    AFG   South Asia            Asia              2006    651 2006-01-01
## 2 Angola         AGO   Sub-Saharan Africa    Africa            2006     13 2006-01-01
## 3 Armenia        ARM   Europe &amp;amp; Central Asia Asia              2006     87 2006-01-01
## 4 Azerbaijan     AZE   Europe &amp;amp; Central Asia Asia              2006     77 2006-01-01
## 5 Belarus        BLR   Europe &amp;amp; Central Asia Europe            2006    350 2006-01-01
## 6 Bhutan         BTN   South Asia            Asia              2006      3 2006-01-01&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cumulative-country-totals-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cumulative country totals over time&lt;/h3&gt;
&lt;p&gt;Note the &lt;code&gt;cumsum()&lt;/code&gt; function—it calculates the cumulative sum of a column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_countries_cumulative &amp;lt;- refugees_clean %&amp;gt;%
  arrange(year_date) %&amp;gt;%
  group_by(origin_country) %&amp;gt;%
  mutate(cumulative_total = cumsum(number))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 7
## # Groups:   origin_country [1]
##   origin_country iso3  origin_continent  year number year_date  cumulative_total
##   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;date&amp;gt;                &amp;lt;dbl&amp;gt;
## 1 Afghanistan    AFG   Asia              2006    651 2006-01-01              651
## 2 Afghanistan    AFG   Asia              2007    441 2007-01-01             1092
## 3 Afghanistan    AFG   Asia              2008    576 2008-01-01             1668
## 4 Afghanistan    AFG   Asia              2009    349 2009-01-01             2017
## 5 Afghanistan    AFG   Asia              2010    515 2010-01-01             2532
## 6 Afghanistan    AFG   Asia              2011    428 2011-01-01             2960&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;continent-totals-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Continent totals over time&lt;/h3&gt;
&lt;p&gt;Note the &lt;code&gt;na.rm = TRUE&lt;/code&gt; argument in &lt;code&gt;sum()&lt;/code&gt;. This makes R ignore any missing data when calculating the total. Without it, if R finds a missing value in the column, it will mark the final sum as &lt;code&gt;NA&lt;/code&gt; too, which we don’t want.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_continents &amp;lt;- refugees_clean %&amp;gt;%
  group_by(origin_continent, year_date) %&amp;gt;%
  summarize(total = sum(number, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;origin_continent&amp;#39;. You can override using the `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 3
## # Groups:   origin_continent [1]
##   origin_continent year_date  total
##   &amp;lt;chr&amp;gt;            &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Africa           2006-01-01 18116
## 2 Africa           2007-01-01 17473
## 3 Africa           2008-01-01  8931
## 4 Africa           2009-01-01  9664
## 5 Africa           2010-01-01 13303
## 6 Africa           2011-01-01  7677&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;cumulative-continent-totals-over-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cumulative continent totals over time&lt;/h3&gt;
&lt;p&gt;Note that there are two &lt;code&gt;group_by()&lt;/code&gt; functions here. First we get the total number of refugees per continent per year, then we group by continent only to get the cumulative sum of refugees across continents.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_continents_cumulative &amp;lt;- refugees_clean %&amp;gt;%
  group_by(origin_continent, year_date) %&amp;gt;%
  summarize(total = sum(number, na.rm = TRUE)) %&amp;gt;%
  arrange(year_date) %&amp;gt;%
  group_by(origin_continent) %&amp;gt;%
  mutate(cumulative_total = cumsum(total))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;origin_continent&amp;#39;. You can override using the `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 4
## # Groups:   origin_continent [1]
##   origin_continent year_date  total cumulative_total
##   &amp;lt;chr&amp;gt;            &amp;lt;date&amp;gt;     &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1 Africa           2006-01-01 18116            18116
## 2 Africa           2007-01-01 17473            35589
## 3 Africa           2008-01-01  8931            44520
## 4 Africa           2009-01-01  9664            54184
## 5 Africa           2010-01-01 13303            67487
## 6 Africa           2011-01-01  7677            75164&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-other-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What about other data?&lt;/h3&gt;
&lt;p&gt;In your prerequisite course, you learned how to use the &lt;code&gt;merge&lt;/code&gt; function (and possibly its Tidyverse cousin, &lt;code&gt;left_join&lt;/code&gt;). Since our refugee data has a standardized country code, we can find other datasets that might have useful information.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;WDI&lt;/code&gt; package acts as an interface to the World Bank Development Inidcators dataset, which has a &lt;em&gt;lot&lt;/em&gt; of information on the countries in our data. To merge WDI data to our refugee data, we’ll need to make a little change to our cleaning code - &lt;code&gt;WDI&lt;/code&gt; needs the iso2 country code, which is a unique 2-letter code instead of the iso3 3-letter code we have. Here’s the cleaning data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_clean &amp;lt;- refugees_raw %&amp;gt;%
  rename(origin_country = `Continent/Country of Nationality`) %&amp;gt;%
  dplyr::filter(!(origin_country %in% non_countries)) %&amp;gt;%
  mutate(iso2 = countrycode(origin_country, &amp;quot;country.name&amp;quot;, &amp;quot;iso2c&amp;quot;,
                            custom_match = c(&amp;quot;Korea, North&amp;quot; = &amp;quot;KP&amp;quot;))) %&amp;gt;%
  mutate(origin_country = countrycode(iso2, &amp;quot;iso2c&amp;quot;, &amp;quot;country.name&amp;quot;),
         origin_region = countrycode(iso2, &amp;quot;iso2c&amp;quot;, &amp;quot;region&amp;quot;),
         origin_continent = countrycode(iso2, &amp;quot;iso2c&amp;quot;, &amp;quot;continent&amp;quot;)) %&amp;gt;%
  gather(year, number, -origin_country, -iso2, -origin_region, -origin_continent) %&amp;gt;%
  mutate(year = as.numeric(year),
         year_date = ymd(paste0(year, &amp;quot;-01-01&amp;quot;)),
         iso2c = iso2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only difference here is that we now have iso2 instead of iso3. If you use your iso3 field in your code, you’ll have to create both.&lt;/p&gt;
&lt;p&gt;Now, the &lt;code&gt;WDI&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(WDI)
myData = WDI(country = refugees_clean$iso2, indicator = &amp;#39;SP.POP.TOTL&amp;#39;, start = 2006, end = 2015)  %&amp;gt;%
      dplyr::select(-country)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function needs four arguments (see &lt;code&gt;?WDI&lt;/code&gt; after installing the &lt;code&gt;WDI&lt;/code&gt; package for more). The first has to be list of iso2 codes, which we have…in the form of the &lt;code&gt;iso2&lt;/code&gt; column. So we pass that column as the first argument, and &lt;code&gt;WDI&lt;/code&gt; will give us data for every country in that column.&lt;/p&gt;
&lt;p&gt;The second argument, &lt;code&gt;indicator&lt;/code&gt;, tells &lt;code&gt;WDI&lt;/code&gt; what data you want. The &lt;a href=&#34;https://databank.worldbank.org/metadataglossary/World-Development-Indicators/series&#34;&gt;WDI data dictionary is here&lt;/a&gt;. Use the drop-down under “Select Database” and choose just the World Development Indicators option to simplify. Then, use the search box to search for intersting indicators. The website shows the “code” of each of the indicators. That’s the code you use. For instance, &lt;code&gt;SP.POP.TOTL&lt;/code&gt; is the total population by country, so you can generate per-capita measures of refugees if you want. Finally, &lt;code&gt;WDI&lt;/code&gt; needs the start and end years. Our data is 2006 to 2015, so that makes the most sense.&lt;/p&gt;
&lt;p&gt;Now, once you have &lt;code&gt;myData&lt;/code&gt; retrieved from &lt;code&gt;WDI&lt;/code&gt;, take a look at it. We need to see which columns to merge on - that is, which columns should R match to put the data together? We want to merge on &lt;code&gt;&#39;iso2c&#39;&lt;/code&gt; and on &lt;code&gt;&#39;year&#39;&lt;/code&gt; because &lt;code&gt;WDI&lt;/code&gt; gives us a tidy data frame by country-year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;refugees_clean_merged = left_join(refugees_clean, myData, by = c(&amp;#39;iso2c&amp;#39;,&amp;#39;year&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We didn’t have an &lt;code&gt;iso2c&lt;/code&gt; column before, just an &lt;code&gt;iso2&lt;/code&gt; column, so you might notice that the new cleaning code I gave you added &lt;code&gt;iso2c = iso2&lt;/code&gt; at the end using &lt;code&gt;mutate&lt;/code&gt;. That way, the column names that we’re using to merge will match.&lt;/p&gt;
&lt;p&gt;You can search the WDI data dictionary and find the “Code” for the indicator you’d like to use, then just merge it into your data. Look at your data closely before you merge, and make sure you know what you’re merging in. Use the slack if you get stuck. Happy data hunting!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(refugees_clean_merged %&amp;gt;% dplyr::select(origin_country, number, iso2, year, SP.POP.TOTL))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 × 5
##   origin_country number iso2   year SP.POP.TOTL
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 Afghanistan       651 AF     2006    26433058
## 2 Angola             13 AO     2006    20149905
## 3 Armenia            87 AM     2006     2958301
## 4 Azerbaijan         77 AZ     2006     8484550
## 5 Belarus           350 BY     2006     9604924
## 6 Bhutan              3 BT     2006      657404&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;For instance, “North Korea”, “Korea, North”, “DPRK”, “Korea, Democratic People’s Republic of”, and “Democratic People’s Republic of Korea”, and “Korea (DPRK)” are all perfectly normal versions of the country’s name and you’ll find them all in the wild.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;See Gleditsch, Kristian S. &amp;amp; Michael D. Ward. 1999. &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/03050629908434958&#34;&gt;“Interstate System Membership: A Revised List of the Independent States since 1816.”&lt;/a&gt; &lt;em&gt;International Interactions&lt;/em&gt; 25: 393-413; or the &lt;a href=&#34;http://www.paulhensel.org/icownames.html&#34;&gt;“ICOW Historical State Names Data Set”&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Weekly Writing</title>
      <link>https://ssc442.netlify.app/assignment/weekly3/</link>
      <pubDate>Tue, 12 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/weekly3/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#weekly-writing-prompt&#34;&gt;Weekly Writing Prompt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;weekly-writing-prompt&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Weekly Writing Prompt&lt;/h3&gt;
&lt;p&gt;This weekly writing is for those who did not attend class.&lt;/p&gt;
&lt;p&gt;This week’s writing has two parts. First, consider examining political results in various legislative districts in Michigan. Suppose you ran a regression with a politician’s vote share (as a percentage) as the target variable and the percent of various races or ethnicities in the voting district (e.g., white, black, indigenous people, Asian, etc.) as the right hand side variables. Brefily describe, in words, the interpretation of the coefficients. Second, write a short paragraph about any concerns you might have with this empirical approach.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;As always, you &lt;strong&gt;must&lt;/strong&gt; knit your output to a PDF and submit that to D2L. No word docs, no .Rmd documents. PDFs, please.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Project 1</title>
      <link>https://ssc442.netlify.app/assignment/project1/</link>
      <pubDate>Fri, 08 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/project1/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#groups&#34;&gt;Groups&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-1-rats-rats-rats.&#34;&gt;Part 1: Rats, rats, rats.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#instructions&#34;&gt;Instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#starter-code&#34;&gt;Starter code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-2-data-hunting&#34;&gt;Part 2: Data Hunting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluations&#34;&gt;Evaluations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;groups&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Groups&lt;/h3&gt;
&lt;p&gt;Your groups are listed &lt;a href=&#34;https://ssc442.netlify.app/projects/grouplist.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Each member of the group must submit a copy of the project to D2L. Please write your group number and the group members’ names across the top.&lt;/p&gt;
&lt;p&gt;Turn in your copies by &lt;strong&gt;11:59pm on October 8th&lt;/strong&gt; (I have extended everyone’s deadline due to slow rollout of groups).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-1-rats-rats-rats.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 1: Rats, rats, rats.&lt;/h2&gt;
&lt;p&gt;New York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.&lt;/p&gt;
&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/PeJUqcbool4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;NYC keeps incredibly detailed data regarding animal sightings, including rats, and &lt;a href=&#34;https://www.kaggle.com/new-york-city/nyc-rat-sightings/data&#34;&gt;it makes this data publicly available&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For this first project, you will use &lt;strong&gt;R and ggplot2&lt;/strong&gt; to tell an interesting story hidden in the data. You must create a story by looking carefully at the data.&lt;/p&gt;
&lt;div id=&#34;instructions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Instructions&lt;/h3&gt;
&lt;p&gt;Here’s what you need to do:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; New York City’s database of rat sightings since 2010:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ssc442.netlify.app/data/Rat_sightings.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Rat_sightings.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Summarize&lt;/strong&gt; the data somehow. The raw data has more than 100,000 rows, which means you’ll need to aggregate the data (&lt;code&gt;filter()&lt;/code&gt;, &lt;code&gt;group_by()&lt;/code&gt;, and &lt;code&gt;summarize()&lt;/code&gt; will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2016.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Create&lt;/strong&gt; an appropriate visualization based on the data you summarized.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Write&lt;/strong&gt; a memo explaining your process. We are specifically looking for a discussion of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What story are you telling with your new graphic?&lt;/li&gt;
&lt;li&gt;How have you applied reasonable standards in visual storytelling?&lt;/li&gt;
&lt;li&gt;What policy implication is there (if any)?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Upload&lt;/strong&gt; the following outputs to D2L:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A PDF file of your memo with your final code and graphic embedded in it.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; This means you’ll need to do all your coding in an &lt;code&gt;R&lt;/code&gt; Markdown file and embed your code in chunks. Note that Part 2 of this project should be included in this PDF (see below).&lt;/li&gt;
&lt;li&gt;A standalone PDF version of your graphic. Use &lt;code&gt;ggsave(plot_name, filename = &#34;output/blah.pdf&#34;, width = XX, height = XX)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;starter-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Starter code&lt;/h3&gt;
&lt;p&gt;I’ve provided some starter code below. A couple comments about it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By default, &lt;code&gt;read_csv()&lt;/code&gt; treats cells that are empty or “NA” as missing values. This rat dataset uses “N/A” to mark missing values, so we need to add that as a possible marker of missingness (hence &lt;code&gt;na = c(&#34;&#34;, &#34;NA&#34;, &#34;N/A&#34;)&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;To make life easier, I’ve renamed some of the key variables you might work with. You can rename others if you want.&lt;/li&gt;
&lt;li&gt;I’ve also created a few date-related variables (&lt;code&gt;sighting_year&lt;/code&gt;, &lt;code&gt;sighting_month&lt;/code&gt;, &lt;code&gt;sighting_day&lt;/code&gt;, and &lt;code&gt;sighting_weekday&lt;/code&gt;). You don’t have to use them, but they’re there if you need them. The functions that create these, like &lt;code&gt;year()&lt;/code&gt; and &lt;code&gt;wday()&lt;/code&gt; are part of the &lt;strong&gt;lubridate&lt;/strong&gt; library.&lt;/li&gt;
&lt;li&gt;The date/time variables are formatted like &lt;code&gt;04/03/2017 12:00:00 AM&lt;/code&gt;, which R is not able to automatically parse as a date when reading the CSV file. You can use the &lt;code&gt;mdy_hms()&lt;/code&gt; function in the &lt;strong&gt;lubridate&lt;/strong&gt; library to parse dates that are structured as “month-day-year-hour-minute”. There are also a bunch of other iterations of this function, like &lt;code&gt;ymd()&lt;/code&gt;, &lt;code&gt;dmy()&lt;/code&gt;, etc., for other date formats.&lt;/li&gt;
&lt;li&gt;There’s one row with an unspecified borough, so I filter that out.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
rats_raw &amp;lt;- read_csv(&amp;quot;data/Rat_Sightings.csv&amp;quot;, na = c(&amp;quot;&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;N/A&amp;quot;))
# If you get an error that says &amp;quot;All formats failed to parse. No formats
# found&amp;quot;, it&amp;#39;s because the mdy_hms function couldn&amp;#39;t parse the date. The date
# variable *should* be in this format: &amp;quot;04/03/2017 12:00:00 AM&amp;quot;, but in some
# rare instances, it might load without the seconds as &amp;quot;04/03/2017 12:00 AM&amp;quot;.
# If there are no seconds, use mdy_hm() instead of mdy_hms().
rats_clean &amp;lt;- rats_raw %&amp;gt;%
  rename(created_date = `Created Date`,
         location_type = `Location Type`,
         borough = Borough) %&amp;gt;%
  mutate(created_date = mdy_hms(created_date)) %&amp;gt;%
  mutate(sighting_year = year(created_date),
         sighting_month = month(created_date),
         sighting_day = day(created_date),
         sighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %&amp;gt;%
  filter(borough != &amp;quot;Unspecified&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll summarize the data with functions from &lt;strong&gt;dplyr&lt;/strong&gt;, including stuff like &lt;code&gt;count()&lt;/code&gt;, &lt;code&gt;arrange()&lt;/code&gt;, &lt;code&gt;filter()&lt;/code&gt;, &lt;code&gt;group_by()&lt;/code&gt;, &lt;code&gt;summarize()&lt;/code&gt;, and &lt;code&gt;mutate()&lt;/code&gt;. Here are some examples of ways to summarize the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# See the count of rat sightings by weekday
rats_clean %&amp;gt;%
  count(sighting_weekday)
# Assign a summarized data frame to an object to use it in a plot
rats_by_weekday &amp;lt;- rats_clean %&amp;gt;%
  count(sighting_weekday, sighting_year)
ggplot(rats_by_weekday, aes(x = fct_rev(sighting_weekday), y = n)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ sighting_year)
# See the count of rat sightings by weekday and borough
rats_clean %&amp;gt;%
  count(sighting_weekday, borough, sighting_year)
# An alternative to count() is to specify the groups with group_by() and then
# be explicit about how you&amp;#39;re summarizing the groups, such as calculating the
# mean, standard deviation, or number of observations (we do that here with
# `n()`).
rats_clean %&amp;gt;%
  group_by(sighting_weekday, borough) %&amp;gt;%
  summarize(n = n())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-data-hunting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 2: Data Hunting&lt;/h2&gt;
&lt;p&gt;For the second part of the project, your task is simple. Your group must identify three different data sources&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; for potential use in your final project. You are not bound to this decision.&lt;/p&gt;
&lt;p&gt;For each, you must write a single paragraph about what about this data interests you. Add this to the memo from Part 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluations&lt;/h2&gt;
&lt;p&gt;I will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 1&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Technical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (10 points)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Appropriateness of visuals: Do the visualizations tell a clear story? Have we learned something? (10 points)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Storytelling: Does your memo clearly convey what you’re doing and why? (9 points)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Part 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each piece of data (and description) is worth 7 points. (21 points total)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;You can approach this in a couple different ways—you can write the memo and then include the full figure and code at the end, &lt;a href=&#34;https://rud.is/b/2017/09/18/mapping-fall-foliage-with-sf/&#34;&gt;similar to this blog post&lt;/a&gt;, or you can write the memo in an incremental way, describing the different steps of creating the figure, ultimately arriving at a clean final figure, &lt;a href=&#34;https://rudeboybert.github.io/fivethirtyeight/articles/bechdel.html&#34;&gt;like this blog post&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The three different sources need not be different websites or from different organizations. For example, three different tables from the US Census would be sufficient&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Final project</title>
      <link>https://ssc442.netlify.app/assignment/final-project/</link>
      <pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://ssc442.netlify.app/assignment/final-project/</guid>
      <description>
&lt;script src=&#34;https://ssc442.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#teams&#34;&gt;Teams&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#suggested-outline&#34;&gt;Suggested outline&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#theory-and-background&#34;&gt;Theory and Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-and-analyses&#34;&gt;Data and Analyses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;requirements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Requirements&lt;/h2&gt;
&lt;p&gt;Data analytics is inherently a hands-on endeavor. Thus, the final project for this class is hands-on. As per the overview page, the final project has the following elements:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For your final project in this class, you will analyze &lt;strong&gt;existing data&lt;/strong&gt; in some area of interest to you.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Aggregating data from multiple sources is encouraged, but is not required.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;You must visualize (at least) three &lt;strong&gt;interesting&lt;/strong&gt; features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.[^4]&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You will submit &lt;strong&gt;three things&lt;/strong&gt; via D2L:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A PDF of your report (see the outline below for details of what this needs to contain) rendered from your R Markdown. You might want to write the prose-heavy sections in a word processor like Word or Google Docs and copy/paste the text into your &lt;code&gt;R&lt;/code&gt; Markdown document, since RStudio doesn’t have a nice spell checker or grammar checker. This should have &lt;em&gt;no visible &lt;code&gt;R&lt;/code&gt; code, warnings, or messages in it&lt;/em&gt;. To do this, you must set &lt;code&gt;echo = FALSE&lt;/code&gt; in the code chunk options &lt;code&gt;knitr::opts_chunk$set(echo = FALSE, ...)&lt;/code&gt; at the beginning of your document template before you knit.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The same PDF as above, but with all the R code in it (set &lt;code&gt;echo = TRUE&lt;/code&gt; at the beginning of your document and reknit the file). Please label files in an obvious way.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A CSV file of your data; or a link to the data online if your code pulls from the internet. This must be a separate file titled “data.csv” or “data.txt” as applicable.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This project is due by &lt;strong&gt;11:59 PM on Tuesday, April 27th, 2021.&lt;/strong&gt; &lt;span style=&#34;color: #81056F; font-weight: bold&#34;&gt; No late work will be accepted. For real. MSU has grading deadlines and I’ve given you every second that can be spared.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There is no final exam. This project is your final exam.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The project will &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Technical skills: Was the project easy? Does it showcase mastery of data analysis? (20%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Visual design: Was the information smartly conveyed and usable? Was it beautiful? (25%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Analytic design: Was the analysis appropriate? Was it sensible, given the dataset? (20%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Story: Did we learn something? (25%)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Following instructions: Did you surpress &lt;code&gt;R&lt;/code&gt; code as asked? Did you submit a separate datafile and label it correctly? (10%)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;teams&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Teams&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;My team sucks; how can I punish them for their lack of effort?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;On this front, we will be more supportive. While you have to put up with your team regardless of their quality, you can indicate that your team members are not carrying their fair share by issuing a &lt;strong&gt;strike&lt;/strong&gt;. This processs works as follows:
1. A team member systematically fails to exert effort on collaborative projects (for example, by not showing up for meetings or not communicating, or by simply leeching off others without contributing.)
2. Your frustration reaches a boiling point. You decide this has to stop. You decide to issue a &lt;strong&gt;strike&lt;/strong&gt;
3. You send an email with the following information:
- &lt;code&gt;Subject line:&lt;/code&gt; [SSC442] Strike against [Last name of Recipient]
- &lt;code&gt;Body:&lt;/code&gt; You do &lt;strong&gt;not&lt;/strong&gt; need to provide detailed reasoning. However, you must discuss the actions (plural) you took to remedy the situation before sending the strike email.&lt;/p&gt;
&lt;p&gt;A strike is a serious matter, and will reduce that team member’s grade on joint work by 10%. If any team-member gets strikes from all other members of his or her team, their grade will be reduced by 50%.&lt;/p&gt;
&lt;p&gt;Strikes are &lt;em&gt;anonymous&lt;/em&gt; so that you do not need to fear social retaliation. However, they are not anonymous to allow you to issue them without thoughtful consideration. Perhaps the other person has a serious issue that is preventing them from completing work (e.g., a relative passing away). Please be thoughtful in using this remedy and consider it a last resort.&lt;/p&gt;
&lt;!-- &gt; Do I really need to create a team GitHub repository? I don&#39;t like GitHub / programming/ work. --&gt;
&lt;!-- Yes, you need to become familiar with GitHub and you and your team will work in a central repository for mini-projects and your final project. --&gt;
&lt;!-- This is for two reasons. First, computer scientists spent a huge amount of time coming up with the solutions that are implemented in GitHub (and other flavors of `git`). Their efforts are largely dedicated toward solving a very concrete goal: how can two people edit the same thing at the same time without creating a ton of new issues. While you could use a paid variant of GitHub (e.g., you could all collaborate over the Microsoft Office suite as implemented by the 360 software that MSU provides), you&#39;d ultimately have the following issues: --&gt;
&lt;!-- 1. The software doesn&#39;t support some file types. --&gt;
&lt;!-- 2. The software doesn&#39;t autosave versions.[^1] If someone accidentally deletes something, you&#39;re in trouble. --&gt;
&lt;!-- 3. You have to learn an entirely new system every time you change classes / universities / jobs, because said institute doesn&#39;t buy the product you love.[^2] --&gt;
&lt;!-- [^1]: Some products, of course, solve this problem a little bit. For example, Dropbox allows users to share files with ease (of any file type) and saves a (coarse) version history. However, Dropbox does not allow multiple users to work on the same file, and has no way of merging edits together. --&gt;
&lt;!-- [^2]: This logic is also why we utilize only free software in this course. It sucks to get really good at, say, `SAS` (as I did many years ago) only to realize that the software costs about $10000 and many firms are unwilling to spent that. We will try our best to avoid giving you dead-end skills. --&gt;
&lt;blockquote&gt;
&lt;p&gt;I’m on a smaller-than-normal team. Does this mean that I have to do more work?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Your instructors are able to count and are aware the teams are imbalanced. Evaluations of final projects will take this into account. While your final product should reflect the best ability of your team, we do not anticipate that the uneven teams will lead to substantively different outputs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;suggested-outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Suggested outline&lt;/h2&gt;
&lt;p&gt;You must write and present your analysis as if presenting to a &lt;strong&gt;C-suite executive&lt;/strong&gt;. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;
- Concretely, this requires a written memo, which describes the data, analyses, and results. This must be clear and easy to understand for a non-expert in your field. Figures and tables do not apply to the page limit.&lt;/p&gt;
&lt;p&gt;Below is a very loose guide to the sort of content that we expect for the final project. Word limits are suggestions only. Note your final report will be approximately&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Describe the motivation for this analysis. Briefly describe the dataset, and explain why the analysis you’re undertaking matters for society. (Or matters for some decision-making. You should not feel constrained to asking only “big questions.” The best projects will be narrow-scope but well-defined.) (&lt;strong&gt;≈300 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theory-and-background&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Theory and Background&lt;/h3&gt;
&lt;p&gt;Provide in-depth background about the data of interest and about your analytics question. (&lt;strong&gt;≈300 words&lt;/strong&gt;)&lt;/p&gt;
&lt;div id=&#34;theory&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;“Theory”&lt;/h4&gt;
&lt;p&gt;Provide some theoretical guidance to the functional relationship you hope to explore. If you’re interested on how, say, height affects scoring in the NBA, write down a proposed function that might map height to scoring. Describe how you might look for this unknown relationship in the data.(&lt;strong&gt;≈300 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hypotheses&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Hypotheses&lt;/h4&gt;
&lt;p&gt;Make predictions. Declare what you think will happen. (Note, this may carry over from second project.) (&lt;strong&gt;≈250 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-and-analyses&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data and Analyses&lt;/h3&gt;
&lt;div id=&#34;data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Data&lt;/h4&gt;
&lt;p&gt;Given your motivations, limits on feasibility, and hypotheses, describe the data you use. (&lt;strong&gt;≈100 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analyses&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Analyses&lt;/h4&gt;
&lt;p&gt;Generate the analyses relevant to your hypotheses and interests. Here you must include three figures and must describe what they contain in simple, easy to digest language. Why did you visualize these elements? Your analyses also must include brief discussion.&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;As many words as you need to fully describe your analysis and results&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;What caveats should we consider? Do you believe this is a truly causal relationship? Why does any of this matter to the decision-maker? (&lt;strong&gt;≈75 words&lt;/strong&gt;)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Note that &lt;strong&gt;existing&lt;/strong&gt; is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged. But you cannot stand with a clipboard outside a store and count visitors (for instance).&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;This exercise provides you with an opportunity to identify your marketable skills and to practice them. I encourage those who will be looking for jobs soon to take this exercise seriously.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
