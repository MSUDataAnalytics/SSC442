<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Examples | Data Analytics</title>
    <link>https://datavizm20.classes.andrewheiss.com/example/</link>
      <atom:link href="https://datavizm20.classes.andrewheiss.com/example/index.xml" rel="self" type="application/rss+xml" />
    <description>Examples</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <image>
      <url>https://datavizm20.classes.andrewheiss.com/img/social-image.png</url>
      <title>Examples</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/</link>
    </image>
    
    <item>
      <title>Illustrating Bias vs. Variance</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/code_aggregated/</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/code_aggregated/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NN = 100   #----&amp;gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = 0.75   #-----&amp;gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.



library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 4.0.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)



f = function(x) {
  x ^ 2
}



get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  data.frame(x, y)
}



set.seed(1)
sim_data = get_sim_data(f)



fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)





set.seed(42)
plot(y ~ x, data = sim_data, col = &amp;quot;grey&amp;quot;, pch = 20,
     main = &amp;quot;Four Polynomial Models fit to a Simulated Dataset&amp;quot;)

grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = &amp;quot;black&amp;quot;, lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = &amp;quot;dodgerblue&amp;quot;,  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &amp;quot;firebrick&amp;quot;,   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &amp;quot;springgreen&amp;quot;, lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = &amp;quot;darkorange&amp;quot;,  lwd = 2, lty = 5)

legend(&amp;quot;topleft&amp;quot;, 
       c(&amp;quot;y ~ 1&amp;quot;, &amp;quot;y ~ poly(x, 1)&amp;quot;, &amp;quot;y ~ poly(x, 2)&amp;quot;,  &amp;quot;y ~ poly(x, 9)&amp;quot;, &amp;quot;truth&amp;quot;), 
       col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;, &amp;quot;black&amp;quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)



for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}



predictions.proc = (predictions)
colnames(predictions.proc) = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;9&amp;quot;)
predictions.proc = as.data.frame(predictions.proc)

tall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)

## Here, you can save your ggplot output 
FinalPlot &amp;lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + 
                      geom_boxplot() +
                      geom_jitter(alpha = .5) +
                      geom_hline(yintercept = f(x = .90)) +
                      labs(col = &amp;#39;Model&amp;#39;, x = &amp;#39;Model&amp;#39;, y = &amp;#39;Prediction&amp;#39;, title = paste0(&amp;#39;Bias v Var - Sample Size: &amp;#39;,NN), subtitle = paste0(&amp;#39;SD of Bayes Err: &amp;#39;,SD.of.Bayes.Error)) +
                      theme_bw()



FinalPlot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/Code_Aggregated_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This is going to aggregate your results for you:
if(!exists(&amp;#39;FinalResults&amp;#39;)) FinalResults = list()

FinalResults[[paste0(&amp;#39;finalPlot.NN.&amp;#39;,NN,&amp;#39;.SDBayes.&amp;#39;,SD.of.Bayes.Error)]] = FinalPlot

# 
# boxplot(value ~ key, data = tall_predictions, border = &amp;quot;darkgrey&amp;quot;, xlab = &amp;quot;Polynomial Degree&amp;quot;, ylab = &amp;quot;Predictions&amp;quot;,
#         main = &amp;quot;Simulated Predictions for Polynomial Models&amp;quot;)
# grid()
# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = &amp;quot;jitter&amp;quot;, jitter = 0.15, pch = 1, col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;))
# abline(h = f(x = 0.90), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This not run automatically.
## To plot the whole list of ggplot objects in FinalResults:
wrap_plots(FinalResults, nrow = 2, guides = &amp;#39;collect&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Illustrating Bias vs. Variance</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/09-example/</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/09-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#review-and-clarify&#34;&gt;Review and Clarify&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#illustration-of-bias-vs.-variance&#34;&gt;Illustration of Bias vs. Variance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-quick-bit-of-r-code-to-help-with-todays-example&#34;&gt;A quick bit of R code to help with todays example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#todays-example&#34;&gt;Today’s Example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simulation&#34;&gt;Simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#heres-the-code&#34;&gt;Here’s the code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;review-and-clarify&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Review and Clarify&lt;/h2&gt;
&lt;p&gt;Bias and Variance are tricky subjects. Hopefully the illustrations from yesterday are helpful. Let’s talk through a few things based on questions some of you have asked since Content 9.&lt;/p&gt;
&lt;div id=&#34;illustration-of-bias-vs.-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Illustration of Bias vs. Variance&lt;/h3&gt;
&lt;p&gt;Bias is about how close you are on average to the correct answer. Variance is about how scattered your estimates would be if you repeated your experiment with new data.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://www.machinelearningplus.com/wp-content/uploads/2020/10/output_31_0.png&#34; alt=&#34;Image from MachineLearningPlus.com&#34;  /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Image from MachineLearningPlus.com
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We care about these things because we usually only have our one dataset (when we’re not creating simulated data, that is), but need to know something about how bias and variance tend to look when we change our model complexity.&lt;/p&gt;
&lt;div id=&#34;deriving-bias-and-variance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Deriving Bias and Variance&lt;/h4&gt;
&lt;p&gt;For this section, recall our model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y = f(x) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This tells us that some of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; can be predicted by the &lt;strong&gt;true&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, and some is just noise &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. In our simulation from last week, &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x^2\)&lt;/span&gt;, so &lt;span class=&#34;math inline&#34;&gt;\(y = x^2 + \epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We want to predict &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. We call our prediction &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Our best guess for &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; is our model. It might be from a linear regression with 1, 2, 9, 15, etc. predictors or interactions of predictors. It might be from a k-nearest-neighbors estimation with &lt;code&gt;k = 4&lt;/code&gt;. It might be from a regression tree with &lt;code&gt;cp = .1&lt;/code&gt; and &lt;code&gt;minsplit=2&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even when we really nail &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; (which means &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x) = f(x)\)&lt;/span&gt;), there is &lt;em&gt;still&lt;/em&gt; error in our prediction because of &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(y \neq \hat{y}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we think of two different measures of error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
EPE = E[(y - \hat{y})^2] = 
\underbrace{\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]}_\textrm{reducible error - MSE from imperfect model} +
\underbrace{\mathbb{V}_{Y \mid X} \left[ Y \mid X = x \right]}_{\textrm{irreducible error from }\epsilon}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MSE(f(x), \hat{f}(x)) = E_{\mathcal{D}}\left[\left(f(x) - \hat{f}(x)\right)^2\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Some of you asked about this equation from last time that decomposed our MSE:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE}\left(f(x), \hat{f}(x)\right) =
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] =
\underbrace{\left(f(x) - \mathbb{E} \left[ \hat{f}(x) \right]  \right)^2}_{\text{bias}^2 \left(\hat{f}(x) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}(x) - \mathbb{E} \left[ \hat{f}(x) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}(x) \right)}
\]&lt;/span&gt;
This can be derived by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] &amp;amp;=&amp;amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)] + E[\hat{f}(x)] - \hat{f}(x)\right)^2 \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)]\right)^2 + \left(E[\hat{f}(x)] - \hat{f}(x)\right)^2 + 2\left(f(x) - E[\hat{f}(x)]\right) \left(E[\hat{f}(x)] - \hat{f}(x)\right) \right] \\
&amp;amp;=&amp;amp; \mathbb{E}_{\mathcal{D}} \left[\left(f(x) - E[\hat{f}(x)]\right)^2\right] + \mathbb{E}_{\mathcal{D}} \left[\left(E[\hat{f}(x)] - \hat{f}(x)\right)^2\right] +  \mathbb{E}_{\mathcal{D}} \left[2\left(f(x) - E[\hat{f}(x)]\right) \left(E[\hat{f}(x)] - \hat{f}(x)\right) \right] \\
&amp;amp;=&amp;amp; \left(f(x) - E[\hat{f}(x)]\right)^2 + Var\left(\hat{f}(x)\right) + 0
\end{eqnarray*}
\]&lt;/span&gt;
Let’s talk about what’s in this equation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MSE is the Mean Squared Error between &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;It does not have the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; in it&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;It is an expectation over all the possible &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; draws of the data we could have
&lt;ul&gt;
&lt;li&gt;Because of this &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}\left[\hat{f}(x)\right]\)&lt;/span&gt; can move out of the expectation. This lets us cancel that last term with the “2” in it.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main takeaway is that, even given the error, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;, we &lt;em&gt;still&lt;/em&gt; have additional error coming from our inability to perfectly get &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x) = f(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-quick-bit-of-r-code-to-help-with-todays-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A quick bit of R code to help with todays example&lt;/h2&gt;
&lt;p&gt;We saw before the usefulness of having a list&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList = list()
myList[[&amp;#39;thisThing&amp;#39;]] = c(1,2,3)
myList[[&amp;#39;thisOtherThing&amp;#39;]] = c(&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;,&amp;#39;C&amp;#39;,&amp;#39;D&amp;#39;)
myList&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $thisThing
## [1] 1 2 3
## 
## $thisOtherThing
## [1] &amp;quot;A&amp;quot; &amp;quot;B&amp;quot; &amp;quot;C&amp;quot; &amp;quot;D&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It worked really well for holding results from models since we could name the things in the list. But if we put it into a loop:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}

print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $I
## [1] &amp;quot;This loop is on I&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened? Every time we used the loop, it re-initiated the list, so we only get the last result!&lt;/p&gt;
&lt;p&gt;So what we want is to create the list &lt;strong&gt;if&lt;/strong&gt; it doesn’t exist, and add to it afterwards. We can do that with &lt;code&gt;exists(&#39;myList&#39;)&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  if(!exists(&amp;#39;myList&amp;#39;)) myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $I
## [1] &amp;quot;This loop is on I&amp;quot;
## 
## $G
## [1] &amp;quot;This loop is on G&amp;quot;
## 
## $H
## [1] &amp;quot;This loop is on H&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re almost there. It turns out, we have our original &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; in there left over from the previous creation of the list. That’s why its out of order. What we want to do is start with a fresh, clean list. If we run &lt;code&gt;rm(myList)&lt;/code&gt;, the old list will no longer exist, and *our code will create a fresh one when we run it again!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(myList)
for(i in c(&amp;#39;G&amp;#39;,&amp;#39;H&amp;#39;,&amp;#39;I&amp;#39;)){
  if(!exists(&amp;#39;myList&amp;#39;)) myList = list()
  myList[[i]] = paste0(&amp;#39;This loop is on &amp;#39;,i)
}
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $G
## [1] &amp;quot;This loop is on G&amp;quot;
## 
## $H
## [1] &amp;quot;This loop is on H&amp;quot;
## 
## $I
## [1] &amp;quot;This loop is on I&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’re going to use this in your breakout rooms today. You’re going to be asked to run some code that stores a plot in a list. To reset the list that stores things, just use &lt;code&gt;rm(listName)&lt;/code&gt; (where &lt;code&gt;listName&lt;/code&gt; is the name of the list).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;todays-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Today’s Example&lt;/h2&gt;
&lt;p&gt;Our goal today is to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;See the code that produced this week’s Content&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why? Because it helps to illustrate the &lt;em&gt;true&lt;/em&gt; sources of noise in the data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;See what larger sample sizes and higher/lower irreducible error does to our Bias vs. Variance tradeoff.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simulation&lt;/h3&gt;
&lt;p&gt;We will use the exact code from Content 9, which I have reproduced here. I have removed the in-between parts with notation so we can focus on the example. I have &lt;strong&gt;copied all of the relevant code into one chunk down at the bottom as well&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We’ll need the following libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(patchwork)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here, I’ve made a little change to Content 9’s code so we can play with sample size &lt;code&gt;NN&lt;/code&gt; and the SD of the irreducible Bayes error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NN = 100   #----&amp;gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = .75   #-----&amp;gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;begin-content-9-code-here&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Begin Content 9 code here:&lt;/h4&gt;
&lt;p&gt;We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f = function(x) {
  x ^ 2
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  data.frame(x, y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To completely specify the data generating process, we have made more model assumptions than simply &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[Y \mid X = x] = x^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{V}[Y \mid X = x] = \sigma ^ 2\)&lt;/span&gt;. In particular,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from a uniform distribution over &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; are independent.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from the conditional normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using this setup, we will generate datasets, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, with a sample size &lt;span class=&#34;math inline&#34;&gt;\(NN\)&lt;/span&gt; and fit four models.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\texttt{predict(fit0, x)} &amp;amp;= \hat{f}_0(x) = \hat{\beta}_0\\
\texttt{predict(fit1, x)} &amp;amp;= \hat{f}_1(x) = \hat{\beta}_0 + \hat{\beta}_1 x \\
\texttt{predict(fit2, x)} &amp;amp;= \hat{f}_2(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \\
\texttt{predict(fit9, x)} &amp;amp;= \hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
sim_data = get_sim_data(f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/09-example_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;p&gt;We will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates for &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; given by these four models at the point &lt;span class=&#34;math inline&#34;&gt;\(x = 0.90\)&lt;/span&gt;. We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compile all of the results:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/09-example_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;864&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;In your breakout room and using the code below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First Breakout&lt;/strong&gt;: set &lt;code&gt;NN&lt;/code&gt; = 100, the value we used in our Content 9 lecture. The value is set in one of the first code chunks. Step through the code to get your finalPlot and make sure it looks like the plot in Content 9. I changed the code to use ggplot (easier to save output), so the formatting and colors will be different - that’s OK, we want to get the same results, not copy the layout of the plot. Note that at the end of the code, a list is created that will hold all of your results. In case you need to clear this list, &lt;code&gt;rm(FinalResults)&lt;/code&gt; will do so and the code will initate a new blank list to hold subsequent results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Second Breakout&lt;/strong&gt;: set NN to a larger number. Usually, more data means more precise predictions. Run your code again stepping through it, until you get to this plot. Note that at the end of the code provided, there is a list that aggregates your results. &lt;strong&gt;Repeat this&lt;/strong&gt; with a 3rd, even larger value for NN. Don’t go much beyond 50,000 or it’ll take too long. Your &lt;code&gt;FinalResults&lt;/code&gt; list should have 3 elements in it. Use &lt;code&gt;wrap_plots(FinalResults, nrow = 1)&lt;/code&gt; to see all 3 side-by-side.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Third Breakout&lt;/strong&gt;: Finally, change the &lt;code&gt;SD.of.Bayes.Error&lt;/code&gt; value to make it higher or lower. Remember, this is the &lt;em&gt;irreducible&lt;/em&gt; error. Run your code again with your first, second, and third different value for sample size &lt;code&gt;NN&lt;/code&gt;. You should have 6 plots in your &lt;code&gt;FinalResults&lt;/code&gt; list - 3 from before, and 3 more with the new SD of Bayes Error. Use &lt;code&gt;wrap_plots&lt;/code&gt; with the right number of rows to see a 2x3 grid of the results.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usually we think larger sample sizes and lower error lead to better overall prediction. Do we see any change in the bias vs. tradeoff relationship with lower/higher sample size &lt;code&gt;NN&lt;/code&gt; and lower/higher SD of Bayes Error?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;heres-the-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Here’s the code&lt;/h3&gt;
&lt;p&gt;I’ve merged all of the code together for you here. Copy this into a new .R script - you don’t need to use a full Markdown.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(patchwork)



NN = 100   #----&amp;gt; In class, we will change this to 
#                 see how our results change in response
SD.of.Bayes.Error = 0.75   #-----&amp;gt; This, too, will change. 

# Note that both of these are used in the next chunk(s) to generate data.







f = function(x) {
  x ^ 2
}



get_sim_data = function(f, sample_size = NN) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = SD.of.Bayes.Error)
  y = f(x) + eps
  data.frame(x, y)
}



set.seed(1)
sim_data = get_sim_data(f)



fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)





set.seed(42)
plot(y ~ x, data = sim_data, col = &amp;quot;grey&amp;quot;, pch = 20,
     main = &amp;quot;Four Polynomial Models fit to a Simulated Dataset&amp;quot;)

grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = &amp;quot;black&amp;quot;, lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = &amp;quot;dodgerblue&amp;quot;,  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = &amp;quot;firebrick&amp;quot;,   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = &amp;quot;springgreen&amp;quot;, lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = &amp;quot;darkorange&amp;quot;,  lwd = 2, lty = 5)

legend(&amp;quot;topleft&amp;quot;, 
       c(&amp;quot;y ~ 1&amp;quot;, &amp;quot;y ~ poly(x, 1)&amp;quot;, &amp;quot;y ~ poly(x, 2)&amp;quot;,  &amp;quot;y ~ poly(x, 9)&amp;quot;, &amp;quot;truth&amp;quot;), 
       col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;, &amp;quot;black&amp;quot;), lty = c(2, 3, 4, 5, 1), lwd = 2)



set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)



for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = get_sim_data(f, sample_size = NN)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}



predictions.proc = (predictions)
colnames(predictions.proc) = c(&amp;quot;0&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;9&amp;quot;)
predictions.proc = as.data.frame(predictions.proc)

tall_predictions = tidyr::gather(predictions.proc, factor_key = TRUE)

## Here, you can save your ggplot output 
FinalPlot &amp;lt;- ggplot(tall_predictions, aes(x = key, y = value, col = as.factor(key))) + 
                      geom_boxplot() +
                      geom_jitter(alpha = .5) +
                      geom_hline(yintercept = f(x = .90)) +
                      labs(col = &amp;#39;Model&amp;#39;, x = &amp;#39;Model&amp;#39;, y = &amp;#39;Prediction&amp;#39;, title = paste0(&amp;#39;Bias v Var - Sample Size: &amp;#39;,NN), subtitle = paste0(&amp;#39;SD of Bayes Err: &amp;#39;,SD.of.Bayes.Error)) +
                      theme_bw()



FinalPlot


## This is going to aggregate your results for you:
if(!exists(&amp;#39;FinalResults&amp;#39;)) FinalResults = list()

FinalResults[[paste0(&amp;#39;finalPlot.NN.&amp;#39;,NN,&amp;#39;.SDBayes.&amp;#39;,SD.of.Bayes.Error)]] = FinalPlot

# 
# boxplot(value ~ key, data = tall_predictions, border = &amp;quot;darkgrey&amp;quot;, xlab = &amp;quot;Polynomial Degree&amp;quot;, ylab = &amp;quot;Predictions&amp;quot;,
#         main = &amp;quot;Simulated Predictions for Polynomial Models&amp;quot;)
# grid()
# stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = &amp;quot;jitter&amp;quot;, jitter = 0.15, pch = 1, col = c(&amp;quot;dodgerblue&amp;quot;, &amp;quot;firebrick&amp;quot;, &amp;quot;springgreen&amp;quot;, &amp;quot;darkorange&amp;quot;))
# abline(h = f(x = 0.90), lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to output &lt;em&gt;whatever is in your list&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## This not run automatically.
## To plot the whole list of ggplot objects in FinalResults:
wrap_plots(FinalResults, nrow = 2, guides = &amp;#39;collect&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonparametric Regression</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/08-example/</link>
      <pubDate>Thu, 18 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/08-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#our-data-and-goal&#34;&gt;Our data and goal&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#some-functions&#34;&gt;Some functions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#breakout-1&#34;&gt;Breakout #1&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#breakout-1-discussion&#34;&gt;Breakout #1 discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#automating-models&#34;&gt;Automating models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#loops&#34;&gt;Loops&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#breakout-2&#34;&gt;Breakout #2&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#breakout-2-discussion&#34;&gt;Breakout #2 discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-rmse-from-the-list&#34;&gt;Getting RMSE from the list&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#breakout-3-if-time&#34;&gt;Breakout #3 (if time)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;our-data-and-goal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Our data and goal&lt;/h2&gt;
&lt;p&gt;We want to use the &lt;code&gt;wooldridge::wage2&lt;/code&gt; data on wages to generate and tune a non-parametric model of wages using a regression tree.&lt;/p&gt;
&lt;p&gt;We’ve learned that our RMSE calculations have a hard time with &lt;code&gt;NA&lt;/code&gt;s in the data. So let’s use the &lt;code&gt;skim&lt;/code&gt; output to drop variables with &lt;code&gt;NA&lt;/code&gt; (see &lt;code&gt;n_missing&lt;/code&gt;). Of course, there are other things we can do (impute the &lt;code&gt;NA&lt;/code&gt;s, or make dummies for them), but for now, it’s easiest to drop.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wage_clean = wage2 %&amp;gt;%
  dplyr::select(-brthord, -meduc, -feduc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once cleaned, we should be able to use &lt;code&gt;rpart(wage ~ ., data = wage_clean)&lt;/code&gt; and not have any &lt;code&gt;NA&lt;/code&gt;s in our prediction.&lt;/p&gt;
&lt;div id=&#34;some-functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Some functions&lt;/h3&gt;
&lt;p&gt;These functions are taken from our previous Content and Examples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;breakout-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Breakout #1&lt;/h2&gt;
&lt;p&gt;I’m going to assign you to breakout rooms to work in groups of 3-4 on live-coding. One person in the group will need to have Rstudio up, be able to share screen, and have the correct packages loaded (&lt;code&gt;caret&lt;/code&gt;, &lt;code&gt;rpart&lt;/code&gt;, and &lt;code&gt;rpart.plot&lt;/code&gt;, plus &lt;code&gt;skimr&lt;/code&gt;). Copy the &lt;code&gt;rmse&lt;/code&gt; and &lt;code&gt;get_rmse&lt;/code&gt; code into a blank R script (you don’t have to use RMarkdown, just use a blank R script and run from there).&lt;/p&gt;
&lt;p&gt;For the first breakout, all I want you to do is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Estimate a default regression tree on &lt;code&gt;wage_clean&lt;/code&gt; using the default parameters.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;rpart.plot&lt;/code&gt; to vizualize your regression tree, and &lt;em&gt;talk through the interpretation&lt;/em&gt; with each other.&lt;/li&gt;
&lt;li&gt;Calculate the RMSE for your regression tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll bring you back in about 5 minutes. Remember, you can use &lt;code&gt;?wage2&lt;/code&gt; to see the variable names. Make sure you know what variables are showing up in the plot and explaining &lt;code&gt;wage&lt;/code&gt; in your model. You may find something odd at first and may need to drop more variables…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5 minutes&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;breakout-1-discussion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Breakout #1 discussion&lt;/h3&gt;
&lt;p&gt;Let’s choose a group to share their plot and discuss the results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;automating-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automating models&lt;/h2&gt;
&lt;p&gt;Let’s talk about a little code shortcut that helps iterate through your model selection.&lt;/p&gt;
&lt;p&gt;First, before, we used &lt;code&gt;list()&lt;/code&gt; to store all of our models. This is because &lt;code&gt;list()&lt;/code&gt; can “hold” anything at all, unlike a &lt;code&gt;matrix&lt;/code&gt;, which is only numeric, or a &lt;code&gt;data.frame&lt;/code&gt; which needs all rows in a column to be the same data type. &lt;code&gt;list()&lt;/code&gt; is also recursive, so each element in a list can be a list. Of lists. Of lists!&lt;/p&gt;
&lt;p&gt;Lists are also really easy to add to iteratively. We can initiate an empty list using &lt;code&gt;myList &amp;lt;- list()&lt;/code&gt;, then we can add things to it. Note that we use the double &lt;code&gt;[&lt;/code&gt; to index:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList &amp;lt;- list()
myList[[&amp;#39;first&amp;#39;]] = &amp;#39;This is the first thing on my list&amp;#39;
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] &amp;quot;This is the first thing on my list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lists let you name the “containers” (much like you can name colums in a &lt;code&gt;data.frame&lt;/code&gt;). Our first one is called “first”. We can add more later:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList[[&amp;#39;second&amp;#39;]] = c(1,2,3)
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] &amp;quot;This is the first thing on my list&amp;quot;
## 
## $second
## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And still more:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList[[&amp;#39;third&amp;#39;]] = data.frame(a = c(1,2,3), b = c(&amp;#39;Albert&amp;#39;,&amp;#39;Alex&amp;#39;,&amp;#39;Alice&amp;#39;))
print(myList)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] &amp;quot;This is the first thing on my list&amp;quot;
## 
## $second
## [1] 1 2 3
## 
## $third
##   a      b
## 1 1 Albert
## 2 2   Alex
## 3 3  Alice&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a &lt;code&gt;data.frame&lt;/code&gt; in there! We can use &lt;code&gt;lapply&lt;/code&gt; to do something to every element in the list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lapply(myList, length) # the length() function with the first entry being the list element&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $first
## [1] 1
## 
## $second
## [1] 3
## 
## $third
## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get back a list of equal length but each (still-named) container is now the length of the original list’s contents.&lt;/p&gt;
&lt;div id=&#34;loops&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Loops&lt;/h3&gt;
&lt;p&gt;R has a very useful looping function that takes the form:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in c(&amp;#39;first&amp;#39;,&amp;#39;second&amp;#39;,&amp;#39;third&amp;#39;)){
print(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;first&amp;quot;
## [1] &amp;quot;second&amp;quot;
## [1] &amp;quot;third&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, R is repeating the thing in the loop (&lt;code&gt;print(i)&lt;/code&gt;) with a different value for &lt;code&gt;i&lt;/code&gt; each time. R repeats only what is &lt;em&gt;inside the curly-brackets&lt;/em&gt;, then when it reaches the close-curly-bracket, it goes back to the top, changes &lt;code&gt;i&lt;/code&gt; to the next element, and repeats.&lt;/p&gt;
&lt;p&gt;We can use this to train our models. First, we clear our list object &lt;code&gt;myList&lt;/code&gt; by setting it equal to an empty list. Then, we loop over some regression tree tuning parameters. First, we have to figure out how to use the loop to set a &lt;em&gt;unique&lt;/em&gt; name for each list container. To do this, we’ll use &lt;code&gt;paste0(&#39;Tuning&#39;,i)&lt;/code&gt; which will result in a character string of &lt;code&gt;Tuning0&lt;/code&gt; when &lt;code&gt;i=0&lt;/code&gt;, &lt;code&gt;Tuning00.1&lt;/code&gt; when &lt;code&gt;i=0.01&lt;/code&gt;, etc.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myList &amp;lt;- list()   # resets the list. Otherwise, you&amp;#39;ll just be adding to your old list!

for(i in c(0, 0.01, 0.02)){
  myList[[paste0(&amp;#39;Tuning&amp;#39;,i)]] = rpart(wage ~ ., data = wage_clean, cp = i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you use &lt;code&gt;names(myList)&lt;/code&gt; you’ll see the result of our &lt;code&gt;paste0&lt;/code&gt; naming. If you want to see the plotted results, you can use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(myList[[&amp;#39;Tuning0.01&amp;#39;]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/08-example_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;breakout-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Breakout #2&lt;/h2&gt;
&lt;p&gt;Let’s send you back to your breakout rooms. Using the loop method, generate 5 regression trees to explain &lt;code&gt;wage&lt;/code&gt; in &lt;code&gt;wage_clean&lt;/code&gt;. You can iterate through values of &lt;code&gt;cp&lt;/code&gt;, the complexity parameter, or &lt;code&gt;minsplit&lt;/code&gt;, the minimum # of points that have to be in each split.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;10 minutes&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;breakout-2-discussion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Breakout #2 discussion&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-rmse-from-the-list&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting RMSE from the list&lt;/h2&gt;
&lt;p&gt;Finally, we want to move towards getting the RMSE for each of these trees. We’ve done this before using &lt;code&gt;lapply&lt;/code&gt;. Let’s introduce a neat coding shortcut, the &lt;code&gt;anonymous function&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;myRMSE &amp;lt;- lapply(myList, function(x){
                  get_rmse(x, wage_clean, &amp;#39;wage&amp;#39;)
                  } )

print(myRMSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $Tuning0
## [1] 292.5856
## 
## $Tuning0.01
## [1] 359.4592
## 
## $Tuning0.02
## [1] 367.2748&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, it gets us the right answer, but whaaaaaat is going on? Curly brackets? &lt;code&gt;x&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;This is an “anonymous function”, or a function created on the fly. Here’s how it works in &lt;code&gt;lapply&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first argument is the list you want to do something to&lt;/li&gt;
&lt;li&gt;The second argument would usually be the function you want to apply, like &lt;code&gt;get_rmse&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Here, we’re going to ask R to &lt;em&gt;temporarily&lt;/em&gt; create a function that takes one argument, &lt;code&gt;x&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is going to be each list element in &lt;code&gt;myList&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Think of it as a loop:
&lt;ul&gt;
&lt;li&gt;Take the first element of &lt;code&gt;myList&lt;/code&gt; and refer to it as &lt;code&gt;x&lt;/code&gt;. Run the function.&lt;/li&gt;
&lt;li&gt;Then it’ll take the second element of &lt;code&gt;myList&lt;/code&gt; and refer to it as &lt;code&gt;x&lt;/code&gt; and run the function.&lt;/li&gt;
&lt;li&gt;Repeat until all elements of &lt;code&gt;x&lt;/code&gt; have been used.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Once the anonymous function has been applied to &lt;code&gt;x&lt;/code&gt;, the result is passed back and saved as the new element of the list output, always in the same position from where the &lt;code&gt;x&lt;/code&gt; was taken.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So you can think of it like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x = myList[[1]]
myList[[1]] = get_rmse(x, wage_clean, &amp;#39;wage&amp;#39;)

x = myList[[2]]
myList[[2]] = get_rmse(x, wage_clean, &amp;#39;wage&amp;#39;)

x = myList[[3]]
myList[[3]] = get_rmse(x, wage_clean, &amp;#39;wage&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can refer to &lt;code&gt;myList&lt;/code&gt; by name:
- &lt;code&gt;myList[[&#39;Tuning0.01&#39;]]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Or you can refer to &lt;code&gt;myList&lt;/code&gt; by the index:
- &lt;code&gt;myList[[2]]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Just like you can refer to &lt;code&gt;data.frame&lt;/code&gt; columns by name or by index.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;breakout-3-if-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Breakout #3 (if time)&lt;/h2&gt;
&lt;p&gt;Let’s send you to the breakout rooms one more time, and use &lt;code&gt;lapply&lt;/code&gt; to get a list of your RMSE’s (one for each of your models). Note that we are not yet splitting into &lt;code&gt;test&lt;/code&gt; and &lt;code&gt;train&lt;/code&gt; (which you &lt;strong&gt;will&lt;/strong&gt; need to do on your lab assignment).&lt;/p&gt;
&lt;p&gt;Once you have your list, &lt;strong&gt;create the plot of RMSEs&lt;/strong&gt; similar to the one we looked at in Content this week. Note: you can use &lt;code&gt;unlist(myRMSE)&lt;/code&gt; to get a numeric vector of the RMSE’s (as long as all of the elements in &lt;code&gt;myRMSE&lt;/code&gt; are numeric). Then, it’s a matter of plotting either with base &lt;code&gt;plot&lt;/code&gt; or with &lt;code&gt;ggplot&lt;/code&gt; (if you use &lt;code&gt;ggplot&lt;/code&gt; you’ll have to &lt;code&gt;tidy&lt;/code&gt; the RMSE by adding the index column or naming the x-axis).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remaining time&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing Uncertainty</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/04-example/</link>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/04-example/</guid>
      <description>
&lt;script src=&#34;https://datavizm20.classes.andrewheiss.com/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://datavizm20.classes.andrewheiss.com/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#part-1-statistical-inference-and-polls&#34;&gt;Part 1: Statistical Inference and Polls&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#polls&#34;&gt;Polls&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-sampling-model-for-polls&#34;&gt;The sampling model for polls&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#populations-samples-parameters-and-estimates&#34;&gt;Populations, samples, parameters, and estimates&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-sample-average&#34;&gt;The sample average&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameters&#34;&gt;Parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#polling-versus-forecasting&#34;&gt;Polling versus forecasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#properties-of-our-estimate-expected-value-and-standard-error&#34;&gt;Properties of our estimate: expected value and standard error&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clt&#34;&gt;Central Limit Theorem&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a-monte-carlo-simulation&#34;&gt;A Monte Carlo simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-spread&#34;&gt;The spread&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-why-not-run-a-very-large-poll&#34;&gt;Bias: why not run a very large poll?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#part-2-supplemental-additional-visualization-techniques&#34;&gt;Part 2: (Supplemental) Additional Visualization Techniques&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34;&gt;Code&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-and-clean-data&#34;&gt;Load and clean data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plots&#34;&gt;Density plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#box-violin-and-rain-cloud-plots&#34;&gt;Box, violin, and rain cloud plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Probabilistic thinking is central in the human experience. How we describe that thinking is mixed, but most of the time we use (rather imprecise) language. With only a few moments of searching, one can find thousands of articles that use probabilistic words to describe events. Here are some examples:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“‘Highly unlikely’ State of the Union will happen amid shutdown” – The Hill&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Tiger Woods makes Masters 15th and most improbable major” – Fox&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Trump predicts ‘very good chance’ of China trade deal” – CNN&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet people don’t have a good sense of what these things mean. Uncertainty is key to data analytics: if we were certain of things, A study in the 1960s explored the perception of probabilistic words like these among NATO officers. A more modern replication of this found the following basic pattern:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/img/words.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A deep, basic fact about humans is that we struggle to understand probabilities. But visualizing things can help. The graphic above shows the uncertainty about uncertainty (very meta). We can convey all manner of uncertainty with clever graphics. Today’s practical example works through some of the myriad of ways to visualize variable data. We’ll cover some territory that we’ve already hit, in hopes of locking in some key concepts.&lt;/p&gt;
&lt;div id=&#34;part-1-statistical-inference-and-polls&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part 1: Statistical Inference and Polls&lt;/h1&gt;
&lt;p&gt;In this Example we will describe, in some detail, how poll aggregators such as FiveThirtyEight use data to predict election outcomes. To understand how they do this, we first need to learn the basics of &lt;em&gt;Statistical Inference&lt;/em&gt;, the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;div id=&#34;polls&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Polls&lt;/h2&gt;
&lt;p&gt;Opinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as &lt;em&gt;inference&lt;/em&gt; and it is the main topic of this chapter.&lt;/p&gt;
&lt;p&gt;Perhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.&lt;/p&gt;
&lt;p&gt;Elections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.&lt;/p&gt;
&lt;p&gt;Although typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.&lt;/p&gt;
&lt;p&gt;Real Clear Politics&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Poll
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Date
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Sample
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
MoE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Clinton
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Trump
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Spread
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Final Results
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48.2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +2.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RCP Average
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/1 - 11/7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +3.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bloomberg
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/4 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
799 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IBD
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/4 - 11/7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1107 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Economist
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/4 - 11/7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3669 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
–
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
49.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
LA Times
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/1 - 11/7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2935 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
4.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
47.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Trump +3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ABC
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/3 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2220 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
49.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FOX News
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/3 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1295 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Monmouth
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/3 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
748 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3.6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NBC News
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/3 - 11/5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1282 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
48.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CBS News
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/2 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1426 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
47.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
43.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Reuters
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
11/2 - 11/6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2196 LV
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Clinton +5
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- (Source: [Real Clear Politics](https://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html)) --&gt;
&lt;p&gt;Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we will go into some detail on this later.&lt;/p&gt;
&lt;p&gt;Let’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different &lt;em&gt;spread&lt;/em&gt;: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled &lt;strong&gt;MoE&lt;/strong&gt; which stands for &lt;em&gt;margin of error&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In this example, we will show how the probability concepts we learned in the previous content can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define &lt;em&gt;estimates&lt;/em&gt; and &lt;em&gt;margins of errors&lt;/em&gt;, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: &lt;em&gt;confidence intervals&lt;/em&gt; and &lt;em&gt;p-values&lt;/em&gt;. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.&lt;/p&gt;
&lt;p&gt;We start by connecting probability theory to the task of using polls to learn about a population.&lt;/p&gt;
&lt;div id=&#34;the-sampling-model-for-polls&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The sampling model for polls&lt;/h3&gt;
&lt;p&gt;To help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this hypothetical urn.&lt;/p&gt;
&lt;p&gt;Before making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you 10 cents per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay \$25 to collect your \$25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;dslabs&lt;/strong&gt; package includes a function that shows a random draw from this urn:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
take_poll(25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/first-simulated-poll-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Think about how you would construct your interval based on the data shown above.&lt;/p&gt;
&lt;p&gt;We have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;populations-samples-parameters-and-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Populations, samples, parameters, and estimates&lt;/h2&gt;
&lt;p&gt;We want to predict the proportion of blue beads in the urn. Let’s call this quantity &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, which then tells us the proportion of red beads &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt;, and the spread &lt;span class=&#34;math inline&#34;&gt;\(p - (1-p)\)&lt;/span&gt;, which simplifies to &lt;span class=&#34;math inline&#34;&gt;\(2p - 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In statistical textbooks, the beads in the urn are called the &lt;em&gt;population&lt;/em&gt;. The proportion of blue beads in the population &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is called a &lt;em&gt;parameter&lt;/em&gt;. The 25 beads we see in the previous plot are called a &lt;em&gt;sample&lt;/em&gt;. The task of statistical inference is to predict the parameter &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; using the observed data in the sample.&lt;/p&gt;
&lt;p&gt;Can we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; &amp;gt; .9 or &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; &amp;lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?&lt;/p&gt;
&lt;p&gt;We want to construct an estimate of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample &lt;span class=&#34;math inline&#34;&gt;\(0.48\)&lt;/span&gt; must be at least related to the actual proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. But do we simply predict &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to be 0.48? First, remember that the sample proportion is a random variable. If we run the command &lt;code&gt;take_poll(25)&lt;/code&gt; four times, we get a different answer each time, since the sample proportion is a random variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/four-simulated-polls-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.&lt;/p&gt;
&lt;div id=&#34;the-sample-average&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The sample average&lt;/h3&gt;
&lt;p&gt;Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an &lt;em&gt;estimate&lt;/em&gt; of the parameter &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Once we have this estimate, we can easily report an estimate for the spread &lt;span class=&#34;math inline&#34;&gt;\(2p-1\)&lt;/span&gt;, but for simplicity we will illustrate the concepts for estimating &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We start by defining the random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as: &lt;span class=&#34;math inline&#34;&gt;\(X=1\)&lt;/span&gt; if we pick a blue bead at random and &lt;span class=&#34;math inline&#34;&gt;\(X=0\)&lt;/span&gt; if it is red. This implies that the population is a list of 0s and 1s. If we sample &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; beads, then the average of the draws &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_N\)&lt;/span&gt; is equivalent to the proportion of blue beads in our sample. This is because adding the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;s is equivalent to counting the blue beads and dividing this count by the total &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is equivalent to computing a proportion. We use the symbol &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant &lt;span class=&#34;math inline&#34;&gt;\(1/N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\bar{X} = 1/N \times \sum_{i=1}^N X_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, the proportion of blue beads.&lt;/p&gt;
&lt;p&gt;Here we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to &lt;strong&gt;estimate&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parameters&lt;/h3&gt;
&lt;p&gt;Just like we use variables to define unknowns in systems of equations, in statistical inference we define &lt;em&gt;parameters&lt;/em&gt; to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; to represent this quantity. &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we are going to &lt;em&gt;estimate this parameter&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polling-versus-forecasting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Polling versus forecasting&lt;/h3&gt;
&lt;p&gt;Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for that moment and not for election day. The &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;properties-of-our-estimate-expected-value-and-standard-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Properties of our estimate: expected value and standard error&lt;/h3&gt;
&lt;p&gt;To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt;. Remember that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is the sum of independent draws so the rules we covered in the probability chapter apply.&lt;/p&gt;
&lt;p&gt;Using what we have learned, the expected value of the sum &lt;span class=&#34;math inline&#34;&gt;\(N\bar{X}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(N \times\)&lt;/span&gt; the average of the urn, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. So dividing by the non-random constant &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; gives us that the expected value of the average &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We can write it using our mathematical notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}(\bar{X}) = p
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can also use what we learned to figure out the standard error: the standard error of the sum is &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{N} \times\)&lt;/span&gt; the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is &lt;span class=&#34;math inline&#34;&gt;\((1-0) \sqrt{p (1-p)}\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p (1-p)}\)&lt;/span&gt;. Because we are dividing the sum by &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, we arrive at the following formula for the standard error of the average:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This result reveals the power of polls. The expected value of the sample proportion &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is the parameter of interest &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and we can make the standard error as small as we want by increasing &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. The law of large numbers tells us that with a large enough poll, our estimate converges to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?&lt;/p&gt;
&lt;p&gt;One problem is that we do not know &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, so we can’t compute the standard error. However, for illustrative purposes, let’s assume that &lt;span class=&#34;math inline&#34;&gt;\(p=0.51\)&lt;/span&gt; and make a plot of the standard error versus the sample size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/standard-error-versus-sample-size-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and &lt;span class=&#34;math inline&#34;&gt;\(p=0.51\)&lt;/span&gt;, the standard error is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(p*(1-p))/sqrt(1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01580823&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or 1.5 percentage points. So even with large polls, for close elections, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;clt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Central Limit Theorem&lt;/h2&gt;
&lt;p&gt;The Central Limit Theorem (CLT) tells us that the distribution function for a sum of draws is approximately normal. You also may recall that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is approximately normal.&lt;/p&gt;
&lt;p&gt;In summary, we have that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; has an approximately normal distribution with expected value &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and standard error &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p(1-p)/N}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now how does this help us? Suppose we want to know what is the probability that we are within 1% from &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We are basically asking what is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(| \bar{X} - p| \leq .01)
\]&lt;/span&gt;
which is the same as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(\bar{X}\leq p + .01) - \mbox{Pr}(\bar{X} \leq p - .01)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Can we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, on the left. Since &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the expected value and &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SE}(\bar{X}) = \sqrt{p(1-p)/N}\)&lt;/span&gt; is the standard error we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq \frac{ \,.01} {\mbox{SE}(\bar{X})} \right) -
\mbox{Pr}\left(Z \leq - \frac{ \,.01} {\mbox{SE}(\bar{X})} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One problem we have is that since we don’t know &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we don’t know &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SE}(\bar{X})\)&lt;/span&gt;. But it turns out that the CLT still works if we estimate the standard error by using &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; in place of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We say that we &lt;em&gt;plug-in&lt;/em&gt; the estimate. Our estimate of the standard error is therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N}
\]&lt;/span&gt;
In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now we continue with our calculation, but dividing by &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mbox{SE}}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N})\)&lt;/span&gt; instead. In our first sample we had 12 blue and 13 red so &lt;span class=&#34;math inline&#34;&gt;\(\bar{X} = 0.48\)&lt;/span&gt; and our estimate of standard error is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x_hat &amp;lt;- 0.48
se &amp;lt;- sqrt(x_hat*(1-x_hat)/25)
se&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09991997&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now we can answer the question of the probability of being close to &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. The answer is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(0.01/se) - pnorm(-0.01/se)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.07971926&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, there is a small chance that we will be close. A poll of only &lt;span class=&#34;math inline&#34;&gt;\(N=25\)&lt;/span&gt; people is not really very useful, at least not for a close election.&lt;/p&gt;
&lt;p&gt;Earlier we mentioned the &lt;em&gt;margin of error&lt;/em&gt;. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1.96*se&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1958431&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq \, 1.96\,\mbox{SE}(\bar{X})  / \mbox{SE}(\bar{X}) \right) -
\mbox{Pr}\left(Z \leq - 1.96\, \mbox{SE}(\bar{X}) / \mbox{SE}(\bar{X}) \right)
\]&lt;/span&gt;
which is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq 1.96 \right) -
\mbox{Pr}\left(Z \leq - 1.96\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which we know is about 95%:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(1.96)-pnorm(-1.96)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9500042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hence, there is a 95% probability that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; will be within &lt;span class=&#34;math inline&#34;&gt;\(1.96\times \hat{SE}(\bar{X})\)&lt;/span&gt;, in our case within about 0.2, of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.&lt;/p&gt;
&lt;p&gt;In summary, the CLT tells us that our poll based on a sample size of &lt;span class=&#34;math inline&#34;&gt;\(25\)&lt;/span&gt; is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.&lt;/p&gt;
&lt;p&gt;From the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt;=0.48 with a sample size of 2,000, our standard error &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mbox{SE}}(\bar{X})\)&lt;/span&gt; would have been 0.0111714. So our result is an estimate of &lt;code&gt;48&lt;/code&gt;% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.&lt;/p&gt;
&lt;div id=&#34;a-monte-carlo-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Monte Carlo simulation&lt;/h3&gt;
&lt;p&gt;Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
N &amp;lt;- 1000
x_hat &amp;lt;- replicate(B, {
  x &amp;lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem is, of course, we don’t know &lt;code&gt;p&lt;/code&gt;. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function &lt;code&gt;take_poll(n=1000)&lt;/code&gt; instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.&lt;/p&gt;
&lt;p&gt;One thing we therefore do to corroborate theoretical results is to pick one or several values of &lt;code&gt;p&lt;/code&gt; and run the simulations. Let’s set &lt;code&gt;p=0.45&lt;/code&gt;. We can then simulate a poll:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- 0.45
N &amp;lt;- 1000

x &amp;lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
x_hat &amp;lt;- mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this particular sample, our estimate is &lt;code&gt;x_hat&lt;/code&gt;. We can use that code to do a Monte Carlo simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
x_hat &amp;lt;- replicate(B, {
  x &amp;lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p))
  mean(x)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To review, the theory tells us that &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is approximately normally distributed, has expected value &lt;span class=&#34;math inline&#34;&gt;\(p=\)&lt;/span&gt; 0.45 and standard error &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{p(1-p)/N}\)&lt;/span&gt; = 0.0157321. The simulation confirms this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4500761&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(x_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01579523&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A histogram and qq-plot confirm that the normal approximation is accurate as well:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/normal-approximation-for-polls-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course, in real life we would never be able to run such an experiment because we don’t know &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. But we could run it for various values of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;N&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-spread&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The spread&lt;/h3&gt;
&lt;p&gt;The competition is to predict the spread, not the proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. However, because we are assuming there are only two parties, we know that the spread is &lt;span class=&#34;math inline&#34;&gt;\(p - (1-p) = 2p - 1\)&lt;/span&gt;. As a result, everything we have done can easily be adapted to an estimate of &lt;span class=&#34;math inline&#34;&gt;\(2p - 1\)&lt;/span&gt;. Once we have our estimate &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mbox{SE}}(\bar{X})\)&lt;/span&gt;, we estimate the spread with &lt;span class=&#34;math inline&#34;&gt;\(2\bar{X} - 1\)&lt;/span&gt; and, since we are multiplying by 2, the standard error is &lt;span class=&#34;math inline&#34;&gt;\(2\hat{\mbox{SE}}(\bar{X})\)&lt;/span&gt;. Note that subtracting 1 does not add any variability so it does not affect the standard error.&lt;/p&gt;
&lt;p&gt;For our 25 item sample above, our estimate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is &lt;code&gt;.48&lt;/code&gt; with margin of error &lt;code&gt;.20&lt;/code&gt; and our estimate of the spread is &lt;code&gt;0.04&lt;/code&gt; with margin of error &lt;code&gt;.40&lt;/code&gt;. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, we have it for the spread &lt;span class=&#34;math inline&#34;&gt;\(2p-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-why-not-run-a-very-large-poll&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bias: why not run a very large poll?&lt;/h3&gt;
&lt;p&gt;For realistic values of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/standard-error-versus-p-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this shortly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-supplemental-additional-visualization-techniques&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part 2: (Supplemental) Additional Visualization Techniques&lt;/h1&gt;
&lt;p&gt;For this second part of the example, we’re going to use historical weather data from &lt;a href=&#34;https://darksky.net/forecast/33.7546,-84.39/us12/en&#34;&gt;Dark Sky&lt;/a&gt; about wind speed and temperature trends for downtown Atlanta (&lt;a href=&#34;https://www.google.com/maps/place/33°45&amp;#39;16.4%22N+84°23&amp;#39;24.0%22W/@33.754557,-84.3921977,17z/&#34;&gt;specifically &lt;code&gt;33.754557, -84.390009&lt;/code&gt;&lt;/a&gt;) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the &lt;a href=&#34;https://github.com/hrbrmstr/darksky&#34;&gt;&lt;strong&gt;darksky&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/data/atl-weather-2019.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;atl-weather-2019.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;div id=&#34;load-and-clean-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load and clean data&lt;/h3&gt;
&lt;p&gt;First, we load the libraries we’ll be using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(ggridges)
library(gghalves)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we load the data with &lt;code&gt;read_csv()&lt;/code&gt;. Here we assume that the CSV file lives in a subfolder in my project named &lt;code&gt;data&lt;/code&gt;. Naturally, you’ll need to point this to wherever you stashed the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_raw &amp;lt;- read_csv(&amp;quot;data/atl-weather-2019.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll add a couple columns that we can use for faceting and filling using the &lt;code&gt;month()&lt;/code&gt; and &lt;code&gt;wday()&lt;/code&gt; functions from &lt;strong&gt;lubridate&lt;/strong&gt; for extracting parts of the date:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl &amp;lt;- weather_atl_raw %&amp;gt;%
  mutate(Month = month(time, label = TRUE, abbr = FALSE),
         Day = wday(time, label = TRUE, abbr = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to go!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;histograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histograms&lt;/h3&gt;
&lt;p&gt;We can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/basic-histogram-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the &lt;code&gt;boundary&lt;/code&gt; argument for that. We also add &lt;code&gt;scale_x_continuous()&lt;/code&gt; to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;, boundary = 1) +
  scale_x_continuous(breaks = seq(0, 12, by = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/basic-histogram-better-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can show the distribution of wind speed by month if we map the &lt;code&gt;Month&lt;/code&gt; column we made onto the fill aesthetic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;, boundary = 1) +
  scale_x_continuous(breaks = seq(0, 12, by = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/histogram-by-month-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_histogram(binwidth = 1, color = &amp;quot;white&amp;quot;, boundary = 1) +
  scale_x_continuous(breaks = seq(0, 12, by = 1)) +
  guides(fill = FALSE) +
  facet_wrap(vars(Month))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/histogram-by-month-facet-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Neat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density plots&lt;/h3&gt;
&lt;p&gt;The code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the &lt;code&gt;geom&lt;/code&gt; layer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_density(color = &amp;quot;grey20&amp;quot;, fill = &amp;quot;grey50&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/basic-density-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we want, we can mess with some of the calculus options like the kernel and bandwidth:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed)) +
  geom_density(color = &amp;quot;grey20&amp;quot;, fill = &amp;quot;grey50&amp;quot;,
               bw = 0.1, kernel = &amp;quot;epanechnikov&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/density-kernel-bw-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_density(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/density-fill-by-month-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, fill = Month)) +
  geom_density(alpha = 0.5) +
  guides(fill = FALSE) +
  facet_wrap(vars(Month))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/density-facet-by-month-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or we can stack the density plots behind each other with &lt;a href=&#34;https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html&#34;&gt;&lt;strong&gt;ggridges&lt;/strong&gt;&lt;/a&gt;. For that to work, we also need to map &lt;code&gt;Month&lt;/code&gt; to the y-axis. We can reverse the y-axis so that January is at the top if we use the &lt;code&gt;fct_rev()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +
  geom_density_ridges() +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/ggridges-basic-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can add some extra information to &lt;code&gt;geom_density_ridges()&lt;/code&gt; with some other arguments like &lt;code&gt;quantile_lines&lt;/code&gt;. We can use the &lt;code&gt;quantiles&lt;/code&gt; argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) +
  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/ggridges-quantile-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have good working code, we can easily substitute in other variables by changing the x mapping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) +
  geom_density_ridges(quantile_lines = TRUE, quantiles = 2) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/ggridges-quantile-temp-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use &lt;code&gt;geom_density_ridges_gradient()&lt;/code&gt;, and we need to change the &lt;code&gt;fill&lt;/code&gt; mapping to the strange looking &lt;code&gt;..x..&lt;/code&gt;, which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, &lt;code&gt;fill = temperatureHigh&lt;/code&gt; doesn’t work 🤷:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) +
  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;) +
  labs(x = &amp;quot;High temperature&amp;quot;, y = NULL, color = &amp;quot;Temp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/ggridges-gradient-temp-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And finally, we can get &lt;em&gt;extra&lt;/em&gt; fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: &lt;code&gt;temperatureLow&lt;/code&gt; and &lt;code&gt;temperatureHigh&lt;/code&gt;. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like &lt;code&gt;linetype&lt;/code&gt;), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using &lt;code&gt;pivot_longer()&lt;/code&gt; from &lt;strong&gt;tidyr&lt;/strong&gt;, which was already loaded with &lt;code&gt;library(tidyverse)&lt;/code&gt;. In the RStudio primers, you did this same thing with &lt;code&gt;gather()&lt;/code&gt;—&lt;code&gt;pivot_longer()&lt;/code&gt; is the newer version of &lt;code&gt;gather()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_long &amp;lt;- weather_atl %&amp;gt;%
  pivot_longer(cols = c(temperatureLow, temperatureHigh),
               names_to = &amp;quot;temp_type&amp;quot;,
               values_to = &amp;quot;temp&amp;quot;) %&amp;gt;%
  # Clean up the new temp_type column so that &amp;quot;temperatureHigh&amp;quot; becomes &amp;quot;High&amp;quot;, etc.
  mutate(temp_type = recode(temp_type,
                            temperatureHigh = &amp;quot;High&amp;quot;,
                            temperatureLow = &amp;quot;Low&amp;quot;)) %&amp;gt;%
  # This is optional—just select a handful of columns
  select(time, temp_type, temp, Month)

# Show the first few rows
head(weather_atl_long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   time                temp_type  temp Month  
##   &amp;lt;dttm&amp;gt;              &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;ord&amp;gt;  
## 1 2019-01-01 05:00:00 Low        50.6 January
## 2 2019-01-01 05:00:00 High       63.9 January
## 3 2019-01-02 05:00:00 Low        49.0 January
## 4 2019-01-02 05:00:00 High       57.4 January
## 5 2019-01-03 05:00:00 Low        53.1 January
## 6 2019-01-03 05:00:00 High       55.3 January&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a column for the temperature (&lt;code&gt;temp&lt;/code&gt;) and a column indicating if it is high or low (&lt;code&gt;temp_type&lt;/code&gt;). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the &lt;code&gt;linetype&lt;/code&gt; aesthetic to show high/low in the border of the plots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month),
                             fill = ..x.., linetype = temp_type)) +
  geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) +
  scale_fill_viridis_c(option = &amp;quot;plasma&amp;quot;) +
  labs(x = &amp;quot;High temperature&amp;quot;, y = NULL, color = &amp;quot;Temp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/ggridges-gradient-temp-high-low-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;box-violin-and-rain-cloud-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Box, violin, and rain cloud plots&lt;/h3&gt;
&lt;p&gt;Finally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the &lt;code&gt;Day&lt;/code&gt; variable we made indicating weekday:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, fill = Day)) +
  geom_boxplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/basic-boxplot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can switch this to a violin plot by just changing the &lt;code&gt;geom&lt;/code&gt; layer and mapping &lt;code&gt;Day&lt;/code&gt; to the x-axis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/basic-violin-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin() +
  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/violin-strip-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also add larger points for the daily averages. We’ll use a special layer for this: &lt;code&gt;stat_summary()&lt;/code&gt;. It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here &lt;code&gt;&#34;mean&#34;&lt;/code&gt;) and then plotting that result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin() +
  stat_summary(geom = &amp;quot;point&amp;quot;, fun = &amp;quot;mean&amp;quot;, size = 5, color = &amp;quot;white&amp;quot;) +
  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/violin-strip-mean-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also show the mean and confidence interval at the same time by changing the summary function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(y = windSpeed, x = Day, fill = Day)) +
  geom_violin() +
  stat_summary(geom = &amp;quot;pointrange&amp;quot;, fun.data = &amp;quot;mean_se&amp;quot;, size = 1, color = &amp;quot;white&amp;quot;) +
  geom_point(size = 0.5, position = position_jitter(width = 0.1)) +
  guides(fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/violin-strip-mean-ci-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Overlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use &lt;a href=&#34;https://github.com/erocoar/gghalves&#34;&gt;the &lt;strong&gt;gghalves&lt;/strong&gt; package&lt;/a&gt;, we can use special halved versions of some of these geoms like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(x = fct_rev(Day), y = temperatureHigh)) +
  geom_half_point(aes(color = Day), side = &amp;quot;l&amp;quot;, size = 0.5) +
  geom_half_boxplot(aes(fill = Day), side = &amp;quot;r&amp;quot;) +
  guides(color = FALSE, fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/gghalves-point-boxplot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note the &lt;code&gt;side&lt;/code&gt; argument for specifying which half of the column the geom goes. We can also use &lt;code&gt;geom_half_violin()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(x = fct_rev(Day), y = temperatureHigh)) +
  geom_half_point(aes(color = Day), side = &amp;quot;l&amp;quot;, size = 0.5) +
  geom_half_violin(aes(fill = Day), side = &amp;quot;r&amp;quot;) +
  guides(color = FALSE, fill = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/gghalves-point-violon-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we flip the plot, we can make a &lt;a href=&#34;https://micahallen.org/2018/03/15/introducing-raincloud-plots/&#34;&gt;rain cloud plot&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl,
       aes(x = fct_rev(Day), y = temperatureHigh)) +
  geom_half_boxplot(aes(fill = Day), side = &amp;quot;l&amp;quot;, width = 0.5, nudge = 0.1) +
  geom_half_point(aes(color = Day), side = &amp;quot;l&amp;quot;, size = 0.5) +
  geom_half_violin(aes(fill = Day), side = &amp;quot;r&amp;quot;) +
  guides(color = FALSE, fill = FALSE) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/04-example_files/figure-html/gghalves-rain-cloud-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.realclearpolitics.com&#34; class=&#34;uri&#34;&gt;http://www.realclearpolitics.com&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html&#34; class=&#34;uri&#34;&gt;http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizations</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/03-example/</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/03-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complete-code&#34;&gt;Complete code&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-and-clean-data&#34;&gt;Load and clean data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#small-multiples&#34;&gt;Small multiples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sparklines&#34;&gt;Sparklines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slopegraphs&#34;&gt;Slopegraphs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bump-charts&#34;&gt;Bump charts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Today’s example will continue (and conclude) some of the discussion from Tuesday. The code may be useful for future work.&lt;/p&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;For today’s example, we’re going to use cross-national data. But instead of using the typical &lt;code&gt;gapminder&lt;/code&gt; dataset as with the Tuesday lecture, we’re going to collect data directly from the &lt;a href=&#34;https://data.worldbank.org/&#34;&gt;World Bank’s Open Data portal&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As”). However, it may be instructive for your group projects to explore the collection process. It’s not particularly hard!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/data/wdi_raw.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;wdi_raw.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;complete-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Complete code&lt;/h2&gt;
&lt;div id=&#34;load-and-clean-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load and clean data&lt;/h3&gt;
&lt;p&gt;First, we load the libraries we’ll be using. Note: there are some new packages below. You will almost surely need to add these. Moreover, these will almost surely throw an error unless you use the &lt;code&gt;dependencies = TRUE&lt;/code&gt; argument when installing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)  # For ggplot, dplyr, and friends
library(WDI)        # For getting data from the World Bank
library(geofacet)   # For map-shaped facets
library(scales)     # For helpful scale functions like dollar()
library(ggrepel)    # For non-overlapping labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The World Bank has a ton of country-level data at &lt;a href=&#34;https://data.worldbank.org/&#34;&gt;data.worldbank.org&lt;/a&gt;. We can use &lt;a href=&#34;https://cran.r-project.org/package=WDI&#34;&gt;a package named &lt;strong&gt;WDI&lt;/strong&gt;&lt;/a&gt; (&lt;strong&gt;w&lt;/strong&gt;orld &lt;strong&gt;d&lt;/strong&gt;evelopment &lt;strong&gt;i&lt;/strong&gt;ndicators) to access their servers and download the data directly into R.&lt;/p&gt;
&lt;p&gt;To do this, we need to find the special World Bank codes for specific variables we want to get. These codes come from the URLs of the World Bank’s website. For instance, if you search for “access to electricity” at the World Bank’s website, you’ll find &lt;a href=&#34;https://data.worldbank.org/indicator/EG.ELC.ACCS.ZS&#34;&gt;this page&lt;/a&gt;. If you look at the end of the URL, you’ll see a cryptic code: &lt;code&gt;EG.ELC.ACCS.ZS&lt;/code&gt;. That’s the World Bank’s ID code for the “Access to electricity (% of population)” indicator.&lt;/p&gt;
&lt;p&gt;We can feed a list of ID codes to the &lt;code&gt;WDI()&lt;/code&gt; function to download data for those specific indicators. We want data from 1995-2015, so we set the start and end years accordingly. The &lt;code&gt;extra=TRUE&lt;/code&gt; argument means that it’ll also include other helpful details like region, aid status, etc. Without it, it would only download the indicators we listed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;indicators &amp;lt;- c(&amp;quot;SP.DYN.LE00.IN&amp;quot;,  # Life expectancy
                &amp;quot;EG.ELC.ACCS.ZS&amp;quot;,  # Access to electricity
                &amp;quot;EN.ATM.CO2E.PC&amp;quot;,  # CO2 emissions
                &amp;quot;NY.GDP.PCAP.KD&amp;quot;)  # GDP per capita
wdi_raw &amp;lt;- WDI(country = &amp;quot;all&amp;quot;, indicators, extra = TRUE,
               start = 1995, end = 2015)
head(wdi_raw)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Downloading data from the World Bank every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). It’s good practice to save this raw data as a CSV file and then work with that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(wdi_raw, &amp;quot;data/wdi_raw.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we care about reproducibility, we still want to include the code we used to get data from the World Bank, we just don’t want it to actually run. You can include chunks but not run them by setting &lt;code&gt;eval=FALSE&lt;/code&gt; in the chunk options. In this little example, we show the code for downloading the data, but we don’t evaluate the chunk. We then include a chunk that loads the data from a CSV file with &lt;code&gt;read_csv()&lt;/code&gt;, but we don’t include it (&lt;code&gt;include=FALSE&lt;/code&gt;). That way, in the knitted file we see the &lt;code&gt;WDI()&lt;/code&gt; code, but in reality it’s loading the data from CSV. Super tricky.&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;I first download data from the World Bank:
```{r get-wdi-data, eval=FALSE}
wdi_raw &amp;lt;- WDI(...)
write_csv(wdi_raw, &amp;quot;data/wdi_raw.csv&amp;quot;)
```
```{r load-wdi-data-real, include=FALSE}
wdi_raw &amp;lt;- read_csv(&amp;quot;data/wdi_raw.csv&amp;quot;)
```&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we clean up the data a little, filtering out rows that aren’t actually countries and renaming the ugly World Bank code columns to actual words:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wdi_clean &amp;lt;- wdi_raw %&amp;gt;%
  filter(region != &amp;quot;Aggregates&amp;quot;) %&amp;gt;%
  select(iso2c, country, year,
         life_expectancy = SP.DYN.LE00.IN,
         access_to_electricity = EG.ELC.ACCS.ZS,
         co2_emissions = EN.ATM.CO2E.PC,
         gdp_per_cap = NY.GDP.PCAP.KD,
         region, income)
head(wdi_clean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 9
##   iso2c country  year life_expectancy access_to_electricity co2_emissions gdp_per_cap region                income     
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;                 &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                 &amp;lt;chr&amp;gt;      
## 1 AD    Andorra  2015              NA                   100         NA         41768. Europe &amp;amp; Central Asia High income
## 2 AD    Andorra  2004              NA                   100          7.36      47033. Europe &amp;amp; Central Asia High income
## 3 AD    Andorra  2001              NA                   100          7.79      41421. Europe &amp;amp; Central Asia High income
## 4 AD    Andorra  2002              NA                   100          7.59      42396. Europe &amp;amp; Central Asia High income
## 5 AD    Andorra  2014              NA                   100          5.83      40790. Europe &amp;amp; Central Asia High income
## 6 AD    Andorra  1995              NA                   100          6.66      32918. Europe &amp;amp; Central Asia High income&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;small-multiples&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Small multiples&lt;/h3&gt;
&lt;p&gt;First we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;life_expectancy_small &amp;lt;- wdi_clean %&amp;gt;%
  filter(country %in% c(&amp;quot;Argentina&amp;quot;, &amp;quot;Bolivia&amp;quot;, &amp;quot;Brazil&amp;quot;,
                        &amp;quot;Belize&amp;quot;, &amp;quot;Canada&amp;quot;, &amp;quot;Chile&amp;quot;))
ggplot(data = life_expectancy_small,
       mapping = aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_wrap(vars(country))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/life-expectancy-small-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Small multiples! That’s all we need to do.&lt;/p&gt;
&lt;p&gt;We can do some fancier things, though. We can make this plot hyper minimalist:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = life_expectancy_small,
       mapping = aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_wrap(vars(country), scales = &amp;quot;free_y&amp;quot;) +
  theme_void() +
  theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/life-expectancy-small-minimalist-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can do a whole part of a continent (poor Iraq and Syria 😞)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;life_expectancy_mena &amp;lt;- wdi_clean %&amp;gt;%
  filter(region == &amp;quot;Middle East &amp;amp; North Africa&amp;quot;)
ggplot(data = life_expectancy_mena,
       mapping = aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_wrap(vars(country), scales = &amp;quot;free_y&amp;quot;, nrow = 3) +
  theme_void() +
  theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/life-expectancy-mena-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use the &lt;a href=&#34;https://hafen.github.io/geofacet/&#34;&gt;&lt;strong&gt;geofacet&lt;/strong&gt; package&lt;/a&gt; to arrange these facets by geography:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;life_expectancy_eu &amp;lt;- wdi_clean %&amp;gt;%
  filter(region == &amp;quot;Europe &amp;amp; Central Asia&amp;quot;)
ggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) +
  geom_line(size = 1) +
  facet_geo(vars(country), grid = &amp;quot;eu_grid1&amp;quot;, scales = &amp;quot;free_y&amp;quot;) +
  labs(x = NULL, y = NULL, title = &amp;quot;Life expectancy from 1995–2015&amp;quot;,
       caption = &amp;quot;Source: The World Bank (SP.DYN.LE00.IN)&amp;quot;) +
  theme_minimal() +
  theme(strip.text = element_text(face = &amp;quot;bold&amp;quot;),
        plot.title = element_text(face = &amp;quot;bold&amp;quot;),
        axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/life-expectancy-eu-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Neat!&lt;/p&gt;
&lt;p&gt;Anybody see any problems here?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sparklines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sparklines&lt;/h3&gt;
&lt;p&gt;Sparklines are just line charts (or bar charts) that are really really small.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;india_co2 &amp;lt;- wdi_clean %&amp;gt;%
  filter(country == &amp;quot;India&amp;quot;)
plot_india &amp;lt;- ggplot(india_co2, aes(x = year, y = co2_emissions)) +
  geom_line() +
  theme_void()
plot_india&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/india-spark-1.png&#34; width=&#34;96&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;india_co2.pdf&amp;quot;, plot_india, width = 1, height = 0.15, units = &amp;quot;in&amp;quot;)
ggsave(&amp;quot;india_co2.png&amp;quot;, plot_india, width = 1, height = 0.15, units = &amp;quot;in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;china_co2 &amp;lt;- wdi_clean %&amp;gt;%
  filter(country == &amp;quot;China&amp;quot;)
plot_china &amp;lt;- ggplot(china_co2, aes(x = year, y = co2_emissions)) +
  geom_line() +
  theme_void()
plot_china&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/china-spark-1.png&#34; width=&#34;96&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;china_co2.pdf&amp;quot;, plot_china, width = 1, heighlt = 0.15, units = &amp;quot;in&amp;quot;)
ggsave(&amp;quot;china_co2.png&amp;quot;, plot_china, width = 1, height = 0.15, units = &amp;quot;in&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then use those saved tiny plots in your text (with a little html extra in there).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Both India &amp;lt;img class=&amp;quot;img-inline&amp;quot; src=&amp;quot;/your/path/to/india-spark-1.png&amp;quot; width = &amp;quot;100&amp;quot;/&amp;gt; and 
China &amp;lt;img class=&amp;quot;img-inline&amp;quot; src=&amp;quot;/your/path/to/china-spark-1.png&amp;quot; width = &amp;quot;100&amp;quot;/&amp;gt; have 
seen increased CO~2~ emissions over the past 20 years.&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Both India &lt;img class=&#34;img-inline&#34; src=&#34;https://datavizm20.classes.andrewheiss.com/example/08-example_files/figure-html/india-spark-1.png&#34; width = &#34;100&#34;/&gt; and China &lt;img class=&#34;img-inline&#34; src=&#34;https://datavizm20.classes.andrewheiss.com/example/08-example_files/figure-html/china-spark-1.png&#34; width = &#34;100&#34;/&gt; have seen increased CO&lt;sub&gt;2&lt;/sub&gt; emissions over the past 20 years.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;slopegraphs&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slopegraphs&lt;/h3&gt;
&lt;p&gt;We can make a slopegraph to show changes in GDP per capita between two time periods. We need to first filter our WDI to include only the start and end years (here 1995 and 2015). Then, to make sure that we’re using complete data, we’ll get rid of any country that has missing data for either 1995 or 2015. The &lt;code&gt;group_by(...) %&amp;gt;% filter(...) %&amp;gt;% ungroup()&lt;/code&gt; pipeline does this, with the &lt;code&gt;!any(is.na(gdp_per_cap))&lt;/code&gt; test keeping any rows where any of the &lt;code&gt;gdp_per_cap&lt;/code&gt; values are not missing for the whole country.&lt;/p&gt;
&lt;p&gt;We then add a couple special columns for labels. The &lt;code&gt;paste0()&lt;/code&gt; function concatenates strings and variables together, so that &lt;code&gt;paste0(&#34;2 + 2 = &#34;, 2 + 2)&lt;/code&gt; would show “2 + 2 = 4”. Here we make labels that say either “Country name: $GDP” or “$GDP” depending on the year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gdp_south_asia &amp;lt;- wdi_clean %&amp;gt;%
  filter(region == &amp;quot;South Asia&amp;quot;) %&amp;gt;%
  filter(year %in% c(1995, 2015)) %&amp;gt;%
  # Look at each country individually
  group_by(country) %&amp;gt;%
  # Remove the country if any of its gdp_per_cap values are missing
  filter(!any(is.na(gdp_per_cap))) %&amp;gt;%
  ungroup() %&amp;gt;%
  # Make year a factor
  mutate(year = factor(year)) %&amp;gt;%
  # Make some nice label columns
  # If the year is 1995, format it like &amp;quot;Country name: $GDP&amp;quot;. If the year is
  # 2015, format it like &amp;quot;$GDP&amp;quot;
  mutate(label_first = ifelse(year == 1995, paste0(country, &amp;quot;: &amp;quot;, dollar(round(gdp_per_cap))), NA),
         label_last = ifelse(year == 2015, dollar(round(gdp_per_cap, 0)), NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the data filtered like this, we can plot it by mapping year to the x-axis, GDP per capita to the y-axis, and coloring by country. To make the lines go across the two categorical labels in the x-axis (since we made year a factor/category), we need to also specify the &lt;code&gt;group&lt;/code&gt; aesthetic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +
  geom_line(size = 1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/slopegraph-sa-simple-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cool! We’re getting closer. We can definitely see different slopes, but with 7 different colors, it’s hard to see exactly which country is which. Instead, we can directly label each of these lines with &lt;code&gt;geom_text()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +
  geom_line(size = 1.5) +
  geom_text(aes(label = country)) +
  guides(color = FALSE)  #&amp;lt;---- this turns off the guide (legend) for the aesthetic &amp;quot;color&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/slopegraph-sa-simple-text-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That gets us a &lt;em&gt;little&lt;/em&gt; closer, but the country labels are hard to see, and we could include more information, like the actual values. Remember those &lt;code&gt;label_first&lt;/code&gt; and &lt;code&gt;label_last&lt;/code&gt; columns we made? Let’s use those instead:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +
  geom_line(size = 1.5) +
  geom_text(aes(label = label_first)) +
  geom_text(aes(label = label_last)) +
  guides(color = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/slopegraph-sa-simple-text-fancier-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we have dollar amounts and country names, but the labels are still overlapping and really hard to read. To fix this, we can make the labels repel away from each other and randomly position in a way that makes them not overlap. The &lt;a href=&#34;https://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html&#34;&gt;&lt;strong&gt;ggrepel&lt;/strong&gt; package&lt;/a&gt; lets us do this with &lt;code&gt;geom_text_repel()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +
  geom_line(size = 1.5) +
  geom_text_repel(aes(label = label_first)) +
  geom_text_repel(aes(label = label_last)) +
  guides(color = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/slopegraph-sa-getting-warmer-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now none of the labels are on top of each other, but the labels are still on top of the lines. Also, some of the labels moved inward and outward along the x-axis, but they don’t need to do that—they just need to shift up and down. We can force the labels to only move up and down by setting the &lt;code&gt;direction = &#34;y&#34;&lt;/code&gt; argument, and we can move all the labels to the left or right with the &lt;code&gt;nudge_x&lt;/code&gt; argument. The &lt;code&gt;seed&lt;/code&gt; argument makes sure that the random label placement is the same every time we run this. It can be whatever number you want—it just has to be a number.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +
  geom_line(size = 1.5) +
  geom_text_repel(aes(label = label_first), direction = &amp;quot;y&amp;quot;, nudge_x = -1, seed = 1234) +
  geom_text_repel(aes(label = label_last), direction = &amp;quot;y&amp;quot;, nudge_x = 1, seed = 1234) +
  guides(color = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/slopegraph-sa-fancier-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s it! Let’s take the theme off completely, change the colors a little, and it should be perfect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) +
  geom_line(size = 1.5) +
  geom_text_repel(aes(label = label_first), direction = &amp;quot;y&amp;quot;, nudge_x = -1, seed = 1234) +
  geom_text_repel(aes(label = label_last), direction = &amp;quot;y&amp;quot;, nudge_x = 1, seed = 1234) +
  guides(color = FALSE) +
  scale_color_viridis_d(option = &amp;quot;magma&amp;quot;, end = 0.9) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/slopegraph-sa-done-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bump-charts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bump charts&lt;/h3&gt;
&lt;p&gt;Finally, we can make a bump chart that shows changes in rankings over time. We’ll look at CO&lt;sub&gt;2&lt;/sub&gt; emissions in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the &lt;code&gt;rank()&lt;/code&gt; function to rank countries by the &lt;code&gt;co2_emissions&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sa_co2 &amp;lt;- wdi_clean %&amp;gt;%
  filter(region == &amp;quot;South Asia&amp;quot;) %&amp;gt;%
  filter(year &amp;gt;= 2004, year &amp;lt; 2015) %&amp;gt;%
  group_by(year) %&amp;gt;%
  mutate(rank = rank(co2_emissions))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then plot this with points and lines, reversing the y-axis so 1 is at the top:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sa_co2, aes(x = year, y = rank, color = country)) +
  geom_line() +
  geom_point() +
  scale_y_reverse(breaks = 1:8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/make-bump-plot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Afghanistan and Nepal switched around for the number 1 spot, while India dropped from 4 to 6, switching places with Pakistan.&lt;/p&gt;
&lt;p&gt;As with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use &lt;code&gt;geom_text()&lt;/code&gt; again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the &lt;code&gt;data&lt;/code&gt; argument in &lt;code&gt;geom_text()&lt;/code&gt; though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(sa_co2, aes(x = year, y = rank, color = country)) +
  geom_line(size = 2) +
  geom_point(size = 4) +
  geom_text(data = filter(sa_co2, year == 2004),
            aes(label = iso2c, x = 2003.25),
            fontface = &amp;quot;bold&amp;quot;) +
  geom_text(data = filter(sa_co2, year == 2014),
            aes(label = iso2c, x = 2014.75),
            fontface = &amp;quot;bold&amp;quot;) +
  guides(color = FALSE) +
  scale_y_reverse(breaks = 1:8) +
  scale_x_continuous(breaks = 2004:2014) +
  scale_color_viridis_d(option = &amp;quot;magma&amp;quot;, begin = 0.2, end = 0.9) +
  labs(x = NULL, y = &amp;quot;Rank&amp;quot;) +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        panel.grid.minor.x = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/03-example_files/figure-html/bump-plot-fancier-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you want to be &lt;em&gt;super&lt;/em&gt; fancy, you can use flags instead of country codes, but that’s a little more complicated (you need to install the &lt;a href=&#34;https://github.com/rensa/ggflags&#34;&gt;&lt;strong&gt;ggflags&lt;/strong&gt; package&lt;/a&gt;. &lt;a href=&#34;https://dominikkoch.github.io/Bump-Chart/&#34;&gt;See here for an example&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ggplot2: Everything you ever wanted to know</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/02-example/</link>
      <pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/02-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ggplot2&#34;&gt;ggplot2&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-components-of-a-graph&#34;&gt;The components of a graph&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ggplot-objects&#34;&gt;&lt;code&gt;ggplot&lt;/code&gt; objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#geometries&#34;&gt;Geometries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aesthetic-mappings&#34;&gt;Aesthetic mappings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#layers&#34;&gt;Layers&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tinkering-with-arguments&#34;&gt;Tinkering with arguments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#global-versus-local-aesthetic-mappings&#34;&gt;Global versus local aesthetic mappings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scales&#34;&gt;Scales&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#labels-and-titles&#34;&gt;Labels and titles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#categories-as-colors&#34;&gt;Categories as colors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#annotation-shapes-and-adjustments&#34;&gt;Annotation, shapes, and adjustments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#add-on-packages&#34;&gt;Add-on packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#putting-it-all-together&#34;&gt;Putting it all together&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qplot&#34;&gt;Quick plots with &lt;code&gt;qplot&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#grids-of-plots&#34;&gt;Grids of plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#in-class-exercises&#34;&gt;In-class exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;ggplot2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;ggplot2&lt;/h1&gt;
&lt;p&gt;Exploratory data visualization is perhaps the greatest strength of &lt;code&gt;R&lt;/code&gt;. One can quickly go from idea to data to plot with a unique balance of flexibility and ease. For example, Excel may be easier than &lt;code&gt;R&lt;/code&gt; for some plots, but it is nowhere near as flexible. &lt;code&gt;D3.js&lt;/code&gt; may be more flexible and powerful than &lt;code&gt;R&lt;/code&gt;, but it takes much longer to generate a plot. One of the reasons we use &lt;code&gt;R&lt;/code&gt; is its incredible flexibility &lt;strong&gt;and&lt;/strong&gt; ease.&lt;/p&gt;
&lt;p&gt;Throughout this course, we will be creating plots using the &lt;strong&gt;ggplot2&lt;/strong&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many other approaches are available for creating plots in &lt;code&gt;R&lt;/code&gt;. In fact, the plotting capabilities that come with a basic installation of &lt;code&gt;R&lt;/code&gt; are already quite powerful. There are also other packages for creating graphics such as &lt;strong&gt;grid&lt;/strong&gt; and &lt;strong&gt;lattice&lt;/strong&gt;. We chose to use &lt;strong&gt;ggplot2&lt;/strong&gt; in this course because it breaks plots into components in a way that permits beginners to create relatively complex and aesthetically pleasing plots using syntax that is intuitive and comparatively easy to remember.&lt;/p&gt;
&lt;p&gt;One reason &lt;strong&gt;ggplot2&lt;/strong&gt; is generally more intuitive for beginners is that it uses a so-called “grammar of graphics”&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, the letters &lt;em&gt;gg&lt;/em&gt; in &lt;strong&gt;ggplot2&lt;/strong&gt;. This is analogous to the way learning grammar can help a beginner construct hundreds of different sentences by learning just a handful of verbs, nouns and adjectives without having to memorize each specific sentence. Similarly, by learning a handful of &lt;strong&gt;ggplot2&lt;/strong&gt; building blocks and its grammar, you will be able to create hundreds of different plots.&lt;/p&gt;
&lt;p&gt;Another reason &lt;strong&gt;ggplot2&lt;/strong&gt; is easy for beginners is that its default behavior is carefully chosen to satisfy the great majority of cases and is visually pleasing. As a result, it is possible to create informative and elegant graphs with relatively simple and readable code.&lt;/p&gt;
&lt;p&gt;One limitation is that &lt;strong&gt;ggplot2&lt;/strong&gt; is designed to work exclusively with data tables in tidy format (where rows are observations and columns are variables). However, a substantial percentage of datasets that beginners work with are in, or can be converted into, this format. An advantage of this approach is that, assuming that our data is tidy, &lt;strong&gt;ggplot2&lt;/strong&gt; simplifies plotting code and the learning of grammar for a variety of plots. You should review the previous content about tidy data if you are feeling lost.&lt;/p&gt;
&lt;p&gt;To use &lt;strong&gt;ggplot2&lt;/strong&gt; you will have to learn several functions and arguments. These are hard to memorize, so we highly recommend you have the ggplot2 cheat sheet handy. You can get a copy here: &lt;a href=&#34;https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf&#34;&gt;https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf&lt;/a&gt; or simply perform an internet search for “ggplot2 cheat sheet”.&lt;/p&gt;
&lt;div id=&#34;the-components-of-a-graph&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The components of a graph&lt;/h2&gt;
&lt;p&gt;We will construct a graph that summarizes the US murders dataset that looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.&lt;/p&gt;
&lt;p&gt;This data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.&lt;/p&gt;
&lt;p&gt;The first step in learning &lt;strong&gt;ggplot2&lt;/strong&gt; is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the &lt;strong&gt;ggplot2&lt;/strong&gt; terminology. The main three components to note are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data&lt;/strong&gt;: The US murders data table is being summarized. We refer to this as the &lt;strong&gt;data&lt;/strong&gt; component.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Geometry&lt;/strong&gt;: The plot above is a scatterplot. This is referred to as the
&lt;strong&gt;geometry&lt;/strong&gt; component. Other possible geometries are barplot, histogram, smooth densities, qqplot, and boxplot. We will learn more about these in the Data Visualization part of the book.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aesthetic mapping&lt;/strong&gt;: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we &lt;em&gt;map&lt;/em&gt; data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the &lt;strong&gt;aesthetic mapping&lt;/strong&gt; component. How we define the mapping depends on what &lt;strong&gt;geometry&lt;/strong&gt; we are using.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The points are labeled with the state abbreviations.&lt;/li&gt;
&lt;li&gt;The range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.&lt;/li&gt;
&lt;li&gt;There are labels, a title, a legend, and we use the style of The Economist magazine.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will now construct the plot piece by piece.&lt;/p&gt;
&lt;p&gt;We start by loading the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ggplot-objects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;ggplot&lt;/code&gt; objects&lt;/h2&gt;
&lt;p&gt;The first step in creating a &lt;strong&gt;ggplot2&lt;/strong&gt; graph is to define a &lt;code&gt;ggplot&lt;/code&gt; object. We do this with the function &lt;code&gt;ggplot&lt;/code&gt;, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also pipe the data in as the first argument. So this line of code is equivalent to the one above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.&lt;/p&gt;
&lt;p&gt;What has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = murders)
class(p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;gg&amp;quot;     &amp;quot;ggplot&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To render the plot associated with this object, we simply print the object &lt;code&gt;p&lt;/code&gt;. The following two lines of code each produce the same plot we see above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(p)
p&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;geometries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Geometries&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;ggplot2&lt;/code&gt; we create graphs by adding &lt;em&gt;layers&lt;/em&gt;. Layers can define geometries, compute summary statistics, define what scales to use, or even change styles.
To add layers, we use the symbol &lt;code&gt;+&lt;/code&gt;. In general, a line of code will look like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;DATA %&amp;gt;% &lt;code&gt;ggplot()&lt;/code&gt; + LAYER 1 + LAYER 2 + … + LAYER N&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Usually, the first added layer defines the geometry. We want to make a scatterplot. What geometry do we use?&lt;/p&gt;
&lt;p&gt;Taking a quick look at the cheat sheet, we see that the function used to create plots with this geometry is &lt;code&gt;geom_point&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/ggplot2-cheatsheeta.png&#34; /&gt;
&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/ggplot2-cheatsheetb.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Image courtesy of RStudio&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. CC-BY-4.0 license&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;!--(Source: [RStudio](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf))--&gt;
&lt;p&gt;Geometry function names follow the pattern: &lt;code&gt;geom_X&lt;/code&gt; where X is the name of some specific geometry. Some examples include &lt;code&gt;geom_point&lt;/code&gt;, &lt;code&gt;geom_bar&lt;/code&gt;, and &lt;code&gt;geom_histogram&lt;/code&gt;. You’ve already seen a few of these.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;geom_point&lt;/code&gt; to run properly we need to provide data and a mapping. We have already connected the object &lt;code&gt;p&lt;/code&gt; with the &lt;code&gt;murders&lt;/code&gt; data table, and if we add the layer &lt;code&gt;geom_point&lt;/code&gt; it defaults to using this data. To find out what mappings are expected, we read the &lt;strong&gt;Aesthetics&lt;/strong&gt; section of the help file &lt;code&gt;geom_point&lt;/code&gt; help file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; Aesthetics
&amp;gt;
&amp;gt; geom_point understands the following aesthetics (required aesthetics are in bold):
&amp;gt;
&amp;gt; x
&amp;gt;
&amp;gt; y
&amp;gt;
&amp;gt; alpha
&amp;gt;
&amp;gt; colour&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and—although it does not show in bold above—we see that at least two arguments are required: &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aesthetic-mappings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Aesthetic mappings&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Aesthetic mappings&lt;/strong&gt; describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The &lt;code&gt;aes&lt;/code&gt; function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the &lt;code&gt;aes&lt;/code&gt; function is often used as the argument of a geometry function. This example produces a scatterplot of total murders versus population in millions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot() +
  geom_point(aes(x = population/10^6, y = total))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can drop the &lt;code&gt;x =&lt;/code&gt; and &lt;code&gt;y =&lt;/code&gt; if we wanted to since these are the first and second expected arguments, as seen in the help page.&lt;/p&gt;
&lt;p&gt;Instead of defining our plot from scratch, we can also add a layer to the &lt;code&gt;p&lt;/code&gt; object that was defined above as &lt;code&gt;p &amp;lt;- ggplot(data = murders)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(population/10^6, total))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The scale and labels are defined by default when adding this layer. Like &lt;strong&gt;dplyr&lt;/strong&gt; functions, &lt;code&gt;aes&lt;/code&gt; also uses the variable names from the object component: we can use &lt;code&gt;population&lt;/code&gt; and &lt;code&gt;total&lt;/code&gt; without having to call them as &lt;code&gt;murders$population&lt;/code&gt; and &lt;code&gt;murders$total&lt;/code&gt;. The behavior of recognizing the variables from the data component is quite specific to &lt;code&gt;aes&lt;/code&gt;. With most functions, if you try to access the values of &lt;code&gt;population&lt;/code&gt; or &lt;code&gt;total&lt;/code&gt; outside of &lt;code&gt;aes&lt;/code&gt; you receive an error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;layers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Layers&lt;/h2&gt;
&lt;p&gt;A second layer in the plot we wish to make involves adding a label to each point to identify the state. The &lt;code&gt;geom_label&lt;/code&gt; and &lt;code&gt;geom_text&lt;/code&gt; functions permit us to add text to the plot with and without a rectangle behind the text, respectively.&lt;/p&gt;
&lt;p&gt;Because each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file, we learn that we supply the mapping between point and label through the &lt;code&gt;label&lt;/code&gt; argument of &lt;code&gt;aes&lt;/code&gt;. So the code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(population/10^6, total)) +
  geom_text(aes(population/10^6, total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have successfully added a second layer to the plot.&lt;/p&gt;
&lt;p&gt;As an example of the unique behavior of &lt;code&gt;aes&lt;/code&gt; mentioned above, note that this call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_test &amp;lt;- p + geom_text(aes(population/10^6, total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is fine, whereas this call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_test &amp;lt;- p + geom_text(aes(population/10^6, total), label = abb)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will give you an error since &lt;code&gt;abb&lt;/code&gt; is not found because it is outside of the &lt;code&gt;aes&lt;/code&gt; function. The layer &lt;code&gt;geom_text&lt;/code&gt; does not know where to find &lt;code&gt;abb&lt;/code&gt; since it is a column name and not a global variable.&lt;/p&gt;
&lt;div id=&#34;tinkering-with-arguments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tinkering with arguments&lt;/h3&gt;
&lt;p&gt;Each geometry function has many arguments other than &lt;code&gt;aes&lt;/code&gt; and &lt;code&gt;data&lt;/code&gt;. They tend to be specific to the function. For example, in the plot we wish to make, the points are larger than the default size. In the help file we see that &lt;code&gt;size&lt;/code&gt; is an aesthetic and we can change it like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(population/10^6, total), size = 3) +
  geom_text(aes(population/10^6, total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;size&lt;/code&gt; is &lt;strong&gt;not&lt;/strong&gt; a mapping: whereas mappings use data from specific observations and need to be inside &lt;code&gt;aes()&lt;/code&gt;, operations we want to affect all the points the same way do not need to be included inside &lt;code&gt;aes&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now because the points are larger it is hard to see the labels. If we read the help file for &lt;code&gt;geom_text&lt;/code&gt;, we see the &lt;code&gt;nudge_x&lt;/code&gt; argument, which moves the text slightly to the right or to the left:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(population/10^6, total), size = 3) +
  geom_text(aes(population/10^6, total, label = abb), nudge_x = 1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is preferred as it makes it easier to read the text. There are alternatives, though, and we will pepper in examples with better labels as we move forward.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;global-versus-local-aesthetic-mappings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Global versus local aesthetic mappings&lt;/h2&gt;
&lt;p&gt;In the previous line of code, we define the mapping &lt;code&gt;aes(population/10^6, total)&lt;/code&gt; twice, once in each geometry. We can avoid this by using a &lt;em&gt;global&lt;/em&gt; aesthetic mapping. We can do this when we define the blank slate &lt;code&gt;ggplot&lt;/code&gt; object. Remember that the function &lt;code&gt;ggplot&lt;/code&gt; contains an argument that permits us to define aesthetic mappings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;args(ggplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (data = NULL, mapping = aes(), ..., environment = parent.frame()) 
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we define a mapping in &lt;code&gt;ggplot&lt;/code&gt;, all the geometries that are added as layers will default to this mapping. We redefine &lt;code&gt;p&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- murders %&amp;gt;% ggplot(aes(population/10^6, total, label = abb))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then we can simply write the following code to produce the previous plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(nudge_x = 1.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We keep the &lt;code&gt;size&lt;/code&gt; and &lt;code&gt;nudge_x&lt;/code&gt; arguments in &lt;code&gt;geom_point&lt;/code&gt; and &lt;code&gt;geom_text&lt;/code&gt;, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in &lt;code&gt;aes&lt;/code&gt; then they would apply to both plots. Also note that the &lt;code&gt;geom_point&lt;/code&gt; function does not need a &lt;code&gt;label&lt;/code&gt; argument and therefore ignores that aesthetic.&lt;/p&gt;
&lt;p&gt;If necessary, we can override the global mapping by defining a new mapping within each layer. These &lt;em&gt;local&lt;/em&gt; definitions override the &lt;em&gt;global&lt;/em&gt;. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(aes(x = 10, y = 800, label = &amp;quot;Hello there!&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, the second call to &lt;code&gt;geom_text&lt;/code&gt; does not use &lt;code&gt;population&lt;/code&gt; and &lt;code&gt;total&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scales&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scales&lt;/h2&gt;
&lt;p&gt;First, our desired scales are in log-scale. This is not the default, so this change needs to be added through a &lt;em&gt;scales&lt;/em&gt; layer. A quick look at the cheat sheet reveals the &lt;code&gt;scale_x_continuous&lt;/code&gt; function lets us control the behavior of scales. We use them like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(nudge_x = 0.05) +
  scale_x_continuous(trans = &amp;quot;log10&amp;quot;) +
  scale_y_continuous(trans = &amp;quot;log10&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Because we are in the log-scale now, the &lt;em&gt;nudge&lt;/em&gt; must be made smaller.&lt;/p&gt;
&lt;p&gt;This particular transformation is so common that &lt;strong&gt;ggplot2&lt;/strong&gt; provides the specialized functions &lt;code&gt;scale_x_log10&lt;/code&gt; and &lt;code&gt;scale_y_log10&lt;/code&gt;, which we can use to rewrite the code like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(nudge_x = 0.05) +
  scale_x_log10() +
  scale_y_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;labels-and-titles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Labels and titles&lt;/h2&gt;
&lt;p&gt;Similarly, the cheat sheet quickly reveals that to change labels and add a title, we use the following functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3) +
  geom_text(nudge_x = 0.05) +
  scale_x_log10() +
  scale_y_log10() +
  xlab(&amp;quot;Populations in millions (log scale)&amp;quot;) +
  ylab(&amp;quot;Total number of murders (log scale)&amp;quot;) +
  ggtitle(&amp;quot;US Gun Murders in 2010&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We are almost there! All we have left to do is add color, a legend, and optional changes to the style.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;categories-as-colors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categories as colors&lt;/h2&gt;
&lt;p&gt;We can change the color of the points using the &lt;code&gt;col&lt;/code&gt; argument in the &lt;code&gt;geom_point&lt;/code&gt; function. To facilitate demonstration of new features, we will redefine &lt;code&gt;p&lt;/code&gt; to be everything except the points layer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;-  murders %&amp;gt;% ggplot(aes(population/10^6, total, label = abb)) +
  geom_text(nudge_x = 0.05) +
  scale_x_log10() +
  scale_y_log10() +
  xlab(&amp;quot;Populations in millions (log scale)&amp;quot;) +
  ylab(&amp;quot;Total number of murders (log scale)&amp;quot;) +
  ggtitle(&amp;quot;US Gun Murders in 2010&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then test out what happens by adding different calls to &lt;code&gt;geom_point&lt;/code&gt;. We can make all the points blue by adding the &lt;code&gt;color&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(size = 3, color =&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This, of course, is not what we want. We want to assign color depending on the geographical region. A nice default behavior of &lt;strong&gt;ggplot2&lt;/strong&gt; is that if we assign a categorical variable to color, it automatically assigns a different color to each category and also adds a legend.&lt;/p&gt;
&lt;p&gt;Since the choice of color is determined by a feature of each observation, this is an aesthetic mapping. To map each point to a color, we need to use &lt;code&gt;aes&lt;/code&gt;. We use the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(col=region), size = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; mappings are inherited from those already defined in &lt;code&gt;p&lt;/code&gt;, so we do not redefine them. We also move &lt;code&gt;aes&lt;/code&gt; to the first argument since that is where mappings are expected in this function call.&lt;/p&gt;
&lt;p&gt;Here we see yet another useful default behavior: &lt;strong&gt;ggplot2&lt;/strong&gt; automatically adds a legend that maps color to region. To avoid adding this legend we set the &lt;code&gt;geom_point&lt;/code&gt; argument &lt;code&gt;show.legend = FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;annotation-shapes-and-adjustments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Annotation, shapes, and adjustments&lt;/h2&gt;
&lt;p&gt;We often want to add shapes or annotation to figures that are not derived directly from the aesthetic mapping; examples include labels, boxes, shaded areas, and lines.&lt;/p&gt;
&lt;p&gt;Here we want to add a line that represents the average murder rate for the entire country. Once we determine the per million rate to be &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, this line is defined by the formula: &lt;span class=&#34;math inline&#34;&gt;\(y = r x\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; our axes: total murders and population in millions, respectively. In the log-scale this line turns into: &lt;span class=&#34;math inline&#34;&gt;\(\log(y) = \log(r) + \log(x)\)&lt;/span&gt;. So in our plot it’s a line with slope 1 and intercept &lt;span class=&#34;math inline&#34;&gt;\(\log(r)\)&lt;/span&gt;. To compute this value, we use our &lt;strong&gt;dplyr&lt;/strong&gt; skills:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- murders %&amp;gt;%
  summarize(rate = sum(total) /  sum(population) * 10^6) %&amp;gt;%
  pull(rate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To add a line we use the &lt;code&gt;geom_abline&lt;/code&gt; function. &lt;strong&gt;ggplot2&lt;/strong&gt; uses &lt;code&gt;ab&lt;/code&gt; in the name to remind us we are supplying the intercept (&lt;code&gt;a&lt;/code&gt;) and slope (&lt;code&gt;b&lt;/code&gt;). The default line has slope 1 and intercept 0 so we only have to define the intercept:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(aes(col=region), size = 3) +
  geom_abline(intercept = log10(r))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/ggplot-example-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;code&gt;geom_abline&lt;/code&gt; does not use any information from the data object.&lt;/p&gt;
&lt;p&gt;We can change the line type and color of the lines using arguments. Also, we draw it first so it doesn’t go over our points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- p + geom_abline(intercept = log10(r), lty = 2, color = &amp;quot;darkgrey&amp;quot;) +
  geom_point(aes(col=region), size = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we have redefined &lt;code&gt;p&lt;/code&gt; and used this new &lt;code&gt;p&lt;/code&gt; below and in the next section.&lt;/p&gt;
&lt;p&gt;The default plots created by &lt;strong&gt;ggplot2&lt;/strong&gt; are already very useful. However, we frequently need to make minor tweaks to the default behavior. Although it is not always obvious how to make these even with the cheat sheet, &lt;strong&gt;ggplot2&lt;/strong&gt; is very flexible.&lt;/p&gt;
&lt;p&gt;For example, we can make changes to the legend via the &lt;code&gt;scale_color_discrete&lt;/code&gt; function. In our plot the word &lt;em&gt;region&lt;/em&gt; is capitalized and we can change it like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- p + scale_color_discrete(name = &amp;quot;Region&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;add-on-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Add-on packages&lt;/h2&gt;
&lt;p&gt;The power of &lt;strong&gt;ggplot2&lt;/strong&gt; is augmented further due to the availability of add-on packages.
The remaining changes needed to put the finishing touches on our plot require the &lt;strong&gt;ggthemes&lt;/strong&gt; and &lt;strong&gt;ggrepel&lt;/strong&gt; packages.&lt;/p&gt;
&lt;p&gt;The style of a &lt;strong&gt;ggplot2&lt;/strong&gt; graph can be changed using the &lt;code&gt;theme&lt;/code&gt; functions. Several themes are included as part of the &lt;strong&gt;ggplot2&lt;/strong&gt; package. In fact, for most of the plots in this book, we use a function in the &lt;strong&gt;dslabs&lt;/strong&gt; package that automatically sets a default theme:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ds_theme_set()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Many other themes are added by the package &lt;strong&gt;ggthemes&lt;/strong&gt;. Among those are the &lt;code&gt;theme_economist&lt;/code&gt; theme that we used. After installing the package, you can change the style by adding a layer like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggthemes)
p + theme_economist()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see how some of the other themes look by simply changing the function. For instance, you might try the &lt;code&gt;theme_fivethirtyeight()&lt;/code&gt; theme instead.&lt;/p&gt;
&lt;p&gt;The final difference has to do with the position of the labels. In our plot, some of the labels fall on top of each other. The add-on package &lt;strong&gt;ggrepel&lt;/strong&gt; includes a geometry that adds labels while ensuring that they don’t fall on top of each other. We simply change &lt;code&gt;geom_text&lt;/code&gt; with &lt;code&gt;geom_text_repel&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-all-together&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;Now that we are done testing, we can write one piece of code that produces our desired plot from scratch.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggthemes)
library(ggrepel)

r &amp;lt;- murders %&amp;gt;%
  summarize(rate = sum(total) /  sum(population) * 10^6) %&amp;gt;%
  pull(rate)

murders %&amp;gt;% ggplot(aes(population/10^6, total, label = abb)) +
  geom_abline(intercept = log10(r), lty = 2, color = &amp;quot;darkgrey&amp;quot;) +
  geom_point(aes(col=region), size = 3) +
  geom_text_repel() +
  scale_x_log10() +
  scale_y_log10() +
  xlab(&amp;quot;Populations in millions (log scale)&amp;quot;) +
  ylab(&amp;quot;Total number of murders (log scale)&amp;quot;) +
  ggtitle(&amp;quot;US Gun Murders in 2010&amp;quot;) +
  scale_color_discrete(name = &amp;quot;Region&amp;quot;) +
  theme_economist_white()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/final-ggplot-example-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;qplot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quick plots with &lt;code&gt;qplot&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;We have learned the powerful approach to generating visualization with ggplot. However, there are instances in which all we want is to make a quick plot of, for example, a histogram of the values in a vector, a scatterplot of the values in two vectors, or a boxplot using categorical and numeric vectors. We demonstrated how to generate these plots with &lt;code&gt;hist&lt;/code&gt;, &lt;code&gt;plot&lt;/code&gt;, and &lt;code&gt;boxplot&lt;/code&gt;. However, if we want to keep consistent with the ggplot style, we can use the function &lt;code&gt;qplot&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If we have values in two vectors, say:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(murders)
x &amp;lt;- log10(murders$population)
y &amp;lt;- murders$total&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we want to make a scatterplot with ggplot, we would have to type something like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(x = x, y = y) %&amp;gt;%
  ggplot(aes(x, y)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This seems like too much code for such a simple plot.
The &lt;code&gt;qplot&lt;/code&gt; function sacrifices the flexibility provided by the &lt;code&gt;ggplot&lt;/code&gt; approach, but allows us to generate a plot quickly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although we won’t discuss &lt;code&gt;qtplot&lt;/code&gt; in much detail, you should feel free to use it in the early stages of your data exploration. Once you’re settled on a final design, then move to &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grids-of-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grids of plots&lt;/h2&gt;
&lt;p&gt;There are often reasons to graph plots next to each other. The &lt;strong&gt;gridExtra&lt;/strong&gt; package permits us to do that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gridExtra)
p1 &amp;lt;- qplot(x)
p2 &amp;lt;- qplot(x,y)
grid.arrange(p1, p2, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/02-example_files/figure-html/gridExtra-example-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;in-class-exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;In-class exercises&lt;/h2&gt;
&lt;p&gt;If we have time, let’s explore some new geom’s.&lt;/p&gt;
&lt;p&gt;I’ll break you out into Breakout Rooms of 4-5 people each. In your room, I want you to explore one of the following 5 &lt;code&gt;ggplot&lt;/code&gt; geoms. Use ?geom_name to learn about the &lt;em&gt;aes&lt;/em&gt; arguments each takes / requires.&lt;/p&gt;
&lt;p&gt;For whichever geom your group chooses, use the &lt;code&gt;dslabs::gapminder&lt;/code&gt; data&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(dslabs)
gapminder = dslabs::gapminder&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;geom_density_2d_filled()&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is a 2d density plot which shows correlation between two variables. Try &lt;code&gt;infant_mortality&lt;/code&gt; and GDP per capita&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: Show infant mortality’s relationship to gdp per capita in the year 2007&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;x = infant_mortality, y = calculate gdp per capita on the fly (in the ggplot call)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;geom_tile()&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is a heatmap type plot&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: Show infant_mortality by year for each country in continent==‘Americas’&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hint: x = year, y = country&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;geom_smooth()&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This is plot that shows the mean and variance (standard deviation) over time&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: Show the distribution of infant mortality across the world for 1960 and 2007, with a separate density for each of those two years&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hint: x = infant_mortality, “+ facet_wrap”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;geom_qq()&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This plot helps us tell if a variable’s distribution is close to normal&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: Show a qq plot of fertility in 2007 across the world. Is it normally distributed?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hint: This one takes a new &lt;code&gt;aes(...)&lt;/code&gt; argument: sample = fertility&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hint: geom_qq and geom_qq_line do two different things, but can be combined. geom_qq_line is a lot like geom_abline()&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;geom_rug()&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This plot shows both the point scatter and the density of each variable. &lt;code&gt;geom_rug&lt;/code&gt; adds density in the margins, use with geom_point&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;:Visualize the relationship between gdp per capita and infant mortality that also communicates the density of each variable&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hint: x = gdp per capita (calculate in ggplot call), y = infant_mortality&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hint: use alpha = .1 to make rug lines show overlap&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start by loading the &lt;strong&gt;dplyr&lt;/strong&gt; and &lt;strong&gt;ggplot2&lt;/strong&gt; library as well as the &lt;code&gt;murders&lt;/code&gt; and &lt;code&gt;heights&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(dslabs)
data(heights)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;With &lt;strong&gt;ggplot2&lt;/strong&gt; plots can be saved as objects. For example we can associate a dataset with a plot object like this&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(data = murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because &lt;code&gt;data&lt;/code&gt; is the first argument we don’t need to spell it out&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- ggplot(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we can also use the pipe:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- murders %&amp;gt;% ggplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is class of the object &lt;code&gt;p&lt;/code&gt;?&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Remember that to print an object you can use the command &lt;code&gt;print&lt;/code&gt; or simply type the object.
Print the object &lt;code&gt;p&lt;/code&gt; defined in exercise one and describe what you see.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Nothing happens.&lt;/li&gt;
&lt;li&gt;A blank slate plot.&lt;/li&gt;
&lt;li&gt;A scatterplot.&lt;/li&gt;
&lt;li&gt;A histogram.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Using the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;, create an object &lt;code&gt;p&lt;/code&gt; but this time associated with the &lt;code&gt;heights&lt;/code&gt; dataset instead of the &lt;code&gt;murders&lt;/code&gt; dataset.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the class of the object &lt;code&gt;p&lt;/code&gt; you have just created?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we are going to add a layer and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the &lt;code&gt;murders&lt;/code&gt; data frame to remind yourself what are the names for these two variables and select the correct answer. &lt;strong&gt;Hint&lt;/strong&gt;: Look at &lt;code&gt;?murders&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;state&lt;/code&gt; and &lt;code&gt;abb&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;total_murders&lt;/code&gt; and &lt;code&gt;population_size&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;total&lt;/code&gt; and &lt;code&gt;population&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To create the scatterplot we add a layer with &lt;code&gt;geom_point&lt;/code&gt;. The aesthetic mappings require us to define the x-axis and y-axis variables, respectively. So the code looks like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot(aes(x = , y = )) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;except we have to define the two variables &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;. Fill this out with the correct variable names.&lt;/p&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Note that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot(aes(population, total)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remake the plot but now with total in the x-axis and population in the y-axis.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If instead of points we want to add text, we can use the &lt;code&gt;geom_text()&lt;/code&gt; or &lt;code&gt;geom_label()&lt;/code&gt; geometries. The following code&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot(aes(population, total)) + geom_label()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will give us the error message: &lt;code&gt;Error: geom_label requires the following missing aesthetics: label&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Why is this?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;We need to map a character to each point through the label argument in aes.&lt;/li&gt;
&lt;li&gt;We need to let &lt;code&gt;geom_label&lt;/code&gt; know what character to use in the plot.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;geom_label&lt;/code&gt; geometry does not require x-axis and y-axis values.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;geom_label&lt;/code&gt; is not a ggplot2 command.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;9&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Rewrite the code above to use abbreviation as the label through &lt;code&gt;aes&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Change the color of the labels to blue. How will we do this?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Adding a column called &lt;code&gt;blue&lt;/code&gt; to &lt;code&gt;murders&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Because each label needs a different color we map the colors through &lt;code&gt;aes&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;color&lt;/code&gt; argument in &lt;code&gt;ggplot&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Because we want all colors to be blue, we do not need to map colors, just use the color argument in &lt;code&gt;geom_label&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;11&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Rewrite the code above to make the labels blue.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now suppose we want to use color to represent the different regions. In this case which of the following is most appropriate:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Adding a column called &lt;code&gt;color&lt;/code&gt; to &lt;code&gt;murders&lt;/code&gt; with the color we want to use.&lt;/li&gt;
&lt;li&gt;Because each label needs a different color we map the colors through the color argument of &lt;code&gt;aes&lt;/code&gt; .&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;color&lt;/code&gt; argument in &lt;code&gt;ggplot&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Because we want all colors to be blue, we do not need to map colors, just use the color argument in &lt;code&gt;geom_label&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;13&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Rewrite the code above to make the labels’ color be determined by the state’s region.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by defining an object &lt;code&gt;p&lt;/code&gt; holding the plot we have made up to now&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- murders %&amp;gt;%
  ggplot(aes(population, total, label = abb, color = region)) +
  geom_label()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To change the y-axis to a log scale we learned about the &lt;code&gt;scale_x_log10()&lt;/code&gt; function. Add this layer to the object &lt;code&gt;p&lt;/code&gt; to change the scale and render the plot.&lt;/p&gt;
&lt;ol start=&#34;15&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Repeat the previous exercise but now change both axes to be in the log scale.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now edit the code above to add the title “Gun murder data” to the plot. Hint: use the &lt;code&gt;ggtitle&lt;/code&gt; function.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://ggplot2.tidyverse.org/&#34; class=&#34;uri&#34;&gt;https://ggplot2.tidyverse.org/&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.springer.com/us/book/9780387245447&#34; class=&#34;uri&#34;&gt;http://www.springer.com/us/book/9780387245447&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&#34; class=&#34;uri&#34;&gt;https://github.com/rstudio/cheatsheets/blob/master/LICENSE&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Working with R and RStudio</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/00-example/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/00-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-examples&#34;&gt;Introduction to Examples&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#getting-started&#34;&gt;Getting started with &lt;code&gt;R&lt;/code&gt; and RStudio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-r-console&#34;&gt;The &lt;code&gt;R&lt;/code&gt; console&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scripts&#34;&gt;Scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rstudio&#34;&gt;RStudio&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-panes&#34;&gt;The panes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#key-bindings&#34;&gt;Key bindings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#running-commands-while-editing-scripts&#34;&gt;Running commands while editing scripts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#installing-r-packages&#34;&gt;Installing &lt;code&gt;R&lt;/code&gt; packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rmarkdown&#34;&gt;Rmarkdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lecture-video&#34;&gt;Lecture Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction-to-examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to Examples&lt;/h1&gt;
&lt;p&gt;Examples in this class are designed to be presented in-class. Accordingly, the notes here are &lt;em&gt;not&lt;/em&gt; comprehensive. Instead, they are intended to guide students through&lt;/p&gt;
&lt;p&gt;I’m also aware that my writing is dry and lifeless. If you’re reading this online without the advantage of seeing it in person, don’t worry—I’ll be “funnier” in class.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting started with &lt;code&gt;R&lt;/code&gt; and RStudio&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is not a programming language like &lt;code&gt;C&lt;/code&gt; or &lt;code&gt;Java&lt;/code&gt;. It was not created by software engineers for software development. Instead, it was developed by statisticians as an interactive environment for data analysis. You can read the full history in the paper A Brief History of S&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. The interactivity is an indispensable feature in data science because, as you will soon learn, the ability to quickly explore data is a necessity for success in this field. However, like in other programming languages, you can save your work as scripts that can be easily executed at any moment. These scripts serve as a record of the analysis you performed, a key feature that facilitates reproducible work. If you are an expert programmer, you should not expect &lt;code&gt;R&lt;/code&gt; to follow the conventions you are used—assuming this will leave you disappointed. If you are patient, you will come to appreciate the unequal power of &lt;code&gt;R&lt;/code&gt; when it comes to data analysis and data visualization.&lt;/p&gt;
&lt;p&gt;Other attractive features of &lt;code&gt;R&lt;/code&gt; are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;R&lt;/code&gt; is free and open source&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;It runs on all major platforms: Windows, Mac OS, UNIX/Linux.&lt;/li&gt;
&lt;li&gt;Scripts and data objects can be shared seamlessly across platforms.&lt;/li&gt;
&lt;li&gt;There is a large, growing, and active community of &lt;code&gt;R&lt;/code&gt; users and, as a result, there are numerous resources for learning and asking questions&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;It is easy for others to contribute add-ons which enables developers to share software implementations of new data science methodologies. The latest methods and tools are developed in &lt;code&gt;R&lt;/code&gt; for a wide variety of disciplines and since social science is so broad, &lt;code&gt;R&lt;/code&gt; is one of the few tools that spans the varied social sciences.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-r-console&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;R&lt;/code&gt; console&lt;/h2&gt;
&lt;p&gt;Interactive data analysis usually occurs on the &lt;em&gt;R console&lt;/em&gt; that executes commands as you type them. There are several ways to gain access to an &lt;code&gt;R&lt;/code&gt; console. One way is to simply start &lt;code&gt;R&lt;/code&gt; on your computer. The console looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/img/R_console2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As a quick example, try using the console to calculate a 15% tip on a meal that cost $19.71:&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.15 * 19.71  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.9565&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note that in this course (at least, on most browsers), grey boxes are used to show &lt;code&gt;R&lt;/code&gt; code typed into the &lt;code&gt;R&lt;/code&gt; console. The symbol &lt;code&gt;##&lt;/code&gt; is used to denote what the &lt;code&gt;R&lt;/code&gt; console outputs.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scripts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scripts&lt;/h2&gt;
&lt;p&gt;One of the great advantages of &lt;code&gt;R&lt;/code&gt; over point-and-click analysis software is that you can save your work as scripts. You can edit and save these scripts using a text editor. The material in this course was developed using the interactive &lt;em&gt;integrated development environment&lt;/em&gt; (IDE) RStudio&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;. RStudio includes an editor with many &lt;code&gt;R&lt;/code&gt; specific features, a console to execute your code, and other useful panes, including one to show figures.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/img/RStudio.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most web-based &lt;code&gt;R&lt;/code&gt; consoles also provide a pane to edit scripts, but not all permit you to save the scripts for later use. On the upper-right part of this webpage you’ll see a little button with the &lt;code&gt;R&lt;/code&gt; logo. You can access a web-based console there.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rstudio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;RStudio&lt;/h2&gt;
&lt;p&gt;RStudio will be our launching pad for data science projects. It not only provides an editor for us to create and edit our scripts but also provides many other useful tools. In this section, we go over some of the basics.&lt;/p&gt;
&lt;div id=&#34;the-panes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The panes&lt;/h3&gt;
&lt;p&gt;When you start RStudio for the first time, you will see three panes. The left pane shows the &lt;code&gt;R&lt;/code&gt; console. On the right, the top pane includes tabs such as &lt;em&gt;Environment&lt;/em&gt; and &lt;em&gt;History&lt;/em&gt;, while the bottom pane shows five tabs: &lt;em&gt;File&lt;/em&gt;, &lt;em&gt;Plots&lt;/em&gt;, &lt;em&gt;Packages&lt;/em&gt;, &lt;em&gt;Help&lt;/em&gt;, and &lt;em&gt;Viewer&lt;/em&gt; (these tabs may change in new versions). You can click on each tab to move across the different features.&lt;/p&gt;
&lt;p&gt;To start a new script, you can click on File, then New File, then &lt;code&gt;R&lt;/code&gt; Script.&lt;/p&gt;
&lt;p&gt;This starts a new pane on the left and it is here where you can start writing your script.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;key-bindings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Key bindings&lt;/h3&gt;
&lt;p&gt;Many tasks we perform with the mouse can be achieved with a combination of key strokes instead. These keyboard versions for performing tasks are referred to as &lt;em&gt;key bindings&lt;/em&gt;. For example, we just showed how to use the mouse to start a new script, but you can also use a key binding: Ctrl+Shift+N on Windows and command+shift+N on the Mac.&lt;/p&gt;
&lt;p&gt;Although in this tutorial we often show how to use the mouse, &lt;strong&gt;we highly recommend that you memorize key bindings for the operations you use most&lt;/strong&gt;. RStudio provides a useful cheat sheet with the most widely used commands. You might want to keep this handy so you can look up key-bindings when you find yourself performing repetitive point-and-clicking.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-commands-while-editing-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Running commands while editing scripts&lt;/h3&gt;
&lt;p&gt;There are many editors specifically made for coding. These are useful because color and indentation are automatically added to make code more readable. RStudio is one of these editors, and it was specifically developed for R. One of the main advantages provided by RStudio over other editors is that we can test our code easily as we edit our scripts. Below we show an example.&lt;/p&gt;
&lt;p&gt;Let’s start by opening a new script as we did before. A next step is to give the script a name. We can do this through the editor by saving the current new unnamed script. To do this, click on the save icon or use the key binding Ctrl+S on Windows and command+S on the Mac.&lt;/p&gt;
&lt;p&gt;When you ask for the document to be saved for the first time, RStudio will prompt you for a name. A good convention is to use a descriptive name, with lower case letters, no spaces, only hyphens to separate words, and then followed by the suffix &lt;em&gt;.R&lt;/em&gt;. We will call this script &lt;em&gt;my-first-script.R&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now we are ready to start editing our first script. The first lines of code in an &lt;code&gt;R&lt;/code&gt; script are dedicated to loading the libraries we will use. Another useful RStudio feature is that once we type &lt;code&gt;library()&lt;/code&gt; it starts auto-completing with libraries that we have installed. Note what happens when we type &lt;code&gt;library(ti)&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;Another feature you may have noticed is that when you type &lt;code&gt;library(&lt;/code&gt; the second parenthesis is automatically added. This will help you avoid one of the most common errors in coding: forgetting to close a parenthesis.&lt;/p&gt;
&lt;p&gt;Now we can continue to write code. As an example, we will make a graph showing murder totals versus population totals by state. Once you are done writing the code needed to make this plot, you can try it out by &lt;em&gt;executing&lt;/em&gt; the code. To do this, click on the &lt;em&gt;Run&lt;/em&gt; button on the upper right side of the editing pane. You can also use the key binding: Ctrl+Shift+Enter on Windows or command+shift+return on the Mac.&lt;/p&gt;
&lt;p&gt;Once you run the code, you will see it appear in the &lt;code&gt;R&lt;/code&gt; console and, in this case, the generated plot appears in the plots console. Note that the plot console has a useful interface that permits you to click back and forward across different plots, zoom in to the plot, or save the plots as files.&lt;/p&gt;
&lt;p&gt;To run one line at a time instead of the entire script, you can use Control-Enter on Windows and command-return on the Mac.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;SETUP TIP&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Change the option &lt;em&gt;Save workspace to .RData on exit&lt;/em&gt; to &lt;em&gt;Never&lt;/em&gt; and uncheck the &lt;em&gt;Restore .RData into workspace at start&lt;/em&gt;. By default, when you exit &lt;code&gt;R&lt;/code&gt; saves all the objects you have created into a file called .RData. This is done so that when you restart the session in the same folder, it will load these objects. I find that this causes confusion especially when sharing code with colleagues or peers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-r-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installing &lt;code&gt;R&lt;/code&gt; packages&lt;/h2&gt;
&lt;p&gt;The functionality provided by a fresh install of &lt;code&gt;R&lt;/code&gt; is only a small fraction of what is possible. In fact, we refer to what you get after your first install as &lt;em&gt;base R&lt;/em&gt;. The extra functionality comes from add-ons available from developers. There are currently hundreds of these available from CRAN and many others shared via other repositories such as GitHub. However, because not everybody needs all available functionality, &lt;code&gt;R&lt;/code&gt; instead makes different components available via &lt;em&gt;packages&lt;/em&gt;. &lt;code&gt;R&lt;/code&gt; makes it very easy to install packages from within R. For example, to install the &lt;strong&gt;dslabs&lt;/strong&gt; package, which we use to share datasets and code related to this book, you would type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;dslabs&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In RStudio, you can navigate to the &lt;em&gt;Tools&lt;/em&gt; tab and select install packages. We can then load the package into our &lt;code&gt;R&lt;/code&gt; sessions using the &lt;code&gt;library&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dslabs&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:gapminder&amp;#39;:
## 
##     gapminder&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you go through this book, you will see that we load packages without installing them. This is because once you install a package, it remains installed and only needs to be loaded with &lt;code&gt;library&lt;/code&gt;. The package remains loaded until we quit the &lt;code&gt;R&lt;/code&gt; session. If you try to load a package and get an error, it probably means you need to
install it first.&lt;/p&gt;
&lt;p&gt;We can install more than one package at once by feeding a character vector to this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;tidyverse&amp;quot;, &amp;quot;dslabs&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One advantage of using RStudio is that it auto-completes package names once you start typing, which is helpful when you do not remember the exact spelling of the package. Once you select your package, we recommend selecting all the defaults. Note that installing &lt;strong&gt;tidyverse&lt;/strong&gt; actually installs several packages. This commonly occurs when a package has &lt;em&gt;dependencies&lt;/em&gt;, or uses functions from other packages. When you load a package using &lt;code&gt;library&lt;/code&gt;, you also load its dependencies.&lt;/p&gt;
&lt;p&gt;Once packages are installed, you can load them into &lt;code&gt;R&lt;/code&gt; and you do not need to install them again, unless you install a fresh version of R. Remember packages are installed in &lt;code&gt;R&lt;/code&gt; not RStudio.&lt;/p&gt;
&lt;p&gt;It is helpful to keep a list of all the packages you need for your work in a script because if you need to perform a fresh install of R, you can re-install all your packages by simply running a script.&lt;/p&gt;
&lt;p&gt;You can see all the packages you have installed using the following function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;installed.packages()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we move through this course, we will constantly be adding to our toolbox of packages. Accordingly, you will need to keep track to ensure you have the requisite package for any given lecture.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rmarkdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rmarkdown&lt;/h2&gt;
&lt;p&gt;Markdown is a general-purpose syntax for laying out documents. Rmarkdown is a combination of R and markdown, as the name implies. When using markdown, one can define headers and tables using specific notation, and depending on the rendering engine, the headers and tables (and a whole lot more) are customized. In fact, this whole website is built in R using Rmarkdown (and a lot of add-ons like Hugo and blogdown). In other contexts, the rendering engine may recognize that your headers are likely to be entries in a table of contents, and does so for you. The table of contents at the top of this document is built from the markdown headers.&lt;/p&gt;
&lt;p&gt;The power of Rmarkdown is that it lets us mix formatted text with R code. That is, you can have a section of the document that understands R code, and a separate section right after that discusses the results from the R code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ajkirkpatrick/FS20/Spring2021/Rmarkdown_templates/SSC442_Weekly_Writing_Template.Rmd&#34;&gt;Try it out using the Weekly Writing Template&lt;/a&gt;. If it opens in your web browser, just right-click the link and select Save As…. &lt;strong&gt;Make sure you save the file to its own folder on your hard drive&lt;/strong&gt;. In converting your Rmarkdown .Rmd file to a .pdf, your system will make multiple interim files&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. It also creates folders to store the output of any plots or graphics you create with your R code.&lt;/p&gt;
&lt;p&gt;If we have time today, let’s open the template linked above and see what happens when we select “knit to pdf”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lecture-video&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lecture Video&lt;/h2&gt;
&lt;p&gt;Video from lecture &lt;a href=&#34;https://mediaspace.msu.edu/media/Jan19_Example0/1_e4uuw4e0&#34;&gt;hosted on Mediaspace &lt;i class=&#34;fas fa-film&#34;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Comments from previous classes indicate that I am not, in fact, funny.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf&#34; class=&#34;uri&#34;&gt;https://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://opensource.org/history&#34; class=&#34;uri&#34;&gt;https://opensource.org/history&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/138/free-resources-for-learning-r&#34; class=&#34;uri&#34;&gt;https://stats.stackexchange.com/questions/138/free-resources-for-learning-r&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.r-project.org/help.html&#34; class=&#34;uri&#34;&gt;https://www.r-project.org/help.html&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/documentation/r/topics&#34; class=&#34;uri&#34;&gt;https://stackoverflow.com/documentation/r/topics&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;But probably tip more than 15%. Times are tough, man.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.rstudio.com/&#34; class=&#34;uri&#34;&gt;https://www.rstudio.com/&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;Specifically, knitr will create an intermediate .md file which is then processed with Pandoc using Latex to create a pdf. Whew!&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Visualization</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/01-example/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/01-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction-to-data-visualization&#34;&gt;Introduction to data visualization&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#video&#34;&gt;Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;introduction-to-data-visualization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to data visualization&lt;/h1&gt;
&lt;p&gt;Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total
## 1    Alabama  AL  South    4779736   135
## 2     Alaska  AK   West     710231    19
## 3    Arizona  AZ   West    6392017   232
## 4   Arkansas  AR  South    2915918    93
## 5 California  CA   West   37253956  1257
## 6   Colorado  CO   West    5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t &lt;strong&gt;learn&lt;/strong&gt; anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggthemes)
library(ggrepel)

r &amp;lt;- murders %&amp;gt;%
  summarize(pop=sum(population), tot=sum(total)) %&amp;gt;%
  mutate(rate = tot/pop*10^6) %&amp;gt;% pull(rate)

murders %&amp;gt;% ggplot(aes(x = population/10^6, y = total, label = abb)) +
  geom_abline(intercept = log10(r), lty=2, col=&amp;quot;darkgrey&amp;quot;) +
  geom_point(aes(color=region), size = 3) +
  geom_text_repel() +
  scale_x_log10() +
  scale_y_log10() +
  xlab(&amp;quot;Populations in millions (log scale)&amp;quot;) +
  ylab(&amp;quot;Total number of murders (log scale)&amp;quot;) +
  ggtitle(&amp;quot;US Gun Murders in 2010&amp;quot;) +
  scale_color_discrete(name=&amp;quot;Region&amp;quot;) +
  theme_economist_white()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/01-example_files/figure-html/ggplot-example-plot-0-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.&lt;/p&gt;
&lt;p&gt;The growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing &lt;em&gt;data journalism&lt;/em&gt; and including effective &lt;em&gt;infographics&lt;/em&gt; as part of their reporting.&lt;/p&gt;
&lt;p&gt;A particularly salient example—given the current state of the world—is a Wall Street Journal article&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/01-example_files/figure-html/wsj-vaccines-example-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: &lt;a href=&#34;http://graphics.wsj.com/infectious-diseases-and-vaccines/&#34;&gt;Wall Street Journal&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Another striking example comes from a New York Times chart&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, which summarizes scores from the NYC Regents Exams. As described in
the article&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/01-example_files/figure-html/regents-exams-example-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: &lt;a href=&#34;http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif&#34;&gt;New York Times&lt;/a&gt; via Amanda Cox)&lt;/p&gt;
&lt;p&gt;The most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.&lt;/p&gt;
&lt;p&gt;This is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call &lt;em&gt;exploratory data analysis&lt;/em&gt; (EDA). John W. Tukey&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, considered the father of EDA, once said,&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“The greatest value of a picture is when it forces us to notice what we never expected to see.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Many widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.&lt;/p&gt;
&lt;p&gt;Data visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; and The Best Stats You’ve Ever Seen&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/01-example_files/figure-html/gampnider-example-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.&lt;/p&gt;
&lt;p&gt;Today, we will discuss the basics of data visualization and exploratory data analysis. We will use the &lt;strong&gt;ggplot2&lt;/strong&gt; package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.&lt;/p&gt;
&lt;p&gt;Of course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ER Tufte (1983) The visual display of quantitative information.
Graphics Press.&lt;/li&gt;
&lt;li&gt;ER Tufte (1990) Envisioning information. Graphics Press.&lt;/li&gt;
&lt;li&gt;ER Tufte (1997) Visual explanations. Graphics Press.&lt;/li&gt;
&lt;li&gt;WS Cleveland (1993) Visualizing data. Hobart Press.&lt;/li&gt;
&lt;li&gt;WS Cleveland (1994) The elements of graphing data. CRC Press.&lt;/li&gt;
&lt;li&gt;A Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach:
Turning tables into graphs. The American Statistician 56:121-130.&lt;/li&gt;
&lt;li&gt;NB Robbins (2004) Creating more effective graphs. Wiley.&lt;/li&gt;
&lt;li&gt;A Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders.&lt;/li&gt;
&lt;li&gt;N Yau (2013) Data points: Visualization that means something. Wiley.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;https://shiny.rstudio.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://d3js.org/&#34;&gt;https://d3js.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;p&gt;Some of the code from today’s class will be available below &lt;em&gt;after&lt;/em&gt; the class.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;video&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Video&lt;/h2&gt;
&lt;p&gt;Video from today’s class will be available below after the class.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e&#34; class=&#34;uri&#34;&gt;http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif&#34; class=&#34;uri&#34;&gt;http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.nytimes.com/2011/02/19/nyregion/19schools.html&#34; class=&#34;uri&#34;&gt;https://www.nytimes.com/2011/02/19/nyregion/19schools.html&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/John_Tukey&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/John_Tukey&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Illustrating Classification</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/10-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/10-example/</guid>
      <description>


&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will come from the “Content” tab.&lt;/p&gt;
&lt;p&gt;We may use last week’s dataset (it is covered in Lab 9). You can find it below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/projects/data/bank.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;bank.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/11-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/11-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-to-be-wrangled&#34;&gt;Data to be wrangled&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;data-to-be-wrangled&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data to be wrangled&lt;/h1&gt;
&lt;p&gt;You work for a travel booking website as a data analyst. A hotel has asked your company for data on corporate bookings at the hotel via your site. Specifically, they have five corporations that are frequent customers of the hotel, and they want to know who spends the most with them. They’ve asked you to help out. Most of the corporate spending is in the form of room reservations, but there are also parking fees that the hotel wants included in the analysis. Your goal: total up spending by corporation and report the biggest and smallest spenders inclusive of rooms and parking.&lt;/p&gt;
&lt;p&gt;Unfortunately, you only have the following data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/data/Example11_booking.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;booking.csv&lt;/code&gt;&lt;/a&gt; - Contains the corporation name, the room type, and the dates someone from the corporation stayed at the hoted.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/data/Example11_roomrates.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;roomrates.csv&lt;/code&gt;&lt;/a&gt; - Contains the price of each room on each day&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/data/Example11_parking.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;parking.csv&lt;/code&gt;&lt;/a&gt; - Contains the corporations who negotiated free parking for employees&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Parking at the hotel is $60 if you don’t have free parking. This hotel is in California, so everyone drives and parks when they stay.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Some tips:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Right-click on each of the links, copy the address, and read the URL in using &lt;code&gt;read.csv&lt;/code&gt; or &lt;code&gt;read_csv&lt;/code&gt; or whatever you prefer to read .csv’s&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You’ll find you need to use most of the tools we covered on Tuesday including &lt;code&gt;gather&lt;/code&gt;, &lt;code&gt;separate&lt;/code&gt; and more.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Text as Data</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/12-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/12-example/</guid>
      <description>


&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will come from the “Content” tab.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Regression</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/05-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/05-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#code&#34;&gt;Code&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#load-and-clean-data&#34;&gt;Load and clean data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#legal-dual-y-axes&#34;&gt;Legal dual y-axes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#combining-plots&#34;&gt;Combining plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scatterplot-matrices&#34;&gt;Scatterplot matrices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlograms&#34;&gt;Correlograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simple-regression&#34;&gt;Simple regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-plots&#34;&gt;Coefficient plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#marginal-effects-plots&#34;&gt;Marginal effects plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will pivot between the content from this week and the example below.&lt;/p&gt;
&lt;p&gt;We will also use the Ames data again (in addition to the Atlanta weather data). The Ames data is linked below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;For this example, we’re again going to use historical weather data from &lt;a href=&#34;https://darksky.net/forecast/33.7546,-84.39/us12/en&#34;&gt;Dark Sky&lt;/a&gt; about wind speed and temperature trends for downtown Atlanta (&lt;a href=&#34;https://www.google.com/maps/place/33°45&amp;#39;16.4%22N+84°23&amp;#39;24.0%22W/@33.754557,-84.3921977,17z/&#34;&gt;specifically &lt;code&gt;33.754557, -84.390009&lt;/code&gt;&lt;/a&gt;) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the &lt;a href=&#34;https://github.com/hrbrmstr/darksky&#34;&gt;&lt;strong&gt;darksky&lt;/strong&gt; package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/data/atl-weather-2019.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;atl-weather-2019.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code&lt;/h2&gt;
&lt;div id=&#34;load-and-clean-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Load and clean data&lt;/h3&gt;
&lt;p&gt;First, we load the libraries we’ll be using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)  # For ggplot, dplyr, and friends
library(patchwork)  # For combining ggplot plots
library(GGally)     # For scatterplot matrices
library(broom)      # For converting model objects to data frames&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we load the data with &lt;code&gt;read_csv()&lt;/code&gt;. Here we assume that the CSV file lives in a subfolder named &lt;code&gt;data&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl &amp;lt;- read_csv(&amp;quot;data/atl-weather-2019.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;legal-dual-y-axes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Legal dual y-axes&lt;/h3&gt;
&lt;p&gt;It is fine (and often helpful) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.&lt;/p&gt;
&lt;p&gt;To do this, you need to add an argument (&lt;code&gt;sec.axis&lt;/code&gt;) to &lt;code&gt;scale_y_continuous()&lt;/code&gt; to tell it to use a second axis. This &lt;code&gt;sec.axis&lt;/code&gt; argument takes a &lt;code&gt;sec_axis()&lt;/code&gt; function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a &lt;code&gt;~&lt;/code&gt;, which indicates that it’s a function, and it needs to use &lt;code&gt;.&lt;/code&gt; to stand in for the original value in the original axis.&lt;/p&gt;
&lt;p&gt;Since the equation for converting Fahrenheit to Celsius is this…&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{C} = (32 - \text{F}) \times -\frac{5}{9}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;…we can specify this with code like so (where &lt;code&gt;.&lt;/code&gt; stands for the Fahrenheit value):&lt;/p&gt;
&lt;pre class=&#34;text&#34;&gt;&lt;code&gt;~ (32 - .) * -5 / 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +
  geom_line() +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,
                                         name = &amp;quot;Celsius&amp;quot;)) +
  labs(x = NULL, y = &amp;quot;Fahrenheit&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/atl-weather-dual-axes-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For fun, we could also convert it to Kelvin, which uses this formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{K} = (\text{F} - 32) \times \frac{5}{9} + 273.15
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +
  geom_line() +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15,
                                         name = &amp;quot;Kelvin&amp;quot;)) +
  labs(x = NULL, y = &amp;quot;Fahrenheit&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/atl-weather-dual-axes-kelvin-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;combining-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Combining plots&lt;/h3&gt;
&lt;p&gt;A good alternative to using two y-axes is to use two plots instead. The &lt;a href=&#34;https://github.com/thomasp85/patchwork&#34;&gt;&lt;strong&gt;patchwork&lt;/strong&gt; package&lt;/a&gt; makes this &lt;em&gt;really&lt;/em&gt; easy to do with R. There are other similar packages that do this, like &lt;strong&gt;cowplot&lt;/strong&gt; and &lt;strong&gt;gridExtra&lt;/strong&gt;, but I’ve found that &lt;strong&gt;patchwork&lt;/strong&gt; is the easiest to use &lt;em&gt;and&lt;/em&gt; it actually aligns the different plot elements like axis lines and legends (yay alignment in CRAP!). The &lt;a href=&#34;https://patchwork.data-imaginist.com/articles/guides/assembly.html&#34;&gt;documentation for &lt;strong&gt;patchwork&lt;/strong&gt;&lt;/a&gt; is really great and full of examples—you should check it out to see all the things you can do with it!&lt;/p&gt;
&lt;p&gt;To use &lt;strong&gt;patchwork&lt;/strong&gt;, we need to (1) save our plots as objects and (2) add them together with &lt;code&gt;+&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Temperature in Atlanta
temp_plot &amp;lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) +
  geom_line() +
  geom_smooth() +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9,
                                         name = &amp;quot;Celsius&amp;quot;)) +
  labs(x = NULL, y = &amp;quot;Fahrenheit&amp;quot;) +
  theme_minimal()
temp_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/create-temp-humid-plots-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Humidity in Atlanta
humidity_plot &amp;lt;- ggplot(weather_atl, aes(x = time, y = humidity)) +
  geom_line() +
  geom_smooth() +
  labs(x = NULL, y = &amp;quot;Humidity&amp;quot;) +
  theme_minimal()
humidity_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/create-temp-humid-plots-2.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Right now, these are two separate plots, but we can combine them with &lt;code&gt;+&lt;/code&gt; if we load &lt;strong&gt;patchwork&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)

temp_plot + humidity_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/patchwork-first-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By default, &lt;strong&gt;patchwork&lt;/strong&gt; will put these side-by-side, but we can change that with the &lt;code&gt;plot_layout()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp_plot + humidity_plot +
  plot_layout(ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/patchwork-vertical-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also play with other arguments in &lt;code&gt;plot_layout()&lt;/code&gt;. If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp_plot + humidity_plot +
  plot_layout(ncol = 1, heights = c(0.7, 0.3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/patchwork-vertical-resized-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplot-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scatterplot matrices&lt;/h3&gt;
&lt;p&gt;We can visualize the correlations between pairs of variables with the &lt;code&gt;ggpairs()&lt;/code&gt; function in the &lt;strong&gt;GGally&lt;/strong&gt; package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into &lt;code&gt;ggpairs()&lt;/code&gt; to see all the correlation information:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(GGally)

weather_correlations &amp;lt;- weather_atl %&amp;gt;%
  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability)

ggpairs(weather_correlations)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/ggpairs-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind spped and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).&lt;/p&gt;
&lt;p&gt;Even though &lt;code&gt;ggpairs()&lt;/code&gt; doesn’t use the standard &lt;code&gt;ggplot(...) + geom_whatever()&lt;/code&gt; syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggpairs(weather_correlations) +
  labs(title = &amp;quot;Correlations!&amp;quot;) +
  theme_dark()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Make a ggpairs plot for some of the Ames data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;correlograms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlograms&lt;/h3&gt;
&lt;p&gt;Scatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In the readings for today, Claus Wilke showed a type of plot called a &lt;a href=&#34;https://serialmentor.com/dataviz/visualizing-associations.html#associations-correlograms&#34;&gt;&lt;em&gt;correlogram&lt;/em&gt;&lt;/a&gt; which &lt;em&gt;is&lt;/em&gt; more appropriate for publication.&lt;/p&gt;
&lt;p&gt;These are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.&lt;/p&gt;
&lt;p&gt;First we need to build a correlation matrix of the main variables we care about. Ordinarily the &lt;code&gt;cor()&lt;/code&gt; function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into &lt;code&gt;cor()&lt;/code&gt; though, it’ll calculate the correlation between each pair of columns&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a correlation matrix
things_to_correlate &amp;lt;- weather_atl %&amp;gt;%
  select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %&amp;gt;%
  cor()

things_to_correlate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   temperatureHigh temperatureLow humidity windSpeed precipProbability
## temperatureHigh              1.00          0.920   -0.030    -0.377            -0.124
## temperatureLow               0.92          1.000    0.112    -0.450            -0.026
## humidity                    -0.03          0.112    1.000     0.011             0.722
## windSpeed                   -0.38         -0.450    0.011     1.000             0.196
## precipProbability           -0.12         -0.026    0.722     0.196             1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to &lt;code&gt;NA&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get rid of the lower triangle
things_to_correlate[lower.tri(things_to_correlate)] &amp;lt;- NA
things_to_correlate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   temperatureHigh temperatureLow humidity windSpeed precipProbability
## temperatureHigh                 1           0.92    -0.03    -0.377            -0.124
## temperatureLow                 NA           1.00     0.11    -0.450            -0.026
## humidity                       NA             NA     1.00     0.011             0.722
## windSpeed                      NA             NA       NA     1.000             0.196
## precipProbability              NA             NA       NA        NA             1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the &lt;code&gt;things_to_correlate&lt;/code&gt; matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named &lt;code&gt;measure1&lt;/code&gt;, and take all the correlation numbers and put them in a column named &lt;code&gt;cor&lt;/code&gt; In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;things_to_correlate_long &amp;lt;- things_to_correlate %&amp;gt;%
  # Convert from a matrix to a data frame
  as.data.frame() %&amp;gt;%
  # Matrixes have column names that don&amp;#39;t get converted to columns when using
  # as.data.frame(), so this adds those names as a column
  rownames_to_column(&amp;quot;measure2&amp;quot;) %&amp;gt;%
  # Make this long. Take all the columns except measure2 and put their names in
  # a column named measure1 and their values in a column named cor
  pivot_longer(cols = -measure2,
               names_to = &amp;quot;measure1&amp;quot;,
               values_to = &amp;quot;cor&amp;quot;) %&amp;gt;%
  # Make a new column with the rounded version of the correlation value
  mutate(nice_cor = round(cor, 2)) %&amp;gt;%
  # Remove rows where the two measures are the same (like the correlation
  # between humidity and humidity)
  filter(measure2 != measure1) %&amp;gt;%
  # Get rid of the empty triangle
  filter(!is.na(cor)) %&amp;gt;%
  # Put these categories in order
  mutate(measure1 = fct_inorder(measure1),
         measure2 = fct_inorder(measure2))

things_to_correlate_long&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 4
##    measure2        measure1              cor nice_cor
##    &amp;lt;fct&amp;gt;           &amp;lt;fct&amp;gt;               &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 temperatureHigh temperatureLow     0.920      0.92
##  2 temperatureHigh humidity          -0.0301    -0.03
##  3 temperatureHigh windSpeed         -0.377     -0.38
##  4 temperatureHigh precipProbability -0.124     -0.12
##  5 temperatureLow  humidity           0.112      0.11
##  6 temperatureLow  windSpeed         -0.450     -0.45
##  7 temperatureLow  precipProbability -0.0255    -0.03
##  8 humidity        windSpeed          0.0108     0.01
##  9 humidity        precipProbability  0.722      0.72
## 10 windSpeed       precipProbability  0.196      0.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Phew. With the data all tidied like that, we can make a correlogram with a heatmap. We have manipulated the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(things_to_correlate_long,
       aes(x = measure2, y = measure1, fill = cor)) +
  geom_tile() +
  geom_text(aes(label = nice_cor)) +
  scale_fill_gradient2(low = &amp;quot;#E16462&amp;quot;, mid = &amp;quot;white&amp;quot;, high = &amp;quot;#0D0887&amp;quot;,
                       limits = c(-1, 1)) +
  labs(x = NULL, y = NULL) +
  coord_equal() +
  theme_minimal() +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/cor-heatmap-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Instead of using a heatmap, we can also use points, which encode the correlation information both as color &lt;em&gt;and&lt;/em&gt; as size. To do that, we just need to switch &lt;code&gt;geom_tile()&lt;/code&gt; to &lt;code&gt;geom_point()&lt;/code&gt; and set the &lt;code&gt;size = cor&lt;/code&gt; mapping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(things_to_correlate_long,
       aes(x = measure2, y = measure1, color = cor)) +
  # Size by the absolute value so that -0.7 and 0.7 are the same size
  geom_point(aes(size = abs(cor))) +
  scale_color_gradient2(low = &amp;quot;#E16462&amp;quot;, mid = &amp;quot;white&amp;quot;, high = &amp;quot;#0D0887&amp;quot;,
                        limits = c(-1, 1)) +
  scale_size_area(max_size = 15, limits = c(-1, 1), guide = FALSE) +
  labs(x = NULL, y = NULL) +
  coord_equal() +
  theme_minimal() +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/cor-points-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Simple regression&lt;/h3&gt;
&lt;p&gt;We finally get to this week’s content. Although correlation is nice, we eventually may want to visualize regression. The lecture has shown us some very intuitive, straightforward ways to think about regression (aka, a line). Simple regression is easy to visualize, since you’re only working with an &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and a &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. For instance, what’s the relationship between humidity and high temperatures during the summer?&lt;/p&gt;
&lt;p&gt;First, let’s filter the data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weather_atl_summer &amp;lt;- weather_atl %&amp;gt;%
  filter(time &amp;gt;= &amp;quot;2019-05-01&amp;quot;, time &amp;lt;= &amp;quot;2019-09-30&amp;quot;) %&amp;gt;%
  mutate(humidity_scaled = humidity * 100,
         moonPhase_scaled = moonPhase * 100,
         precipProbability_scaled = precipProbability * 100,
         cloudCover_scaled = cloudCover * 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can build a simple regression model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_simple &amp;lt;- lm(temperatureHigh ~ humidity_scaled,
                   data = weather_atl_summer)

tidy(model_simple, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   term            estimate std.error statistic  p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)      104.       2.35       44.3  1.88e-88   99.5     109.   
## 2 humidity_scaled   -0.241    0.0358     -6.74 3.21e-10   -0.312    -0.170&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can interpret these coefficients like so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The intercept shows that the average temperature when there’s 0% humidity is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line.&lt;/li&gt;
&lt;li&gt;The coefficient for &lt;code&gt;humidity_scaled&lt;/code&gt; shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Visualizing this model is simple, since there are only two variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(weather_atl_summer,
       aes(x = humidity_scaled, y = temperatureHigh)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/plot-simple-model-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And indeed, as humidity increases, temperatures decrease.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Coefficient plots&lt;/h3&gt;
&lt;p&gt;But if we use multiple variables in the model (and we will do this a lot going forward), it gets really hard to visualize the results since we’re working with multiple dimensions. Instead, we can use coefficient plots to see the individual coefficients in the model.&lt;/p&gt;
&lt;p&gt;First, let’s build a more complex model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_complex &amp;lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled +
                      precipProbability_scaled + windSpeed + pressure + cloudCover_scaled,
                    data = weather_atl_summer)
tidy(model_complex, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 7
##   term                     estimate std.error statistic   p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;                       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)              262.      125.         2.09  0.0380    14.8      510.    
## 2 humidity_scaled           -0.111     0.0757    -1.47  0.143     -0.261      0.0381
## 3 moonPhase_scaled           0.0116    0.0126     0.917 0.360     -0.0134     0.0366
## 4 precipProbability_scaled   0.0356    0.0203     1.75  0.0820    -0.00458    0.0758
## 5 windSpeed                 -1.78      0.414     -4.29  0.0000326 -2.59      -0.958 
## 6 pressure                  -0.157     0.122     -1.28  0.203     -0.398      0.0854
## 7 cloudCover_scaled         -0.0952    0.0304    -3.14  0.00207   -0.155     -0.0352&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can interpret these coefficients like so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Holding everything else constant, a 1% increase in humidity is associated with a 0.11° decrease in the high temperature, on average, but the effect is not statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1% increase in moon visibility is associated with a 0.01° increase in the high temperature, on average, and the effect is not statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1% increase in the probability of precipitation is associated with a 0.04° increase in the high temperature, on average, and the effect is not statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1 mph increase in the wind speed is associated with a 1.8° decrease in the high temperature, on average, and the effect &lt;em&gt;is&lt;/em&gt; statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1 unit increase in barometric pressure is associated with a 0.15° decrease in the high temperature, on average, and the effect is not statistically significant&lt;/li&gt;
&lt;li&gt;Holding everything else constant, a 1% increase in cloud cover is associated with a 0.01° decrease in the high temperature, on average, and the effect &lt;em&gt;is&lt;/em&gt; statistically significant&lt;/li&gt;
&lt;li&gt;The intercept is pretty useless. It shows that the predicted temperature will be 262° when humidity is 0%, the moon is invisible, there’s no chance of precipitation, no wind, no barometric pressure, and no cloud cover. Yikes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To plot all these things at once, we’ll store the results of &lt;code&gt;tidy(model_complex)&lt;/code&gt; as a data frame, remove the useless intercept, and plot it using &lt;code&gt;geom_pointrange()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_tidied &amp;lt;- tidy(model_complex, conf.int = TRUE) %&amp;gt;%
  filter(term != &amp;quot;(Intercept)&amp;quot;)

ggplot(model_tidied,
       aes(x = estimate, y = term)) +
  geom_vline(xintercept = 0, color = &amp;quot;red&amp;quot;, linetype = &amp;quot;dotted&amp;quot;) +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) +
  labs(x = &amp;quot;Coefficient estimate&amp;quot;, y = NULL) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/coef-plot-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Neat! Now we can see how big these different coefficients are and how close they are to zero. Wind speed has a big significant effect on temperature. The others are all very close to zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-effects-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Marginal effects plots&lt;/h3&gt;
&lt;p&gt;Instead of just looking at the coefficients, we can also see the effect of moving different variables up and down like sliders and switches. Remember that regression coefficients allow us to build actual mathematical formulas that predict the value of Y. The coefficients from &lt;code&gt;model_compex&lt;/code&gt; yield the following big hairy ugly equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat{\text{High temperature}} =&amp;amp; 262 - 0.11 \times \text{humidity_scaled } \\
&amp;amp; + 0.01 \times \text{moonPhase_scaled } + 0.04 \times \text{precipProbability_scaled } \\
&amp;amp; - 1.78 \times \text{windSpeed} - 0.16 \times \text{pressure} - 0.095 \times \text{cloudCover_scaled}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we plug in values for each of the explanatory variables, we can get the predicted value of high temperature, or &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;augment()&lt;/code&gt; function in the &lt;strong&gt;broom&lt;/strong&gt; library allows us to take a data frame of explanatory variable values, plug them into the model equation, and get predictions out. For example, let’s set each of the variables to some arbitrary values (50% for humidity, moon phase, chance of rain, and cloud cover; 1000 for pressure, and 1 MPH for wind speed).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata_example &amp;lt;- tibble(humidity_scaled = 50, moonPhase_scaled = 50,
                          precipProbability_scaled = 50, windSpeed = 1,
                          pressure = 1000, cloudCover_scaled = 50)
newdata_example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   humidity_scaled moonPhase_scaled precipProbability_scaled windSpeed pressure cloudCover_scaled
##             &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;                    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
## 1              50               50                       50         1     1000                50&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plug these values into the model with &lt;code&gt;augment()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# I use select() here because augment() returns columns for all the explanatory
# variables, and the .fitted column with the predicted value is on the far right
# and gets cut off
augment(model_complex, newdata = newdata_example, se_fit=TRUE) %&amp;gt;%
  select(.fitted, .se.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given these weather conditions, the predicted high temperature is 96.2°. Now you’re an armchair meteorologist!&lt;/p&gt;
&lt;p&gt;We can follow the same pattern to show how the predicted temperature changes as specific variables change across a whole range. Here, we create a data frame of possible wind speeds and keep all the other explanatory variables at their means:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata &amp;lt;- tibble(windSpeed = seq(0, 8, 0.5),
                  pressure = mean(weather_atl_summer$pressure),
                  precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),
                  moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),
                  humidity_scaled = mean(weather_atl_summer$humidity_scaled),
                  cloudCover_scaled = mean(weather_atl_summer$cloudCover_scaled))
newdata&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17 x 6
##    windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled
##        &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;                    &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
##  1       0      1016.                     40.2             50.7            64.8              29.5
##  2       0.5    1016.                     40.2             50.7            64.8              29.5
##  3       1      1016.                     40.2             50.7            64.8              29.5
##  4       1.5    1016.                     40.2             50.7            64.8              29.5
##  5       2      1016.                     40.2             50.7            64.8              29.5
##  6       2.5    1016.                     40.2             50.7            64.8              29.5
##  7       3      1016.                     40.2             50.7            64.8              29.5
##  8       3.5    1016.                     40.2             50.7            64.8              29.5
##  9       4      1016.                     40.2             50.7            64.8              29.5
## 10       4.5    1016.                     40.2             50.7            64.8              29.5
## 11       5      1016.                     40.2             50.7            64.8              29.5
## 12       5.5    1016.                     40.2             50.7            64.8              29.5
## 13       6      1016.                     40.2             50.7            64.8              29.5
## 14       6.5    1016.                     40.2             50.7            64.8              29.5
## 15       7      1016.                     40.2             50.7            64.8              29.5
## 16       7.5    1016.                     40.2             50.7            64.8              29.5
## 17       8      1016.                     40.2             50.7            64.8              29.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we feed this big data frame into &lt;code&gt;augment()&lt;/code&gt;, we can get the predicted high temperature for each row. We can also use the &lt;code&gt;.se.fit&lt;/code&gt; column to calculate the 95% confidence interval for each predicted value. We take the standard error, multiply it by -1.96 and 1.96 (or the quantile of the normal distribution at 2.5% and 97.5%), and add that value to the estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted_values &amp;lt;- augment(model_complex, newdata = newdata, se_fit=TRUE) %&amp;gt;%
  mutate(conf.low = .fitted + (-1.96 * .se.fit),
         conf.high = .fitted + (1.96 * .se.fit))

predicted_values %&amp;gt;%
  select(windSpeed, .fitted, .se.fit, conf.low, conf.high) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   windSpeed .fitted .se.fit conf.low conf.high
##       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1       0      95.3   1.63      92.2      98.5
## 2       0.5    94.5   1.42      91.7      97.2
## 3       1      93.6   1.22      91.2      96.0
## 4       1.5    92.7   1.03      90.7      94.7
## 5       2      91.8   0.836     90.1      93.4
## 6       2.5    90.9   0.653     89.6      92.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Cool! Just looking at the data in the table, we can see that predicted temperature drops as windspeed increases. We can plot this to visualize the effect:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(predicted_values, aes(x = windSpeed, y = .fitted)) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high),
              fill = &amp;quot;#BF3984&amp;quot;, alpha = 0.5) +
  geom_line(size = 1, color = &amp;quot;#BF3984&amp;quot;) +
  labs(x = &amp;quot;Wind speed (MPH)&amp;quot;, y = &amp;quot;Predicted high temperature (F)&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/mfx-plot-simple-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We just manipulated one of the model coefficients and held everything else at its mean. We can manipulate multiple variables too and encode them all on the graph. For instance, what is the effect of windspeed &lt;em&gt;and&lt;/em&gt; cloud cover on the temperature?&lt;/p&gt;
&lt;p&gt;We’ll follow the same process, but vary both &lt;code&gt;windSpeed&lt;/code&gt; and &lt;code&gt;cloudCover_scaled&lt;/code&gt;. Instead of using &lt;code&gt;tibble()&lt;/code&gt;, we use &lt;code&gt;exapnd_grid()&lt;/code&gt;, which creates every combination of the variables we specify. Instead of varying cloud cover by every possible value (like from 0 to 100), we’ll choose four possible cloud cover types: 0%, 33%, 66%, and 100%. Everything else will be at its mean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata_fancy &amp;lt;- expand_grid(windSpeed = seq(0, 8, 0.5),
                             pressure = mean(weather_atl_summer$pressure),
                             precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled),
                             moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled),
                             humidity_scaled = mean(weather_atl_summer$humidity_scaled),
                             cloudCover_scaled = c(0, 33, 66, 100))
newdata_fancy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 68 x 6
##    windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled
##        &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;                    &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt;
##  1       0      1016.                     40.2             50.7            64.8                 0
##  2       0      1016.                     40.2             50.7            64.8                33
##  3       0      1016.                     40.2             50.7            64.8                66
##  4       0      1016.                     40.2             50.7            64.8               100
##  5       0.5    1016.                     40.2             50.7            64.8                 0
##  6       0.5    1016.                     40.2             50.7            64.8                33
##  7       0.5    1016.                     40.2             50.7            64.8                66
##  8       0.5    1016.                     40.2             50.7            64.8               100
##  9       1      1016.                     40.2             50.7            64.8                 0
## 10       1      1016.                     40.2             50.7            64.8                33
## # ... with 58 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice now that &lt;code&gt;windSpeed&lt;/code&gt; repeats four times (0, 0, 0, 0, 0.5, 0.5, etc.), since there are four possible &lt;code&gt;cloudCover_scaled&lt;/code&gt; values (0, 33, 66, 100).&lt;/p&gt;
&lt;p&gt;We can plot this now, just like before, with wind speed on the x-axis, the predicted temperature on the y-axis, and colored and faceted by cloud cover:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predicted_values_fancy &amp;lt;- augment(model_complex, newdata = newdata_fancy, se_fit=TRUE) %&amp;gt;%
  mutate(conf.low = .fitted + (-1.96 * .se.fit),
         conf.high = .fitted + (1.96 * .se.fit)) %&amp;gt;%
  # Make cloud cover a categorical variable
  mutate(cloudCover_scaled = factor(cloudCover_scaled))

ggplot(predicted_values_fancy, aes(x = windSpeed, y = .fitted)) +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled),
              alpha = 0.5) +
  geom_line(aes(color = cloudCover_scaled), size = 1) +
  labs(x = &amp;quot;Wind speed (MPH)&amp;quot;, y = &amp;quot;Predicted high temperature (F)&amp;quot;) +
  theme_minimal() +
  guides(fill = FALSE, color = FALSE) +
  facet_wrap(vars(cloudCover_scaled), nrow = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/05-example_files/figure-html/mfx-complex-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nice. Temperatures go down slightly as cloud cover increases. If we wanted to improve the model, we’d add an interaction term between cloud cover and windspeed so that each line would have a different slope in addition to a different intercept, but that’s beyond the scope of this class.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression: Interpreting Coefficients</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/06-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/06-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preliminaries&#34;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dummy-variables&#34;&gt;Dummy Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interactions&#34;&gt;Interactions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#factor-variables&#34;&gt;Factor Variables&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#factors-with-more-than-two-levels&#34;&gt;Factors with More Than Two Levels&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#parameterization&#34;&gt;Parameterization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#building-larger-models&#34;&gt;Building Larger Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will pivot between the content from this week and the example below.&lt;/p&gt;
&lt;p&gt;We will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;So far in each of our analyses, we have only used numeric variables as predictors. We have also only used &lt;em&gt;additive models&lt;/em&gt;, meaning the effect any predictor had on the response was not dependent on the other predictors. In this chapter, we will remove both of these restrictions. We will fit models with categorical predictors, and use models that allow predictors to &lt;em&gt;interact&lt;/em&gt;. The mathematics of multiple regression will remain largely unchanging, however, we will pay close attention to interpretation, as well as some difference in &lt;code&gt;R&lt;/code&gt; usage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dummy-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dummy Variables&lt;/h2&gt;
&lt;p&gt;For this example and discussion, we will briefly use the built in dataset &lt;code&gt;mtcars&lt;/code&gt; before returning to our favorite &lt;code&gt;autompg&lt;/code&gt; dataset. During the in-class lecture / example, I will also use much more interesting datasets. The reason to use these easy, straightforward datasets is that they make visualization of the &lt;strong&gt;entire dataset&lt;/strong&gt; trivially easy. Accordingly, the &lt;code&gt;mtcars&lt;/code&gt; dataset is small, so we’ll quickly take a look at the entire dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
## Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
## Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
## Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
## Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
## Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
## Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
## Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
## Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
## Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
## Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
## Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
## Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
## Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
## Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
## Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
## Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
## Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
## AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
## Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
## Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
## Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
## Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
## Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
## Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
## Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
## Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
## Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will be interested in three of the variables: &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;hp&lt;/code&gt;, and &lt;code&gt;am&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mpg&lt;/code&gt;: fuel efficiency, in miles per gallon.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hp&lt;/code&gt;: horsepower, in foot-pounds per second.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;am&lt;/code&gt;: transmission. Automatic or manual.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As we often do, we will start by plotting the data. We are interested in &lt;code&gt;mpg&lt;/code&gt; as the response variable, and &lt;code&gt;hp&lt;/code&gt; as a predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, cex = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/06-example_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Since we are also interested in the transmission type, we could also label the points accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Automatic&amp;quot;, &amp;quot;Manual&amp;quot;), col = c(1, 2), pch = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/06-example_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now fit the SLR model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;hp&lt;/code&gt;. For notational brevity, we drop the index &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; for observations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp_slr = lm(mpg ~ hp, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then re-plot the data and add the fitted line to the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
abline(mpg_hp_slr, lwd = 3, col = &amp;quot;grey&amp;quot;)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Automatic&amp;quot;, &amp;quot;Manual&amp;quot;), col = c(1, 2), pch = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/06-example_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We should notice a pattern here. The red, manual observations largely fall above the line, while the black, automatic observations are mostly below the line. This means our model underestimates the fuel efficiency of manual transmissions, and overestimates the fuel efficiency of automatic transmissions. To correct for this, we will add a predictor to our model, namely, &lt;code&gt;am&lt;/code&gt; as &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Our new model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; remain the same, but now&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_2 =
  \begin{cases}
   1 &amp;amp; \text{manual transmission} \\
   0       &amp;amp; \text{automatic transmission}
  \end{cases}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, we call &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; a &lt;strong&gt;dummy variable&lt;/strong&gt;. A dummy variable (also called an “indicator” variable) is somewhat unfortunately named, as it is in no way “dumb”. In fact, it is actually somewhat clever. A dummy variable is a numerical variable that is used in a regression analysis to “code” for a binary categorical variable. Let’s see how this works.&lt;/p&gt;
&lt;p&gt;First, note that &lt;code&gt;am&lt;/code&gt; is already a dummy variable, since it uses the values &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; to represent automatic and manual transmissions. Often, a variable like &lt;code&gt;am&lt;/code&gt; would store the character values &lt;code&gt;auto&lt;/code&gt; and &lt;code&gt;man&lt;/code&gt; and we would either have to convert these to &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, or, as we will see later, &lt;code&gt;R&lt;/code&gt; will take care of creating dummy variables for us.&lt;/p&gt;
&lt;p&gt;So, to fit the above model, we do so like any other multiple regression model we have seen before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp_add = lm(mpg ~ hp + am, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Briefly checking the output, we see that &lt;code&gt;R&lt;/code&gt; has estimated the three &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp_add&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ hp + am, data = mtcars)
## 
## Coefficients:
## (Intercept)           hp           am  
##    26.58491     -0.05889      5.27709&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; can only take values &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, we can effectively write two different models, one for manual and one for automatic transmissions.&lt;/p&gt;
&lt;p&gt;For automatic transmissions, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 0\)&lt;/span&gt;, we have,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then for manual transmissions, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 1\)&lt;/span&gt;, we have,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = (\beta_0 + \beta_2) + \beta_1 x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice that these models share the same slope, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, but have different intercepts, differing by &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt;. So the change in &lt;code&gt;mpg&lt;/code&gt; is the same for both models, but on average &lt;code&gt;mpg&lt;/code&gt; differs by &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; between the two transmission types.&lt;/p&gt;
&lt;p&gt;We’ll now calculate the estimated slope and intercept of these two models so that we can add them to a plot. Note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0\)&lt;/span&gt; = &lt;code&gt;coef(mpg_hp_add)[1]&lt;/code&gt; = 26.5849137&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt; = &lt;code&gt;coef(mpg_hp_add)[2]&lt;/code&gt; = -0.0588878&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_2\)&lt;/span&gt; = &lt;code&gt;coef(mpg_hp_add)[3]&lt;/code&gt; = 5.2770853&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can then combine these to calculate the estimated slope and intercepts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_auto = coef(mpg_hp_add)[1]
int_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3]

slope_auto = coef(mpg_hp_add)[2]
slope_manu = coef(mpg_hp_add)[2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Re-plotting the data, we use these slopes and intercepts to add the “two” fitted models to the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2)
abline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # add line for auto
abline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # add line for manual
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Automatic&amp;quot;, &amp;quot;Manual&amp;quot;), col = c(1, 2), pch = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/06-example_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We notice right away that the points are no longer systematically incorrect. The red, manual observations vary about the red line in no particular pattern without underestimating the observations as before. The black, automatic points vary about the black line, also without an obvious pattern.&lt;/p&gt;
&lt;p&gt;They say a picture is worth a thousand words, but as a statistician, sometimes a picture is worth an entire analysis. The above picture makes it plainly obvious that &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; is significant, but let’s verify mathematically. Essentially we would like to test:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_2 = 0 \quad \text{vs} \quad H_1: \beta_2 \neq 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is nothing new. Again, the math is the same as the multiple regression analyses we have seen before. We could perform either a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test here. The only difference is a slight change in interpretation. We could think of this as testing a model with a single line (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) against a model that allows two lines (&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;To obtain the test statistic and p-value for the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test, we would use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mpg_hp_add)$coefficients[&amp;quot;am&amp;quot;,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Estimate   Std. Error      t value     Pr(&amp;gt;|t|) 
## 5.277085e+00 1.079541e+00 4.888270e+00 3.460318e-05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To do the same for the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test, we would use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_hp_slr, mpg_hp_add)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ hp
## Model 2: mpg ~ hp + am
##   Res.Df    RSS Df Sum of Sq      F   Pr(&amp;gt;F)    
## 1     30 447.67                                 
## 2     29 245.44  1    202.24 23.895 3.46e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that these are indeed testing the same thing, as the p-values are exactly equal. (And the &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; test statistic is the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; test statistic squared.)&lt;/p&gt;
&lt;p&gt;Recapping some interpretations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 = 26.5849137\)&lt;/span&gt; is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a car with an automatic transmission and &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;hp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 + \hat{\beta}_2 = 31.8619991\)&lt;/span&gt; is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a car with a manual transmission and &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;hp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_2 = 5.2770853\)&lt;/span&gt; is the estimated &lt;strong&gt;difference&lt;/strong&gt; in average &lt;code&gt;mpg&lt;/code&gt; for cars with manual transmissions as compared to those with automatic transmission, for &lt;strong&gt;any&lt;/strong&gt; &lt;code&gt;hp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 = -0.0588878\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in one &lt;code&gt;hp&lt;/code&gt;, for &lt;strong&gt;either&lt;/strong&gt; transmission types.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We should take special notice of those last two. In the model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we see &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the average change in &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for an increase in &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;, &lt;em&gt;no matter&lt;/em&gt; the value of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. Also, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; is always the difference in the average of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for &lt;em&gt;any&lt;/em&gt; value of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;. These are two restrictions we won’t always want, so we need a way to specify a more flexible model.&lt;/p&gt;
&lt;p&gt;Here we restricted ourselves to a single numerical predictor &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and one dummy variable &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. However, the concept of a dummy variable can be used with larger multiple regression models. We only use a single numerical predictor here for ease of visualization since we can think of the “two lines” interpretation. But in general, we can think of a dummy variable as creating “two models,” one for each category of a binary categorical variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interactions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactions&lt;/h2&gt;
&lt;p&gt;To remove the “same slope” restriction, we will now discuss &lt;strong&gt;interaction&lt;/strong&gt;. To illustrate this concept, we will return to the &lt;code&gt;autompg&lt;/code&gt; dataset we created in the last chapter, with a few more modifications.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# read data frame from the web
autompg = read.table(
  &amp;quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&amp;quot;,
  quote = &amp;quot;\&amp;quot;&amp;quot;,
  comment.char = &amp;quot;&amp;quot;,
  stringsAsFactors = FALSE)
# give the dataframe headers
colnames(autompg) = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;acc&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;origin&amp;quot;, &amp;quot;name&amp;quot;)
# remove missing data, which is stored as &amp;quot;?&amp;quot;
autompg = subset(autompg, autompg$hp != &amp;quot;?&amp;quot;)
# remove the plymouth reliant, as it causes some issues
autompg = subset(autompg, autompg$name != &amp;quot;plymouth reliant&amp;quot;)
# give the dataset row names, based on the engine, year and name
rownames(autompg) = paste(autompg$cyl, &amp;quot;cylinder&amp;quot;, autompg$year, autompg$name)
# remove the variable for name
autompg = subset(autompg, select = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;acc&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;origin&amp;quot;))
# change horsepower from character to numeric
autompg$hp = as.numeric(autompg$hp)
# create a dummary variable for foreign vs domestic cars. domestic = 1.
autompg$domestic = as.numeric(autompg$origin == 1)
# remove 3 and 5 cylinder cars (which are very rare.)
autompg = autompg[autompg$cyl != 5,]
autompg = autompg[autompg$cyl != 3,]
# the following line would verify the remaining cylinder possibilities are 4, 6, 8
#unique(autompg$cyl)
# change cyl to a factor variable
autompg$cyl = as.factor(autompg$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    383 obs. of  9 variables:
##  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...
##  $ cyl     : Factor w/ 3 levels &amp;quot;4&amp;quot;,&amp;quot;6&amp;quot;,&amp;quot;8&amp;quot;: 3 3 3 3 3 3 3 3 3 3 ...
##  $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...
##  $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...
##  $ wt      : num  3504 3693 3436 3433 3449 ...
##  $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
##  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...
##  $ origin  : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ve removed cars with &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; cylinders , as well as created a new variable &lt;code&gt;domestic&lt;/code&gt; which indicates whether or not a car was built in the United States. Removing the &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;5&lt;/code&gt; cylinders is simply for ease of demonstration later in the chapter and would not be done in practice. The new variable &lt;code&gt;domestic&lt;/code&gt; takes the value &lt;code&gt;1&lt;/code&gt; if the car was built in the United States, and &lt;code&gt;0&lt;/code&gt; otherwise, which we will refer to as “foreign.” (We are arbitrarily using the United States as the reference point here.) We have also made &lt;code&gt;cyl&lt;/code&gt; and &lt;code&gt;origin&lt;/code&gt; into factor variables, which we will discuss later.&lt;/p&gt;
&lt;p&gt;We’ll now be concerned with three variables: &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;disp&lt;/code&gt;, and &lt;code&gt;domestic&lt;/code&gt;. We will use &lt;code&gt;mpg&lt;/code&gt; as the response. We can fit a model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;domestic&lt;/code&gt; as described above, which is a dummy variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_2 =
  \begin{cases}
   1 &amp;amp; \text{Domestic} \\
   0 &amp;amp; \text{Foreign}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will fit this model, extract the slope and intercept for the “two lines,” plot the data and add the lines.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_add = lm(mpg ~ disp + domestic, data = autompg)

int_for = coef(mpg_disp_add)[1]
int_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3]

slope_for = coef(mpg_disp_add)[2]
slope_dom = coef(mpg_disp_add)[2]

plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)
abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # add line for foreign cars
abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # add line for domestic cars
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Foreign&amp;quot;, &amp;quot;Domestic&amp;quot;), pch = c(1, 2), col = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/06-example_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a model that allows for two &lt;em&gt;parallel&lt;/em&gt; lines, meaning the &lt;code&gt;mpg&lt;/code&gt; can be different on average between foreign and domestic cars of the same engine displacement, but the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in displacement is the same for both. We can see this model isn’t doing very well here. The red line fits the red points fairly well, but the black line isn’t doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes.&lt;/p&gt;
&lt;p&gt;Consider the following model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are the same as before, but we have added a new &lt;strong&gt;interaction&lt;/strong&gt; term &lt;span class=&#34;math inline&#34;&gt;\(x_1 x_2\)&lt;/span&gt; which multiplies &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, so we also have an additional &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This model essentially creates two slopes and two intercepts, &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; being the difference in intercepts and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; being the difference in slopes. To see this, we will break down the model into the two “sub-models” for foreign and domestic cars.&lt;/p&gt;
&lt;p&gt;For foreign cars, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 0\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For domestic cars, that is &lt;span class=&#34;math inline&#34;&gt;\(x_2 = 1\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These two models have both different slopes and intercepts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a foreign car with &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt;, for &lt;strong&gt;foreign&lt;/strong&gt; cars.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_2\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a domestic car with &lt;strong&gt;0&lt;/strong&gt; &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1 + \beta_3\)&lt;/span&gt; is the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt;, for &lt;strong&gt;domestic&lt;/strong&gt; cars.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How do we fit this model in &lt;code&gt;R&lt;/code&gt;? There are a number of ways.&lt;/p&gt;
&lt;p&gt;One method would be to simply create a new variable, then fit a model like any other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autompg$x3 = autompg$disp * autompg$domestic # THIS CODE NOT RUN!
do_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # THIS CODE NOT RUN!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should only do this as a last resort. We greatly prefer not to have to modify our data simply to fit a model. Instead, we can tell &lt;code&gt;R&lt;/code&gt; we would like to use the existing data with an interaction term, which it will create automatically when we use the &lt;code&gt;:&lt;/code&gt; operator.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An alternative method, which will fit the exact same model as above would be to use the &lt;code&gt;*&lt;/code&gt; operator. This method automatically creates the interaction term, as well as any “lower order terms,” which in this case are the first order terms for &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;domestic&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can quickly verify that these are doing the same thing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(mpg_disp_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)          disp      domestic disp:domestic 
##    46.0548423    -0.1569239   -12.5754714     0.1025184&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(mpg_disp_int2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)          disp      domestic disp:domestic 
##    46.0548423    -0.1569239   -12.5754714     0.1025184&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that both the variables, and their coefficient estimates are indeed the same for both models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mpg_disp_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp + domestic + disp:domestic, data = autompg)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.8332  -2.8956  -0.8332   2.2828  18.7749 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)    46.05484    1.80582  25.504  &amp;lt; 2e-16 ***
## disp           -0.15692    0.01668  -9.407  &amp;lt; 2e-16 ***
## domestic      -12.57547    1.95644  -6.428 3.90e-10 ***
## disp:domestic   0.10252    0.01692   6.060 3.29e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 4.308 on 379 degrees of freedom
## Multiple R-squared:  0.7011, Adjusted R-squared:  0.6987 
## F-statistic: 296.3 on 3 and 379 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that using &lt;code&gt;summary()&lt;/code&gt; gives the usual output for a multiple regression model. We pay close attention to the row for &lt;code&gt;disp:domestic&lt;/code&gt; which tests,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_3 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, testing for &lt;span class=&#34;math inline&#34;&gt;\(\beta_3 = 0\)&lt;/span&gt; is testing for two lines with parallel slopes versus two lines with possibly different slopes. The &lt;code&gt;disp:domestic&lt;/code&gt; line in the &lt;code&gt;summary()&lt;/code&gt; output uses a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test to perform the test.&lt;/p&gt;
&lt;p&gt;We could also use an ANOVA &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test. The additive model, without interaction is our null model, and the interaction model is the alternative.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_disp_add, mpg_disp_int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + domestic
## Model 2: mpg ~ disp + domestic + disp:domestic
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    380 7714.0                                  
## 2    379 7032.6  1    681.36 36.719 3.294e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again we see this test has the same p-value as the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test. Also the p-value is extremely low, so between the two, we choose the interaction model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_for = coef(mpg_disp_int)[1]
int_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3]

slope_for = coef(mpg_disp_int)[2]
slope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we again calculate the slope and intercepts for the two lines for use in plotting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)
abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # line for foreign cars
abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # line for domestic cars
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;Foreign&amp;quot;, &amp;quot;Domestic&amp;quot;), pch = c(1, 2), col = c(1, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/06-example_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that these lines fit the data much better, which matches the result of our tests.&lt;/p&gt;
&lt;p&gt;So far we have only seen interaction between a categorical variable (&lt;code&gt;domestic&lt;/code&gt;) and a numerical variable (&lt;code&gt;disp&lt;/code&gt;). While this is easy to visualize, since it allows for different slopes for two lines, it is not the only type of interaction we can use in a model. We can also consider interactions between two numerical variables.&lt;/p&gt;
&lt;p&gt;Consider the model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;hp&lt;/code&gt;, the horsepower, in foot-pounds per second.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;How does &lt;code&gt;mpg&lt;/code&gt; change based on &lt;code&gt;disp&lt;/code&gt; in this model? We can rearrange some terms to see how.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + (\beta_1 + \beta_3 x_2) x_1 + \beta_2 x_2 + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, for a one unit increase in &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;), the mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;) increases &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 + \beta_3 x_2\)&lt;/span&gt;, which is a different value depending on the value of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; (&lt;code&gt;hp&lt;/code&gt;)!&lt;/p&gt;
&lt;p&gt;Since we’re now working in three dimensions, this model can’t be easily justified via visualizations like the previous example. Instead, we will have to rely on a test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg)
mpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg)
summary(mpg_disp_int_hp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * hp, data = autompg)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.7849  -2.3104  -0.5699   2.1453  17.9211 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  5.241e+01  1.523e+00   34.42   &amp;lt;2e-16 ***
## disp        -1.002e-01  6.638e-03  -15.09   &amp;lt;2e-16 ***
## hp          -2.198e-01  1.987e-02  -11.06   &amp;lt;2e-16 ***
## disp:hp      5.658e-04  5.165e-05   10.96   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.896 on 379 degrees of freedom
## Multiple R-squared:  0.7554, Adjusted R-squared:  0.7535 
## F-statistic: 390.2 on 3 and 379 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;summary()&lt;/code&gt; we focus on the row for &lt;code&gt;disp:hp&lt;/code&gt; which tests,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_3 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, we see a very low p-value so we reject the null (additive model) in favor of the interaction model. Again, there is an equivalent &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_disp_add_hp, mpg_disp_int_hp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + hp
## Model 2: mpg ~ disp * hp
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    380 7576.6                                  
## 2    379 5754.2  1    1822.3 120.03 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can take a closer look at the coefficients of our fitted interaction model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(mpg_disp_int_hp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)          disp            hp       disp:hp 
## 52.4081997848 -0.1001737655 -0.2198199720  0.0005658269&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 = 52.4081998\)&lt;/span&gt; is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a car with 0 &lt;code&gt;disp&lt;/code&gt; and 0 &lt;code&gt;hp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 = -0.1001738\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in 1 &lt;code&gt;disp&lt;/code&gt;, &lt;strong&gt;for a car with 0 &lt;code&gt;hp&lt;/code&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_2 = -0.21982\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in 1 &lt;code&gt;hp&lt;/code&gt;, &lt;strong&gt;for a car with 0 &lt;code&gt;disp&lt;/code&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_3 = 5.658269\times 10^{-4}\)&lt;/span&gt; is an estimate of the modification to the change in average &lt;code&gt;mpg&lt;/code&gt; for an increase in &lt;code&gt;disp&lt;/code&gt;, for a car of a certain &lt;code&gt;hp&lt;/code&gt; (or vice versa).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That last coefficient needs further explanation. Recall the rearrangement we made earlier&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + (\beta_1 + \beta_3 x_2) x_1 + \beta_2 x_2 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, our estimate for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 + \beta_3 x_2\)&lt;/span&gt;, is &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 + \hat{\beta}_3 x_2\)&lt;/span&gt;, which in this case is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-0.1001738 + 5.658269\times 10^{-4} x_2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This says that, for an increase of one &lt;code&gt;disp&lt;/code&gt; we see an estimated change in average &lt;code&gt;mpg&lt;/code&gt; of &lt;span class=&#34;math inline&#34;&gt;\(-0.1001738 + 5.658269\times 10^{-4} x_2\)&lt;/span&gt;. So how &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;mpg&lt;/code&gt; are related, depends on the &lt;code&gt;hp&lt;/code&gt; of the car.&lt;/p&gt;
&lt;p&gt;So for a car with 50 &lt;code&gt;hp&lt;/code&gt;, the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-0.1001738 + 5.658269\times 10^{-4} \cdot 50 = -0.0718824
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And for a car with 350 &lt;code&gt;hp&lt;/code&gt;, the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
-0.1001738 + 5.658269\times 10^{-4} \cdot 350 = 0.0978657
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice the sign changed!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factor-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Factor Variables&lt;/h2&gt;
&lt;p&gt;So far in this chapter, we have limited our use of categorical variables to binary categorical variables. Specifically, we have limited ourselves to dummy variables which take a value of &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt; and represent a categorical variable numerically.&lt;/p&gt;
&lt;p&gt;We will now discuss &lt;strong&gt;factor&lt;/strong&gt; variables, which is a special way that &lt;code&gt;R&lt;/code&gt; deals with categorical variables. With factor variables, a human user can simply think about the categories of a variable, and &lt;code&gt;R&lt;/code&gt; will take care of the necessary dummy variables without any 0/1 assignment being done by the user.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(autompg$domestic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Earlier when we used the &lt;code&gt;domestic&lt;/code&gt; variable, it was &lt;strong&gt;not&lt;/strong&gt; a factor variable. It was simply a numerical variable that only took two possible values, &lt;code&gt;1&lt;/code&gt; for domestic, and &lt;code&gt;0&lt;/code&gt; for foreign. Let’s create a new variable &lt;code&gt;origin&lt;/code&gt; that stores the same information, but in a different way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autompg$origin[autompg$domestic == 1] = &amp;quot;domestic&amp;quot;
autompg$origin[autompg$domestic == 0] = &amp;quot;foreign&amp;quot;
head(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot; &amp;quot;domestic&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the &lt;code&gt;origin&lt;/code&gt; variable stores &lt;code&gt;&#34;domestic&#34;&lt;/code&gt; for domestic cars and &lt;code&gt;&#34;foreign&#34;&lt;/code&gt; for foreign cars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this is simply a vector of character values. A vector of car models is a character variable in &lt;code&gt;R&lt;/code&gt;. A vector of Vehicle Identification Numbers (VINs) is a character variable as well. But those don’t represent a short list of levels that might influence a response variable. We will want to &lt;strong&gt;coerce&lt;/strong&gt; this origin variable to be something more: a factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autompg$origin = as.factor(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now when we check the structure of the &lt;code&gt;autompg&lt;/code&gt; dataset, we see that &lt;code&gt;origin&lt;/code&gt; is a factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    383 obs. of  9 variables:
##  $ mpg     : num  18 15 18 16 17 15 14 14 14 15 ...
##  $ cyl     : Factor w/ 3 levels &amp;quot;4&amp;quot;,&amp;quot;6&amp;quot;,&amp;quot;8&amp;quot;: 3 3 3 3 3 3 3 3 3 3 ...
##  $ disp    : num  307 350 318 304 302 429 454 440 455 390 ...
##  $ hp      : num  130 165 150 150 140 198 220 215 225 190 ...
##  $ wt      : num  3504 3693 3436 3433 3449 ...
##  $ acc     : num  12 11.5 11 12 10.5 10 9 8.5 10 8.5 ...
##  $ year    : int  70 70 70 70 70 70 70 70 70 70 ...
##  $ origin  : Factor w/ 2 levels &amp;quot;domestic&amp;quot;,&amp;quot;foreign&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ domestic: num  1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Factor variables have &lt;strong&gt;levels&lt;/strong&gt; which are the possible values (categories) that the variable may take, in this case foreign or domestic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(autompg$origin)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;domestic&amp;quot; &amp;quot;foreign&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recall that previously we have fit the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;domestic&lt;/code&gt; a dummy variable where &lt;code&gt;1&lt;/code&gt; indicates a domestic car.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mod_dummy = lm(mpg ~ disp * domestic, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * domestic, data = autompg)
## 
## Coefficients:
##   (Intercept)           disp       domestic  disp:domestic  
##       46.0548        -0.1569       -12.5755         0.1025&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here we see that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 + \hat{\beta}_2 = 46.0548423 + -12.5754714 = 33.4793709
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a &lt;strong&gt;domestic&lt;/strong&gt; car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now let’s try to do the same, but using our new factor variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mod_factor = lm(mpg ~ disp * origin, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * origin, data = autompg)
## 
## Coefficients:
##        (Intercept)                disp       originforeign  disp:originforeign  
##           33.47937            -0.05441            12.57547            -0.10252&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems that it doesn’t produce the same results. Right away we notice that the intercept is different, as is the the coefficient in front of &lt;code&gt;disp&lt;/code&gt;. We also notice that the remaining two coefficients are of the same magnitude as their respective counterparts using the domestic variable, but with a different sign. Why is this happening?&lt;/p&gt;
&lt;p&gt;It turns out, that by using a factor variable, &lt;code&gt;R&lt;/code&gt; is automatically creating a dummy variable for us. However, it is not the dummy variable that we had originally used ourselves.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is fitting the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; &lt;strong&gt;is a dummy variable created by &lt;code&gt;R&lt;/code&gt;.&lt;/strong&gt; It uses &lt;code&gt;1&lt;/code&gt; to represent a &lt;strong&gt;foreign car&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So now,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 = 33.4793709
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is the estimated average &lt;code&gt;mpg&lt;/code&gt; for a &lt;strong&gt;domestic&lt;/strong&gt; car with 0 &lt;code&gt;disp&lt;/code&gt;, which is indeed the same as before.&lt;/p&gt;
&lt;p&gt;When &lt;code&gt;R&lt;/code&gt; created &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, the dummy variable, it used domestic cars as the &lt;strong&gt;reference&lt;/strong&gt; level, that is the default value of the factor variable. So when the dummy variable is &lt;code&gt;0&lt;/code&gt;, the model represents this reference level, which is domestic. (&lt;code&gt;R&lt;/code&gt; makes this choice because domestic comes before foreign alphabetically.)&lt;/p&gt;
&lt;p&gt;So the two models have different estimated coefficients, but due to the different model representations, they are actually the same model.&lt;/p&gt;
&lt;div id=&#34;factors-with-more-than-two-levels&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factors with More Than Two Levels&lt;/h3&gt;
&lt;p&gt;Let’s now consider a factor variable with more than two levels. In this dataset, &lt;code&gt;cyl&lt;/code&gt; is an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(autompg$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(autompg$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;4&amp;quot; &amp;quot;6&amp;quot; &amp;quot;8&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the &lt;code&gt;cyl&lt;/code&gt; variable has three possible levels: &lt;code&gt;4&lt;/code&gt;, &lt;code&gt;6&lt;/code&gt;, and &lt;code&gt;8&lt;/code&gt;. You may wonder, why not simply use &lt;code&gt;cyl&lt;/code&gt; as a numerical variable? You certainly could.&lt;/p&gt;
&lt;p&gt;However, that would force the difference in average &lt;code&gt;mpg&lt;/code&gt; between &lt;code&gt;4&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; cylinders to be the same as the difference in average mpg between &lt;code&gt;6&lt;/code&gt; and &lt;code&gt;8&lt;/code&gt; cylinders. That usually make senses for a continuous variable, but not for a discrete variable with so few possible values. In the case of this variable, there is no such thing as a 7-cylinder engine or a 6.23-cylinder engine in personal vehicles. For these reasons, we will simply consider &lt;code&gt;cyl&lt;/code&gt; to be categorical. This is a decision that will commonly need to be made with ordinal variables. Often, with a large number of categories, the decision to treat them as numerical variables is appropriate because, otherwise, a large number of dummy variables are then needed to represent these variables.&lt;/p&gt;
&lt;p&gt;Let’s define three dummy variables related to the &lt;code&gt;cyl&lt;/code&gt; factor variable.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v_1 =
  \begin{cases}
   1 &amp;amp; \text{4 cylinder} \\
   0       &amp;amp; \text{not 4 cylinder}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v_2 =
  \begin{cases}
   1 &amp;amp; \text{6 cylinder} \\
   0       &amp;amp; \text{not 6 cylinder}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
v_3 =
  \begin{cases}
   1 &amp;amp; \text{8 cylinder} \\
   0       &amp;amp; \text{not 8 cylinder}
  \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s fit an additive model in &lt;code&gt;R&lt;/code&gt;, using &lt;code&gt;mpg&lt;/code&gt; as the response, and &lt;code&gt;disp&lt;/code&gt; and &lt;code&gt;cyl&lt;/code&gt; as predictors. This should be a model that uses “three regression lines” to model &lt;code&gt;mpg&lt;/code&gt;, one for each of the possible &lt;code&gt;cyl&lt;/code&gt; levels. They will all have the same slope (since it is an additive model), but each will have its own intercept.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp + cyl, data = autompg)
## 
## Coefficients:
## (Intercept)         disp         cyl6         cyl8  
##    34.99929     -0.05217     -3.63325     -2.03603&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The question is, what is the model that &lt;code&gt;R&lt;/code&gt; has fit here? It has chosen to use the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;, the fuel efficiency in miles per gallon,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_3\)&lt;/span&gt; are the dummy variables define above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Why doesn’t &lt;code&gt;R&lt;/code&gt; use &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;? Essentially because it doesn’t need to. To create three lines, it only needs two dummy variables since it is using a reference level, which in this case is a 4 cylinder car. The three “sub models” are then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_2) + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_3) + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that they all have the same slope. However, using the two dummy variables, we achieve the three intercepts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a 4 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_2\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a 6 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_3\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; for a 8 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So because 4 cylinder is the reference level, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is specific to 4 cylinders, but &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; are used to represent quantities relative to 4 cylinders.&lt;/p&gt;
&lt;p&gt;As we have done before, we can extract these intercepts and slopes for the three lines, and plot them accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_4cyl = coef(mpg_disp_add_cyl)[1]
int_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3]
int_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4]

slope_all_cyl = coef(mpg_disp_add_cyl)[2]

plot_colors = c(&amp;quot;Darkorange&amp;quot;, &amp;quot;Darkgrey&amp;quot;, &amp;quot;Dodgerblue&amp;quot;)
plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))
abline(int_4cyl, slope_all_cyl, col = plot_colors[1], lty = 1, lwd = 2)
abline(int_6cyl, slope_all_cyl, col = plot_colors[2], lty = 2, lwd = 2)
abline(int_8cyl, slope_all_cyl, col = plot_colors[3], lty = 3, lwd = 2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;4 Cylinder&amp;quot;, &amp;quot;6 Cylinder&amp;quot;, &amp;quot;8 Cylinder&amp;quot;),
       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/06-example_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On this plot, we have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: orange dots, solid orange line.&lt;/li&gt;
&lt;li&gt;6 Cylinder: grey dots, dashed grey line.&lt;/li&gt;
&lt;li&gt;8 Cylinder: blue dots, dotted blue line.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The odd result here is that we’re estimating that 8 cylinder cars have better fuel efficiency than 6 cylinder cars at &lt;strong&gt;any&lt;/strong&gt; displacement! The dotted blue line is always above the dashed grey line. That doesn’t seem right. Maybe for very large displacement engines that could be true, but that seems wrong for medium to low displacement.&lt;/p&gt;
&lt;p&gt;To attempt to fix this, we will try using an interaction model, that is, instead of simply three intercepts and one slope, we will allow for three slopes. Again, we’ll let &lt;code&gt;R&lt;/code&gt; take the wheel, (no pun intended) then figure out what model it has applied.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * cyl, data = autompg)
## 
## Coefficients:
## (Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  
##    43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# could also use mpg ~ disp + cyl + disp:cyl&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; has again chosen to use 4 cylinder cars as the reference level, but this also now has an effect on the interaction terms. &lt;code&gt;R&lt;/code&gt; has fit the model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \gamma_2 x v_2 + \gamma_3 x v_3 + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’re using &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; like a &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter for simplicity, so that, for example &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_2\)&lt;/span&gt; are both associated with &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, the three “sub models” are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \epsilon\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_2) + (\beta_1 + \gamma_2) x + \epsilon\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = (\beta_0 + \beta_3) + (\beta_1 + \gamma_3) x + \epsilon\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Interpreting some parameters and coefficients then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\beta_0 + \beta_2)\)&lt;/span&gt; is the average &lt;code&gt;mpg&lt;/code&gt; of a 6 cylinder car with 0 &lt;code&gt;disp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((\hat{\beta}_1 + \hat{\gamma}_3) = -0.1306935 + 0.1081714 = -0.0225221\)&lt;/span&gt; is the estimated change in average &lt;code&gt;mpg&lt;/code&gt; for an increase of one &lt;code&gt;disp&lt;/code&gt;, for an 8 cylinder car.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, as we have seen before &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; change the intercepts for 6 and 8 cylinder cars relative to the reference level of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; for 4 cylinder cars.&lt;/p&gt;
&lt;p&gt;Now, similarly &lt;span class=&#34;math inline&#34;&gt;\(\gamma_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_3\)&lt;/span&gt; change the slopes for 6 and 8 cylinder cars relative to the reference level of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; for 4 cylinder cars.&lt;/p&gt;
&lt;p&gt;Once again, we extract the coefficients and plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;int_4cyl = coef(mpg_disp_int_cyl)[1]
int_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3]
int_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4]

slope_4cyl = coef(mpg_disp_int_cyl)[2]
slope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5]
slope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6]

plot_colors = c(&amp;quot;Darkorange&amp;quot;, &amp;quot;Darkgrey&amp;quot;, &amp;quot;Dodgerblue&amp;quot;)
plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl))
abline(int_4cyl, slope_4cyl, col = plot_colors[1], lty = 1, lwd = 2)
abline(int_6cyl, slope_6cyl, col = plot_colors[2], lty = 2, lwd = 2)
abline(int_8cyl, slope_8cyl, col = plot_colors[3], lty = 3, lwd = 2)
legend(&amp;quot;topright&amp;quot;, c(&amp;quot;4 Cylinder&amp;quot;, &amp;quot;6 Cylinder&amp;quot;, &amp;quot;8 Cylinder&amp;quot;),
       col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/06-example_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks much better! We can see that for medium displacement cars, 6 cylinder cars now perform better than 8 cylinder cars, which seems much more reasonable than before.&lt;/p&gt;
&lt;p&gt;To completely justify the interaction model (i.e., a unique slope for each &lt;code&gt;cyl&lt;/code&gt; level) compared to the additive model (single slope), we can perform an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test. Notice first, that there is no &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test that will be able to do this since the difference between the two models is not a single parameter.&lt;/p&gt;
&lt;p&gt;We will test,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \gamma_2 = \gamma_3 = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which represents the parallel regression lines we saw before,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Again, this is a difference of two parameters, thus no &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-test will be useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mpg_disp_add_cyl, mpg_disp_int_cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + cyl
## Model 2: mpg ~ disp * cyl
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    379 7299.5                                  
## 2    377 6551.7  2    747.79 21.515 1.419e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, we see a very low p-value, and thus reject the null. We prefer the interaction model over the additive model.&lt;/p&gt;
&lt;p&gt;Recapping a bit:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Null Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \epsilon\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Number of parameters: &lt;span class=&#34;math inline&#34;&gt;\(q = 4\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Full Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x + \beta_2 v_2 + \beta_3 v_3 + \gamma_2 x v_2 + \gamma_3 x v_3 + \epsilon\)&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Number of parameters: &lt;span class=&#34;math inline&#34;&gt;\(p = 6\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(coef(mpg_disp_int_cyl)) - length(coef(mpg_disp_add_cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see there is a difference of two parameters, which is also displayed in the resulting ANOVA table from &lt;code&gt;R&lt;/code&gt;. Notice that the following two values also appear on the ANOVA table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(autompg) - length(coef(mpg_disp_int_cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 377&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(autompg) - length(coef(mpg_disp_add_cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 379&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;parameterization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Parameterization&lt;/h2&gt;
&lt;p&gt;So far we have been simply letting &lt;code&gt;R&lt;/code&gt; decide how to create the dummy variables, and thus &lt;code&gt;R&lt;/code&gt; has been deciding the parameterization of the models. To illustrate the ability to use alternative parameterizations, we will recreate the data, but directly creating the dummy variables ourselves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_param_data = data.frame(
  y = autompg$mpg,
  x = autompg$disp,
  v1 = 1 * as.numeric(autompg$cyl == 4),
  v2 = 1 * as.numeric(autompg$cyl == 6),
  v3 = 1 * as.numeric(autompg$cyl == 8))

head(new_param_data, 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     y   x v1 v2 v3
## 1  18 307  0  0  1
## 2  15 350  0  0  1
## 3  18 318  0  0  1
## 4  16 304  0  0  1
## 5  17 302  0  0  1
## 6  15 429  0  0  1
## 7  14 454  0  0  1
## 8  14 440  0  0  1
## 9  14 455  0  0  1
## 10 15 390  0  0  1
## 11 15 383  0  0  1
## 12 14 340  0  0  1
## 13 15 400  0  0  1
## 14 14 455  0  0  1
## 15 24 113  1  0  0
## 16 22 198  0  1  0
## 17 18 199  0  1  0
## 18 21 200  0  1  0
## 19 27  97  1  0  0
## 20 26  97  1  0  0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; is &lt;code&gt;mpg&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is &lt;code&gt;disp&lt;/code&gt;, the displacement in cubic inches,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;v1&lt;/code&gt;, &lt;code&gt;v2&lt;/code&gt;, and &lt;code&gt;v3&lt;/code&gt; are dummy variables as defined above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First let’s try to fit an additive model using &lt;code&gt;x&lt;/code&gt; as well as the three dummy variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ x + v1 + v2 + v3, data = new_param_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + v1 + v2 + v3, data = new_param_data)
## 
## Coefficients:
## (Intercept)            x           v1           v2           v3  
##    32.96326     -0.05217      2.03603     -1.59722           NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is happening here? Notice that &lt;code&gt;R&lt;/code&gt; is essentially ignoring &lt;code&gt;v3&lt;/code&gt;, but why? Well, because &lt;code&gt;R&lt;/code&gt; uses an intercept, it cannot also use &lt;code&gt;v3&lt;/code&gt;. This is because&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{1} = v_1 + v_2 + v_3
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which means that &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{1}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(v_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(v_3\)&lt;/span&gt; are linearly dependent. This would make the &lt;span class=&#34;math inline&#34;&gt;\(X^\top X\)&lt;/span&gt; matrix singular, but we need to be able to invert it to solve the normal equations and obtain &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}.\)&lt;/span&gt; With the intercept, &lt;code&gt;v1&lt;/code&gt;, and &lt;code&gt;v2&lt;/code&gt;, &lt;code&gt;R&lt;/code&gt; can make the necessary “three intercepts”. So, in this case &lt;code&gt;v3&lt;/code&gt; is the reference level.&lt;/p&gt;
&lt;p&gt;If we remove the intercept, then we can directly obtain all “three intercepts” without a reference level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ 0 + x + v1 + v2 + v3, data = new_param_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ 0 + x + v1 + v2 + v3, data = new_param_data)
## 
## Coefficients:
##        x        v1        v2        v3  
## -0.05217  34.99929  31.36604  32.96326&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we are fitting the model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \mu_1 v_1 + \mu_2 v_2 + \mu_3 v_3 + \beta x +\epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_1 + \beta x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_2 + \beta x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_3 + \beta x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We could also do something similar with the interaction model, and give each line an intercept and slope, without the need for a reference level.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data)
## 
## Coefficients:
##       v1        v2        v3      v1:x      v2:x      v3:x  
## 43.59052  30.39026  22.73346  -0.13069  -0.04770  -0.02252&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \mu_1 v_1 + \mu_2 v_2 + \mu_3 v_3 + \beta_1 x v_1 + \beta_2 x v_2 + \beta_3 x v_3 +\epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_1 + \beta_1 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;6 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_2 + \beta_2 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;8 Cylinder: &lt;span class=&#34;math inline&#34;&gt;\(Y = \mu_3 + \beta_3 x + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using the original data, we have (at least) three equivalent ways to specify the interaction model with &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(mpg ~ disp * cyl, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * cyl, data = autompg)
## 
## Coefficients:
## (Intercept)         disp         cyl6         cyl8    disp:cyl6    disp:cyl8  
##    43.59052     -0.13069    -13.20026    -20.85706      0.08299      0.10817&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ 0 + cyl + disp:cyl, data = autompg)
## 
## Coefficients:
##      cyl4       cyl6       cyl8  cyl4:disp  cyl6:disp  cyl8:disp  
##  43.59052   30.39026   22.73346   -0.13069   -0.04770   -0.02252&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ 0 + disp + cyl + disp:cyl, data = autompg)
## 
## Coefficients:
##      disp       cyl4       cyl6       cyl8  disp:cyl6  disp:cyl8  
##  -0.13069   43.59052   30.39026   22.73346    0.08299    0.10817&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They all fit the same model, importantly each using six parameters, but the coefficients mean slightly different things in each. However, once they are interpreted as slopes and intercepts for the “three lines” they will have the same result.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;?all.equal&lt;/code&gt; to learn about the &lt;code&gt;all.equal()&lt;/code&gt; function, and think about how the following code verifies that the residuals of the two models are the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)),
          fitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-larger-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building Larger Models&lt;/h2&gt;
&lt;p&gt;Now that we have seen how to incorporate categorical predictors as well as interaction terms, we can start to build much larger, much more flexible models which can potentially fit data better.&lt;/p&gt;
&lt;p&gt;Let’s define a “big” model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;code&gt;mpg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is &lt;code&gt;disp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is &lt;code&gt;hp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt; is &lt;code&gt;domestic&lt;/code&gt;, which is a dummy variable we defined, where &lt;code&gt;1&lt;/code&gt; is a domestic vehicle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First thing to note here, we have included a new term &lt;span class=&#34;math inline&#34;&gt;\(x_1 x_2 x_3\)&lt;/span&gt; which is a three-way interaction. Interaction terms can be larger and larger, up to the number of predictors in the model.&lt;/p&gt;
&lt;p&gt;Since we are using the three-way interaction term, we also use all possible two-way interactions, as well as each of the first order (&lt;strong&gt;main effect&lt;/strong&gt;) terms. This is the concept of a &lt;strong&gt;hierarchy&lt;/strong&gt;. Any time a “higher-order” term is in a model, the related “lower-order” terms should also be included. Mathematically their inclusion or exclusion is sometimes irrelevant, but from an interpretation standpoint, it is best to follow the hierarchy rules.&lt;/p&gt;
&lt;p&gt;Let’s do some rearrangement to obtain a “coefficient” in front of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_2 x_2 + \beta_3 x_3 + \beta_6 x_2 x_3 + (\beta_1 + \beta_4 x_2 + \beta_5 x_3 + \beta_7 x_2 x_3)x_1 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Specifically, the “coefficient” in front of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + \beta_5 x_3 + \beta_7 x_2 x_3).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s discuss this “coefficient” to help us understand the idea of the &lt;em&gt;flexibility&lt;/em&gt; of a model. Recall that,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the coefficient for a first order term,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_4\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_5\)&lt;/span&gt; are coefficients for two-way interactions,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_7\)&lt;/span&gt; is the coefficient for the three-way interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the two and three way interactions were not in the model, the whole “coefficient” would simply be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\beta_1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus, no matter the values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; would determine the relationship between &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;With the addition of the two-way interactions, now the “coefficient” would be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + \beta_5 x_3).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, changing &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;) has a different effect on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;), depending on the values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Lastly, adding the three-way interaction gives the whole “coefficient”&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + \beta_5 x_3 + \beta_7 x_2 x_3)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is even more flexible. Now changing &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; (&lt;code&gt;disp&lt;/code&gt;) has a different effect on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (&lt;code&gt;mpg&lt;/code&gt;), depending on the values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt;, but in a more flexible way which we can see with some more rearrangement. Now the “coefficient” in front of &lt;span class=&#34;math inline&#34;&gt;\(x_3\)&lt;/span&gt; in this “coefficient” is dependent on &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\beta_1 + \beta_4 x_2 + (\beta_5 + \beta_7 x_2) x_3)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is so flexible, it is becoming hard to interpret!&lt;/p&gt;
&lt;p&gt;Let’s fit this three-way interaction model in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;big_model = lm(mpg ~ disp * hp * domestic, data = autompg)
summary(big_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ disp * hp * domestic, data = autompg)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.9410  -2.2147  -0.4008   1.9430  18.4094 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)       6.065e+01  6.600e+00   9.189  &amp;lt; 2e-16 ***
## disp             -1.416e-01  6.344e-02  -2.232   0.0262 *  
## hp               -3.545e-01  8.123e-02  -4.364 1.65e-05 ***
## domestic         -1.257e+01  7.064e+00  -1.780   0.0759 .  
## disp:hp           1.369e-03  6.727e-04   2.035   0.0426 *  
## disp:domestic     4.933e-02  6.400e-02   0.771   0.4414    
## hp:domestic       1.852e-01  8.709e-02   2.126   0.0342 *  
## disp:hp:domestic -9.163e-04  6.768e-04  -1.354   0.1766    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.88 on 375 degrees of freedom
## Multiple R-squared:   0.76,  Adjusted R-squared:  0.7556 
## F-statistic: 169.7 on 7 and 375 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do we actually need this large of a model? Let’s first test for the necessity of the three-way interaction term. That is,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_7 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \beta_7 x_1 x_2 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Null Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We fit the null model in &lt;code&gt;R&lt;/code&gt; as &lt;code&gt;two_way_int_mod&lt;/code&gt;, then use &lt;code&gt;anova()&lt;/code&gt; to perform an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test as usual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;two_way_int_mod = lm(mpg ~ disp * hp + disp * domestic + hp * domestic, data = autompg)
#two_way_int_mod = lm(mpg ~ (disp + hp + domestic) ^ 2, data = autompg)
anova(two_way_int_mod, big_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp * hp + disp * domestic + hp * domestic
## Model 2: mpg ~ disp * hp * domestic
##   Res.Df    RSS Df Sum of Sq      F Pr(&amp;gt;F)
## 1    376 5673.2                           
## 2    375 5645.6  1    27.599 1.8332 0.1766&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see the p-value is somewhat large, so we would fail to reject. We prefer the smaller, less flexible, null model, without the three-way interaction.&lt;/p&gt;
&lt;p&gt;A quick note here: the full model does still “fit better.” Notice that it has a smaller RMSE than the null model, which means the full model makes smaller (squared) errors on average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(resid(big_model) ^ 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14.74053&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(resid(two_way_int_mod) ^ 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14.81259&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, it is not much smaller. We could even say that, the difference is insignificant. This is an idea we will return to later in greater detail.&lt;/p&gt;
&lt;p&gt;Now that we have chosen the model without the three-way interaction, can we go further? Do we need the two-way interactions? Let’s test&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H_0: \beta_4 = \beta_5 = \beta_6 = 0.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Remember we already chose &lt;span class=&#34;math inline&#34;&gt;\(\beta_7 = 0\)&lt;/span&gt;, so,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Null Model: &lt;span class=&#34;math inline&#34;&gt;\(Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We fit the null model in &lt;code&gt;R&lt;/code&gt; as &lt;code&gt;additive_mod&lt;/code&gt;, then use &lt;code&gt;anova()&lt;/code&gt; to perform an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;-test as usual.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;additive_mod = lm(mpg ~ disp + hp + domestic, data = autompg)
anova(additive_mod, two_way_int_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: mpg ~ disp + hp + domestic
## Model 2: mpg ~ disp * hp + disp * domestic + hp * domestic
##   Res.Df    RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1    379 7369.7                                  
## 2    376 5673.2  3    1696.5 37.478 &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the p-value is small, so we reject the null, and we prefer the full (alternative) model. Of the models we have considered, our final preference is for&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_1 x_2 + \beta_5 x_1 x_3 + \beta_6 x_2 x_3 + \epsilon.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression: Model Selection</title>
      <link>https://datavizm20.classes.andrewheiss.com/example/07-example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://datavizm20.classes.andrewheiss.com/example/07-example/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model-selection&#34;&gt;Model Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#assesing-model-accuracy&#34;&gt;Assesing Model Accuracy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-complexity&#34;&gt;Model Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#test-train-split&#34;&gt;Test-Train Split&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-flexibility-to-linear-models&#34;&gt;Adding Flexibility to Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-a-model&#34;&gt;Choosing a Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;Today’s example will pivot between the content from this week and the example below.&lt;/p&gt;
&lt;p&gt;We will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://datavizm20.classes.andrewheiss.com/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;model-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Selection&lt;/h2&gt;
&lt;p&gt;Often when we are developing a linear regression model, part of our goal is to &lt;strong&gt;explain&lt;/strong&gt; a relationship. Now, we will ignore much of what we have learned and instead simply use regression as a tool to &lt;strong&gt;predict&lt;/strong&gt;. Instead of a model which explains relationships, we seek a model which minimizes errors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/07-example_files/regression.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First, note that a linear model is one of many methods used in regression.&lt;/p&gt;
&lt;p&gt;To discuss linear models in the context of prediction, we introduce the (very boring) &lt;code&gt;Advertising&lt;/code&gt; data that is discussed in the ISL text (see supplemental readings).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Advertising&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 200 x 4
##       TV Radio Newspaper Sales
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 230.   37.8      69.2  22.1
##  2  44.5  39.3      45.1  10.4
##  3  17.2  45.9      69.3   9.3
##  4 152.   41.3      58.5  18.5
##  5 181.   10.8      58.4  12.9
##  6   8.7  48.9      75     7.2
##  7  57.5  32.8      23.5  11.8
##  8 120.   19.6      11.6  13.2
##  9   8.6   2.1       1     4.8
## 10 200.    2.6      21.2  10.6
## # ... with 190 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
featurePlot(x = Advertising[ , c(&amp;quot;TV&amp;quot;, &amp;quot;Radio&amp;quot;, &amp;quot;Newspaper&amp;quot;)], y = Advertising$Sales)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/07-example_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assesing-model-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assesing Model Accuracy&lt;/h2&gt;
&lt;p&gt;There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}(\hat{f}, \text{Data}) = \sqrt{\frac{1}{n}\displaystyle\sum_{i = 1}^{n}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For a linear model , the estimate of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;, is given by the fitted regression line.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{y}({\bf{x}_i}) = \hat{f}({\bf{x}_i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can write an &lt;code&gt;R&lt;/code&gt; function that will be useful for performing this calculation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-complexity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Complexity&lt;/h2&gt;
&lt;p&gt;Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We write a simple &lt;code&gt;R&lt;/code&gt; function to extract this information from a model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_complexity = function(model) {
  length(coef(model)) - 1
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;test-train-split&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Test-Train Split&lt;/h2&gt;
&lt;p&gt;There is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts. It is essentially cheating! As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down, or in very specific cases, stay the same.&lt;/p&gt;
&lt;p&gt;This would suggest that to predict well, we should use the largest possible model! However, in reality we have hard fit to a specific dataset, but as soon as we see new data, a large model may in fact predict poorly. This is called &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Frequently we will take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the &lt;strong&gt;training&lt;/strong&gt; data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the &lt;strong&gt;test&lt;/strong&gt; data. Test data should &lt;em&gt;never&lt;/em&gt; be used to train a model.&lt;/p&gt;
&lt;p&gt;Note that sometimes the terms &lt;em&gt;evaluation set&lt;/em&gt; and &lt;em&gt;test set&lt;/em&gt; are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.&lt;/p&gt;
&lt;p&gt;Here we use the &lt;code&gt;sample()&lt;/code&gt; function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the &lt;code&gt;set.seed()&lt;/code&gt; function to allow use to reproduce the same random split each time we perform this analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(9)
num_obs = nrow(Advertising)

train_index = sample(num_obs, size = trunc(0.50 * num_obs))
train_data = Advertising[train_index, ]
test_data = Advertising[-train_index, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will look at two measures that assess how well a model is predicting, the &lt;strong&gt;train RMSE&lt;/strong&gt; and the &lt;strong&gt;test RMSE&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_{\text{Train}} = \text{RMSE}(\hat{f}, \text{Train Data}) = \sqrt{\frac{1}{n_{\text{Tr}}}\displaystyle\sum_{i \in \text{Train}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Tr}\)&lt;/span&gt; is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{RMSE}_{\text{Test}} = \text{RMSE}(\hat{f}, \text{Test Data}) = \sqrt{\frac{1}{n_{\text{Te}}}\displaystyle\sum_{i \in \text{Test}}^{}\left(y_i - \hat{f}(\bf{x}_i)\right)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(n_{Te}\)&lt;/span&gt; is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict &lt;strong&gt;in general&lt;/strong&gt;, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.&lt;/p&gt;
&lt;p&gt;We will start with the simplest possible linear model, that is, a model with no predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(Sales ~ 1, data = train_data)
get_complexity(fit_0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
sqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.529258&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
sqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.914163&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train RMSE
rmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.529258&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test RMSE
rmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.914163&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.529258&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_0, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.914163&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-flexibility-to-linear-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding Flexibility to Linear Models&lt;/h2&gt;
&lt;p&gt;Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_1 = lm(Sales ~ ., data = train_data)
get_complexity(fit_1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_1, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.888488&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_1, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.461661&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)
get_complexity(fit_2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_2, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.016822&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_2, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9117228&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)
get_complexity(fit_3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_3, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6553091&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_3, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6633375&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_4 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)
get_complexity(fit_4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_4, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6421909&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_4, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7465957&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_5 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)
get_complexity(fit_5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_5, data = train_data, response = &amp;quot;Sales&amp;quot;) # train RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6120887&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_rmse(model = fit_5, data = test_data, response = &amp;quot;Sales&amp;quot;) # test RMSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7864181&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing a Model&lt;/h2&gt;
&lt;p&gt;To better understand the relationship between train RMSE, test RMSE, and model complexity, we summarize our results, as the above is somewhat cluttered.&lt;/p&gt;
&lt;p&gt;First, we recap the models that we have fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_1 = lm(Sales ~ ., data = train_data)
fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)
fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)
fit_4 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)
fit_5 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we create a list of the models fit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then obtain train RMSE, test RMSE, and model complexity for each.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_rmse = sapply(model_list, get_rmse, data = train_data, response = &amp;quot;Sales&amp;quot;)
test_rmse = sapply(model_list, get_rmse, data = test_data, response = &amp;quot;Sales&amp;quot;)
model_complexity = sapply(model_list, get_complexity)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(model_complexity, train_rmse, type = &amp;quot;b&amp;quot;,
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02,
              max(c(train_rmse, test_rmse)) + 0.02),
     col = &amp;quot;dodgerblue&amp;quot;,
     xlab = &amp;quot;Model Size&amp;quot;,
     ylab = &amp;quot;RMSE&amp;quot;)
lines(model_complexity, test_rmse, type = &amp;quot;b&amp;quot;, col = &amp;quot;darkorange&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://datavizm20.classes.andrewheiss.com/example/07-example_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We also summarize the results as a table. &lt;code&gt;fit_1&lt;/code&gt; is the least flexible, and &lt;code&gt;fit_5&lt;/code&gt; is the most flexible. We see the Train RMSE decrease as flexibility increases. We see that the Test RMSE is smallest for &lt;code&gt;fit_3&lt;/code&gt;, thus is the model we believe will perform the best on future data not used to train the model. Note this may not be the best model, but it is the best model of the models we have seen in this example.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Train RMSE&lt;/th&gt;
&lt;th&gt;Test RMSE&lt;/th&gt;
&lt;th&gt;Predictors&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;1.8884884&lt;/td&gt;
&lt;td&gt;1.4616608&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_2&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;1.0168223&lt;/td&gt;
&lt;td&gt;0.9117228&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.6553091&lt;/td&gt;
&lt;td&gt;0.6633375&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.6421909&lt;/td&gt;
&lt;td&gt;0.7465957&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;code&gt;fit_5&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;0.6120887&lt;/td&gt;
&lt;td&gt;0.7864181&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To summarize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Underfitting models:&lt;/strong&gt; In general &lt;em&gt;High&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE. Seen in &lt;code&gt;fit_1&lt;/code&gt; and &lt;code&gt;fit_2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overfitting models:&lt;/strong&gt; In general &lt;em&gt;Low&lt;/em&gt; Train RMSE, &lt;em&gt;High&lt;/em&gt; Test RMSE. Seen in &lt;code&gt;fit_4&lt;/code&gt; and &lt;code&gt;fit_5&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Specifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE. Then a model is underfitting if there exists a more complex model with lower Test RMSE.&lt;/p&gt;
&lt;p&gt;A number of notes on these results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The labels of under and overfitting are &lt;em&gt;relative&lt;/em&gt; to the best model we see, &lt;code&gt;fit_3&lt;/code&gt;. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.&lt;/li&gt;
&lt;li&gt;The train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. Here we see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later.&lt;/li&gt;
&lt;li&gt;Often we expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get lucky and this will not be true.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that &lt;strong&gt;predicted&lt;/strong&gt; well, and paid no attention to a model for &lt;strong&gt;explaination&lt;/strong&gt;. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
