<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Readings, lectures, and videos | Data Analytics</title>
    <link>/content/</link>
      <atom:link href="/content/index.xml" rel="self" type="application/rss+xml" />
    <description>Readings, lectures, and videos</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><lastBuildDate>Tue, 01 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/social-image.png</url>
      <title>Readings, lectures, and videos</title>
      <link>/content/</link>
    </image>
    
    <item>
      <title>Classification</title>
      <link>/content/10-content/</link>
      <pubDate>Mon, 09 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/content/10-content/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#classification-overview&#34;&gt;Overview&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#visualization-for-classification&#34;&gt;Visualization for Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a-simple-classifier&#34;&gt;A Simple Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#metrics-for-classification&#34;&gt;Metrics for Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic Regression&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linear-regression-and-binary-responses&#34;&gt;Linear Regression and Binary Responses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bayes-classifier&#34;&gt;Bayes Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression-with-glm&#34;&gt;Logistic Regression with &lt;code&gt;glm()&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#roc-curves&#34;&gt;ROC Curves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multinomial-logistic-regression&#34;&gt;Multinomial Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf&#34;&gt;Chapter 4&lt;/a&gt; in &lt;em&gt;Introduction to Statistical Learning with Applications in R&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How do we make predictions about binary responses?&lt;/li&gt;
&lt;li&gt;Why should we be concerned about using simple linear regression?&lt;/li&gt;
&lt;li&gt;What is the right way to assess the accuracy of such a model?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt; is a form of &lt;strong&gt;supervised learning&lt;/strong&gt; where the response variable is categorical, as opposed to numeric for regression. &lt;em&gt;Our goal is to find a rule, algorithm, or function which takes as input a feature vector, and outputs a category which is the true category as often as possible.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./10-content_files/classification.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That is, the classifier &lt;span class=&#34;math inline&#34;&gt;\(\hat{C}(x)\)&lt;/span&gt; returns the predicted category &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{y}(x) = \hat{C}(x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To build our first classifier, we will use the &lt;code&gt;Default&lt;/code&gt; dataset from the &lt;code&gt;ISLR&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ISLR)
library(tibble)
as_tibble(Default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,000 x 4
##    default student balance income
##    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 No      No         730. 44362.
##  2 No      Yes        817. 12106.
##  3 No      No        1074. 31767.
##  4 No      No         529. 35704.
##  5 No      No         786. 38463.
##  6 No      Yes        920.  7492.
##  7 No      No         826. 24905.
##  8 No      Yes        809. 17600.
##  9 No      No        1161. 37469.
## 10 No      No           0  29275.
## # … with 9,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our goal is to properly classify individuals as defaulters based on student status, credit card balance, and income. Be aware that the response &lt;code&gt;default&lt;/code&gt; is a factor, as is the predictor &lt;code&gt;student&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(Default$default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.factor(Default$student)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we did with regression, we test-train split our data. In this case, using 50% for each.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42069)
default_idx   = sample(nrow(Default), 5000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;visualization-for-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualization for Classification&lt;/h2&gt;
&lt;p&gt;Often, some simple visualizations can suggest simple classification rules. To quickly create some useful visualizations, we use the &lt;code&gt;featurePlot()&lt;/code&gt; function from the &lt;code&gt;caret()&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A density plot can often suggest a simple split based on a numeric predictor. Essentially this plot graphs a density estimate&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{f}_{X_i}(x_i \mid Y = k)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for each numeric predictor &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and each category &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of the response &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;featurePlot(x = default_trn[, c(&amp;quot;balance&amp;quot;, &amp;quot;income&amp;quot;)],
            y = default_trn$default,
            plot = &amp;quot;density&amp;quot;,
            scales = list(x = list(relation = &amp;quot;free&amp;quot;),
                          y = list(relation = &amp;quot;free&amp;quot;)),
            adjust = 1.5,
            pch = &amp;quot;|&amp;quot;,
            layout = c(2, 1),
            auto.key = list(columns = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/10-content_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some notes about the arguments to this function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is a data frame containing only &lt;strong&gt;numeric predictors&lt;/strong&gt;. It would be nonsensical to estimate a density for a categorical predictor.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;y&lt;/code&gt; is the response variable. It needs to be a factor variable. If coded as &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, you will need to coerce to factor for plotting.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;plot&lt;/code&gt; specifies the type of plot, here &lt;code&gt;density&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scales&lt;/code&gt; defines the scale of the axes for each plot. By default, the axis of each plot would be the same, which often is not useful, so the arguments here, a different axis for each plot, will almost always be used.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;adjust&lt;/code&gt; specifies the amount of smoothing used for the density estimate.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pch&lt;/code&gt; specifies the &lt;strong&gt;p&lt;/strong&gt;lot &lt;strong&gt;ch&lt;/strong&gt;aracter used for the bottom of the plot.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;layout&lt;/code&gt; places the individual plots into rows and columns. For some odd reason, it is given as (col, row).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;auto.key&lt;/code&gt; defines the key at the top of the plot. The number of columns should be the number of categories.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It seems that the income variable by itself is not particularly useful. However, there seems to be a big difference in default status at a &lt;code&gt;balance&lt;/code&gt; of about 1400. We will use this information shortly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;featurePlot(x = default_trn[, c(&amp;quot;balance&amp;quot;, &amp;quot;income&amp;quot;)],
            y = default_trn$student,
            plot = &amp;quot;density&amp;quot;,
            scales = list(x = list(relation = &amp;quot;free&amp;quot;),
                          y = list(relation = &amp;quot;free&amp;quot;)),
            adjust = 1.5,
            pch = &amp;quot;|&amp;quot;,
            layout = c(2, 1),
            auto.key = list(columns = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/10-content_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, we create a similar plot, except with &lt;code&gt;student&lt;/code&gt; as the response. We see that students often carry a slightly larger balance, and have far lower income. This will be useful to know when making more complicated classifiers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;featurePlot(x = default_trn[, c(&amp;quot;student&amp;quot;, &amp;quot;balance&amp;quot;, &amp;quot;income&amp;quot;)],
            y = default_trn$default,
            plot = &amp;quot;pairs&amp;quot;,
            auto.key = list(columns = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/10-content_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use &lt;code&gt;plot = &#34;pairs&#34;&lt;/code&gt; to consider multiple variables at the same time. This plot reinforces using &lt;code&gt;balance&lt;/code&gt; to create a classifier, and again shows that &lt;code&gt;income&lt;/code&gt; seems not that useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ellipse)
featurePlot(x = default_trn[, c(&amp;quot;balance&amp;quot;, &amp;quot;income&amp;quot;)],
            y = default_trn$default,
            plot = &amp;quot;ellipse&amp;quot;,
            auto.key = list(columns = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/10-content_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar to &lt;code&gt;pairs&lt;/code&gt; is a plot of type &lt;code&gt;ellipse&lt;/code&gt;, which requires the &lt;code&gt;ellipse&lt;/code&gt; package. Here we only use numeric predictors, as essentially we are assuming multivariate normality. The ellipses mark points of equal density. This will be useful later when discussing LDA and QDA.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simple-classifier&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Simple Classifier&lt;/h2&gt;
&lt;p&gt;A very simple classifier is a rule based on a boundary &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; for a particular input variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; x &amp;gt; b \\
      0 &amp;amp; x \leq b
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Based on the first plot, we believe we can use &lt;code&gt;balance&lt;/code&gt; to create a reasonable classifier. In particular,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(\texttt{balance}) =
\begin{cases}
      \text{Yes} &amp;amp; \texttt{balance} &amp;gt; 1400 \\
      \text{No} &amp;amp; \texttt{balance} \leq 1400
   \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we predict an individual is a defaulter if their &lt;code&gt;balance&lt;/code&gt; is above 1400, and not a defaulter if the balance is 1400 or less.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simple_class = function(x, boundary, above = 1, below = 0) {
  ifelse(x &amp;gt; boundary, above, below)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We write a simple &lt;code&gt;R&lt;/code&gt; function that compares a variable to a boundary, then use it to make predictions on the train and test sets with our chosen variable and boundary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;default_trn_pred = simple_class(x = default_trn$balance,
                                boundary = 1400, above = &amp;quot;Yes&amp;quot;, below = &amp;quot;No&amp;quot;)
default_tst_pred = simple_class(x = default_tst$balance,
                                boundary = 1400, above = &amp;quot;Yes&amp;quot;, below = &amp;quot;No&amp;quot;)
head(default_tst_pred, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot; &amp;quot;No&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;metrics-for-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Metrics for Classification&lt;/h2&gt;
&lt;p&gt;In the classification setting, there are a large number of metrics to assess how well a classifier is performing.&lt;/p&gt;
&lt;p&gt;One of the most obvious things to do is arrange predictions and true values in a cross table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(trn_tab = table(predicted = default_trn_pred, actual = default_trn$default))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          actual
## predicted   No  Yes
##       No  4361   24
##       Yes  464  151&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(tst_tab = table(predicted = default_tst_pred, actual = default_tst$default))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          actual
## predicted   No  Yes
##       No  4319   28
##       Yes  523  130&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Often we give specific names to individual cells of these tables, and in the predictive setting, we would call this table a &lt;a href=&#34;https://en.wikipedia.org/wiki/Confusion_matrix&#34;&gt;&lt;strong&gt;confusion matrix&lt;/strong&gt;&lt;/a&gt;. Be aware, that the placement of Actual and Predicted values affects the names of the cells, and often the matrix may be presented transposed.&lt;/p&gt;
&lt;p&gt;In statistics, we label the errors Type I and Type II, but these are hard to remember. False Positive and False Negative are more descriptive, so we choose to use these.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/confusion.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;confusionMatrix()&lt;/code&gt; function from the &lt;code&gt;caret&lt;/code&gt; package can be used to obtain a wealth of additional information, which we see output below for the test data. Note that we specify which category is considered “positive.”&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trn_con_mat  = confusionMatrix(trn_tab, positive = &amp;quot;Yes&amp;quot;)
(tst_con_mat = confusionMatrix(tst_tab, positive = &amp;quot;Yes&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##          actual
## predicted   No  Yes
##       No  4319   28
##       Yes  523  130
##                                           
##                Accuracy : 0.8898          
##                  95% CI : (0.8808, 0.8984)
##     No Information Rate : 0.9684          
##     P-Value [Acc &amp;gt; NIR] : 1               
##                                           
##                   Kappa : 0.2842          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : &amp;lt;2e-16          
##                                           
##             Sensitivity : 0.8228          
##             Specificity : 0.8920          
##          Pos Pred Value : 0.1991          
##          Neg Pred Value : 0.9936          
##              Prevalence : 0.0316          
##          Detection Rate : 0.0260          
##    Detection Prevalence : 0.1306          
##       Balanced Accuracy : 0.8574          
##                                           
##        &amp;#39;Positive&amp;#39; Class : Yes             
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most common, and most important metric is the &lt;strong&gt;classification error rate&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{err}(\hat{C}, \text{Data}) = \frac{1}{n}\sum_{i = 1}^{n}I(y_i \neq \hat{C}(x_i))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; is an indicator function, so we are essentially calculating the proportion of predicted classes that match the true class.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
I(y_i \neq \hat{C}(x)) =
\begin{cases}
  1 &amp;amp; y_i \neq \hat{C}(x) \\
  0 &amp;amp; y_i = \hat{C}(x) \\
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is also common to discuss the &lt;strong&gt;accuracy&lt;/strong&gt;, which is simply one minus the error.&lt;/p&gt;
&lt;p&gt;Like regression, we often split the data, and then consider Train (Classification) Error and Test (Classification) Error will be used as a measure of how well a classifier will work on unseen future data.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{err}_{\texttt{trn}}(\hat{C}, \text{Train Data}) = \frac{1}{n_{\texttt{trn}}}\sum_{i \in \texttt{trn}}^{}I(y_i \neq \hat{C}(x_i))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{err}_{\texttt{tst}}(\hat{C}, \text{Test Data}) = \frac{1}{n_{\texttt{tst}}}\sum_{i \in \texttt{tst}}^{}I(y_i \neq \hat{C}(x_i))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Accuracy values can be found by calling &lt;code&gt;confusionMatrix()&lt;/code&gt;, or, if stored, can be accessed directly. Here, we use them to obtain error rates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - trn_con_mat$overall[&amp;quot;Accuracy&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Accuracy 
##   0.0976&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - tst_con_mat$overall[&amp;quot;Accuracy&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Accuracy 
##   0.1102&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sometimes guarding against making certain errors, FP or FN, are more important than simply finding the best accuracy. Thus, sometimes we will consider &lt;strong&gt;sensitivity&lt;/strong&gt; and &lt;strong&gt;specificity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Sens} = \text{True Positive Rate} = \frac{\text{TP}}{\text{P}} = \frac{\text{TP}}{\text{TP + FN}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tst_con_mat$byClass[&amp;quot;Sensitivity&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sensitivity 
##   0.8227848&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Spec} = \text{True Negative Rate} = \frac{\text{TN}}{\text{N}} = \frac{\text{TN}}{\text{TN + FP}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tst_con_mat$byClass[&amp;quot;Specificity&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Specificity 
##   0.8919868&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like accuracy, these can easily be found using &lt;code&gt;confusionMatrix()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When considering how well a classifier is performing, often, it is understandable to assume that any accuracy in a binary classification problem above 0.50, is a reasonable classifier. This however is not the case. We need to consider the &lt;strong&gt;balance&lt;/strong&gt; of the classes. To do so, we look at the &lt;strong&gt;prevalence&lt;/strong&gt; of positive cases.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Prev} = \frac{\text{P}}{\text{Total Obs}}= \frac{\text{TP + FN}}{\text{Total Obs}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trn_con_mat$byClass[&amp;quot;Prevalence&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Prevalence 
##      0.035&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tst_con_mat$byClass[&amp;quot;Prevalence&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Prevalence 
##     0.0316&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we see an extremely low prevalence, which suggests an even simpler classifier than our current based on &lt;code&gt;balance&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(\texttt{balance}) =
\begin{cases}
      \text{No} &amp;amp; \texttt{balance} &amp;gt; 1400 \\
      \text{No} &amp;amp; \texttt{balance} \leq 1400
   \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This classifier simply classifies all observations as negative cases.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_all_no = simple_class(default_tst$balance,
                           boundary = 1400, above = &amp;quot;No&amp;quot;, below = &amp;quot;No&amp;quot;)
table(predicted = pred_all_no, actual = default_tst$default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          actual
## predicted   No  Yes
##        No 4842  158&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;confusionMatrix()&lt;/code&gt; function won’t even accept this table as input, because it isn’t a full matrix, only one row, so we calculate error rates directly. To do so, we write a function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_class_err(actual = default_tst$default,
               predicted = pred_all_no)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0316&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see that the error rate is exactly the prevelance of the minority class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(default_tst$default) / length(default_tst$default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##     No    Yes 
## 0.9684 0.0316&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This classifier does better than the previous. But the point is, in reality, to create a good classifier, we should obtain a test error better than 0.033, which is obtained by simply manipulating the prevalences. Next section, we’ll introduce much better classifiers which should have no problem accomplishing this task.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Logistic Regression&lt;/h1&gt;
&lt;p&gt;In this section, we continue our discussion of classification. We introduce our first model for classification, logistic regression. To begin, we return to the &lt;code&gt;Default&lt;/code&gt; dataset from above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ISLR)
library(tibble)
as_tibble(Default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,000 x 4
##    default student balance income
##    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 No      No         730. 44362.
##  2 No      Yes        817. 12106.
##  3 No      No        1074. 31767.
##  4 No      No         529. 35704.
##  5 No      No         786. 38463.
##  6 No      Yes        920.  7492.
##  7 No      No         826. 24905.
##  8 No      Yes        809. 17600.
##  9 No      No        1161. 37469.
## 10 No      No           0  29275.
## # … with 9,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also repeat the test-train split from above (you need not repeat this step if you have this saved).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
default_idx = sample(nrow(Default), 5000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;linear-regression-and-binary-responses&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Regression and Binary Responses&lt;/h2&gt;
&lt;p&gt;Before moving on to logistic regression, why not plain, old, linear regression?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;default_trn_lm = default_trn
default_tst_lm = default_tst&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since linear regression expects a numeric response variable, we coerce the response to be numeric. (Notice that we also shift the results, as we require &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;, not &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt;.) Notice we have also copied the dataset so that we can return the original data with factors later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;default_trn_lm$default = as.numeric(default_trn_lm$default) - 1
default_tst_lm$default = as.numeric(default_tst_lm$default) - 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why would we think this should work? Recall that,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mathbb{E}}[Y \mid X = x] = X\hat{\beta}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is limited to values of &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[Y \mid X = x] = P(Y = 1 \mid X = x).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It would then seem reasonable that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\hat{\beta}\)&lt;/span&gt; is a reasonable estimate of &lt;span class=&#34;math inline&#34;&gt;\(P(Y = 1 \mid X = x)\)&lt;/span&gt;. We test this on the &lt;code&gt;Default&lt;/code&gt; data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_lm = lm(default ~ balance, data = default_trn_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Everything seems to be working, until we plot the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(default ~ balance, data = default_trn_lm,
     col = &amp;quot;darkorange&amp;quot;, pch = &amp;quot;|&amp;quot;, ylim = c(-0.2, 1),
     main = &amp;quot;Using Linear Regression for Classification&amp;quot;)
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
abline(model_lm, lwd = 3, col = &amp;quot;dodgerblue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/10-content_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Two issues arise. First, all of the predicted probabilities are below 0.5. That means, we would classify every observation as a &lt;code&gt;&#34;No&#34;&lt;/code&gt;. This is certainly possible, but not what we would expect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(predict(model_lm) &amp;lt; 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next, and bigger issue, is predicted probabilities less than 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;any(predict(model_lm) &amp;lt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-classifier&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes Classifier&lt;/h2&gt;
&lt;p&gt;Why are we using a predicted probability of 0.5 as the cutoff for classification? Recall, the Bayes Classifier, which minimizes the classification error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
C^B(x) = \underset{g}{\mathrm{argmax}} \ P(Y = g \mid  X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, in the binary classification problem, we will use predicted probabilities&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid { X = x})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{P}(Y = 0 \mid { X = x})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and then classify to the larger of the two. We actually only need to consider a single probability, usually &lt;span class=&#34;math inline&#34;&gt;\(\hat{P}(Y = 1 \mid { X = x})\)&lt;/span&gt;. Since we use it so often, we give it the shorthand notation, &lt;span class=&#34;math inline&#34;&gt;\(\hat{p}(x)\)&lt;/span&gt;. Then the classifier is written,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; \hat{p}(x) &amp;gt; 0.5 \\
      0 &amp;amp; \hat{p}(x) \leq 0.5
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This classifier is essentially estimating the Bayes Classifier, thus, is seeking to minimize classification errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression-with-glm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic Regression with &lt;code&gt;glm()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;To better estimate the probability&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(x) = P(Y = 1 \mid {X = x})
\]&lt;/span&gt;
we turn to logistic regression. The model is written&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\log\left(\frac{p(x)}{1 - p(x)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Rearranging, we see the probabilities can be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p)}} = \sigma(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots  + \beta_p x_p)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice, we use the sigmoid function as shorthand notation, which appears often in deep learning literature. It takes any real input, and outputs a number between 0 and 1. How useful! (This is actualy a particular sigmoid function called the logistic function, but since it is by far the most popular sigmoid function, often sigmoid function is used to refer to the logistic function)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sigma(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The model is fit by numerically maximizing the likelihood, which we will let &lt;code&gt;R&lt;/code&gt; take care of.&lt;/p&gt;
&lt;p&gt;We start with a single predictor example, again using &lt;code&gt;balance&lt;/code&gt; as our single predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glm = glm(default ~ balance, data = default_trn, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fitting this model looks very similar to fitting a simple linear regression. Instead of &lt;code&gt;lm()&lt;/code&gt; we use &lt;code&gt;glm()&lt;/code&gt;. The only other difference is the use of &lt;code&gt;family = &#34;binomial&#34;&lt;/code&gt; which indicates that we have a two-class categorical response. Using &lt;code&gt;glm()&lt;/code&gt; with &lt;code&gt;family = &#34;gaussian&#34;&lt;/code&gt; would perform the usual linear regression.&lt;/p&gt;
&lt;p&gt;First, we can obtain the fitted coefficients the same way we did with linear regression.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(model_glm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)       balance 
## -10.493158288   0.005424994&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next thing we should understand is how the &lt;code&gt;predict()&lt;/code&gt; function works with &lt;code&gt;glm()&lt;/code&gt;. So, let’s look at some predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_glm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2369      5273      9290      1252      8826       356 
## -5.376670 -4.875653 -5.018746 -4.007664 -6.538414 -6.601582&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, &lt;code&gt;predict.glm()&lt;/code&gt; uses &lt;code&gt;type = &#34;link&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_glm, type = &amp;quot;link&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      2369      5273      9290      1252      8826       356 
## -5.376670 -4.875653 -5.018746 -4.007664 -6.538414 -6.601582&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is, &lt;code&gt;R&lt;/code&gt; is returning&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p
\]&lt;/span&gt;
for each observation.&lt;/p&gt;
&lt;p&gt;Importantly, these are &lt;strong&gt;not&lt;/strong&gt; predicted probabilities. To obtain the predicted probabilities&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we need to use &lt;code&gt;type = &#34;response&#34;&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_glm, type = &amp;quot;response&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        2369        5273        9290        1252        8826         356 
## 0.004601914 0.007572331 0.006569370 0.017851333 0.001444691 0.001356375&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that these are probabilities, &lt;strong&gt;not&lt;/strong&gt; classifications. To obtain classifications, we will need to compare to the correct cutoff value with an &lt;code&gt;ifelse()&lt;/code&gt; statement.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glm_pred = ifelse(predict(model_glm, type = &amp;quot;link&amp;quot;) &amp;gt; 0, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)
# model_glm_pred = ifelse(predict(model_glm, type = &amp;quot;response&amp;quot;) &amp;gt; 0.5, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The line that is run is performing&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; \hat{f}(x) &amp;gt; 0 \\
      0 &amp;amp; \hat{f}(x) \leq 0
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{f}(x) =\hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + \cdots  + \hat{\beta}_p x_p.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The commented line, which would give the same results, is performing&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; \hat{p}(x) &amp;gt; 0.5 \\
      0 &amp;amp; \hat{p}(x) \leq 0.5
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid X = x).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Once we have classifications, we can calculate metrics such as the trainging classification error rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_class_err = function(actual, predicted) {
  mean(actual != predicted)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_class_err(actual = default_trn$default, predicted = model_glm_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0284&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we saw previously, the &lt;code&gt;table()&lt;/code&gt; and &lt;code&gt;confusionMatrix()&lt;/code&gt; functions can be used to quickly obtain many more metrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_tab = table(predicted = model_glm_pred, actual = default_trn$default)
library(caret)
train_con_mat = confusionMatrix(train_tab, positive = &amp;quot;Yes&amp;quot;)
c(train_con_mat$overall[&amp;quot;Accuracy&amp;quot;],
  train_con_mat$byClass[&amp;quot;Sensitivity&amp;quot;],
  train_con_mat$byClass[&amp;quot;Specificity&amp;quot;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Accuracy Sensitivity Specificity 
##   0.9716000   0.2941176   0.9954451&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also write a custom function for the error for use with trained logist regression models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_logistic_error = function(mod, data, res = &amp;quot;y&amp;quot;, pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = &amp;quot;response&amp;quot;)
  preds = ifelse(probs &amp;gt; cut, pos, neg)
  calc_class_err(actual = data[, res], predicted = preds)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function will be useful later when calculating train and test errors for several models at the same time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_logistic_error(model_glm, data = default_trn,
                   res = &amp;quot;default&amp;quot;, pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0284&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see how much better logistic regression is for this task, we create the same plot we used for linear regression.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(default ~ balance, data = default_trn_lm,
     col = &amp;quot;darkorange&amp;quot;, pch = &amp;quot;|&amp;quot;, ylim = c(-0.2, 1),
     main = &amp;quot;Using Logistic Regression for Classification&amp;quot;)
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
curve(predict(model_glm, data.frame(balance = x), type = &amp;quot;response&amp;quot;),
      add = TRUE, lwd = 3, col = &amp;quot;dodgerblue&amp;quot;)
abline(v = -coef(model_glm)[1] / coef(model_glm)[2], lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/10-content_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot contains a wealth of information.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The orange &lt;code&gt;|&lt;/code&gt; characters are the data, &lt;span class=&#34;math inline&#34;&gt;\((x_i, y_i)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The blue “curve” is the predicted probabilities given by the fitted logistic regression. That is,
&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid { X = x})
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The solid vertical black line represents the &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_boundary&#34;&gt;decision boundary&lt;/a&gt;&lt;/strong&gt;, the &lt;code&gt;balance&lt;/code&gt; that obtains a predicted probability of 0.5. In this case &lt;code&gt;balance&lt;/code&gt; = 1934.2247145.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The decision boundary is found by solving for points that satisfy&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{p}(x) = \hat{P}(Y = 1 \mid { X = x}) = 0.5
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is equivalent to point that satisfy&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\beta}_0 + \hat{\beta}_1 x_1 = 0.
\]&lt;/span&gt;
Thus, for logistic regression with a single predictor, the decision boundary is given by the &lt;em&gt;point&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_1 = \frac{-\hat{\beta}_0}{\hat{\beta}_1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following is not run, but an alternative way to add the logistic curve to the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid = seq(0, max(default_trn$balance), by = 0.01)

sigmoid = function(x) {
  1 / (1 + exp(-x))
}

lines(grid, sigmoid(coef(model_glm)[1] + coef(model_glm)[2] * grid), lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the usual formula syntax, it is easy to add or remove complexity from logistic regressions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_1 = glm(default ~ 1, data = default_trn, family = &amp;quot;binomial&amp;quot;)
model_2 = glm(default ~ ., data = default_trn, family = &amp;quot;binomial&amp;quot;)
model_3 = glm(default ~ . ^ 2 + I(balance ^ 2),
              data = default_trn, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, using polynomial transformations of predictors will allow a linear model to have non-linear decision boundaries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_list = list(model_1, model_2, model_3)
train_errors = sapply(model_list, get_logistic_error, data = default_trn,
                      res = &amp;quot;default&amp;quot;, pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.5)
test_errors  = sapply(model_list, get_logistic_error, data = default_tst,
                      res = &amp;quot;default&amp;quot;, pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see the misclassification error rates for each model. The train decreases, and the test decreases, until it starts to increases. Everything we learned about the bias-variance tradeoff for regression also applies here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff(train_errors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.0066  0.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff(test_errors)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.0068  0.0006&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We call &lt;code&gt;model_2&lt;/code&gt; the &lt;strong&gt;additive&lt;/strong&gt; logistic model, which we will use quite often.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curves&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ROC Curves&lt;/h2&gt;
&lt;p&gt;Let’s return to our simple model with only balance as a predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_glm = glm(default ~ balance, data = default_trn, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We write a function which allows use to make predictions based on different probability cutoffs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_logistic_pred = function(mod, data, res = &amp;quot;y&amp;quot;, pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = &amp;quot;response&amp;quot;)
  ifelse(probs &amp;gt; cut, pos, neg)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{C}(x) =
\begin{cases}
      1 &amp;amp; \hat{p}(x) &amp;gt; c \\
      0 &amp;amp; \hat{p}(x) \leq c
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s use this to obtain predictions using a low, medium, and high cutoff. (0.1, 0.5, and 0.9)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_pred_10 = get_logistic_pred(model_glm, data = default_tst, res = &amp;quot;default&amp;quot;,
                                 pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.1)
test_pred_50 = get_logistic_pred(model_glm, data = default_tst, res = &amp;quot;default&amp;quot;,
                                 pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.5)
test_pred_90 = get_logistic_pred(model_glm, data = default_tst, res = &amp;quot;default&amp;quot;,
                                 pos = &amp;quot;Yes&amp;quot;, neg = &amp;quot;No&amp;quot;, cut = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we evaluate accuracy, sensitivity, and specificity for these classifiers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_tab_10 = table(predicted = test_pred_10, actual = default_tst$default)
test_tab_50 = table(predicted = test_pred_50, actual = default_tst$default)
test_tab_90 = table(predicted = test_pred_90, actual = default_tst$default)

test_con_mat_10 = confusionMatrix(test_tab_10, positive = &amp;quot;Yes&amp;quot;)
test_con_mat_50 = confusionMatrix(test_tab_50, positive = &amp;quot;Yes&amp;quot;)
test_con_mat_90 = confusionMatrix(test_tab_90, positive = &amp;quot;Yes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;metrics = rbind(

  c(test_con_mat_10$overall[&amp;quot;Accuracy&amp;quot;],
    test_con_mat_10$byClass[&amp;quot;Sensitivity&amp;quot;],
    test_con_mat_10$byClass[&amp;quot;Specificity&amp;quot;]),

  c(test_con_mat_50$overall[&amp;quot;Accuracy&amp;quot;],
    test_con_mat_50$byClass[&amp;quot;Sensitivity&amp;quot;],
    test_con_mat_50$byClass[&amp;quot;Specificity&amp;quot;]),

  c(test_con_mat_90$overall[&amp;quot;Accuracy&amp;quot;],
    test_con_mat_90$byClass[&amp;quot;Sensitivity&amp;quot;],
    test_con_mat_90$byClass[&amp;quot;Specificity&amp;quot;])

)

rownames(metrics) = c(&amp;quot;c = 0.10&amp;quot;, &amp;quot;c = 0.50&amp;quot;, &amp;quot;c = 0.90&amp;quot;)
metrics&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Accuracy Sensitivity Specificity
## c = 0.10   0.9328  0.71779141   0.9400455
## c = 0.50   0.9730  0.31288344   0.9952450
## c = 0.90   0.9688  0.04294479   1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see then sensitivity decreases as the cutoff is increased. Conversely, specificity increases as the cutoff increases. This is useful if we are more interested in a particular error, instead of giving them equal weight.&lt;/p&gt;
&lt;p&gt;Note that usually the best accuracy will be seen near &lt;span class=&#34;math inline&#34;&gt;\(c = 0.50\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead of manually checking cutoffs, we can create an ROC curve (receiver operating characteristic curve) which will sweep through all possible cutoffs, and plot the sensitivity and specificity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pROC)
test_prob = predict(model_glm, newdata = default_tst, type = &amp;quot;response&amp;quot;)
test_roc = roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/10-content_files/figure-html/unnamed-chunk-52-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(test_roc$auc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9492866&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A good model will have a high AUC, that is as often as possible a high sensitivity and specificity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multinomial-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multinomial Logistic Regression&lt;/h2&gt;
&lt;p&gt;What if the response contains more than two categories? For that we need multinomial logistic regression.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(Y = k \mid { X = x}) = \frac{e^{\beta_{0k} + \beta_{1k} x_1 + \cdots +  + \beta_{pk} x_p}}{\sum_{g = 1}^{G} e^{\beta_{0g} + \beta_{1g} x_1 + \cdots + \beta_{pg} x_p}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will omit the details, as ISL has as well. If you are interested, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multinomial_logistic_regression&#34;&gt;Wikipedia page&lt;/a&gt; provides a rather thorough coverage. Also note that the above is an example of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Softmax_function&#34;&gt;softmax function&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As an example of a dataset with a three category response, we use the &lt;code&gt;iris&lt;/code&gt; dataset, which is so famous, it has its own &lt;a href=&#34;https://en.wikipedia.org/wiki/Iris_flower_data_set&#34;&gt;Wikipedia entry&lt;/a&gt;. It is also a default dataset in &lt;code&gt;R&lt;/code&gt;, so no need to load it.&lt;/p&gt;
&lt;p&gt;Before proceeding, we test-train split this data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(430)
iris_obs = nrow(iris)
iris_idx = sample(iris_obs, size = trunc(0.50 * iris_obs))
iris_trn = iris[iris_idx, ]
iris_test = iris[-iris_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To perform multinomial logistic regression, we use the &lt;code&gt;multinom&lt;/code&gt; function from the &lt;code&gt;nnet&lt;/code&gt; package. Training using &lt;code&gt;multinom()&lt;/code&gt; is done using similar syntax to &lt;code&gt;lm()&lt;/code&gt; and &lt;code&gt;glm()&lt;/code&gt;. We add the &lt;code&gt;trace = FALSE&lt;/code&gt; argument to suppress information about updates to the optimization routine as the model is trained.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nnet)
model_multi = multinom(Species ~ ., data = iris_trn, trace = FALSE)
summary(model_multi)$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width
## versicolor    16.77474    -7.855576   -13.98668     25.13860    4.270375
## virginica    -33.94895   -37.519645   -94.22846     97.82691   73.487162&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice we are only given coefficients for two of the three class, much like only needing coefficients for one class in logistic regression.&lt;/p&gt;
&lt;p&gt;A difference between &lt;code&gt;glm()&lt;/code&gt; and &lt;code&gt;multinom()&lt;/code&gt; is how the &lt;code&gt;predict()&lt;/code&gt; function operates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_multi, newdata = iris_trn))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] setosa     versicolor versicolor setosa     virginica  versicolor
## Levels: setosa versicolor virginica&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(predict(model_multi, newdata = iris_trn, type = &amp;quot;prob&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           setosa   versicolor     virginica
## 1   1.000000e+00 1.910554e-16 6.118616e-176
## 92  8.542846e-22 1.000000e+00  1.372168e-18
## 77  8.343856e-23 1.000000e+00  2.527471e-14
## 38  1.000000e+00 1.481126e-16 5.777917e-180
## 108 1.835279e-73 1.403654e-36  1.000000e+00
## 83  1.256090e-16 1.000000e+00  2.223689e-32&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that by default, classifications are returned. When obtaining probabilities, we are given the predicted probability for &lt;strong&gt;each&lt;/strong&gt; class.&lt;/p&gt;
&lt;p&gt;Interestingly, you’ve just fit a neural network, and you didn’t even know it! (Hence the &lt;code&gt;nnet&lt;/code&gt; package.) Later we will discuss the connections between logistic regression, multinomial logistic regression, and simple neural networks.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Bias-Variance Tradeoff</title>
      <link>/content/09-content/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/content/09-content/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-biasvariance-tradeoff&#34;&gt;The Bias–Variance Tradeoff&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-setup-and-source&#34;&gt;R Setup and Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-regression-setup&#34;&gt;The Regression Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reducible-and-irreducible-error&#34;&gt;Reducible and Irreducible Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bias-variance-decomposition&#34;&gt;Bias-Variance Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-simulation-to-estimate-bias-and-variance&#34;&gt;Using Simulation to Estimate Bias and Variance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimating-expected-prediction-error&#34;&gt;Estimating Expected Prediction Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-flexibility&#34;&gt;Model Flexibility&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linear-models&#34;&gt;Linear Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbors&#34;&gt;k-Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decision-trees&#34;&gt;Decision Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://trevorhastie.github.io/ISLR/ISLR%20Seventh%20Printing.pdf&#34;&gt;Chapter 2&lt;/a&gt; in &lt;em&gt;Introduction to Statistical Learning with Applications in R&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;What is the relationship between &lt;strong&gt;bias&lt;/strong&gt;, &lt;strong&gt;variance&lt;/strong&gt;, and &lt;strong&gt;mean squared error?&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;What is the relationship between &lt;strong&gt;model flexibility&lt;/strong&gt; and training error?&lt;/li&gt;
&lt;li&gt;What is the relationship between &lt;strong&gt;model flexibility&lt;/strong&gt; and validation (or test) error?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-biasvariance-tradeoff&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Bias–Variance Tradeoff&lt;/h1&gt;
&lt;p&gt;This lecture will begin to dig into some theoretical details of estimating regression functions, in particular how the &lt;strong&gt;bias-variance tradeoff&lt;/strong&gt; helps explain the relationship between &lt;strong&gt;model flexibility&lt;/strong&gt; and the errors a model makes.&lt;/p&gt;
&lt;p&gt;This content is currently &lt;strong&gt;under construction&lt;/strong&gt;. You can expect it to be a lot less polished than other sections.&lt;/p&gt;
&lt;p&gt;Don’t freak out if this seems mathematically overwhelming. We’ll walk through relatively slowly. It’s not super important to follow the nitty-gritty details; but the broad takeaways are quite important.&lt;/p&gt;
&lt;div id=&#34;r-setup-and-source&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R Setup and Source&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tibble)     # data frame printing
library(dplyr)      # data manipulation

library(caret)      # fitting knn
library(rpart)      # fitting trees
library(rpart.plot) # plotting trees&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-regression-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Regression Setup&lt;/h2&gt;
&lt;p&gt;Consider the general regression setup where we are given a random pair &lt;span class=&#34;math inline&#34;&gt;\((X, Y) \in \mathbb{R}^p \times \mathbb{R}\)&lt;/span&gt;. We would like to “predict” &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; with some function of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, say, &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To clarify what we mean by “predict,” we specify that we would like &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; to be “close” to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. To further clarify what we mean by “close,” we define the &lt;strong&gt;squared error loss&lt;/strong&gt; of estimating &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L(Y, f(X)) \triangleq (Y - f(X)) ^ 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the &lt;strong&gt;risk&lt;/strong&gt; of estimating &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R(Y, f(X)) \triangleq \mathbb{E}[L(Y, f(X))] = \mathbb{E}_{X, Y}[(Y - f(X)) ^ 2]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Before attempting to minimize the risk, we first re-write the risk after conditioning on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}_{X, Y} \left[ (Y - f(X)) ^ 2 \right] = \mathbb{E}_{X} \mathbb{E}_{Y \mid X} \left[ ( Y - f(X) ) ^ 2 \mid X = x \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Minimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to &lt;span class=&#34;math inline&#34;&gt;\(Y \mid X\)&lt;/span&gt;, essentially minimizing the risk pointwise, for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It turns out, that the risk is minimized by setting &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; to be equal the conditional mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(x) = \mathbb{E}(Y \mid X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which we call the &lt;strong&gt;regression function&lt;/strong&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L(Y, f(X)) \triangleq | Y - f(X) |
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The risk would then be minimized setting &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; equal to the conditional median.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
f(x) = \text{median}(Y \mid X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Despite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations.&lt;/p&gt;
&lt;p&gt;Now, given data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}\)&lt;/span&gt;, our goal becomes finding some &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; that is a good estimate of the regression function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;. We’ll see that this amounts to minimizing what we call the reducible error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reducible-and-irreducible-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reducible and Irreducible Error&lt;/h2&gt;
&lt;p&gt;Suppose that we obtain some &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;, how well does it estimate &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;? We define the &lt;strong&gt;expected prediction error&lt;/strong&gt; of predicting &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt;. A good &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; will have a low expected prediction error.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}(X)\right) \triangleq \mathbb{E}_{X, Y, \mathcal{D}} \left[  \left( Y - \hat{f}(X) \right)^2 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This expectation is over &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and also &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;. The estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; is actually random depending on the data, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, used to estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;. We could actually write &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X, \mathcal{D})\)&lt;/span&gt; to make this dependence explicit, but our notation will become cumbersome enough as it is.&lt;/p&gt;
&lt;p&gt;Like before, we’ll condition on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. This results in the expected prediction error of predicting &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}(x)\right) =
\mathbb{E}_{Y \mid X, \mathcal{D}} \left[  \left(Y - \hat{f}(X) \right)^2 \mid X = x \right] =
\underbrace{\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]}_\textrm{reducible error} +
\underbrace{\mathbb{V}_{Y \mid X} \left[ Y \mid X = x \right]}_\textrm{irreducible error}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A number of things to note here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The expected prediction error is for a random &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given a fixed &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and a random &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;. As such, the expectation is over &lt;span class=&#34;math inline&#34;&gt;\(Y \mid X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;. Our estimated function &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; is random depending on the data, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, which is used to perform the estimation.&lt;/li&gt;
&lt;li&gt;The expected prediction error of predicting &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt; has been decomposed into two errors:
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;reducible error&lt;/strong&gt;, which is the expected squared error loss of estimation &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; at a fixed point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. The only thing that is random here is &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, the data used to obtain &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt;. (Both &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; are fixed.) We’ll often call this reducible error the &lt;strong&gt;mean squared error&lt;/strong&gt; of estimating &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; at a fixed point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.
&lt;span class=&#34;math display&#34;&gt;\[ \text{MSE}\left(f(x), \hat{f}(x)\right) \triangleq \mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;irreducible error&lt;/strong&gt;. This is simply the variance of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given that &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt;, essentially noise that we do not want to learn. This is also called the &lt;strong&gt;Bayes error&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As the name suggests, the reducible error is the error that we have some control over. But how do we control this error?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-variance-decomposition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bias-Variance Decomposition&lt;/h2&gt;
&lt;p&gt;After decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error.&lt;/p&gt;
&lt;p&gt;Recall the definition of the &lt;strong&gt;bias&lt;/strong&gt; of an estimator.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{bias}(\hat{\theta}) \triangleq \mathbb{E}\left[\hat{\theta}\right] - \theta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also recall the definition of the &lt;strong&gt;variance&lt;/strong&gt; of an estimator.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{V}(\hat{\theta}) = \text{var}(\hat{\theta}) \triangleq \mathbb{E}\left [ ( \hat{\theta} -\mathbb{E}\left[\hat{\theta}\right] )^2 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using this, we further decompose the reducible error (mean squared error) into bias squared and variance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE}\left(f(x), \hat{f}(x)\right) =
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] =
\underbrace{\left(f(x) - \mathbb{E} \left[ \hat{f}(x) \right]  \right)^2}_{\text{bias}^2 \left(\hat{f}(x) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}(x) - \mathbb{E} \left[ \hat{f}(x) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}(x) \right)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; at some point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE}\left(f(x), \hat{f}(x)\right) = \text{bias}^2 \left(\hat{f}(x) \right) + \text{var} \left(\hat{f}(x) \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a perfect world, we would be able to find some &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; which is &lt;strong&gt;unbiased&lt;/strong&gt;, that is &lt;span class=&#34;math inline&#34;&gt;\(\text{bias}\left(\hat{f}(x) \right) = 0\)&lt;/span&gt;, which also has low variance. In practice, this isn’t always possible.&lt;/p&gt;
&lt;p&gt;It turns out, there is a &lt;strong&gt;bias-variance tradeoff&lt;/strong&gt;. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance.&lt;/p&gt;
&lt;p&gt;In the context of regression, models are biased when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parametric: The form of the model &lt;a href=&#34;https://en.wikipedia.org/wiki/Omitted-variable_bias&#34;&gt;does not incorporate all the necessary variables&lt;/a&gt;, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic.&lt;/li&gt;
&lt;li&gt;Non-parametric: The model provides too much smoothing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the context of regression, models are variable when:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parametric: The form of the model incorporates too many variables, or the form of the relationship is too flexible. For example, a parametric model assumes a cubic relationship, but the true relationship is linear.&lt;/li&gt;
&lt;li&gt;Non-parametric: The model does not provide enough smoothing. It is very, “wiggly.”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data.&lt;/p&gt;
&lt;p&gt;Recall that when fitting models, we’ve seen that train RMSE decreases as model flexibility is increasing. (Technically it is non-increasing.) For validation RMSE, we expect to see a U-shaped curve. Importantly, validation RMSE decreases, until a certain flexibility, then begins to increase.&lt;/p&gt;
&lt;p&gt;Now we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the flexibility that minimizes the prediction error.&lt;/p&gt;
&lt;p&gt;To summarize, if we assume that irreducible error can be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{V}[Y \mid X = x] = \sigma ^ 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;then we can write the full decomposition of the expected prediction error of predicting &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt; as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}(x)\right) =
\underbrace{\text{bias}^2\left(\hat{f}(x)\right) + \text{var}\left(\hat{f}(x)\right)}_\textrm{reducible error} + \sigma^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As model flexibility increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model flexibility to find a model that will predict well on unseen observations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/error-vs-flex-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Tying this all together, the above image shows how we “expect” training and validation error to behavior in relation to model flexibility.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; In practice, we won’t always see such a nice “curve” in the validation error, but we expect to see the general trends.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-simulation-to-estimate-bias-and-variance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using Simulation to Estimate Bias and Variance&lt;/h2&gt;
&lt;p&gt;We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function &lt;span class=&#34;math inline&#34;&gt;\(f(x) = x^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f = function(x) {
  x ^ 2
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More specifically, we’d like to predict an observation, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, given that &lt;span class=&#34;math inline&#34;&gt;\(X = x\)&lt;/span&gt; by using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(x)\)&lt;/span&gt; where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[Y \mid X = x] = f(x) = x^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{V}[Y \mid X = x] = \sigma ^ 2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Alternatively, we could write this as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = f(X) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[\epsilon] = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{V}[\epsilon] = \sigma ^ 2\)&lt;/span&gt;. In this formulation, we call &lt;span class=&#34;math inline&#34;&gt;\(f(X)\)&lt;/span&gt; the &lt;strong&gt;signal&lt;/strong&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; the &lt;strong&gt;noise&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following &lt;code&gt;R&lt;/code&gt; code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_sim_data = function(f, sample_size = 100) {
  x = runif(n = sample_size, min = 0, max = 1)
  y = rnorm(n = sample_size, mean = f(x), sd = 0.3)
  tibble(x, y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also note that if you prefer to think of this situation using the &lt;span class=&#34;math inline&#34;&gt;\(Y = f(X) + \epsilon\)&lt;/span&gt; formulation, the following code represents the same data generating process.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_sim_data = function(f, sample_size = 100) {
  x = runif(n = sample_size, min = 0, max = 1)
  eps = rnorm(n = sample_size, mean = 0, sd = 0.75)
  y = f(x) + eps
  tibble(x, y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To completely specify the data generating process, we have made more model assumptions than simply &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[Y \mid X = x] = x^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{V}[Y \mid X = x] = \sigma ^ 2\)&lt;/span&gt;. In particular,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from a uniform distribution over &lt;span class=&#34;math inline&#34;&gt;\([0, 1]\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; are independent.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; are sampled from the conditional normal distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y \mid X \sim N(f(x), \sigma^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using this setup, we will generate datasets, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, with a sample size &lt;span class=&#34;math inline&#34;&gt;\(n = 100\)&lt;/span&gt; and fit four models.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\texttt{predict(fit0, x)} &amp;amp;= \hat{f}_0(x) = \hat{\beta}_0\\
\texttt{predict(fit1, x)} &amp;amp;= \hat{f}_1(x) = \hat{\beta}_0 + \hat{\beta}_1 x \\
\texttt{predict(fit2, x)} &amp;amp;= \hat{f}_2(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \\
\texttt{predict(fit9, x)} &amp;amp;= \hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
sim_data = gen_sim_data(f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that technically we’re being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes.&lt;/p&gt;
&lt;p&gt;Plotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.&lt;/p&gt;
&lt;p&gt;While the ninth degree model doesn’t appear to be correct for any of these three simulations, we’ll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn’t the only goal we’re after, since in practice, we’ll only have a single dataset. This is why we’d also like our models to exhibit low variance.&lt;/p&gt;
&lt;p&gt;We could have also fit &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-nearest neighbors models to these three datasets.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see that when &lt;span class=&#34;math inline&#34;&gt;\(k = 100\)&lt;/span&gt; we have a biased model with very low variance.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; When &lt;span class=&#34;math inline&#34;&gt;\(k = 5\)&lt;/span&gt;, we again have a highly variable model.&lt;/p&gt;
&lt;p&gt;These two sets of plots reinforce our intuition about the bias-variance tradeoff. Flexible models (ninth degree polynomial and &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and &lt;span class=&#34;math inline&#34;&gt;\(k = 100\)&lt;/span&gt;) are very biased, but have extremely low variance.&lt;/p&gt;
&lt;p&gt;We will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; given by these four models at the point &lt;span class=&#34;math inline&#34;&gt;\(x = 0.90\)&lt;/span&gt;. We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (sim in 1:n_sims) {

  # simulate new, random, training data
  # this is the only random portion of the bias, var, and mse calculations
  # this allows us to calculate the expectation over D
  sim_data = gen_sim_data(f)

  # fit models
  fit_0 = lm(y ~ 1,                   data = sim_data)
  fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)

  # get predictions
  predictions[sim, 1] = predict(fit_0, x)
  predictions[sim, 2] = predict(fit_1, x)
  predictions[sim, 3] = predict(fit_2, x)
  predictions[sim, 4] = predict(fit_9, x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this is one of many ways we could have accomplished this task using &lt;code&gt;R&lt;/code&gt;. For example we could have used a combination of &lt;code&gt;replicate()&lt;/code&gt; and &lt;code&gt;*apply()&lt;/code&gt; functions. Alternatively, we could have used a &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; approach, which likely would have used some combination of &lt;a href=&#34;http://dplyr.tidyverse.org/&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;http://tidyr.tidyverse.org/&#34;&gt;&lt;code&gt;tidyr&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;http://purrr.tidyverse.org/&#34;&gt;&lt;code&gt;purrr&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our approach, which would be considered a &lt;code&gt;base&lt;/code&gt; &lt;code&gt;R&lt;/code&gt; approach, was chosen to make it as clear as possible what is being done. The &lt;code&gt;tidyverse&lt;/code&gt; approach is rapidly gaining popularity in the &lt;code&gt;R&lt;/code&gt; community, but might make it more difficult to see what is happening here, unless you are already familiar with that approach.&lt;/p&gt;
&lt;p&gt;Also of note, while it may seem like the output stored in &lt;code&gt;predictions&lt;/code&gt; would meet the definition of &lt;a href=&#34;http://vita.had.co.nz/papers/tidy-data.html&#34;&gt;tidy data&lt;/a&gt; given by &lt;a href=&#34;http://hadley.nz/&#34;&gt;Hadley Wickham&lt;/a&gt; since each row represents a simulation, it actually falls slightly short. For our data to be tidy, a row should store the simulation number, the model, and the resulting prediction. We’ve actually already aggregated one level above this. Our observational unit is a simulation (with four predictions), but for tidy data, it should be a single prediction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, &lt;span class=&#34;math inline&#34;&gt;\(f(x = 0.90) = (0.9)^2 = 0.81\)&lt;/span&gt;, is given by the solid black horizontal line.&lt;/p&gt;
&lt;p&gt;Two things are immediately clear:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As flexibility &lt;em&gt;increases&lt;/em&gt;, &lt;strong&gt;bias decreases&lt;/strong&gt;. The mean of a model’s predictions is closer to the truth.&lt;/li&gt;
&lt;li&gt;As flexibility &lt;em&gt;increases&lt;/em&gt;, &lt;strong&gt;variance increases&lt;/strong&gt;. The variance about the mean of a model’s predictions increases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal of this simulation study is to show that the following holds true for each of the four models.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE}\left(f(0.90), \hat{f}_k(0.90)\right) =
\underbrace{\left(\mathbb{E} \left[ \hat{f}_k(0.90) \right] - f(0.90) \right)^2}_{\text{bias}^2 \left(\hat{f}_k(0.90) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}_k(0.90) - \mathbb{E} \left[ \hat{f}_k(0.90) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}_k(0.90) \right)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’ll use the empirical results of our simulations to estimate these quantities. (Yes, we’re using estimation to justify facts about estimation.) Note that we’ve actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we’ve performed just enough simulations to obtain the desired results. (Since we’re estimating estimation, the bigger the sample size, the better.)&lt;/p&gt;
&lt;p&gt;To estimate the mean squared error of our predictions, we’ll use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\widehat{\text{MSE}}\left(f(0.90), \hat{f}_k(0.90)\right) = \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}} \left(f(0.90) - \hat{f}_k^{[i]}(0.90) \right)^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}_k^{[i]}(0.90)\)&lt;/span&gt; is the estimate of &lt;span class=&#34;math inline&#34;&gt;\(f(0.90)\)&lt;/span&gt; using the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th from the polynomial degree &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; model.&lt;/p&gt;
&lt;p&gt;We also write an accompanying &lt;code&gt;R&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_mse = function(truth, estimate) {
  mean((estimate - truth) ^ 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Similarly, for the bias of our predictions we use,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\widehat{\text{bias}} \left(\hat{f}(0.90) \right)  = \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}} \left(\hat{f}_k^{[i]}(0.90) \right) - f(0.90)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And again, we write an accompanying &lt;code&gt;R&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_bias = function(estimate, truth) {
  mean(estimate) - truth
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, for the variance of our predictions we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\widehat{\text{var}} \left(\hat{f}(0.90) \right) = \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}} \left(\hat{f}_k^{[i]}(0.90) - \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}}\hat{f}_k^{[i]}(0.90) \right)^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While there is already &lt;code&gt;R&lt;/code&gt; function for variance, the following is more appropriate in this situation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_var = function(estimate) {
  mean((estimate - mean(estimate)) ^ 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To quickly obtain these results for each of the four models, we utilize the &lt;code&gt;apply()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bias = apply(predictions, 2, get_bias, truth = f(x = 0.90))
variance = apply(predictions, 2, get_var)
mse = apply(predictions, 2, get_mse, truth = f(x = 0.90))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We summarize these results in the following table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Degree&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Mean Squared Error&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Bias Squared&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Variance&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.22643&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.22476&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00829&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00508&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00322&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00387&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00005&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00381&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;9&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.01019&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.00002&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.01017&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A number of things to notice here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as flexibility increases.&lt;/li&gt;
&lt;li&gt;The squared bias trend which we see here is &lt;strong&gt;decreasing&lt;/strong&gt; as flexibility increases, which we expect to see in general.&lt;/li&gt;
&lt;li&gt;The exact opposite is true of variance. As model flexibility increases, variance &lt;strong&gt;increases&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can check for these trends with the &lt;code&gt;diff()&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(diff(bias ^ 2) &amp;lt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(diff(variance) &amp;gt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff(mse) &amp;lt; 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     1     2     9 
##  TRUE  TRUE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance.&lt;/p&gt;
&lt;p&gt;Models with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, since least squares estimation is unbiased, importantly,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}\left[\hat{\beta}_d\right] = \beta_d = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(d = 3, 4, \ldots 9\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}\left[\hat{f}_9(x)\right] = \beta_0 + \beta_1 x + \beta_2 x^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we can finally verify the bias-variance decomposition.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bias ^ 2 + variance == mse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     0     1     2     9 
## FALSE FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But wait, this says it isn’t true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(bias ^ 2 + variance, mse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See &lt;code&gt;?all.equal()&lt;/code&gt; for details.&lt;/p&gt;
&lt;p&gt;So far, we’ve focused our efforts on looking at the mean squared error of estimating &lt;span class=&#34;math inline&#34;&gt;\(f(0.90)\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(0.90)\)&lt;/span&gt;. We could also look at the expected prediction error of using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(X = 0.90\)&lt;/span&gt; to estimate &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}_k(0.90)\right) =
\mathbb{E}_{Y \mid X, \mathcal{D}} \left[  \left(Y - \hat{f}_k(X) \right)^2 \mid X = 0.90 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can estimate this quantity for each of the four models using the simulation study we already performed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_epe = function(realized, estimate) {
  mean((realized - estimate) ^ 2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y = rnorm(n = nrow(predictions), mean = f(x = 0.9), sd = 0.3)
epe = apply(predictions, 2, get_epe, realized = y)
epe&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         0         1         2         9 
## 0.3180470 0.1104055 0.1095955 0.1205570&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about the unconditional expected prediction error. That is, for any &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, not just &lt;span class=&#34;math inline&#34;&gt;\(0.90\)&lt;/span&gt;. Specifically, the expected prediction error of estimating &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; using &lt;span class=&#34;math inline&#34;&gt;\(\hat{f}(X)\)&lt;/span&gt;. The following (new) simulation study provides an estimate of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}_k(X)\right) = \mathbb{E}_{X, Y, \mathcal{D}} \left[  \left( Y - \hat{f}_k(X) \right)^2 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for the quadratic model, that is &lt;span class=&#34;math inline&#34;&gt;\(k = 2\)&lt;/span&gt; as we have defined &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
n_sims = 2500
X = runif(n = n_sims, min = 0, max = 1)
Y = rnorm(n = n_sims, mean = f(X), sd = 0.3)

f_hat_X = rep(0, length(X))

for (i in seq_along(X)) {
  sim_data = gen_sim_data(f)
  fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
  f_hat_X[i] = predict(fit_2, newdata = data.frame(x = X[i]))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# truth
0.3 ^ 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# via simulation
mean((Y - f_hat_X) ^ 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.09566445&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that in practice, we should use many more simulations in this study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-expected-prediction-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating Expected Prediction Error&lt;/h2&gt;
&lt;p&gt;While previously, we only decomposed the expected prediction error conditionally, a similar argument holds unconditionally.&lt;/p&gt;
&lt;p&gt;Assuming&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{V}[Y \mid X = x] = \sigma ^ 2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{EPE}\left(Y, \hat{f}(X)\right) =
\mathbb{E}_{X, Y, \mathcal{D}} \left[  (Y - \hat{f}(X))^2 \right] =
\underbrace{\mathbb{E}_{X} \left[\text{bias}^2\left(\hat{f}(X)\right)\right] + \mathbb{E}_{X} \left[\text{var}\left(\hat{f}(X)\right)\right]}_\textrm{reducible error} + \sigma^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Lastly, we note that if&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{D} = \mathcal{D}_{\texttt{trn}} \cup \mathcal{D}_{\texttt{tst}} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i = 1, 2, \ldots n
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{D}_{\texttt{trn}} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i \in \texttt{trn}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{D}_{\texttt{tst}} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i \in \texttt{tst}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, if we have a model fit to the training data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}_{\texttt{trn}}\)&lt;/span&gt;, we can use the test mean squared error&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i \in \texttt{tst}}\left(y_i - \hat{f}(x_i)\right) ^ 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;as an estimate of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}_{X, Y, \mathcal{D}} \left[  (Y - \hat{f}(X))^2 \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the expected prediction error.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;How good is this estimate? Well, if &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; is a random sample from &lt;span class=&#34;math inline&#34;&gt;\((X, Y)\)&lt;/span&gt;, and the &lt;span class=&#34;math inline&#34;&gt;\(\texttt{tst}\)&lt;/span&gt; data are randomly sampled observations randomly sampled from &lt;span class=&#34;math inline&#34;&gt;\(i = 1, 2, \ldots, n\)&lt;/span&gt;, then it is a reasonable estimate. However, it is rather variable due to the randomness of selecting the observations for the test set, if the test set is small.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-flexibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Flexibility&lt;/h2&gt;
&lt;p&gt;Let’s return to the simiulated dataset we used occaisionally in the linear regression content. Recall there was a single feature &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; with the following properties:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define regression function
cubic_mean = function(x) {
  1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then generated some data around this function with some added noise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define full data generating process
gen_slr_data = function(sample_size = 100, mu) {
  x = runif(n = sample_size, min = -1, max = 1)
  y = mu(x) + rnorm(n = sample_size)
  tibble(x, y)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After defining the data generating process, we generate and split the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate entire dataset
set.seed(3)
sim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean)

# test-train split
slr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data))
slr_trn = sim_slr_data[slr_trn_idx, ]
slr_tst = sim_slr_data[-slr_trn_idx, ]

# estimation-validation split
slr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn))
slr_est = slr_trn[slr_est_idx, ]
slr_val = slr_trn[-slr_est_idx, ]

# check data
head(slr_trn, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##         x      y
##     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1  0.573 -1.18 
##  2  0.807  0.576
##  3  0.272 -0.973
##  4 -0.813 -1.78 
##  5 -0.161  0.833
##  6  0.736  1.07 
##  7 -0.242  2.97 
##  8  0.520 -1.64 
##  9 -0.664  0.269
## 10 -0.777 -2.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For validating models, we will use RMSE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# helper function for calculating RMSE
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/error-vs-flex-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This picture is an idealized version of what we expect to see, but we’ll illustrate the sorts of validate “curves” that we might see in practice.&lt;/p&gt;
&lt;p&gt;Note that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details.&lt;/p&gt;
&lt;div id=&#34;linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Linear Models&lt;/h3&gt;
&lt;p&gt;First up, linear models. We will fit polynomial models with degree from one to nine, and then validate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit polynomial models
poly_mod_est_list = list(
  poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est),
  poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est),
  poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est),
  poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est),
  poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est),
  poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est),
  poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est),
  poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est),
  poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est)
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below visualizes the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What do we see here? As the polynomial degree &lt;em&gt;increases&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The training error &lt;em&gt;decreases&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The validation error &lt;em&gt;decreases&lt;/em&gt;, then &lt;em&gt;increases&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This more of less matches the idealized version above, but the validation “curve” is much more jagged. This is something that we can expect in practice.&lt;/p&gt;
&lt;p&gt;We have previously noted that training error isn’t particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;k-Nearest Neighbors&lt;/h3&gt;
&lt;p&gt;Next up, k-nearest neighbors. We will consider values for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; that are odd and between &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(45\)&lt;/span&gt; inclusive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# helper function for fitting knn models
fit_knn_mod = function(neighbors) {
  knnreg(y ~ x, data = slr_est, k = neighbors)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define values of tuning parameter k to evaluate
k_to_try = seq(from = 1, to = 45, by = 2)

# fit knn models
knn_mod_est_list = lapply(k_to_try, fit_knn_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below visualizes the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see the “opposite” of the usual plot. Why? Because with k-nearest neighbors, a small value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; generates a flexible model compared to larger values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. So visually, this plot is flipped. That is we see that as &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; &lt;em&gt;increases&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The training error &lt;em&gt;increases&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The validation error &lt;em&gt;decreases&lt;/em&gt;, then &lt;em&gt;increases&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Important to note here: the pattern above only holds “in general,” that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-trees&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Decision Trees&lt;/h3&gt;
&lt;p&gt;Lastly, we evaluate some decision tree models. We choose some arbitrary values of &lt;code&gt;cp&lt;/code&gt; to evaluate, while holding &lt;code&gt;minsplit&lt;/code&gt; constant at &lt;code&gt;5&lt;/code&gt;. There are arbitrary choices that produce a plot that is useful for discussion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# helper function for fitting decision tree models
tree_knn_mod = function(flex) {
  rpart(y ~ x, data = slr_est, cp = flex, minsplit = 5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# define values of tuning parameter cp to evaluate
cp_to_try = c(0.5, 0.3, 0.1, 0.05, 0.01, 0.001, 0.0001)

# fit decision tree models
tree_mod_est_list = lapply(cp_to_try, tree_knn_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below visualizes the results.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/09-content_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Based on this plot, how is &lt;code&gt;cp&lt;/code&gt; related to model flexibility?&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Note that in this section, we will refer to &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; as the regression function instead of &lt;span class=&#34;math inline&#34;&gt;\(\mu(x)\)&lt;/span&gt; for unimportant and arbitrary reasons.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Someday, someone will tell you this is a lie. They aren’t wrong. In modern deep learning, there is a concept called &lt;a href=&#34;https://openai.com/blog/deep-double-descent/&#34;&gt;Deep Double Descent&lt;/a&gt;. See also &lt;span class=&#34;citation&#34;&gt;@belkin2018reconciling&lt;/span&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;It’s actually the same as the 0 predictor linear model. Can you see why?&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;In practice we prefer RMSE to MSE for comparing models and reporting because of the units.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;In practice, if you already know how your model’s flexibility works, by checking that the training error goes down as you increase flexibility, you can check that you have done your coding and model training correctly.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;As &lt;code&gt;cp&lt;/code&gt; increases, model flexibility decreases.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nonparametric Regression</title>
      <link>/content/08-content/</link>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/content/08-content/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-ideas&#34;&gt;Guiding Ideas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nonparametric-regression&#34;&gt;Nonparametric Regression&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#r-setup&#34;&gt;R Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mathematical-setup&#34;&gt;Mathematical Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-nearest-neighbors&#34;&gt;k-Nearest Neighbors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#decision-trees&#34;&gt;Decision Trees&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-credit-card-data&#34;&gt;Example: Credit Card Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-ideas&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Ideas&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How to use &lt;strong&gt;k-nearest neighbors&lt;/strong&gt; for regression through the use of the &lt;code&gt;knnreg()&lt;/code&gt; function from the &lt;code&gt;caret&lt;/code&gt; package&lt;/li&gt;
&lt;li&gt;How to use &lt;strong&gt;decision trees&lt;/strong&gt; for regression through the use of the &lt;code&gt;rpart()&lt;/code&gt; function from the &lt;code&gt;rpart&lt;/code&gt; package.&lt;/li&gt;
&lt;li&gt;How “making predictions” can be thought of as &lt;strong&gt;estimating the regression function&lt;/strong&gt;, that is, the conditional mean of the response given values of the features.&lt;/li&gt;
&lt;li&gt;The difference between &lt;strong&gt;parametric&lt;/strong&gt; and &lt;strong&gt;nonparametric&lt;/strong&gt; methods.&lt;/li&gt;
&lt;li&gt;The difference between &lt;strong&gt;model parameters&lt;/strong&gt; and &lt;strong&gt;tuning parameters&lt;/strong&gt; methods.&lt;/li&gt;
&lt;li&gt;How these nonparametric methods deal with &lt;strong&gt;categorical variables&lt;/strong&gt; and &lt;strong&gt;interactions&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;What is &lt;strong&gt;model flexibility&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;What is &lt;strong&gt;overfitting&lt;/strong&gt; and how do we avoid it?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;nonparametric-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nonparametric Regression&lt;/h1&gt;
&lt;p&gt;In the next weeks, we will continue to explore models for making &lt;strong&gt;predictions&lt;/strong&gt;, but now we will introduce &lt;strong&gt;nonparametric models&lt;/strong&gt; that will contrast the &lt;strong&gt;parametric models&lt;/strong&gt; that we have used previously.&lt;/p&gt;
&lt;p&gt;This content is currently &lt;strong&gt;under construction&lt;/strong&gt;. You can expect it to be a lot less polished than other sections.&lt;/p&gt;
&lt;div id=&#34;r-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R Setup&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tibble)     # data frame printing
library(dplyr)      # data manipulation

library(caret)      # fitting knn
library(rpart)      # fitting trees
library(rpart.plot) # plotting trees

library(knitr)      # creating tables
library(kableExtra) # styling tables&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mathematical-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mathematical Setup&lt;/h2&gt;
&lt;p&gt;Let’s return to the setup we defined in the previous lectures. Consider a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; which represents a &lt;strong&gt;response&lt;/strong&gt; variable, and &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; &lt;strong&gt;feature&lt;/strong&gt; variables &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{X} = (X_1, X_2, \ldots, X_p)\)&lt;/span&gt;. We assume that the response variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is some function of the features, plus some random noise.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = f(\boldsymbol{X}) + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our goal is to find some &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(f(\boldsymbol{X})\)&lt;/span&gt; is close to &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. More specifically we want to minimize the risk under squared error loss.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}_{\boldsymbol{X}, Y} \left[ (Y - f(\boldsymbol{X})) ^ 2 \right] = \mathbb{E}_{\boldsymbol{X}} \mathbb{E}_{Y \mid \boldsymbol{X}} \left[ ( Y - f(\boldsymbol{X}) ) ^ 2 \mid \boldsymbol{X} = \boldsymbol{x} \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We saw previously (see the slides from last two content days) that this risk is minimized by the &lt;strong&gt;conditional mean&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{X}\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu(\boldsymbol{x}) \triangleq \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which we call the &lt;strong&gt;regression function&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Our goal then is to &lt;strong&gt;estimate&lt;/strong&gt; this &lt;strong&gt;regression function&lt;/strong&gt;. Let’s use an example where we know the true probability model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = 1 - 2x - 3x ^ 2 + 5x ^ 3 + \epsilon
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon \sim \text{N}(0, \sigma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recall that this implies that the regression function is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}] = 1 - 2x - 3x ^ 2 + 5x ^ 3
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s also pretend that we do not actually know this information, but instead have some data, &lt;span class=&#34;math inline&#34;&gt;\((x_i, y_i)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i = 1, 2, \ldots, n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We simulate enough data to make the “pattern” clear-ish to recognize.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we use a linear model, we first need to make an &lt;strong&gt;assumption&lt;/strong&gt; about the form of the regression function.&lt;/p&gt;
&lt;p&gt;For example, we could assume that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}] = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is fit in R using the &lt;code&gt;lm()&lt;/code&gt; function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ x + I(x ^ 2) + I(x ^ 3), data = sim_slr_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + I(x^2) + I(x^3), data = sim_slr_data)
## 
## Coefficients:
## (Intercept)            x       I(x^2)       I(x^3)  
##      0.8397      -2.7257      -2.3752       6.0906&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that what is returned are (maximum likelihood or least squares) estimates of the unknown &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; coefficients. That is, the “learning” that takes place with a linear models is “learning” the values of the coefficients.&lt;/p&gt;
&lt;p&gt;For this reason, we call linear regression models &lt;strong&gt;parametric&lt;/strong&gt; models. They have unknown &lt;strong&gt;model parameters&lt;/strong&gt;, in this case the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; coefficients that must be learned from the data. The form of the regression function is assumed.&lt;/p&gt;
&lt;p&gt;What if we don’t want to make an assumption about the form of the regression function? While in this case, you might look at the plot and arrive at a reasonable guess of assuming a third order polynomial, what if it isn’t so clear? What if you have 100 features? Making strong assumptions might not work well.&lt;/p&gt;
&lt;p&gt;Enter &lt;strong&gt;nonparametric&lt;/strong&gt; models. We will consider two examples: &lt;strong&gt;k-nearest neighbors&lt;/strong&gt; and &lt;strong&gt;decision trees&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k-nearest-neighbors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;k-Nearest Neighbors&lt;/h2&gt;
&lt;p&gt;We’ll start with &lt;strong&gt;k-nearest neighbors&lt;/strong&gt; which is possibly a more intuitive procedure than linear models.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If our goal is to estimate the mean function,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the most natural approach would be to use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{average}(\{ y_i : x_i = x \}).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That is, to estimate the conditional mean at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, average the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values for each data point where &lt;span class=&#34;math inline&#34;&gt;\(x_i = x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;While this sounds nice, it has an obvious flaw. For most values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; there will not be any &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in the data where &lt;span class=&#34;math inline&#34;&gt;\(x_i = x\)&lt;/span&gt;!&lt;/p&gt;
&lt;p&gt;So what’s the next best thing? Pick values of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; that are “close” to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{average}( \{ y_i : x_i \text{ equal to (or very close to) x} \} ).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the main idea behind many nonparametric approaches. The details often just amount to very specifically defining what “close” means.&lt;/p&gt;
&lt;p&gt;In the case of k-nearest neighbors we use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\mu}_k(x) = \frac{1}{k} \sum_{ \{i \ : \ x_i \in \mathcal{N}_k(x, \mathcal{D}) \} } y_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;as our estimate of the regression function at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. While this looks complicated, it is actually very simple. Here, we are using an average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of for the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; nearest neighbors to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; “nearest” neighbors are the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; data points &lt;span class=&#34;math inline&#34;&gt;\((x_i, y_i)\)&lt;/span&gt; that have &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; values that are nearest to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We can define “nearest” using any distance we like, but unless otherwise noted, we are referring to euclidean distance.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; We are using the notation &lt;span class=&#34;math inline&#34;&gt;\(\{i \ : \ x_i \in \mathcal{N}_k(x, \mathcal{D}) \}\)&lt;/span&gt; to define the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; observations that have &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; values that are nearest to the value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in a dataset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;, in other words, the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; nearest neighbors.&lt;/p&gt;
&lt;p&gt;The plots below begin to illustrate this idea.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the left plot, to estimate the mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = -0.5\)&lt;/span&gt; we use the three nearest neighbors, which are highlighted with green. Our estimate is the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of these three points indicated by the black x.&lt;/li&gt;
&lt;li&gt;In the middle plot, to estimate the mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = 0\)&lt;/span&gt; we use the five nearest neighbors, which are highlighted with green. Our estimate is the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of these five points indicated by the black x.&lt;/li&gt;
&lt;li&gt;In the right plot, to estimate the mean of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(x = 0.75\)&lt;/span&gt; we use the nine nearest neighbors, which are highlighted with green. Our estimate is the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of these nine points indicated by the black x.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You might begin to notice a bit of an issue here. We have to do a new calculation each time we want to estimate the regression function at a different value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;! For this reason, k-nearest neighbors is often said to be “fast to train” and “slow to predict.” Training, is instant. You just memorize the data! Prediction involves finding the distance between the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; considered and all &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; in the data!&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, how then, do we choose the value of the &lt;strong&gt;tuning&lt;/strong&gt; parameter &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;? We &lt;em&gt;&lt;strong&gt;validate&lt;/strong&gt;&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;First, let’s take a look at what happens with this data if we consider three different values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For each plot, the black dashed curve is the true mean function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In the left plot we use &lt;span class=&#34;math inline&#34;&gt;\(k = 25\)&lt;/span&gt;. The red “curve” is the estimate of the mean function for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; shown in the plot.&lt;/li&gt;
&lt;li&gt;In the left plot we use &lt;span class=&#34;math inline&#34;&gt;\(k = 5\)&lt;/span&gt;. The blue “curve” is the estimate of the mean function for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; shown in the plot.&lt;/li&gt;
&lt;li&gt;In the left plot we use &lt;span class=&#34;math inline&#34;&gt;\(k = 1\)&lt;/span&gt;. The green “curve” is the estimate of the mean function for each &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; shown in the plot.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some things to notice here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The left plot with &lt;span class=&#34;math inline&#34;&gt;\(k = 25\)&lt;/span&gt; is performing poorly. The estimated “curve” does not “move” enough. This is an example of an &lt;strong&gt;inflexible&lt;/strong&gt; model.&lt;/li&gt;
&lt;li&gt;The right plot with &lt;span class=&#34;math inline&#34;&gt;\(k = 1\)&lt;/span&gt; might not perform too well. The estimated “curve” seems to “move” too much. (Notice, that it goes through each point. We’ve fit to the noise.) This is an example of a &lt;strong&gt;flexible&lt;/strong&gt; model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While the middle plot with &lt;span class=&#34;math inline&#34;&gt;\(k = 5\)&lt;/span&gt; is not “perfect” it seems to roughly capture the “motion” of the true regression function. We can begin to see that if we generated new data, this estimated regression function would perform better than the other two.&lt;/p&gt;
&lt;p&gt;But remember, in practice, we won’t know the true regression function, so we will need to determine how our model performs using only the available data!&lt;/p&gt;
&lt;p&gt;This &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, the number of neighbors, is an example of a &lt;strong&gt;tuning parameter&lt;/strong&gt;. Instead of being learned from the data, like model parameters such as the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; coefficients in linear regression, a tuning parameter tells us &lt;em&gt;how&lt;/em&gt; to learn from data. It is user-specified. To determine the value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; that should be used, many models are fit to the estimation data, then evaluated on the validation. Using the information from the validation data, a value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is chosen. (More on this in a bit.)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Model parameters&lt;/strong&gt; are “learned” using the same data that was used to fit the model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tuning parameters&lt;/strong&gt; are “chosen” using data not used to fit the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tuning parameter &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; also defines the &lt;strong&gt;flexibility&lt;/strong&gt; of the model. In KNN, a small value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is a flexible model, while a large value of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is inflexible.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Before moving to an example of tuning a KNN model, we will first introduce decision trees.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Decision Trees&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Decision trees&lt;/strong&gt; are similar to k-nearest neighbors but instead of looking for neighbors, decision trees create neighborhoods. We won’t explore the full details of trees, but just start to understand the basic concepts, as well as learn to fit them in R.&lt;/p&gt;
&lt;p&gt;Neighborhoods are created via recursive binary partitions. In simpler terms, pick a feature and a possible cutoff value. Data that have a value less than the cutoff for the selected feature are in one neighborhood (the left) and data that have a value greater than the cutoff are in another (the right). Within these two neighborhoods, repeat this procedure until a stopping rule is satisfied. To make a prediction, check which neighborhood a new piece of data would belong to and predict the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values of data in that neighborhood.&lt;/p&gt;
&lt;p&gt;With the data above, which has a single feature &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, consider three possible cutoffs: -0.5, 0.0, and 0.75.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For each plot, the black vertical line defines the neighborhoods. The green horizontal lines are the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values for the points in the left neighborhood. The red horizontal lines are the average of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; values for the points in the right neighborhood.&lt;/p&gt;
&lt;p&gt;What makes a cutoff good? Large differences in the average &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; between the two neighborhoods. More formally we want to find a cutoff value that minimizes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i \in N_L} \left( y_i - \hat{\mu}_{N_L} \right) ^ 2 + \sum_{i \in N_R} \left(y_i - \hat{\mu}_{N_R} \right) ^ 2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N_L\)&lt;/span&gt; are the data in the left neighborhood, that is &lt;span class=&#34;math inline&#34;&gt;\(x &amp;lt; c\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N_R\)&lt;/span&gt; are the data in the right neighborhood, that is &lt;span class=&#34;math inline&#34;&gt;\(x &amp;gt; c\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_{N_L}\)&lt;/span&gt; is the mean of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; for data in the left neighborhood&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}_{N_R}\)&lt;/span&gt; is the mean of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; for data in the right neighborhood&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This quantity is the sum of two sum of squared errors, one for the left neighborhood, and one for the right neighborhood.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Cutoff
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Total SSE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Left SSE
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Right SSE
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
45.02
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21.28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23.74
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
58.94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44.68
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.26
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
56.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
55.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.25
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The table above summarizes the results of the three potential splits. We see that (of the splits considered, which are not exhaustive&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;) the split based on a cutoff of &lt;span class=&#34;math inline&#34;&gt;\(x = -0.50\)&lt;/span&gt; creates the best partitioning of the space.&lt;/p&gt;
&lt;p&gt;Now let’s consider building a full tree.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the plot above, the true regression function is the dashed black curve, and the solid orange curve is the estimated regression function using a decision tree. We see that there are two splits, which we can visualize as a tree.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above “tree”&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; shows the splits that were made. It informs us of the variable used, the cutoff value, and some summary of the resulting neighborhood. In “tree” terminology the resulting neighborhoods are “terminal nodes” of the tree. In contrast, “internal nodes” are neighborhoods that are created, but then further split.&lt;/p&gt;
&lt;p&gt;The “root node” is the neighborhood contains all observations, before any splitting, and can be seen at the top of the image above. We see that this node represents 100% of the data. The other number, 0.21, is the mean of the response variable, in this case, &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Looking at a terminal node, for example the bottom left node, we see that 23% of the data is in this node. The average value of the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in this node is -1, which can be seen in the plot above.&lt;/p&gt;
&lt;p&gt;We also see that the first split is based on the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; variable, and a cutoff of &lt;span class=&#34;math inline&#34;&gt;\(x = -0.52\)&lt;/span&gt;. Note that because there is only one variable here, all splits are based on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, but in the future, we will have multiple features that can be split and neighborhoods will no longer be one-dimensional. However, this is hard to plot.&lt;/p&gt;
&lt;p&gt;Let’s build a bigger, more flexible tree.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are two tuning parameters at play here which we will call by their names in R which we will see soon:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cp&lt;/code&gt; or the “complexity parameter” as it is called.&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; This parameter determines which splits are accepted. A split must improve the performance of the tree by more than &lt;code&gt;cp&lt;/code&gt; in order to be used. When we get to R, we will see that the default value is 0.1.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;minsplit&lt;/code&gt;, the minimum number of observations in a node (neighborhood) in order to consider splitting within a neighborhood.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are actually many more possible tuning parameters for trees, possibly differing depending on who wrote the code you’re using. We will limit discussion to these two.&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; Note that they effect each other, and they effect other parameters which we are not discussing. The main takeaway should be how they effect model flexibility.&lt;/p&gt;
&lt;p&gt;First let’s look at what happens for a fixed &lt;code&gt;minsplit&lt;/code&gt; by variable &lt;code&gt;cp&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that as &lt;code&gt;cp&lt;/code&gt; &lt;em&gt;decreases&lt;/em&gt;, model flexibility &lt;strong&gt;increases&lt;/strong&gt;. We see more splits, because the increase in performance needed to accept a split is smaller as &lt;code&gt;cp&lt;/code&gt; is reduced.&lt;/p&gt;
&lt;p&gt;Now the reverse, fix &lt;code&gt;cp&lt;/code&gt; and vary &lt;code&gt;minsplit&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;1152&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that as &lt;code&gt;minsplit&lt;/code&gt; &lt;em&gt;decreases&lt;/em&gt;, model flexibility &lt;strong&gt;increases&lt;/strong&gt;. By allowing splits of neighborhoods with fewer observations, we obtain more splits, which results in a more flexible model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-credit-card-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example: Credit Card Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load data, coerce to tibble
crdt = as_tibble(ISLR::Credit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we are using the &lt;code&gt;Credit&lt;/code&gt; data form the &lt;code&gt;ISLR&lt;/code&gt; package. Note: &lt;strong&gt;this is not real data.&lt;/strong&gt; It has been simulated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data prep
crdt = crdt %&amp;gt;%
  select(-ID) %&amp;gt;%
  select(-Rating, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We remove the &lt;code&gt;ID&lt;/code&gt; variable as it should have no predictive power. We also move the &lt;code&gt;Rating&lt;/code&gt; variable to the last column with a clever &lt;code&gt;dplyr&lt;/code&gt; trick. This is in no way necessary, but is useful in creating some plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test-train split
set.seed(1)
crdt_trn_idx = sample(nrow(crdt), size = 0.8 * nrow(crdt))
crdt_trn = crdt[crdt_trn_idx, ]
crdt_tst = crdt[-crdt_trn_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimation-validation split
crdt_est_idx = sample(nrow(crdt_trn), size = 0.8 * nrow(crdt_trn))
crdt_est = crdt_trn[crdt_est_idx, ]
crdt_val = crdt_trn[-crdt_est_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After train-test (with 80% of the data) and estimation-validation splitting the data, we look at the train data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check data
head(crdt_trn, n = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 11
##    Income Limit Cards   Age Education Gender Student Married Ethnicity Balance
##     &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;  &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;       &amp;lt;int&amp;gt;
##  1  183.  13913     4    98        17 &amp;quot; Mal… No      Yes     Caucasian    1999
##  2   35.7  2880     2    35        15 &amp;quot; Mal… No      No      African …       0
##  3  123.   8376     2    89        17 &amp;quot; Mal… Yes     No      African …    1259
##  4   20.8  2672     1    70        18 &amp;quot;Fema… No      No      African …       0
##  5   39.1  5565     4    48        18 &amp;quot;Fema… No      Yes     Caucasian     772
##  6   36.5  3806     2    52        13 &amp;quot; Mal… No      No      African …     188
##  7   45.1  3762     3    80         8 &amp;quot; Mal… No      Yes     Caucasian      70
##  8   43.5  2906     4    69        11 &amp;quot; Mal… No      No      Caucasian       0
##  9   23.1  3476     2    50        15 &amp;quot;Fema… No      No      Caucasian     209
## 10   53.2  4943     2    46        16 &amp;quot;Fema… No      Yes     Asian         382
## # … with 1 more variable: Rating &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this simulated data, we would like to predict the &lt;code&gt;Rating&lt;/code&gt; variable. For now, let’s try to use only demographic information as predictors.&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; In particular, let’s focus on &lt;code&gt;Age&lt;/code&gt; (numeric), &lt;code&gt;Gender&lt;/code&gt; (categorical), and &lt;code&gt;Student&lt;/code&gt; (categorical).&lt;/p&gt;
&lt;p&gt;Let’s fit KNN models with these features, and various values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;. To do so, we use the &lt;code&gt;knnreg()&lt;/code&gt; function from the &lt;code&gt;caret&lt;/code&gt; package.&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt; Use &lt;code&gt;?knnreg&lt;/code&gt; for documentation and details.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 1)
crdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 10)
crdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we fit three models to the estimation data. We supply the variables that will be used as features as we would with &lt;code&gt;lm()&lt;/code&gt;. We also specify how many neighbors to consider via the &lt;code&gt;k&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;But wait a second, what is the distance from non-student to student? From male to female? In other words, how does KNN handle categorical variables? It doesn’t! Like &lt;code&gt;lm()&lt;/code&gt; it creates dummy variables under the hood.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To this point, and until we specify otherwise, we will always coerce categorical variables to be factor variables in R. We will then let modeling functions such as &lt;code&gt;lm()&lt;/code&gt; or &lt;code&gt;knnreg()&lt;/code&gt; deal with the creation of dummy variables internally.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(crdt_knn_10$learn$X)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Age GenderFemale StudentYes
## 1  30            0          0
## 2  25            0          0
## 3  44            0          0
## 4  73            1          0
## 5  44            0          1
## 6  71            0          0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once these dummy variables have been created, we have a numeric &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; matrix, which makes distance calculations easy.&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; For example, the distance between the 3rd and 4th observation here is 29.017.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dist(head(crdt_knn_10$learn$X))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           1         2         3         4         5
## 2  5.000000                                        
## 3 14.000000 19.000000                              
## 4 43.011626 48.010416 29.017236                    
## 5 14.035669 19.026298  1.000000 29.034462          
## 6 41.000000 46.000000 27.000000  2.236068 27.018512&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(sum((crdt_knn_10$learn$X[3, ] - crdt_knn_10$learn$X[4, ]) ^ 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 29.01724&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about interactions? Basically, you’d have to create them the same way as you do for linear models. We only mention this to contrast with trees in a bit.&lt;/p&gt;
&lt;p&gt;OK, so of these three models, which one performs best? (Where for now, “best” is obtaining the lowest validation RMSE.)&lt;/p&gt;
&lt;p&gt;First, note that we return to the &lt;code&gt;predict()&lt;/code&gt; function as we did with &lt;code&gt;lm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(crdt_knn_10, crdt_val[1:5, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 337.7857 356.0000 295.7692 360.8182 306.8000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This uses the 10-NN (10 nearest neighbors) model to make predictions (estimate the regression function) given the first five observations of the validation data. &lt;strong&gt;Note:&lt;/strong&gt; We did not name the second argument to &lt;code&gt;predict()&lt;/code&gt;. Again, you’ve been warned.&lt;/p&gt;
&lt;p&gt;Now that we know how to use the &lt;code&gt;predict()&lt;/code&gt; function, let’s calculate the validation RMSE for each of these models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knn_mod_list = list(
  crdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 1),
  crdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 10),
  crdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 25)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knn_val_pred = lapply(knn_mod_list, predict, crdt_val)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sapply(knn_val_pred, calc_rmse, crdt_val$Rating)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## crdt_knn_01 crdt_knn_10 crdt_knn_25 
##    182.3469    149.2172    138.6527&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, of these three values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, the model with &lt;span class=&#34;math inline&#34;&gt;\(k = 25\)&lt;/span&gt; achieves the lowest validation RMSE.&lt;/p&gt;
&lt;p&gt;This process, fitting a number of models with different values of the &lt;em&gt;tuning parameter&lt;/em&gt;, in this case &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, and then finding the “best” tuning parameter value based on performance on the validation data is called &lt;strong&gt;tuning&lt;/strong&gt;. In practice, we would likely consider more values of &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, but this should illustrate the point.&lt;/p&gt;
&lt;p&gt;In the next lectures, we will discuss the details of model flexibility and model tuning, and how these concepts are tied together. However, even though we will present some theory behind this relationship, in practice, &lt;strong&gt;you must tune and validate your models&lt;/strong&gt;. There is no theory that will inform you ahead of tuning and validation which model will be the best. By teaching you &lt;em&gt;how&lt;/em&gt; to fit KNN models in R and how to calculate validation RMSE, you already have all a set of tools you can use to find a good model.&lt;/p&gt;
&lt;p&gt;Let’s turn to decision trees which we will fit with the &lt;code&gt;rpart()&lt;/code&gt; function from the &lt;code&gt;rpart&lt;/code&gt; package. Use &lt;code&gt;?rpart&lt;/code&gt; and &lt;code&gt;?rpart.control&lt;/code&gt; for documentation and details. In particular, &lt;code&gt;?rpart.control&lt;/code&gt; will detail the many tuning parameters of this implementation of decision tree models in R.&lt;/p&gt;
&lt;p&gt;We’ll start by using default tuning parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_tree = rpart(Rating ~ Age + Gender + Student, data = crdt_est)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_tree&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## n= 256 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 256 6667400.0 357.0781  
##    2) Age&amp;lt; 82.5 242 5865419.0 349.3719  
##      4) Age&amp;gt;=69.5 52 1040678.0 313.0385 *
##      5) Age&amp;lt; 69.5 190 4737307.0 359.3158  
##       10) Age&amp;lt; 38.5 55  700013.2 326.6000 *
##       11) Age&amp;gt;=38.5 135 3954443.0 372.6444  
##         22) Student=Yes 14  180764.4 297.7857 *
##         23) Student=No 121 3686148.0 381.3058  
##           46) Age&amp;gt;=50.5 64 1881299.0 359.2344  
##             92) Age&amp;lt; 53.5 9   48528.0 278.3333 *
##             93) Age&amp;gt;=53.5 55 1764228.0 372.4727 *
##           47) Age&amp;lt; 50.5 57 1738665.0 406.0877 *
##    3) Age&amp;gt;=82.5 14  539190.9 490.2857 *&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Above we see the resulting tree printed, however, this is difficult to read. Instead, we use the &lt;code&gt;rpart.plot()&lt;/code&gt; function from the &lt;code&gt;rpart.plot&lt;/code&gt; package to better visualize the tree.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(crdt_tree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At each split, the variable used to split is listed together with a condition. If the condition is true for a data point, send it to the left neighborhood. Although the &lt;code&gt;Gender&lt;/code&gt; available for creating splits, we only see splits based on &lt;code&gt;Age&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt;. This hints at the relative importance of these variables for prediction. More on this much later.&lt;/p&gt;
&lt;p&gt;Categorical variables are split based on potential categories! This is &lt;em&gt;excellent&lt;/em&gt;. This means that trees naturally handle categorical features without needing to convert to numeric under the hood. We see a split that puts students into one neighborhood, and non-students into another.&lt;/p&gt;
&lt;p&gt;Notice that the splits happen in order. So for example, the third terminal node (with an average rating of 298) is based on splits of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Age &amp;lt; 83&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Age &amp;lt; 70&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Age &amp;gt; 39&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Student = Yes&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, individuals in this terminal node are students who are between the ages of 39 and 70. (Only 5% of the data is represented here.) This is basically an interaction between &lt;code&gt;Age&lt;/code&gt; and &lt;code&gt;Student&lt;/code&gt; without any need to directly specify it! What a great feature of trees.&lt;/p&gt;
&lt;p&gt;To recap:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trees do not make assumptions about the form of the regression function.&lt;/li&gt;
&lt;li&gt;Trees automatically handle categorical features.&lt;/li&gt;
&lt;li&gt;Trees naturally incorporate interaction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let’s fit another tree that is more flexible by relaxing some tuning parameters. Recall that by default, &lt;code&gt;cp = 0.1&lt;/code&gt; and &lt;code&gt;minsplit = 20&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_tree_big = rpart(Rating ~ Age + Gender + Student, data = crdt_est,
                      cp = 0.0, minsplit = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(crdt_tree_big)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To make the tree even bigger, we could reduce &lt;code&gt;minsplit&lt;/code&gt;, but in practice we mostly consider the &lt;code&gt;cp&lt;/code&gt; parameter. Since &lt;code&gt;minsplit&lt;/code&gt; has been kept the same, but &lt;code&gt;cp&lt;/code&gt; was reduced, we see the same splits as the smaller tree, but many additional splits.&lt;/p&gt;
&lt;p&gt;Now let’s fit a bunch of trees, with different values of &lt;code&gt;cp&lt;/code&gt;, for tuning.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tree_mod_list = list(
  crdt_tree_0000 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.000),
  crdt_tree_0001 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.001),
  crdt_tree_0010 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.010),
  crdt_tree_0100 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.100)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tree_val_pred = lapply(tree_mod_list, predict, crdt_val)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sapply(tree_val_pred, calc_rmse, crdt_val$Rating)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## crdt_tree_0000 crdt_tree_0001 crdt_tree_0010 crdt_tree_0100 
##       156.3527       155.4262       151.9081       140.0806&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we see the least flexible model, with &lt;code&gt;cp = 0.100&lt;/code&gt;, performs best.&lt;/p&gt;
&lt;p&gt;Note that by only using these three features, we are severely limiting our models performance. Let’s quickly assess using all available predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crdt_tree_all = rpart(Rating ~ ., data = crdt_est)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(crdt_tree_all)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/08-content_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that this model &lt;strong&gt;only&lt;/strong&gt; splits based on &lt;code&gt;Limit&lt;/code&gt; despite using all features. This should be a big hint about which variables are useful for prediction.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;calc_rmse(
  actual = crdt_val$Rating,
  predicted = predict(crdt_tree_all, crdt_val)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 28.8498&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model performs much better. You should try something similar with the KNN models above. Also, consider comparing this result to results from last lectures using linear models.&lt;/p&gt;
&lt;p&gt;Notice that we’ve been using that trusty &lt;code&gt;predict()&lt;/code&gt; function here again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(crdt_tree_all, crdt_val[1:5, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1        2        3        4        5 
## 292.8182 467.5152 467.5152 467.5152 772.4000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does this code do? It estimates the mean &lt;code&gt;Rating&lt;/code&gt; given the feature information (the “x” values) from the first five observations from the validation data using a decision tree model with default tuning parameters. Hopefully a theme is emerging.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;We chose to start with linear regression because most students the social sciences should already be familiar with the basic notion.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The usual distance when you hear distance. That is, unless you drive a &lt;a href=&#34;https://en.wikipedia.org/wiki/Taxicab_geometry&#34;&gt;taxicab&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;For this reason, KNN is often not used in practice, but it is very useful learning tool.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Many texts use the term complex instead of flexible. We feel this is confusing as complex is often associated with difficult. KNN with &lt;span class=&#34;math inline&#34;&gt;\(k = 1\)&lt;/span&gt; is actually a very simple model to understand, but it is very flexible as defined here.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;To exhaust all possible splits of a variable, we would need to consider the midpoint between each of the order statistics of the variable. To exhaust all possible splits, we would need to do this for each of the feature variables.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;It’s really an upside tree isn’t it?&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Flexibility parameter would be a better name.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;The &lt;code&gt;rpart&lt;/code&gt; function in R would allow us to use others, but we will always just leave their values as the default values.&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;There is a question of whether or not we &lt;em&gt;should&lt;/em&gt; use these variables. For example, should men and women be given different ratings when all other variables are the same? Using the &lt;code&gt;Gender&lt;/code&gt; variable allows for this to happen. Also, you might think, just don’t use the &lt;code&gt;Gender&lt;/code&gt; variable. Unfortunately, it’s not that easy. There is an increasingly popular field of study centered around these ideas called &lt;a href=&#34;https://en.wikipedia.org/wiki/Fairness_(machine_learning)&#34;&gt;machine learning &lt;strong&gt;fairness&lt;/strong&gt;&lt;/a&gt;.&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;There are many other KNN functions in &lt;code&gt;R&lt;/code&gt;. However, the operation and syntax of &lt;code&gt;knnreg()&lt;/code&gt; better matches other functions we use in this course.&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;Wait. Doesn’t this sort of create an arbitrary distance between the categories? Why &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and not &lt;span class=&#34;math inline&#34;&gt;\(-42\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(51\)&lt;/span&gt;? Good question. This hints at the notion of pre-processing. We’re going to hold off on this for now, but, often when performing k-nearest neighbors, you should try scaling all of the features to have mean &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression III</title>
      <link>/content/07-content/</link>
      <pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/content/07-content/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-question&#34;&gt;Guiding Question&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#association-is-not-causation&#34;&gt;Association is not causation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#spurious-correlation&#34;&gt;Spurious correlation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#outliers&#34;&gt;Outliers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reversing-cause-and-effect&#34;&gt;Reversing cause and effect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confounders&#34;&gt;Confounders&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example-uc-berkeley-admissions&#34;&gt;Example: UC Berkeley admissions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confounding-explained-graphically&#34;&gt;Confounding explained graphically&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-after-stratifying&#34;&gt;Average after stratifying&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#simpsons-paradox&#34;&gt;Simpson’s paradox&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Question&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;When can we make causal claims about the relationship between variables?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slides&lt;/h3&gt;
&lt;p&gt;Note that the slides below are from last lecture; we will reference these and they contain a lot of useful information.&lt;/p&gt;
&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;slide-tabs&#34; role=&#34;tablist&#34;&gt;
&lt;li class=&#34;nav-item&#34;&gt;
&lt;a class=&#34;nav-link active&#34; id=&#34;introduction-tab&#34; data-toggle=&#34;tab&#34; href=&#34;#introduction&#34; role=&#34;tab&#34; aria-controls=&#34;introduction&#34; aria-selected=&#34;true&#34;&gt;Introduction&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;slide-tabs&#34; class=&#34;tab-content&#34;&gt;
&lt;div id=&#34;introduction&#34; class=&#34;tab-pane fade show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;introduction-tab&#34;&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;/slides/06-slides.html#Introduction&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;association-is-not-causation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Association is not causation&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Association is not causation&lt;/em&gt; is perhaps the most important lesson one learns in a statistics class. &lt;em&gt;Correlation is not causation&lt;/em&gt; is another way to say this. Throughout the previous parts of this class, we have described tools useful for quantifying associations between variables. However, we must be careful not to over-interpret these associations.&lt;/p&gt;
&lt;p&gt;There are many reasons that a variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; can be correlated with a variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; without having any direct effect on &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Here we examine four common ways that can lead to misinterpreting data.&lt;/p&gt;
&lt;div id=&#34;spurious-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Spurious correlation&lt;/h2&gt;
&lt;p&gt;The following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/divorce-versus-margarine-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Does this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a &lt;em&gt;spurious correlation&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You can see many more absurd examples on the Spurious Correlations website&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The cases presented in the spurious correlation site are all instances of what is generally called &lt;em&gt;data dredging&lt;/em&gt;, &lt;em&gt;data fishing&lt;/em&gt;, or &lt;em&gt;data snooping&lt;/em&gt;. It’s basically a form of what in the US they call &lt;em&gt;cherry picking&lt;/em&gt;. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.&lt;/p&gt;
&lt;p&gt;A Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 25
g &amp;lt;- 1000000
sim_data &amp;lt;- tibble(group = rep(1:g, each=N),
                   x = rnorm(N * g),
                   y = rnorm(N * g))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first column denotes group. We created groups and for each one we generated a pair of independent vectors, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are not correlated.&lt;/p&gt;
&lt;p&gt;Next, we compute the correlation between &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;Y&lt;/code&gt; for each group and look at the max:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res &amp;lt;- sim_data %&amp;gt;%
  group_by(group) %&amp;gt;%
  summarize(r = cor(x, y)) %&amp;gt;%
  arrange(desc(r))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,000,000 x 2
##     group     r
##     &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
##  1 861679 0.815
##  2 387275 0.786
##  3 455283 0.786
##  4 442746 0.783
##  5 737678 0.777
##  6 113036 0.775
##  7 454360 0.773
##  8 553579 0.766
##  9 656133 0.758
## 10 660249 0.749
## # … with 999,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see a maximum correlation of 0.8146411 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are in fact correlated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_data %&amp;gt;% filter(group == res$group[which.max(res$r)]) %&amp;gt;%
  ggplot(aes(x, y)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/dredging-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Remember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res %&amp;gt;% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/null-corr-hist-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.2041007, the largest one will be close to 1.&lt;/p&gt;
&lt;p&gt;If we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)
sim_data %&amp;gt;%
  filter(group == res$group[which.max(res$r)]) %&amp;gt;%
  do(tidy(lm(y ~ x, data = .))) %&amp;gt;%
  filter(term == &amp;quot;x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 5
##   term  estimate std.error statistic     p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 x         1.04     0.154      6.74 0.000000716&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This particular form of data dredging is referred to as &lt;em&gt;p-hacking&lt;/em&gt;. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you can learn methods to adjust for these multiple comparisons.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outliers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outliers&lt;/h2&gt;
&lt;p&gt;Suppose we take measurements from two independent outcomes, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1985)
x &amp;lt;- rnorm(100,100,1)
y &amp;lt;- rnorm(100,84,1)
x[-23] &amp;lt;- scale(x[-23])
y[-23] &amp;lt;- scale(y[-23])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(x, y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/outlier-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not surprisingly, the correlation is very high:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x,y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9878382&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x[-23], y[-23])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.04419032&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Previously, we (briefly) described alternatives to the average and standard deviation that are robust to outliers. There is also an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called &lt;em&gt;Spearman correlation&lt;/em&gt;. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(rank(x), rank(y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/scatter-plot-of-ranks-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The outlier is no longer associated with a very large value and the correlation comes way down:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(rank(x), rank(y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.002508251&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spearman correlation can also be calculated like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(x, y, method = &amp;quot;spearman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.002508251&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber &amp;amp; Elvezio M. Ronchetti.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reversing-cause-and-effect&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reversing cause and effect&lt;/h2&gt;
&lt;p&gt;Another way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.&lt;/p&gt;
&lt;p&gt;A form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. Consider this quote from the article:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.&lt;/p&gt;
&lt;p&gt;We can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_i = \beta_0 + \beta_1 y_i + \varepsilon_i, i=1, \dots, N\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to the father and son height data, with &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; the father height and &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; the son height, we do get a statistically significant result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HistData)
data(&amp;quot;GaltonFamilies&amp;quot;)
GaltonFamilies %&amp;gt;%
  filter(childNum == 1 &amp;amp; gender == &amp;quot;male&amp;quot;) %&amp;gt;%
  select(father, childHeight) %&amp;gt;%
  rename(son = childHeight) %&amp;gt;%
  do(tidy(lm(father ~ son, data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)   34.0      4.57        7.44 4.31e-12
## 2 son            0.499    0.0648      7.70 9.47e-13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confounders&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confounders&lt;/h2&gt;
&lt;p&gt;Confounders are perhaps the most common reason that leads to associations begin misinterpreted.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are correlated, we call &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; a &lt;em&gt;confounder&lt;/em&gt; if changes in &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; causes changes in both &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.&lt;/p&gt;
&lt;p&gt;Incorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.&lt;/p&gt;
&lt;div id=&#34;example-uc-berkeley-admissions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example: UC Berkeley admissions&lt;/h3&gt;
&lt;p&gt;Admission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and
&lt;!--compute the percent of men and women that were accepted like this:


```r
data(admissions)
admissions %&gt;% group_by(gender) %&gt;%
  summarize(percentage =
              round(sum(admitted*applicants)/sum(applicants),1))
```

```
## `summarise()` ungrouping output (override with `.groups` argument)
```

```
## # A tibble: 2 x 2
##   gender percentage
##   &lt;chr&gt;       &lt;dbl&gt;
## 1 men          44.5
## 2 women        30.3
```
--&gt;
a statistical test, which clearly rejects the hypothesis that gender and admission are independent:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(admissions)
admissions %&amp;gt;% group_by(gender) %&amp;gt;%
  summarize(total_admitted = round(sum(admitted / 100 * applicants)),
            not_admitted = sum(applicants) - sum(total_admitted)) %&amp;gt;%
  select(-gender) %&amp;gt;%
  do(tidy(chisq.test(.))) %&amp;gt;% .$p.value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.055797e-21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But closer inspection shows a paradoxical result. Here are the percent admissions by major:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;admissions %&amp;gt;% select(major, gender, admitted) %&amp;gt;%
  spread(gender, admitted) %&amp;gt;%
  mutate(women_minus_men = women - men)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   major men women women_minus_men
## 1     A  62    82              20
## 2     B  63    68               5
## 3     C  37    34              -3
## 4     D  33    35               2
## 5     E  28    24              -4
## 6     F   6     7               1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Four out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.&lt;/p&gt;
&lt;p&gt;The paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.&lt;/p&gt;
&lt;p&gt;So let’s define three variables: &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is 1 for men and 0 for women, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is 1 for those admitted and 0 otherwise, and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; quantifies the selectivity of the major. A gender bias claim would be based on the fact that &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(Y=1 | X = x)\)&lt;/span&gt; is higher for &lt;span class=&#34;math inline&#34;&gt;\(x=1\)&lt;/span&gt; than &lt;span class=&#34;math inline&#34;&gt;\(x=0\)&lt;/span&gt;. However, &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is an important confounder to consider. Clearly &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; is associated with &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, as the more selective a major, the lower &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(Y=1 | Z = z)\)&lt;/span&gt;. But is major selectivity &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; associated with gender &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;One way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;admissions %&amp;gt;%
  group_by(major) %&amp;gt;%
  summarize(major_selectivity = sum(admitted * applicants)/sum(applicants),
            percent_women_applicants = sum(applicants * (gender==&amp;quot;women&amp;quot;)) /
                                             sum(applicants) * 100) %&amp;gt;%
  ggplot(aes(major_selectivity, percent_women_applicants, label = major)) +
  geom_text()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/uc-berkeley-majors-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There seems to be association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confounding-explained-graphically&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Confounding explained graphically&lt;/h3&gt;
&lt;p&gt;The following plot shows the number of applicants that were admitted and those that were not by:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/confounding-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;!--

```r
admissions %&gt;%
  mutate(percent_admitted = admitted * applicants/sum(applicants)) %&gt;%
  ggplot(aes(gender, y = percent_admitted, fill = major)) +
  geom_bar(stat = &#34;identity&#34;, position = &#34;stack&#34;)
```

&lt;img src=&#34;/content/07-content_files/figure-html/confounding-2-1.png&#34; width=&#34;672&#34; /&gt;
--&gt;
&lt;p&gt;It also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-after-stratifying&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Average after stratifying&lt;/h3&gt;
&lt;p&gt;In this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;admissions %&amp;gt;%
  ggplot(aes(major, admitted, col = gender, size = applicants)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/admission-by-major-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.&lt;/p&gt;
&lt;p&gt;If we average the difference by major, we find that the percent is actually 3.5% higher for women.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;admissions %&amp;gt;%  group_by(gender) %&amp;gt;% summarize(average = mean(admitted))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   gender average
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 men       38.2
## 2 women     41.7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simpsons-paradox&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simpson’s paradox&lt;/h2&gt;
&lt;p&gt;The case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; and that we observe realizations of these. Here is a plot of simulated observations for &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; along with the sample correlation:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/simpsons-paradox-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are negatively correlated. However, once we stratify by &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; (shown in different colors below) another pattern emerges:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/07-content_files/figure-html/simpsons-paradox-explained-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is really &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; that is negatively correlated with &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. If we stratify by &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are actually positively correlated as seen in the plot above.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://tylervigen.com/spurious-correlations&#34; class=&#34;uri&#34;&gt;http://tylervigen.com/spurious-correlations&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated&#34; class=&#34;uri&#34;&gt;https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression II</title>
      <link>/content/06-content/</link>
      <pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/content/06-content/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#supplemental-readings&#34;&gt;Supplemental Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-models-ii&#34;&gt;Linear Models II&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-moneyball&#34;&gt;Case study: Moneyball&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sabermetics&#34;&gt;Sabermetics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#baseball-basics&#34;&gt;Baseball basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#no-awards-for-bb&#34;&gt;No awards for BB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#base-on-balls-or-stolen-bases&#34;&gt;Base on balls or stolen bases?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regression-applied-to-baseball-statistics&#34;&gt;Regression applied to baseball statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#confounding&#34;&gt;Confounding&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#understanding-confounding-through-stratification&#34;&gt;Understanding confounding through stratification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multivariate-regression&#34;&gt;Multivariate regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lse&#34;&gt;Least squares estimates&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interpreting-linear-models&#34;&gt;Interpreting linear models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#least-squares-estimates-lse&#34;&gt;Least Squares Estimates (LSE)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-lm-function&#34;&gt;The &lt;code&gt;lm&lt;/code&gt; function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lse-are-random-variables&#34;&gt;LSE are random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predicted-values-are-random-variables&#34;&gt;Predicted values are random variables&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exercises&#34;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-regression-in-the-tidyverse&#34;&gt;Linear regression in the tidyverse&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-broom-package&#34;&gt;The broom package&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-moneyball-continued&#34;&gt;Case study: Moneyball (continued)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#adding-salary-and-position-information&#34;&gt;Adding salary and position information&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#picking-nine-players&#34;&gt;Picking nine players&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-regression-fallacy&#34;&gt;The regression fallacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#measurement-error-models&#34;&gt;Measurement error models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;supplemental-readings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Supplemental Readings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; Chapter 3 in &lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf&#34;&gt;Introduction to Statistical Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How do we interpret linear regression outputs?&lt;/li&gt;
&lt;li&gt;How are the standard errors derived?&lt;/li&gt;
&lt;li&gt;When should we turn to linear regression versus alternative approaches?&lt;/li&gt;
&lt;li&gt;Why do we use linear regression so often in data analytics?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slides&lt;/h3&gt;
&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;slide-tabs&#34; role=&#34;tablist&#34;&gt;
&lt;li class=&#34;nav-item&#34;&gt;
&lt;a class=&#34;nav-link active&#34; id=&#34;introduction-tab&#34; data-toggle=&#34;tab&#34; href=&#34;#introduction&#34; role=&#34;tab&#34; aria-controls=&#34;introduction&#34; aria-selected=&#34;true&#34;&gt;Introduction&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;slide-tabs&#34; class=&#34;tab-content&#34;&gt;
&lt;div id=&#34;introduction&#34; class=&#34;tab-pane fade show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;introduction-tab&#34;&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;/slides/06-slides.html#Introduction&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-models-ii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Linear Models II&lt;/h1&gt;
&lt;p&gt;Since Galton’s original development, regression has become one of the most widely used tools in data science. One reason has to do with the fact that regression permits us to find relationships between two variables taking into account the effects of other variables that affect both. This has been particularly popular in fields where randomized experiments are hard to run, such as economics and epidemiology.&lt;/p&gt;
&lt;p&gt;When we are not able to randomly assign each individual to a treatment or control group, confounding is particularly prevalent. For example, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in a jurisdiction. Fast food consumers are more likely to be smokers, drinkers, and have lower incomes. Therefore, a naive regression model may lead to an overestimate of the negative health effect of fast food. So how do we account for confounding in practice? In this lecture we learn how linear models can help with such situations and can be used to describe how one or more variables affect an outcome variable.&lt;/p&gt;
&lt;div id=&#34;case-study-moneyball&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: Moneyball&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Moneyball: The Art of Winning an Unfair Game&lt;/em&gt; is a book by Michael Lewis about the Oakland Athletics (A’s) baseball team and its general manager, the person tasked with building the team, Billy Beane.&lt;/p&gt;
&lt;p&gt;Traditionally, baseball teams use &lt;em&gt;scouts&lt;/em&gt; to help them decide what players to hire. These scouts evaluate players by observing them perform. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand. This in turn drives up their salaries.&lt;/p&gt;
&lt;p&gt;From 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to buy the best players and, during that time, they were one of the best teams. However, in 1995 the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball. He could no longer afford the most sought-after players. Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to scouts, as a method for finding low-cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a large role in this approach.&lt;/p&gt;
&lt;p&gt;As motivation for this lecture, we will pretend it is 2002 (holy shit I’m old) and try to build a baseball team with a limited budget, just like the A’s had to do. To appreciate what you are up against, note that in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s $39,679,746:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/mlb-2002-payroll-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;sabermetics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sabermetics&lt;/h3&gt;
&lt;p&gt;Statistics have been used in baseball since its beginnings. The dataset we will be using, included in the &lt;strong&gt;Lahman&lt;/strong&gt; library, goes back to the 19th century. For example, a summary statistics we will describe soon, the &lt;em&gt;batting average&lt;/em&gt;, has been used for decades to summarize a batter’s success. Other statistics&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; such as home runs (HR), runs batted in (RBI), and stolen bases (SB) are reported for each player in the game summaries included in the sports section of newspapers, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily decided on without much thought as to whether they actually predicted anything or were related to helping a team win.&lt;/p&gt;
&lt;p&gt;This changed with Bill James&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. In the late 1970s, this aspiring writer and baseball fan started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to predict what outcomes best predicted if a team would win &lt;em&gt;sabermetrics&lt;/em&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Currently, sabermetrics popularity is no longer limited to just baseball; other sports have started to use this approach as well.&lt;/p&gt;
&lt;p&gt;In this lecture, to simplify the exercise, we will focus on scoring runs and ignore the two other important aspects of the game: pitching and fielding. We will see how regression analysis can help develop strategies to build a competitive baseball team with a constrained budget. The approach can be divided into two separate data analyses. In the first, we determine which recorded player-specific statistics predict runs. In the second, we examine if players were undervalued based on what our first analysis predicts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;baseball-basics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Baseball basics&lt;/h3&gt;
&lt;p&gt;To see how regression will help us find undervalued players, we actually don’t need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem.&lt;/p&gt;
&lt;p&gt;The goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team’s &lt;em&gt;pitcher&lt;/em&gt; throws the ball and the batter tries to hit it. The PA ends with an binary outcome: the batter either makes an &lt;em&gt;out&lt;/em&gt; (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases). Each team gets nine tries, referred to as &lt;em&gt;innings&lt;/em&gt;, to score runs and each inning ends after three outs (three failures).&lt;/p&gt;
&lt;p&gt;Here is a video showing a success: &lt;a href=&#34;https://www.youtube.com/watch?v=HL-XjMCPfio&#34;&gt;https://www.youtube.com/watch?v=HL-XjMCPfio&lt;/a&gt;. And here is one showing a failure: &lt;a href=&#34;https://www.youtube.com/watch?v=NeloljCx-1g&#34;&gt;https://www.youtube.com/watch?v=NeloljCx-1g&lt;/a&gt;. In these videos, we see how luck is involved in the process. When at bat, the batter wants to hit the ball hard. If the batter hits it hard enough, it is a HR, the best possible outcome as the batter gets at least one automatic run. But sometimes, due to chance, the batter hits the ball very hard and a defender catches it, resulting in an out. In contrast, sometimes the batter hits the ball softly, but it lands just in the right place. The fact that there is chance involved hints at why probability models will be involved.&lt;/p&gt;
&lt;p&gt;Now there are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many &lt;em&gt;bases&lt;/em&gt; as possible. There are four bases with the fourth one called &lt;em&gt;home plate&lt;/em&gt;. Home plate is where batters start by trying to hit, so the bases form a cycle.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./06-content_files/Baseball_Diamond1.png&#34; /&gt;
(Courtesy of Cburnett&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. CC BY-SA 3.0 license&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;.)
&lt;!--Source: [Wikipedia Commons](https://commons.wikimedia.org/wiki/File:Baseball_diamond_simplified.svg))--&gt;&lt;/p&gt;
&lt;p&gt;A batter who &lt;em&gt;goes around the bases&lt;/em&gt; and arrives home, scores a run.&lt;/p&gt;
&lt;p&gt;We are simplifying a bit, but there are five ways a batter can succeed, that is, not make an out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base.&lt;/li&gt;
&lt;li&gt;Single - Batter hits the ball and gets to first base.&lt;/li&gt;
&lt;li&gt;Double (2B) - Batter hits the ball and gets to second base.&lt;/li&gt;
&lt;li&gt;Triple (3B) - Batter hits the ball and gets to third base.&lt;/li&gt;
&lt;li&gt;Home Run (HR) - Batter hits the ball and goes all the way home and scores a run.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an example of a HR:
&lt;a href=&#34;https://www.youtube.com/watch?v=xYxSZJ9GZ-w&#34;&gt;https://www.youtube.com/watch?v=xYxSZJ9GZ-w&lt;/a&gt;.
If a batter gets to a base, the batter still has a chance of getting home and scoring a run if the next batter hits successfully. While the batter is &lt;em&gt;on base&lt;/em&gt;, the batter can also try to steal a base (SB). If a batter runs fast enough, the batter can try to go from one base to the next without the other team tagging the runner. [Here] is an example of a stolen base: &lt;a href=&#34;https://www.youtube.com/watch?v=JSE5kfxkzfk&#34;&gt;https://www.youtube.com/watch?v=JSE5kfxkzfk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All these events are kept track of during the season and are available to us through the &lt;strong&gt;Lahman&lt;/strong&gt; package. Now we will start discussing how data analysis can help us decide how to use these statistics to evaluate players.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;no-awards-for-bb&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;No awards for BB&lt;/h3&gt;
&lt;p&gt;Historically, the &lt;em&gt;batting average&lt;/em&gt; has been considered the most important offensive statistic. To define this average, we define a &lt;em&gt;hit&lt;/em&gt; (H) and an &lt;em&gt;at bat&lt;/em&gt; (AB). Singles, doubles, triples, and home runs are hits. The fifth way to be successful, BB, is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. Today this success rate ranges from 20% to 38%. We refer to the batting average in thousands so, for example, if your success rate is 28%, we call it &lt;em&gt;batting 280&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./06-content_files/JumboTron.png&#34; /&gt;
(Picture courtesy of Keith Allison&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;. CC BY-SA 2.0 license&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;One of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. He proposed we use the &lt;em&gt;on base percentage&lt;/em&gt; (OBP) instead of batting average. He defined OBP as (H+BB)/(AB+BB) which is simply the proportion of plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic. In contrast, total stolen bases were considered important and an award&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data science to determine if it’s better to pay for players with high BB or SB?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;base-on-balls-or-stolen-bases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Base on balls or stolen bases?&lt;/h3&gt;
&lt;p&gt;One of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. We do keep track of the number of runs scored by a player. However, remember that if a player X bats right before someone who hits many HRs, batter X will score many runs. But these runs don’t necessarily happen if we hire player X but not his HR hitting teammate. However, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? We have data! Let’s examine some.&lt;/p&gt;
&lt;p&gt;Let’s start with an obvious one: HRs. Do teams that hit more home runs score more runs? We examine data from 1961 to 2001. The visualization of choice when exploring the relationship between two variables, such as HRs and wins, is a scatterplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Lahman)

Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(HR_per_game = HR / G, R_per_game = R / G) %&amp;gt;%
  ggplot(aes(HR_per_game, R_per_game)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/runs-vs-hrs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot shows a strong association: teams with more HRs tend to score more runs. Now let’s examine the relationship between stolen bases and runs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(SB_per_game = SB / G, R_per_game = R / G) %&amp;gt;%
  ggplot(aes(SB_per_game, R_per_game)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/runs-vs-sb-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here the relationship is not as clear. Finally, let’s examine the relationship between BB and runs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(BB_per_game = BB/G, R_per_game = R/G) %&amp;gt;%
  ggplot(aes(BB_per_game, R_per_game)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/runs-vs-bb-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here again we see a clear association. But does this mean that increasing a team’s BBs &lt;strong&gt;causes&lt;/strong&gt; an increase in runs? One of the most important lessons you learn in this book is that &lt;strong&gt;association is not causation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In fact, it looks like BBs and HRs are also associated:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;% filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(HR_per_game = HR/G, BB_per_game = BB/G) %&amp;gt;%
  ggplot(aes(HR_per_game, BB_per_game)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/bb-vs-hrs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We know that HRs cause runs because, as the name “home run” implies, when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is &lt;em&gt;confounding&lt;/em&gt;, an important concept we will learn more about throughout this lecture.&lt;/p&gt;
&lt;p&gt;Linear regression will help us parse all this out and quantify the associations. This will then help us determine what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BBs, but keep the HRs fixed? Regression will help us answer questions like this one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regression-applied-to-baseball-statistics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression applied to baseball statistics&lt;/h3&gt;
&lt;p&gt;Can we use regression with these data? First, notice that the HR and Run data appear to be bivariate normal. We save the plot into the object &lt;code&gt;p&lt;/code&gt; as we will use it again later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Lahman)
p &amp;lt;- Teams %&amp;gt;% filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(HR_per_game = HR/G, R_per_game = R/G) %&amp;gt;%
  ggplot(aes(HR_per_game, R_per_game)) +
  geom_point(alpha = 0.5)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/hr-runs-bivariate-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The qq-plots confirm that the normal approximation is useful here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;% filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(z_HR = round((HR - mean(HR))/sd(HR)),
         R_per_game = R/G) %&amp;gt;%
  filter(z_HR %in% -2:3) %&amp;gt;%
  ggplot() +
  stat_qq(aes(sample=R_per_game)) +
  facet_wrap(~z_HR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/hr-by-runs-qq-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we are ready to use linear regression to predict the number of runs a team will score if we know how many home runs the team hits. All we need to do is compute the five summary statistics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary_stats &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(HR_per_game = HR/G, R_per_game = R/G) %&amp;gt;%
  summarize(avg_HR = mean(HR_per_game),
            s_HR = sd(HR_per_game),
            avg_R = mean(R_per_game),
            s_R = sd(R_per_game),
            r = cor(HR_per_game, R_per_game))
summary_stats&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      avg_HR      s_HR    avg_R       s_R         r
## 1 0.8547104 0.2429707 4.355262 0.5885791 0.7615597&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and use the formulas given above to create the regression lines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reg_line &amp;lt;- summary_stats %&amp;gt;% summarize(slope = r*s_R/s_HR,
                            intercept = avg_R - slope*avg_HR)

p + geom_abline(intercept = reg_line$intercept, slope = reg_line$slope)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/hr-versus-runs-regression-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Soon we will learn R functions, such as &lt;code&gt;lm&lt;/code&gt;, that make fitting regression lines much easier. Another example is the &lt;strong&gt;ggplot2&lt;/strong&gt; function &lt;code&gt;geom_smooth&lt;/code&gt; which computes and adds a regression line to plot along with confidence intervals, which we also learn about later. We use the argument &lt;code&gt;method = &#34;lm&#34;&lt;/code&gt; which stands for &lt;em&gt;linear model&lt;/em&gt;, the title of an upcoming section. So we can simplify the code above like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/hr-versus-runs-regression-easy-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the example above, the slope is 1.8448241. So this tells us that teams that hit 1 more HR per game than the average team, score 1.8448241 more runs per game than the average team. Given that the most common final score is a difference of a run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because we are working on a budget, we will need to find some other way to increase wins. So in the next section we move our attention to BB.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;confounding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confounding&lt;/h2&gt;
&lt;p&gt;Previously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(Lahman)
get_slope &amp;lt;- function(x, y) cor(x, y) * sd(y) / sd(x)

bb_slope &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(BB_per_game = BB/G, R_per_game = R/G) %&amp;gt;%
  summarize(slope = get_slope(BB_per_game, R_per_game))

bb_slope&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       slope
## 1 0.7353288&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score 1.5 more runs per game?&lt;/p&gt;
&lt;p&gt;We are again reminded that association is not causation. The data does provide strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game. But this does not mean that BB are the cause.&lt;/p&gt;
&lt;p&gt;Note that if we compute the regression line slope for singles we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;singles_slope &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %&amp;gt;%
  summarize(slope = get_slope(Singles_per_game, R_per_game))

singles_slope&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       slope
## 1 0.4494253&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is a lower value than what we obtain for BB.&lt;/p&gt;
&lt;p&gt;Also, notice that a single gets you to first base just like a BB. Those that know about baseball will tell you that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %&amp;gt;%
  summarize(cor(BB, HR), cor(Singles, HR), cor(BB, Singles))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   cor(BB, HR) cor(Singles, HR) cor(BB, Singles)
## 1   0.4039313       -0.1737435      -0.05603822&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BBs and a team with many HRs will also have more BBs. Although it may appear that BBs cause runs, it is actually the HRs that cause most of these runs. We say that BBs are &lt;em&gt;confounded&lt;/em&gt; with HRs. Nonetheless, could it be that BBs still help? To find out, we somehow have to adjust for the HR effect. Regression can help with this as well.&lt;/p&gt;
&lt;div id=&#34;understanding-confounding-through-stratification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Understanding confounding through stratification&lt;/h3&gt;
&lt;p&gt;A first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest ten. We filter out the strata with few points to avoid highly variable estimates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(HR_strata = round(HR/G, 1),
         BB_per_game = BB / G,
         R_per_game = R / G) %&amp;gt;%
  filter(HR_strata &amp;gt;= 0.4 &amp;amp; HR_strata &amp;lt;=1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then make a scatterplot for each strata:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  ggplot(aes(BB_per_game, R_per_game)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  facet_wrap( ~ HR_strata)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/runs-vs-bb-by-hr-strata-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Remember that the regression slope for predicting runs with BB was 0.7. Once we stratify by HR, these slopes are substantially reduced:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR_strata) %&amp;gt;%
  summarize(slope = get_slope(BB_per_game, R_per_game))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 2
##   HR_strata slope
##       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1       0.4 0.734
## 2       0.5 0.566
## 3       0.6 0.412
## 4       0.7 0.285
## 5       0.8 0.365
## 6       0.9 0.261
## 7       1   0.512
## 8       1.1 0.454
## 9       1.2 0.440&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The slopes are reduced, but they are not 0, which indicates that BBs are helpful for producing runs, just not as much as previously thought.
In fact, the values above are closer to the slope we obtained from singles, 0.45, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.&lt;/p&gt;
&lt;p&gt;Although our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BBs to get this plot:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/runs-vs-hr-by-bb-strata-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, the slopes do not change much from the original:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% group_by(BB_strata) %&amp;gt;%
   summarize(slope = get_slope(HR_per_game, R_per_game))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 2
##    BB_strata slope
##        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1       2.8  1.52
##  2       2.9  1.57
##  3       3    1.52
##  4       3.1  1.49
##  5       3.2  1.58
##  6       3.3  1.56
##  7       3.4  1.48
##  8       3.5  1.63
##  9       3.6  1.83
## 10       3.7  1.45
## 11       3.8  1.70
## 12       3.9  1.30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They are reduced a bit, which is consistent with the fact that BB do in fact cause some runs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hr_slope &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001 ) %&amp;gt;%
  mutate(HR_per_game = HR/G, R_per_game = R/G) %&amp;gt;%
  summarize(slope = get_slope(HR_per_game, R_per_game))

hr_slope&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      slope
## 1 1.844824&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Regardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multivariate-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multivariate regression&lt;/h3&gt;
&lt;p&gt;It is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1(x_2) x_1 + \beta_2(x_1) x_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with the slopes for &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; changing for different values of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; and vice versa. But is there an easier approach?&lt;/p&gt;
&lt;p&gt;If we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that &lt;span class=&#34;math inline&#34;&gt;\(\beta_1(x_2)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2(x_1)\)&lt;/span&gt; are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This model suggests that if the number of HR is fixed at &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;, we observe a linear relationship between runs and BB with an intercept of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_2 x_2\)&lt;/span&gt;. Our exploratory data analysis suggested this. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by &lt;span class=&#34;math inline&#34;&gt;\(\beta_1 x_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In this analysis, referred to as &lt;em&gt;multivariate regression&lt;/em&gt;, you will often hear people say that the BB slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is &lt;em&gt;adjusted&lt;/em&gt; for the HR effect. If the model is correct then confounding has been accounted for. But how do we estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; from the data? For this, we learn about linear models and least squares estimates.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Least squares estimates&lt;/h2&gt;
&lt;p&gt;We have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a &lt;em&gt;linear model&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We note that “linear” here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, &lt;span class=&#34;math inline&#34;&gt;\(3x - 4y + 5z\)&lt;/span&gt; is a linear combination of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;. We can also add a constant so &lt;span class=&#34;math inline&#34;&gt;\(2 + 3x - 4y + 5z\)&lt;/span&gt; is also linear combination of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_1 x_1 + \beta_2 x_2\)&lt;/span&gt;, is a linear combination of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.
The simplest linear model is a constant &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;; the second simplest is a line &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_1 x\)&lt;/span&gt;. If we were to specify a linear model for Galton’s data, we would denote the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; observed father heights with &lt;span class=&#34;math inline&#34;&gt;\(x_1, \dots, x_n\)&lt;/span&gt;, then we model the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; son heights we are trying to predict with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \, i=1,\dots,N.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the father’s height, which is fixed (not random) due to the conditioning, and &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; is the random son’s height that we want to predict. We further assume that &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt; are independent from each other, have expected value 0 and the standard deviation, call it &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, does not depend on &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the above model, we know the &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, but to have a useful model for prediction, we need &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;. We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We show how to do this in the next section.&lt;/p&gt;
&lt;p&gt;Note that if we further assume that the &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; is normally distributed, then this model is exactly the same one we derived earlier by assuming bivariate normal data. A somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and that the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;s is not specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.&lt;/p&gt;
&lt;div id=&#34;interpreting-linear-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpreting linear models&lt;/h3&gt;
&lt;p&gt;One reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; for each inch we increase the father’s height &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Because not all sons with fathers of height &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; are of equal height, we need the term &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;, which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.&lt;/p&gt;
&lt;p&gt;Given how we wrote the model above, the intercept &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 (x_i - \bar{x}) + \varepsilon_i, \, i=1,\dots,N
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\bar{x} = 1/N \sum_{i=1}^N x_i\)&lt;/span&gt; the average of the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. In this case &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; represents the height when &lt;span class=&#34;math inline&#34;&gt;\(x_i = \bar{x}\)&lt;/span&gt;, which is the height of the son of an average father.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;least-squares-estimates-lse&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Least Squares Estimates (LSE)&lt;/h3&gt;
&lt;p&gt;For linear models to be useful, we have to estimate the unknown &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;s. The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this lecture. For Galton’s data, we would write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
RSS = \sum_{i=1}^n \left\{  y_i - \left(\beta_0 + \beta_1 x_i \right)\right\}^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1\)&lt;/span&gt;. Let’s demonstrate this with the previously defined dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(HistData)
data(&amp;quot;GaltonFamilies&amp;quot;)
set.seed(1983)
galton_heights &amp;lt;- GaltonFamilies %&amp;gt;%
  filter(gender == &amp;quot;male&amp;quot;) %&amp;gt;%
  group_by(family) %&amp;gt;%
  sample_n(1) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(father, childHeight) %&amp;gt;%
  rename(son = childHeight)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s write a function that computes the RSS for any pair of values &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rss &amp;lt;- function(beta0, beta1, data){
  resid &amp;lt;- galton_heights$son - (beta0+beta1*galton_heights$father)
  return(sum(resid^2))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So for any pair of values, we get an RSS. Here is a plot of the RSS as a function of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; when we keep the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; fixed at 25.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta1 = seq(0, 1, len=nrow(galton_heights))
results &amp;lt;- data.frame(beta1 = beta1,
                      rss = sapply(beta1, rss, beta0 = 25))
results %&amp;gt;% ggplot(aes(beta1, rss)) + geom_line() +
  geom_line(aes(beta1, rss))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/rss-versus-estimate-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see a clear minimum for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; at around 0.65. However, this minimum for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is for when &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 = 25\)&lt;/span&gt;, a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.&lt;/p&gt;
&lt;p&gt;Trial and error is not going to work in this case. We could search for a minimum within a fine grid of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; values, but this is unnecessarily time-consuming since we can use calculus: take the partial derivatives, set them to 0 and solve for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt;. Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will learn these next. To learn the mathematics behind this, you can consult a book on linear models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-lm-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;lm&lt;/code&gt; function&lt;/h3&gt;
&lt;p&gt;In R, we can obtain the least squares estimates using the &lt;code&gt;lm&lt;/code&gt; function. To fit the model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; the son’s height and &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; the father’s height, we can use this code to obtain the least squares estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- lm(son ~ father, data = galton_heights)
fit$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)      father 
##   37.287605    0.461392&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The most common way we use &lt;code&gt;lm&lt;/code&gt; is by using the character &lt;code&gt;~&lt;/code&gt; to let &lt;code&gt;lm&lt;/code&gt; know which is the variable we are predicting (left of &lt;code&gt;~&lt;/code&gt;) and which we are using to predict (right of &lt;code&gt;~&lt;/code&gt;). The intercept is added automatically to the model that will be fit.&lt;/p&gt;
&lt;p&gt;The object &lt;code&gt;fit&lt;/code&gt; includes more information about the fit. We can use the function &lt;code&gt;summary&lt;/code&gt; to extract more of this information (not shown):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = son ~ father, data = galton_heights)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.3543 -1.5657 -0.0078  1.7263  9.4150 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 37.28761    4.98618   7.478 3.37e-12 ***
## father       0.46139    0.07211   6.398 1.36e-09 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.45 on 177 degrees of freedom
## Multiple R-squared:  0.1878, Adjusted R-squared:  0.1833 
## F-statistic: 40.94 on 1 and 177 DF,  p-value: 1.36e-09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lse-are-random-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LSE are random variables&lt;/h3&gt;
&lt;p&gt;The LSE is derived from the data &lt;span class=&#34;math inline&#34;&gt;\(y_1,\dots,y_N\)&lt;/span&gt;, which are a realization of random variables &lt;span class=&#34;math inline&#34;&gt;\(Y_1, \dots, Y_N\)&lt;/span&gt;. This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size &lt;span class=&#34;math inline&#34;&gt;\(N=50\)&lt;/span&gt;, and compute the regression slope coefficient for each one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 1000
N &amp;lt;- 50
lse &amp;lt;- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %&amp;gt;%
    lm(son ~ father, data = .) %&amp;gt;%
    .$coef
})
lse &amp;lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the variability of the estimates by plotting their distributions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gridExtra&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/lse-distributions-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The reason these look normal is because the central limit theorem applies here as well: for large enough &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, the least squares estimates will be approximately normal with expected value &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, respectively. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the &lt;code&gt;lm&lt;/code&gt; function. Here it is for one of our simulated data sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; sample_n(galton_heights, N, replace = TRUE) %&amp;gt;%
  lm(son ~ father, data = .) %&amp;gt;%
  summary %&amp;gt;% .$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Estimate Std. Error  t value     Pr(&amp;gt;|t|)
## (Intercept) 19.2791952 11.6564590 1.653950 0.1046637693
## father       0.7198756  0.1693834 4.249977 0.0000979167&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see that the standard errors estimates reported by the &lt;code&gt;summary&lt;/code&gt; are close to the standard errors from the simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lse %&amp;gt;% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      se_0      se_1
## 1 8.83591 0.1278812&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;summary&lt;/code&gt; function also reports t-statistics (&lt;code&gt;t value&lt;/code&gt;) and p-values (&lt;code&gt;Pr(&amp;gt;|t|)&lt;/code&gt;). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt;s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_0 / \hat{\mbox{SE}}(\hat{\beta}_0 )\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_1 / \hat{\mbox{SE}}(\hat{\beta}_1 )\)&lt;/span&gt;, follow a t-distribution with &lt;span class=&#34;math inline&#34;&gt;\(N-p\)&lt;/span&gt; degrees of freedom, with &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; the number of parameters in our model. In the case of height &lt;span class=&#34;math inline&#34;&gt;\(p=2\)&lt;/span&gt;, the two p-values are testing the null hypothesis that &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1=0\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Remember that, as we described in Section &lt;a href=&#34;#t-dist&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; for large enough &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about &lt;strong&gt;broom&lt;/strong&gt;, an add-on package that makes this easy.&lt;/p&gt;
&lt;p&gt;Although we do not show examples in this book, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predicted-values-are-random-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predicted values are random variables&lt;/h3&gt;
&lt;p&gt;Once we fit our model, we can obtain prediction of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; by plugging in the estimates into the regression model. For example, if the father’s height is &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, then our prediction &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; for the son’s height will be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 x\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When we plot &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; versus &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we see the regression line.&lt;/p&gt;
&lt;p&gt;Keep in mind that the prediction &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the &lt;strong&gt;ggplot2&lt;/strong&gt; layer &lt;code&gt;geom_smooth(method = &#34;lm&#34;)&lt;/code&gt; that we previously used plots &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; and surrounds it by confidence intervals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;% ggplot(aes(son, father)) +
  geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/father-son-regression-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The R function &lt;code&gt;predict&lt;/code&gt; takes an &lt;code&gt;lm&lt;/code&gt; object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- galton_heights %&amp;gt;% lm(son ~ father, data = .)

y_hat &amp;lt;- predict(fit, se.fit = TRUE)

names(y_hat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;fit&amp;quot;            &amp;quot;se.fit&amp;quot;         &amp;quot;df&amp;quot;             &amp;quot;residual.scale&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;We have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing how stable they are across the years. Since we have to pick players based on their previous performances, we will prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BBs.&lt;/p&gt;
&lt;p&gt;1. Before we get started, we want to generate two tables. One for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics. Here is how we create the 2017 table. Keeping only players with more than 100 plate appearances.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Lahman)
dat &amp;lt;- Batting %&amp;gt;% filter(yearID == 2002) %&amp;gt;%
  mutate(pa = AB + BB,
         singles = (H - X2B - X3B - HR) / pa, bb = BB / pa) %&amp;gt;%
  filter(pa &amp;gt;= 100) %&amp;gt;%
  select(playerID, singles, bb)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now compute a similar table but with rates computed over 1999-2001.&lt;/p&gt;
&lt;p&gt;2. In Section &lt;a href=&#34;#joins&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we learn about the &lt;code&gt;inner_join&lt;/code&gt;, which you can use to have the 2001 data and averages in the same table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- inner_join(dat, avg, by = &amp;quot;playerID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compute the correlation between 2002 and the previous seasons for singles and BB.&lt;/p&gt;
&lt;p&gt;3. Note that the correlation is higher for BB. To quickly get an idea of the uncertainty associated with this correlation estimate, we will fit a linear model and compute confidence intervals for the slope coefficient. However, first make scatterplots to confirm that fitting a linear model is appropriate.&lt;/p&gt;
&lt;p&gt;4. Now fit a linear model for each metric and use the &lt;code&gt;confint&lt;/code&gt; function to compare the estimates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-regression-in-the-tidyverse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear regression in the tidyverse&lt;/h2&gt;
&lt;p&gt;To see how we use the &lt;code&gt;lm&lt;/code&gt; function in a more complex analysis, let’s go back to the baseball example. In a previous example, we estimated regression lines to predict runs for BB in different HR strata. We first constructed a data frame similar to this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- Teams %&amp;gt;% filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(HR = round(HR/G, 1),
         BB = BB/G,
         R = R/G) %&amp;gt;%
  select(HR, BB, R) %&amp;gt;%
  filter(HR &amp;gt;= 0.4 &amp;amp; HR&amp;lt;=1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we didn’t know the &lt;code&gt;lm&lt;/code&gt; function, to compute the regression line in each strata, we used the formula directly like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_slope &amp;lt;- function(x, y) cor(x, y) * sd(y) / sd(x)
dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  summarize(slope = get_slope(BB, R))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We argued that the slopes are similar and that the differences were perhaps due to random variation. To provide a more rigorous defense of the slopes being the same, which led to our multivariate model, we could compute confidence intervals for each slope. We have not learned the formula for this, but the &lt;code&gt;lm&lt;/code&gt; function provides enough information to construct them.&lt;/p&gt;
&lt;p&gt;First, note that if we try to use the &lt;code&gt;lm&lt;/code&gt; function to get the estimated slope like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  lm(R ~ BB, data = .) %&amp;gt;% .$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)          BB 
##   2.1983658   0.6378804&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we don’t get the result we want. The &lt;code&gt;lm&lt;/code&gt; function ignores the &lt;code&gt;group_by&lt;/code&gt;. This is expected because &lt;code&gt;lm&lt;/code&gt; is not part of the &lt;strong&gt;tidyverse&lt;/strong&gt; and does not know how to handle the outcome of a grouped tibble.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;tidyverse&lt;/strong&gt; functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;, &lt;strong&gt;tidyverse&lt;/strong&gt; functions consistently return data frames, since this assures that the output of a function is accepted as the input of another.
But most R functions do not recognize grouped tibbles nor do they return data frames. The &lt;code&gt;lm&lt;/code&gt; function is an example. The &lt;code&gt;do&lt;/code&gt; functions serves as a bridge between R functions, such as &lt;code&gt;lm&lt;/code&gt;, and the &lt;strong&gt;tidyverse&lt;/strong&gt;. The &lt;code&gt;do&lt;/code&gt; function understands grouped tibbles and always returns a data frame.&lt;/p&gt;
&lt;p&gt;So, let’s try to use the &lt;code&gt;do&lt;/code&gt; function to fit a regression line to each HR strata:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(fit = lm(R ~ BB, data = .))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 2
## # Rowwise: 
##      HR fit   
##   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;
## 1   0.4 &amp;lt;lm&amp;gt;  
## 2   0.5 &amp;lt;lm&amp;gt;  
## 3   0.6 &amp;lt;lm&amp;gt;  
## 4   0.7 &amp;lt;lm&amp;gt;  
## 5   0.8 &amp;lt;lm&amp;gt;  
## 6   0.9 &amp;lt;lm&amp;gt;  
## 7   1   &amp;lt;lm&amp;gt;  
## 8   1.1 &amp;lt;lm&amp;gt;  
## 9   1.2 &amp;lt;lm&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that we did in fact fit a regression line to each strata. The &lt;code&gt;do&lt;/code&gt; function will create a data frame with the first column being the strata value and a column named &lt;code&gt;fit&lt;/code&gt; (we chose the name, but it can be anything). The column will contain the result of the &lt;code&gt;lm&lt;/code&gt; call. Therefore, the returned tibble has a column with &lt;code&gt;lm&lt;/code&gt; objects, which is not very useful.&lt;/p&gt;
&lt;p&gt;Also, if we do not name a column (note above we named it &lt;code&gt;fit&lt;/code&gt;), then &lt;code&gt;do&lt;/code&gt; will return the actual output of &lt;code&gt;lm&lt;/code&gt;, not a data frame, and this will result in an error since &lt;code&gt;do&lt;/code&gt; is expecting a data frame as output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(lm(R ~ BB, data = .))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Error: Results 1, 2, 3, 4, 5, ... must be data frames, not lm&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For a useful data frame to be constructed, the output of the function must be a data frame too. We could build a function that returns only what we want in the form of a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_slope &amp;lt;- function(data){
  fit &amp;lt;- lm(R ~ BB, data = data)
  data.frame(slope = fit$coefficients[2],
             se = summary(fit)$coefficient[2,2])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then use &lt;code&gt;do&lt;/code&gt; &lt;strong&gt;without&lt;/strong&gt; naming the output, since we are already getting a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(get_slope(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
## # Groups:   HR [9]
##      HR slope     se
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1   0.4 0.734 0.208 
## 2   0.5 0.566 0.110 
## 3   0.6 0.412 0.0974
## 4   0.7 0.285 0.0705
## 5   0.8 0.365 0.0653
## 6   0.9 0.261 0.0751
## 7   1   0.512 0.0751
## 8   1.1 0.454 0.0855
## 9   1.2 0.440 0.0801&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we name the output, then we get something we do not want, a column containing data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(slope = get_slope(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 2
## # Rowwise: 
##      HR slope           
##   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;          
## 1   0.4 &amp;lt;df[,2] [1 × 2]&amp;gt;
## 2   0.5 &amp;lt;df[,2] [1 × 2]&amp;gt;
## 3   0.6 &amp;lt;df[,2] [1 × 2]&amp;gt;
## 4   0.7 &amp;lt;df[,2] [1 × 2]&amp;gt;
## 5   0.8 &amp;lt;df[,2] [1 × 2]&amp;gt;
## 6   0.9 &amp;lt;df[,2] [1 × 2]&amp;gt;
## 7   1   &amp;lt;df[,2] [1 × 2]&amp;gt;
## 8   1.1 &amp;lt;df[,2] [1 × 2]&amp;gt;
## 9   1.2 &amp;lt;df[,2] [1 × 2]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is not very useful, so let’s cover one last feature of &lt;code&gt;do&lt;/code&gt;. If the data frame being returned has more than one row, these will be concatenated appropriately. Here is an example in which we return both estimated parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_lse &amp;lt;- function(data){
  fit &amp;lt;- lm(R ~ BB, data = data)
  data.frame(term = names(fit$coefficients),
    slope = fit$coefficients,
    se = summary(fit)$coefficient[,2])
}

dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(get_lse(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18 x 4
## # Groups:   HR [9]
##       HR term        slope     se
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1   0.4 (Intercept) 1.36  0.631 
##  2   0.4 BB          0.734 0.208 
##  3   0.5 (Intercept) 2.01  0.344 
##  4   0.5 BB          0.566 0.110 
##  5   0.6 (Intercept) 2.53  0.305 
##  6   0.6 BB          0.412 0.0974
##  7   0.7 (Intercept) 3.21  0.225 
##  8   0.7 BB          0.285 0.0705
##  9   0.8 (Intercept) 3.07  0.213 
## 10   0.8 BB          0.365 0.0653
## 11   0.9 (Intercept) 3.54  0.251 
## 12   0.9 BB          0.261 0.0751
## 13   1   (Intercept) 2.88  0.256 
## 14   1   BB          0.512 0.0751
## 15   1.1 (Intercept) 3.21  0.300 
## 16   1.1 BB          0.454 0.0855
## 17   1.2 (Intercept) 3.40  0.291 
## 18   1.2 BB          0.440 0.0801&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you think this is all a bit too complicated, you are not alone. To simplify things, we introduce the &lt;strong&gt;broom&lt;/strong&gt; package which was designed to facilitate the use of model fitting functions, such as &lt;code&gt;lm&lt;/code&gt;, with the &lt;strong&gt;tidyverse&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;the-broom-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The broom package&lt;/h3&gt;
&lt;p&gt;Our original task was to provide an estimate and confidence interval for the slope estimates of each strata. The &lt;strong&gt;broom&lt;/strong&gt; package will make this quite easy.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;broom&lt;/strong&gt; package has three main functions, all of which extract information from the object returned by &lt;code&gt;lm&lt;/code&gt; and return it in a &lt;strong&gt;tidyverse&lt;/strong&gt; friendly data frame. These functions are &lt;code&gt;tidy&lt;/code&gt;, &lt;code&gt;glance&lt;/code&gt;, and &lt;code&gt;augment&lt;/code&gt;. The &lt;code&gt;tidy&lt;/code&gt; function returns estimates and related information as a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)
fit &amp;lt;- lm(R ~ BB, data = dat)
tidy(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)    2.20     0.113       19.4 1.12e-70
## 2 BB             0.638    0.0344      18.5 1.35e-65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can add other important summaries, such as confidence intervals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)    2.20     0.113       19.4 1.12e-70    1.98      2.42 
## 2 BB             0.638    0.0344      18.5 1.35e-65    0.570     0.705&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because the outcome is a data frame, we can immediately use it with &lt;code&gt;do&lt;/code&gt; to string together the commands that produce the table we are after. Because a data frame is returned, we can filter and select the rows and columns we want, which facilitates working with &lt;strong&gt;ggplot2&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  group_by(HR) %&amp;gt;%
  do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %&amp;gt;%
  filter(term == &amp;quot;BB&amp;quot;) %&amp;gt;%
  select(HR, estimate, conf.low, conf.high) %&amp;gt;%
  ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/do-tidy-example-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we return to discussing our original task of determining if slopes changed. The plot we just made, using &lt;code&gt;do&lt;/code&gt; and &lt;code&gt;tidy&lt;/code&gt;, shows that the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slope does not change is safe.&lt;/p&gt;
&lt;p&gt;The other functions provided by &lt;strong&gt;broom&lt;/strong&gt;, &lt;code&gt;glance&lt;/code&gt;, and &lt;code&gt;augment&lt;/code&gt;, relate to model-specific and observation-specific outcomes, respectively. Here, we can see the model fit summaries &lt;code&gt;glance&lt;/code&gt; returns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glance(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 12
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
##       &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     0.266         0.265 0.454      343. 1.35e-65     1  -596. 1199. 1214.
## # … with 3 more variables: deviance &amp;lt;dbl&amp;gt;, df.residual &amp;lt;int&amp;gt;, nobs &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can learn more about these summaries in any regression text book.&lt;/p&gt;
&lt;p&gt;We will see an example of &lt;code&gt;augment&lt;/code&gt; in the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-moneyball-continued&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: Moneyball (continued)&lt;/h2&gt;
&lt;p&gt;In trying to answer how well BBs predict runs, data exploration led us to a model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[R \mid BB = x_1, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, the data is approximately normal and conditional distributions were also normal. Thus, we are justified in using a linear model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; runs per game for team &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_{i,1}\)&lt;/span&gt; walks per game, and &lt;span class=&#34;math inline&#34;&gt;\(x_{i,2}\)&lt;/span&gt;. To use &lt;code&gt;lm&lt;/code&gt; here, we need to let the function know we have two predictor variables. So we use the &lt;code&gt;+&lt;/code&gt; symbol as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(BB = BB/G, HR = HR/G,  R = R/G) %&amp;gt;%
  lm(R ~ BB + HR, data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use &lt;code&gt;tidy&lt;/code&gt; to see a nice summary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 7
##   term        estimate std.error statistic   p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)    1.74     0.0824      21.2 7.62e- 83    1.58      1.91 
## 2 BB             0.387    0.0270      14.3 1.20e- 42    0.334     0.440
## 3 HR             1.56     0.0490      31.9 1.78e-155    1.47      1.66&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we fit the model with only one variable, the estimated slopes were 0.7353288 and 1.8448241 for BB and HR, respectively. Note that when fitting the multivariate model both go down, with the BB effect decreasing much more.&lt;/p&gt;
&lt;p&gt;Now we want to construct a metric to pick players, we need to consider singles, doubles, and triples as well. Can we build a model that predicts runs based on all these outcomes?&lt;/p&gt;
&lt;p&gt;We now are going to take somewhat of a “leap of faith” and assume that these five variables are jointly normal. This means that if we pick any one of them, and hold the other four fixed, the relationship with the outcome is linear and the slope does not depend on the four values held constant. If this is true, then a linear model for our data is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \beta_3 x_{i,3}+ \beta_4 x_{i,4} + \beta_5 x_{i,5} + \varepsilon_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\)&lt;/span&gt; representing BB, singles, doubles, triples, and HR respectively.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;lm&lt;/code&gt;, we can quickly find the LSE for the parameters using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- Teams %&amp;gt;%
  filter(yearID %in% 1961:2001) %&amp;gt;%
  mutate(BB = BB / G,
         singles = (H - X2B - X3B - HR) / G,
         doubles = X2B / G,
         triples = X3B / G,
         HR = HR / G,
         R = R / G) %&amp;gt;%
  lm(R ~ BB + singles + doubles + triples + HR, data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see the coefficients using &lt;code&gt;tidy&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs &amp;lt;- tidy(fit, conf.int = TRUE)

coefs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
##   term        estimate std.error statistic   p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)   -2.77     0.0862     -32.1 4.76e-157   -2.94     -2.60 
## 2 BB             0.371    0.0117      31.6 1.87e-153    0.348     0.394
## 3 singles        0.519    0.0127      40.8 8.67e-217    0.494     0.544
## 4 doubles        0.771    0.0226      34.1 8.44e-171    0.727     0.816
## 5 triples        1.24     0.0768      16.1 2.12e- 52    1.09      1.39 
## 6 HR             1.44     0.0243      59.3 0.           1.40      1.49&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function &lt;code&gt;predict&lt;/code&gt;, then make a plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Teams %&amp;gt;%
  filter(yearID %in% 2002) %&amp;gt;%
  mutate(BB = BB/G,
         singles = (H-X2B-X3B-HR)/G,
         doubles = X2B/G,
         triples =X3B/G,
         HR=HR/G,
         R=R/G)  %&amp;gt;%
  mutate(R_hat = predict(fit, newdata = .)) %&amp;gt;%
  ggplot(aes(R_hat, R, label = teamID)) +
  geom_point() +
  geom_text(nudge_x=0.1, cex = 2) +
  geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/model-predicts-runs-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our model does quite a good job as demonstrated by the fact that points from the observed versus predicted plot fall close to the identity line.&lt;/p&gt;
&lt;p&gt;So instead of using batting average, or just number of HR, as a measure of picking players, we can use our fitted model to form a metric that relates more directly to run production. Specifically, to define a metric for player A, we imagine a team made up of players just like player A and use our fitted regression model to predict how many runs this team would produce. The formula would look like this:
-2.7691857 +
0.3712147 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; BB +
0.5193923 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; singles +
0.7711444 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; doubles +
1.2399696 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; triples +
1.4433701 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; HR.&lt;/p&gt;
&lt;p&gt;To define a player-specific metric, we have a bit more work to do. A challenge here is that we derived the metric for teams, based on team-level summary statistics. For example, the HR value that is entered into the equation is HR per game for the entire team. If we compute the HR per game for a player, it will be much lower since the total is accumulated by 9 batters. Furthermore, if a player only plays part of the game and gets fewer opportunities than average, it is still considered a game played. For players, a rate that takes into account opportunities is the per-plate-appearance rate.&lt;/p&gt;
&lt;p&gt;To make the per-game team rate comparable to the per-plate-appearance player rate, we compute the average number of team plate appearances per game:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pa_per_game &amp;lt;- Batting %&amp;gt;% filter(yearID == 2002) %&amp;gt;%
  group_by(teamID) %&amp;gt;%
  summarize(pa_per_game = sum(AB+BB)/max(G)) %&amp;gt;%
  pull(pa_per_game) %&amp;gt;%
  mean&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We compute the per-plate-appearance rates for players available in 2002 on data from 1997-2001. To avoid small sample artifacts, we filter players with less than 200 plate appearances per year. Here is the entire calculation in one line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players &amp;lt;- Batting %&amp;gt;% filter(yearID %in% 1997:2001) %&amp;gt;%
  group_by(playerID) %&amp;gt;%
  mutate(PA = BB + AB) %&amp;gt;%
  summarize(G = sum(PA)/pa_per_game,
    BB = sum(BB)/G,
    singles = sum(H-X2B-X3B-HR)/G,
    doubles = sum(X2B)/G,
    triples = sum(X3B)/G,
    HR = sum(HR)/G,
    AVG = sum(H)/sum(AB),
    PA = sum(PA)) %&amp;gt;%
  filter(PA &amp;gt;= 1000) %&amp;gt;%
  select(-G) %&amp;gt;%
  mutate(R_hat = predict(fit, newdata = .))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The player-specific predicted runs computed here can be interpreted as the number of runs we predict a team will score if all batters are exactly like that player. The distribution shows that there is wide variability across players:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(R_hat, data = players, binwidth = 0.5, color = I(&amp;quot;black&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/r-hat-hist-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;adding-salary-and-position-information&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding salary and position information&lt;/h3&gt;
&lt;p&gt;To actually build the team, we will need to know their salaries as well as their defensive position. For this, we join the &lt;code&gt;players&lt;/code&gt; data frame we just created with the player information data frame included in some of the other Lahman data tables. We will learn more about the join function (and we will discuss this further in a later lecture).&lt;/p&gt;
&lt;p&gt;Start by adding the 2002 salary of each player:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players &amp;lt;- Salaries %&amp;gt;%
  filter(yearID == 2002) %&amp;gt;%
  select(playerID, salary) %&amp;gt;%
  right_join(players, by=&amp;quot;playerID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we add their defensive position. This is a somewhat complicated task because players play more than one position each year. The &lt;strong&gt;Lahman&lt;/strong&gt; package table &lt;code&gt;Appearances&lt;/code&gt; tells how many games each player played in each position, so we can pick the position that was most played using &lt;code&gt;which.max&lt;/code&gt; on each row. We use &lt;code&gt;apply&lt;/code&gt; to do this. However, because some players are traded, they appear more than once on the table, so we first sum their appearances across teams.
Here, we pick the one position the player most played using the &lt;code&gt;top_n&lt;/code&gt; function. To make sure we only pick one position, in the case of ties, we pick the first row of the resulting data frame. We also remove the &lt;code&gt;OF&lt;/code&gt; position which stands for outfielder, a generalization of three positions: left field (LF), center field (CF), and right field (RF). We also remove pitchers since they don’t bat in the league in which the A’s play.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;position_names &amp;lt;-
  paste0(&amp;quot;G_&amp;quot;, c(&amp;quot;p&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;1b&amp;quot;,&amp;quot;2b&amp;quot;,&amp;quot;3b&amp;quot;,&amp;quot;ss&amp;quot;,&amp;quot;lf&amp;quot;,&amp;quot;cf&amp;quot;,&amp;quot;rf&amp;quot;, &amp;quot;dh&amp;quot;))

tmp &amp;lt;- Appearances %&amp;gt;%
  filter(yearID == 2002) %&amp;gt;%
  group_by(playerID) %&amp;gt;%
  summarize_at(position_names, sum) %&amp;gt;%
  ungroup()

pos &amp;lt;- tmp %&amp;gt;%
  select(position_names) %&amp;gt;%
  apply(., 1, which.max)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Note: Using an external vector in selections is ambiguous.
## ℹ Use `all_of(position_names)` instead of `position_names` to silence this message.
## ℹ See &amp;lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&amp;gt;.
## This message is displayed once per session.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players &amp;lt;- tibble(playerID = tmp$playerID, POS = position_names[pos]) %&amp;gt;%
  mutate(POS = str_to_upper(str_remove(POS, &amp;quot;G_&amp;quot;))) %&amp;gt;%
  filter(POS != &amp;quot;P&amp;quot;) %&amp;gt;%
  right_join(players, by=&amp;quot;playerID&amp;quot;) %&amp;gt;%
  filter(!is.na(POS)  &amp;amp; !is.na(salary))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we add their first and last name:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players &amp;lt;- Master %&amp;gt;%
  select(playerID, nameFirst, nameLast, debut) %&amp;gt;%
  mutate(debut = as.Date(debut)) %&amp;gt;%
  right_join(players, by=&amp;quot;playerID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are a baseball fan, you will recognize the top 10 players:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players %&amp;gt;% select(nameFirst, nameLast, POS, salary, R_hat) %&amp;gt;%
  arrange(desc(R_hat)) %&amp;gt;% top_n(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Selecting by R_hat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    nameFirst nameLast POS   salary    R_hat
## 1      Barry    Bonds  LF 15000000 8.441480
## 2      Larry   Walker  RF 12666667 8.344316
## 3       Todd   Helton  1B  5000000 7.764649
## 4      Manny  Ramirez  LF 15462727 7.714582
## 5      Sammy     Sosa  RF 15000000 7.559582
## 6       Jeff  Bagwell  1B 11000000 7.405572
## 7       Mike   Piazza   C 10571429 7.343984
## 8      Jason   Giambi  1B 10428571 7.263690
## 9      Edgar Martinez  DH  7086668 7.259399
## 10       Jim    Thome  1B  8000000 7.231955&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;picking-nine-players&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Picking nine players&lt;/h3&gt;
&lt;p&gt;On average, players with a higher metric have higher salaries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;players %&amp;gt;% ggplot(aes(salary, R_hat, color = POS)) +
  geom_point() +
  scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/predicted-runs-vs-salary-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;!--Notice the very high salaries for most players. We do see some low-cost players with very high metrics. These will be great for our team. Some of these are likely young players that have not yet been able to negotiate a salary and are unavailable.

Here we remake plot without players that debuted before 1998. We use the __lubridate__ function `year`, introduced in Section \@ref(lubridate).

```r
library(lubridate)
players %&gt;% filter(year(debut) &lt; 1998) %&gt;%
 ggplot(aes(salary, R_hat, color = POS)) +
  geom_point() +
  scale_x_log10()
```

&lt;img src=&#34;/content/06-content_files/figure-html/predicted-runs-vs-salary-no-rookies-1.png&#34; width=&#34;672&#34; /&gt;
--&gt;
&lt;p&gt;We can search for good deals by looking at players who produce many more runs than others with similar salaries. We can use this table to decide what players to pick and keep our total salary below the 40 million dollars Billy Beane had to work with. This can be done using what computer scientists call linear programming. This is not something we teach, but here are the position players selected with this approach:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameFirst
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
POS
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
salary
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
R_hat
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Todd
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Helton
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1B
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.764649
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mike
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Piazza
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
C
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10571429
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.343984
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Edgar
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Martinez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
DH
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7086668
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.259399
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jim
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Edmonds
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CF
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7333333
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.552456
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jeff
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kent
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2B
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6000000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.391614
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Phil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nevin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
3B
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2600000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.163936
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Matt
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Stairs
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RF
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
500000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.062372
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Henry
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Rodriguez
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
LF
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
300000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.938315
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
John
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Valentin
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SS
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
550000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.273441
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that all these players have above average BB and most have above average HR rates, while the same is not true for singles. Here is a table with statistics standardized across players so that, for example, above average HR hitters have values above 0.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
BB
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
singles
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
doubles
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
triples
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
HR
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
AVG
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
R_hat
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Helton
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9088340
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.2147828
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.6489997
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.3105275
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.5221254
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.6704562
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.5316660
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Piazza
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3281058
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4231217
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2037161
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.4181571
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.8253653
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.1990055
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0890701
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Martinez
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.1352215
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0051702
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2649044
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.2242578
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8079817
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.2032836
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0000756
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Edmonds
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0706548
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5579104
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7912381
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1517126
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9730052
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8543566
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2562767
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kent
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2316321
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7322902
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0113988
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4483097
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7658693
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7871932
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0870488
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Nevin
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3066863
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9051225
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4787634
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1908955
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1927055
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1048721
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8475017
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Stairs
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0996635
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5127562
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0460876
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.1285395
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1209081
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5608456
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7406428
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Rodriguez
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2011513
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.5963595
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3324557
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.7823620
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.3202734
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.6723416
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6101181
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Valentin
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1802855
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.9287069
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.7940379
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4348410
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0452462
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.4717038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.0894187
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-regression-fallacy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The regression fallacy&lt;/h2&gt;
&lt;p&gt;Wikipedia defines the &lt;em&gt;sophomore slump&lt;/em&gt; as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The &lt;em&gt;sophmore slump&lt;/em&gt; phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this Fox Sports article&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”.&lt;/p&gt;
&lt;p&gt;Does the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for batting average, we see that this observation holds true for the top performing ROYs:&lt;/p&gt;
&lt;!--The data is available in the Lahman library, but we have to do some work to create a table with the statistics for all the ROY. First we create a table with player ID, their names, and their most played position.--&gt;
&lt;!--
Now, we will create a table with only the ROY award winners and add their batting statistics. We filter out pitchers, since pitchers are not given awards for batting and we are going to focus on offense. Specifically, we will focus on batting average since it is the summary that most pundits talk about when discussing the sophomore slump:
--&gt;
&lt;!--
We also will keep only the rookie and sophomore seasons and remove players that did not play sophomore seasons:
--&gt;
&lt;!--
Finally, we will use the `spread` function to have one column for the rookie and sophomore years batting averages:
--&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameFirst
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rookie_year
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rookie
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sophomore
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Willie
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
McCovey
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1959
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3541667
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2384615
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ichiro
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Suzuki
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2001
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3497110
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3214838
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Al
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Bumbry
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1973
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3370787
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2333333
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fred
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Lynn
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1975
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3314394
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3136095
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Albert
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pujols
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2001
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3288136
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3135593
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In fact, the proportion of players that have a lower batting average their sophomore year is 0.6862745.&lt;/p&gt;
&lt;p&gt;So is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).&lt;/p&gt;
&lt;!--We perform similar operations to what we did above: --&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;playerID&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same pattern arises when we look at the top performers: batting averages go down for most of the top performers.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameFirst
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2013
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2014
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Miguel
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cabrera
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3477477
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3126023
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Hanley
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Ramirez
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3453947
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2828508
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Michael
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Cuddyer
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3312883
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3315789
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Scooter
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Gennett
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3239437
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2886364
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Joe
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mauer
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3235955
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2769231
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But these are not rookies! Also, look at what happens to the worst performers of 2013:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameFirst
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
nameLast
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2013
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2014
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Danny
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Espinosa
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1582278
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2192192
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Dan
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Uggla
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1785714
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1489362
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jeff
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mathis
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1810345
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2000000
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
B. J.
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Upton
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1841432
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2080925
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adam
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Rosales
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1904762
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2621951
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Their batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/regression-fallacy-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The correlation is 0.460254 and
the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for any given player that had a 2013 batting average &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{Y - .255}{.032} = 0.46 \left( \frac{X - .261}{.023}\right) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; so it is expected that &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; will regress to the mean.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-error-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measurement error models&lt;/h2&gt;
&lt;p&gt;Up to now, all our linear regression examples have been applied to two or more random variables. We assume the pairs are bivariate normal and use this to motivate a linear model. This approach covers most real-life examples of linear regression. The other major application comes from measurement errors models. In these applications, it is common to have a non-random covariate, such as time, and randomness is introduced from measurement error rather than sampling or natural variability.&lt;/p&gt;
&lt;p&gt;To understand these models, imagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let’s simulate some data using the equations we know today and adding some measurement error. The &lt;strong&gt;dslabs&lt;/strong&gt; function &lt;code&gt;rfalling_object&lt;/code&gt; generates these simulations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
falling_object &amp;lt;- rfalling_object()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The assistants hand the data to Galileo and this is what he sees:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;falling_object %&amp;gt;%
  ggplot(aes(time, observed_distance)) +
  geom_point() +
  ylab(&amp;quot;Distance in meters&amp;quot;) +
  xlab(&amp;quot;Time in seconds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/gravity-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Galileo does not know the exact equation, but by looking at the plot above, he deduces that the position should follow a parabola, which we can write like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(x) = \beta_0 + \beta_1 x + \beta_2 x^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The data does not fall exactly on a parabola. Galileo knows this is due to measurement error. His helpers make mistakes when measuring the distance. To account for this, he models the data with:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i, i=1,\dots,n \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt; representing distance in meters, &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; representing time in seconds, and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; accounting for measurement error. The measurement error is assumed to be random, independent from each other, and having the same distribution for each &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. We also assume that there is no bias, which means the expected value &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}[\varepsilon] = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that this is a linear model because it is a linear combination of known quantities (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x^2\)&lt;/span&gt; are known) and unknown parameters (the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;s are unknown parameters to Galileo). Unlike our previous examples, here &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is a fixed quantity; we are not conditioning.&lt;/p&gt;
&lt;p&gt;To pose a new physical theory and start making predictions about other falling objects, Galileo needs actual numbers, rather than unknown parameters. Using LSE seems like a reasonable approach. How do we find the LSE?&lt;/p&gt;
&lt;p&gt;LSE calculations do not require the errors to be approximately normal. The &lt;code&gt;lm&lt;/code&gt; function will find the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; s that will minimize the residual sum of squares:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- falling_object %&amp;gt;%
  mutate(time_sq = time^2) %&amp;gt;%
  lm(observed_distance~time+time_sq, data=.)
tidy(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)   56.1       0.592    94.9   2.23e-17
## 2 time          -0.786     0.845    -0.930 3.72e- 1
## 3 time_sq       -4.53      0.251   -18.1   1.58e- 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check if the estimated parabola fits the data. The &lt;strong&gt;broom&lt;/strong&gt; function &lt;code&gt;augment&lt;/code&gt; lets us do this easily:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;augment(fit) %&amp;gt;%
  ggplot() +
  geom_point(aes(time, observed_distance)) +
  geom_line(aes(time, .fitted), col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/06-content_files/figure-html/falling-object-fit-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Thanks to my high school physics teacher, I know that the equation for the trajectory of a falling object is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d = h_0 + v_0 t -  0.5 \times 9.8 t^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(h_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v_0\)&lt;/span&gt; the starting height and velocity, respectively. The data we simulated above followed this equation and added measurement error to simulate &lt;code&gt;n&lt;/code&gt; observations for dropping the ball &lt;span class=&#34;math inline&#34;&gt;\((v_0=0)\)&lt;/span&gt; from the tower of Pisa &lt;span class=&#34;math inline&#34;&gt;\((h_0=55.86)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These are consistent with the parameter estimates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit, conf.int = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 (Intercept)   56.1       0.592    94.9   2.23e-17    54.8      57.4 
## 2 time          -0.786     0.845    -0.930 3.72e- 1    -2.65      1.07
## 3 time_sq       -4.53      0.251   -18.1   1.58e- 9    -5.08     -3.98&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Tower of Pisa height is within the confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, the initial velocity 0 is in the confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; (note the p-value is larger than 0.05), and the acceleration constant is in a confidence interval for &lt;span class=&#34;math inline&#34;&gt;\(-2 \times \beta_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\mbox{BB}}{\mbox{PA}} + \frac{\mbox{Singles} + 2 \mbox{Doubles} + 3 \mbox{Triples} + 4\mbox{HR}}{\mbox{AB}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;They called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Compute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For every year since 1961, compute the correlation between runs per game and OPS; then plot these correlations as a function of year.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note that we can rewrite OPS as a weighted average of BBs, singles, doubles, triples, and HRs. We know that the weights for doubles, triples, and HRs are 2, 3, and 4 times that of singles. But what about BB? What is the weight for BB relative to singles? Hint: the weight for BB relative to singles will be a function of AB and PA.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Note that the weight for BB, &lt;span class=&#34;math inline&#34;&gt;\(\frac{\mbox{AB}}{\mbox{PA}}\)&lt;/span&gt;, will change from team to team. To see how variable it is, compute and plot this quantity for each team for each year since 1961. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;So now we know that the formula for OPS is proportional to &lt;span class=&#34;math inline&#34;&gt;\(0.91 \times \mbox{BB} + \mbox{singles} + 2 \times \mbox{doubles} + 3 \times \mbox{triples} + 4 \times \mbox{HR}\)&lt;/span&gt;. Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1961, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1961, compute the OPS, the predicted runs with the regression model and compute the correlation between the two as well as the correlation with runs per game.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://mlb.mlb.com/stats/league_leaders.jsp&#34; class=&#34;uri&#34;&gt;http://mlb.mlb.com/stats/league_leaders.jsp&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Bill_James&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Bill_James&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Sabermetrics&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Sabermetrics&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/User:Cburnett&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/User:Cburnett&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/3.0/deed.en&#34; class=&#34;uri&#34;&gt;https://creativecommons.org/licenses/by-sa/3.0/deed.en&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.flickr.com/people/27003603@N00&#34; class=&#34;uri&#34;&gt;https://www.flickr.com/people/27003603@N00&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/2.0&#34; class=&#34;uri&#34;&gt;https://creativecommons.org/licenses/by-sa/2.0&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.baseball-almanac.com/awards/lou_brock_award.shtml&#34; class=&#34;uri&#34;&gt;http://www.baseball-almanac.com/awards/lou_brock_award.shtml&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715&#34; class=&#34;uri&#34;&gt;http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715&lt;/a&gt;&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Regression</title>
      <link>/content/05-content/</link>
      <pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/content/05-content/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#supplemental-readings&#34;&gt;Supplemental Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#regression&#34;&gt;Introduction to Linear Regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-is-height-hereditary&#34;&gt;Case study: is height hereditary?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#corr-coef&#34;&gt;The correlation coefficient&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sample-correlation-is-a-random-variable&#34;&gt;Sample correlation is a random variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#correlation-is-not-always-a-useful-summary&#34;&gt;Correlation is not always a useful summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-expectation&#34;&gt;Conditional expectations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-regression-line&#34;&gt;The regression line&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#regression-improves-precision&#34;&gt;Regression improves precision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bivariate-normal-distribution-advanced&#34;&gt;Bivariate normal distribution (advanced)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance-explained&#34;&gt;Variance explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#warning-there-are-two-regression-lines&#34;&gt;Warning: there are two regression lines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;supplemental-readings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Supplemental Readings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Coming soon.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Coming soon.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slides&lt;/h3&gt;
&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;slide-tabs&#34; role=&#34;tablist&#34;&gt;
&lt;li class=&#34;nav-item&#34;&gt;
&lt;a class=&#34;nav-link active&#34; id=&#34;introduction-tab&#34; data-toggle=&#34;tab&#34; href=&#34;#introduction&#34; role=&#34;tab&#34; aria-controls=&#34;introduction&#34; aria-selected=&#34;true&#34;&gt;Introduction&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;slide-tabs&#34; class=&#34;tab-content&#34;&gt;
&lt;div id=&#34;introduction&#34; class=&#34;tab-pane fade show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;introduction-tab&#34;&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;/slides/05-slides.html#Introduction&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Today’s lecture will ask you to touch real data during the lecture. Please download the following dataset and load it into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This dataset is from houses in Ames, Iowa. (Thrilling!) We will use this dataset during the lecture to illustrate some of the points discussed below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction to Linear Regression&lt;/h1&gt;
&lt;p&gt;Up to this point, this class has focused mainly on single variables. However, in data analytics applications, it is very common to be interested in the &lt;strong&gt;relationship&lt;/strong&gt; between two or more variables. For instance, in the coming days we will use a data-driven approach that examines the relationship between player statistics and success to guide the building of a baseball team with a limited budget. Before delving into this more complex example, we introduce necessary concepts needed to understand regression using a simpler illustration. We actually use the dataset from which regression was born.&lt;/p&gt;
&lt;p&gt;The example is from genetics. Francis Galton&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity. While doing this, he developed the concepts of correlation and regression, as well as a connection to pairs of data that follow a normal distribution. Of course, at the time this data was collected our knowledge of genetics was quite limited compared to what we know today. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height? The technique he developed to answer this question, regression, can also be applied to our baseball question. Regression can be applied in many other circumstances as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Historical note&lt;/strong&gt;: Galton made important contributions to statistics and genetics, but he was also one of the first proponents of eugenics, a scientifically flawed philosophical movement favored by many biologists of Galton’s time but with horrific historical consequences. These consequences still reverberate to this day, and form the basis for much of the Western world’s racist policies. You can read more about it here: &lt;a href=&#34;https://pged.org/history-eugenics-and-genetics/&#34;&gt;https://pged.org/history-eugenics-and-genetics/&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;case-study-is-height-hereditary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: is height hereditary?&lt;/h2&gt;
&lt;p&gt;We have access to Galton’s family height data through the &lt;strong&gt;HistData&lt;/strong&gt; package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(HistData)
data(&amp;quot;GaltonFamilies&amp;quot;)

set.seed(1983)
galton_heights &amp;lt;- GaltonFamilies %&amp;gt;%
  filter(gender == &amp;quot;male&amp;quot;) %&amp;gt;%
  group_by(family) %&amp;gt;%
  sample_n(1) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(father, childHeight) %&amp;gt;%
  rename(son = childHeight)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the exercises, we will look at other relationships including mothers and daughters.&lt;/p&gt;
&lt;p&gt;Suppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;%
  summarize(mean(father), sd(father), mean(son), sd(son))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 4
##   `mean(father)` `sd(father)` `mean(son)` `sd(son)`
##            &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1           69.1         2.55        69.2      2.71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;% ggplot(aes(father, son)) +
  geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/scatterplot-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We will learn that the correlation coefficient is an informative summary of how two variables move together and then see how this can be used to predict one variable using the other.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;corr-coef&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The correlation coefficient&lt;/h2&gt;
&lt;p&gt;The correlation coefficient is defined for a list of pairs &lt;span class=&#34;math inline&#34;&gt;\((x_1, y_1), \dots, (x_n,y_n)\)&lt;/span&gt; as the average of the product of the standardized values:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)
\]&lt;/span&gt;
with &lt;span class=&#34;math inline&#34;&gt;\(\mu_x, \mu_y\)&lt;/span&gt; the averages of &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots, x_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_1, \dots, y_n\)&lt;/span&gt;, respectively, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x, \sigma_y\)&lt;/span&gt; the standard deviations. The Greek letter &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is commonly used in statistics books to denote the correlation. The Greek letter for &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, because it is the first letter of regression. Soon we learn about the connection between correlation and regression. We can represent the formula above with R code using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rho &amp;lt;- mean(scale(x) * scale(y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To understand why this equation does in fact summarize how two variables move together, consider the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th entry of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\left( \frac{x_i-\mu_x}{\sigma_x} \right)\)&lt;/span&gt; SDs away from the average. Similarly, the &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; that is paired with &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, is &lt;span class=&#34;math inline&#34;&gt;\(\left( \frac{y_1-\mu_y}{\sigma_y} \right)\)&lt;/span&gt; SDs away from the average &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are unrelated, the product &lt;span class=&#34;math inline&#34;&gt;\(\left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)\)&lt;/span&gt; will be positive ( &lt;span class=&#34;math inline&#34;&gt;\(+ \times +\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(- \times -\)&lt;/span&gt; ) as often as negative (&lt;span class=&#34;math inline&#34;&gt;\(+ \times -\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(- \times +\)&lt;/span&gt;) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products ( &lt;span class=&#34;math inline&#34;&gt;\(+ \times +\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(- \times -\)&lt;/span&gt;) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation.&lt;/p&gt;
&lt;p&gt;The correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)^2 =
\frac{1}{\sigma_x^2} \frac{1}{n} \sum_{i=1}^n \left( x_i-\mu_x \right)^2 =
\frac{1}{\sigma_x^2} \sigma^2_x =
1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A similar derivation, but with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and its exact opposite, proves the correlation has to be bigger or equal to -1.&lt;/p&gt;
&lt;p&gt;For other pairs, the correlation is in between -1 and 1. The correlation between father and son’s heights is about 0.5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;% summarize(r = cor(father, son)) %&amp;gt;% pull(r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4334102&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see what data looks like for different values of &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, here are six examples of pairs with correlations ranging from -0.9 to 0.99:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/what-correlation-looks-like-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;sample-correlation-is-a-random-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sample correlation is a random variable&lt;/h3&gt;
&lt;p&gt;Before we continue connecting correlation to regression, let’s remind ourselves about random variability.&lt;/p&gt;
&lt;p&gt;In most data science applications, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest but rather for a random sample. As with the average and standard deviation, the &lt;em&gt;sample correlation&lt;/em&gt; is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.&lt;/p&gt;
&lt;p&gt;By way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;R &amp;lt;- sample_n(galton_heights, 25, replace = TRUE) %&amp;gt;%
  summarize(r = cor(father, son)) %&amp;gt;% pull(r)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is a random variable. We can run a Monte Carlo simulation to see its distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 1000
N &amp;lt;- 25
R &amp;lt;- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %&amp;gt;%
    summarize(r=cor(father, son)) %&amp;gt;%
    pull(r)
})
qplot(R, geom = &amp;quot;histogram&amp;quot;, binwidth = 0.05, color = I(&amp;quot;black&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/sample-correlation-distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the expected value of &lt;code&gt;R&lt;/code&gt; is the population correlation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4307393&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and that it has a relatively high standard error relative to the range of values &lt;code&gt;R&lt;/code&gt; can take:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(R)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1609393&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.&lt;/p&gt;
&lt;p&gt;Also, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, the distribution of &lt;code&gt;R&lt;/code&gt; is approximately normal with expected value &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. The standard deviation, which is somewhat complex to derive, is &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{1-r^2}{N-2}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In our example, &lt;span class=&#34;math inline&#34;&gt;\(N=25\)&lt;/span&gt; does not seem to be large enough to make the approximation a good one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(aes(sample=R), data = data.frame(R)) +
  stat_qq() +
  geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/small-sample-correlation-not-normal-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you increase &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, you will see the distribution converging to normal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-is-not-always-a-useful-summary&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correlation is not always a useful summary&lt;/h3&gt;
&lt;p&gt;Correlation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/ascombe-quartet-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Correlation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-expectation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional expectations&lt;/h2&gt;
&lt;p&gt;Suppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?&lt;/p&gt;
&lt;p&gt;It turns out that if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.&lt;/p&gt;
&lt;p&gt;In general, we call this approach &lt;em&gt;conditioning&lt;/em&gt;. The general idea is that we stratify a population into groups and compute summaries in each group. To provide a mathematical description of conditioning, consider we have a population of pairs of values &lt;span class=&#34;math inline&#34;&gt;\((x_1,y_1),\dots,(x_n,y_n)\)&lt;/span&gt;, for example all father and son heights in England. In the previous week’s content, we learned that if you take a random pair &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;, the expected value and best predictor of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}(Y) = \mu_y\)&lt;/span&gt;, the population average &lt;span class=&#34;math inline&#34;&gt;\(1/n\sum_{i=1}^n y_i\)&lt;/span&gt;. However, we are no longer interested in the general population, instead we are interested in only the subset of a population with a specific &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; value, 72 inches in our example. This subset of the population, is also a population and thus the same principles and properties we have learned apply. The &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; in the subpopulation have a distribution, referred to as the &lt;em&gt;conditional distribution&lt;/em&gt;, and this distribution has an expected value referred to as the &lt;em&gt;conditional expectation&lt;/em&gt;. In our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches. The statistical notation for the conditional expectation is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}(Y \mid X = x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; representing the fixed value that defines that subset, for example 72 inches. Similarly, we denote the standard deviation of the strata with&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SD}(Y \mid X = x) = \sqrt{\mbox{Var}(Y \mid X = x)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because the conditional expectation &lt;span class=&#34;math inline&#34;&gt;\(E(Y\mid X=x)\)&lt;/span&gt; is the best predictor for the random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for an individual in the strata defined by &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt;, many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.&lt;/p&gt;
&lt;p&gt;In the example we have been considering, we are interested in computing the average son height &lt;em&gt;conditioned&lt;/em&gt; on the father being 72 inches tall. We want to estimate &lt;span class=&#34;math inline&#34;&gt;\(E(Y|X=72)\)&lt;/span&gt; using the sample collected by Galton. We previously learned that the sample average is the preferred approach to estimating the population average. However, a challenge when using this approach to estimating conditional expectations is that for continuous data we don’t have many data points matching exactly one value in our sample. For example, we have only:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(galton_heights$father == 72)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;fathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(galton_heights$father == 72.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conditional_avg &amp;lt;- galton_heights %&amp;gt;%
  filter(round(father) == 72) %&amp;gt;%
  summarize(avg = mean(son)) %&amp;gt;%
  pull(avg)
conditional_avg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 70.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that a 72-inch father is taller than average – specifically, 72 - 69.1/2.5 =
1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son. The sons of 72-inch fathers have &lt;em&gt;regressed&lt;/em&gt; some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later lecture, this is not a coincidence.&lt;/p&gt;
&lt;p&gt;If we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;% mutate(father_strata = factor(round(father))) %&amp;gt;%
  ggplot(aes(father_strata, son)) +
  geom_boxplot() +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/boxplot-1-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not surprisingly, the centers of the groups are increasing with height.
Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/conditional-averages-follow-line-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the &lt;em&gt;regression line&lt;/em&gt;, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-regression-line&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The regression line&lt;/h2&gt;
&lt;p&gt;If we are predicting a random variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; knowing the value of another &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt; using a regression line, then we predict that for every standard deviation, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_X\)&lt;/span&gt;, that &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; increases above the average &lt;span class=&#34;math inline&#34;&gt;\(\mu_X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; increase &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; standard deviations &lt;span class=&#34;math inline&#34;&gt;\(\sigma_Y\)&lt;/span&gt; above the average &lt;span class=&#34;math inline&#34;&gt;\(\mu_Y\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; the correlation between &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. The formula for the regression is therefore:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left( \frac{Y-\mu_Y}{\sigma_Y} \right) = \rho \left( \frac{x-\mu_X}{\sigma_X} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can rewrite it like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y = \mu_Y + \rho \left( \frac{x-\mu_X}{\sigma_X} \right) \sigma_Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; at all for the prediction and simply predict the average &lt;span class=&#34;math inline&#34;&gt;\(\mu_Y\)&lt;/span&gt;. For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.&lt;/p&gt;
&lt;p&gt;Note that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, is to the average of the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;s. This is why we call it &lt;em&gt;regression&lt;/em&gt;: the son regresses to the average height. In fact, the title of Galton’s paper was: &lt;em&gt;Regression toward mediocrity in hereditary stature&lt;/em&gt;. To add regression lines to plots, we will need the above formula in the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y= b + mx \mbox{ with slope } m = \rho \frac{\sigma_y}{\sigma_x} \mbox{ and intercept } b=\mu_y - m \mu_x
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we add the regression line to the original data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu_x &amp;lt;- mean(galton_heights$father)
mu_y &amp;lt;- mean(galton_heights$son)
s_x &amp;lt;- sd(galton_heights$father)
s_y &amp;lt;- sd(galton_heights$son)
r &amp;lt;- cor(galton_heights$father, galton_heights$son)

galton_heights %&amp;gt;%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/regression-line-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. You can make same plot, but using standard units like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;%
  ggplot(aes(scale(father), scale(son))) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = r)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;regression-improves-precision&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regression improves precision&lt;/h3&gt;
&lt;p&gt;Let’s compare the two approaches to prediction that we have presented:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Round fathers’ heights to closest inch, stratify, and then take the average.&lt;/li&gt;
&lt;li&gt;Compute the regression line and use it to predict.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We use a Monte Carlo simulation sampling &lt;span class=&#34;math inline&#34;&gt;\(N=50\)&lt;/span&gt; families:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 1000
N &amp;lt;- 50

set.seed(1983)
conditional_avg &amp;lt;- replicate(B, {
  dat &amp;lt;- sample_n(galton_heights, N)
  dat %&amp;gt;% filter(round(father) == 72) %&amp;gt;%
    summarize(avg = mean(son)) %&amp;gt;%
    pull(avg)
  })

regression_prediction &amp;lt;- replicate(B, {
  dat &amp;lt;- sample_n(galton_heights, N)
  mu_x &amp;lt;- mean(dat$father)
  mu_y &amp;lt;- mean(dat$son)
  s_x &amp;lt;- sd(dat$father)
  s_y &amp;lt;- sd(dat$son)
  r &amp;lt;- cor(dat$father, dat$son)
  mu_y + r*(72 - mu_x)/s_x*s_y
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although the expected value of these two random variables is about the same:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(conditional_avg, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 70.49368&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(regression_prediction)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 70.50941&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The standard error for the regression prediction is substantially smaller:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(conditional_avg, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9635814&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(regression_prediction)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4520833&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use &lt;code&gt;na.rm=TRUE&lt;/code&gt;. The regression always uses all the data.&lt;/p&gt;
&lt;p&gt;So why not always use the regression for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The justification, which we include in the next section, is somewhat more advanced than the rest of the chapter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-normal-distribution-advanced&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bivariate normal distribution (advanced)&lt;/h3&gt;
&lt;p&gt;Correlation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.&lt;/p&gt;
&lt;p&gt;The main way we motivate the use of correlation involves what is called the &lt;em&gt;bivariate normal distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;When a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. These ovals can be thin (high correlation) or circle-shaped (no correlation).&lt;/p&gt;
&lt;!--
&lt;img src=&#34;/content/05-content_files/figure-html/bivariate-ovals-1.png&#34; width=&#34;672&#34; /&gt;
--&gt;
&lt;p&gt;A more technical way to define the bivariate normal distribution is the following: if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a normally distributed random variable, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is also a normally distributed random variable, and the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt; is approximately normal, then the pair is approximately bivariate normal.&lt;/p&gt;
&lt;p&gt;If we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;%
  mutate(z_father = round((father - mean(father)) / sd(father))) %&amp;gt;%
  filter(z_father %in% -2:2) %&amp;gt;%
  ggplot() +
  stat_qq(aes(sample = son)) +
  facet_wrap( ~ z_father)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/qqnorm-of-strata-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the expected value of the &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; in pairs for which &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}(Y | X=x) = \mu_Y +  \rho \frac{X-\mu_X}{\sigma_X}\sigma_Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the regression line, with slope &lt;span class=&#34;math display&#34;&gt;\[\rho \frac{\sigma_Y}{\sigma_X}\]&lt;/span&gt; and intercept &lt;span class=&#34;math inline&#34;&gt;\(\mu_y - m\mu_X\)&lt;/span&gt;. It is equivalent to the regression equation we showed earlier which can be written like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\mbox{E}(Y \mid X=x)  - \mu_Y}{\sigma_Y} = \rho \frac{x-\mu_X}{\sigma_X}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.&lt;/p&gt;
&lt;p&gt;In summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given we know the value of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, is given by the regression line.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-explained&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance explained&lt;/h3&gt;
&lt;p&gt;The bivariate normal theory also tells us that the standard deviation of the &lt;em&gt;conditional&lt;/em&gt; distribution described above is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SD}(Y \mid X=x ) = \sigma_Y \sqrt{1-\rho^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To see why this is intuitive, notice that without conditioning, &lt;span class=&#34;math inline&#34;&gt;\(\mbox{SD}(Y) = \sigma_Y\)&lt;/span&gt;, we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.&lt;/p&gt;
&lt;p&gt;Specifically, it is reduced to &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{1-\rho^2} = \sqrt{1 - 0.25}\)&lt;/span&gt; = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.&lt;/p&gt;
&lt;p&gt;The statement “&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by &lt;span class=&#34;math inline&#34;&gt;\(1-\rho^2\)&lt;/span&gt;, so we say that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; explains &lt;span class=&#34;math inline&#34;&gt;\(1- (1-\rho^2)=\rho^2\)&lt;/span&gt; (the correlation squared) of the variance.&lt;/p&gt;
&lt;p&gt;But it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;warning-there-are-two-regression-lines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Warning: there are two regression lines&lt;/h3&gt;
&lt;p&gt;We computed a regression line to predict the son’s height from father’s height. We used these calculations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu_x &amp;lt;- mean(galton_heights$father)
mu_y &amp;lt;- mean(galton_heights$son)
s_x &amp;lt;- sd(galton_heights$father)
s_y &amp;lt;- sd(galton_heights$son)
r &amp;lt;- cor(galton_heights$father, galton_heights$son)
m_1 &amp;lt;-  r * s_y / s_x
b_1 &amp;lt;- mu_y - m_1*mu_x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which gives us the function &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}(Y\mid X=x) =\)&lt;/span&gt; 37.3 + 0.46 &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;What if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function:
&lt;span class=&#34;math inline&#34;&gt;\(x = \{ \mbox{E}(Y\mid X=x) -\)&lt;/span&gt; 37.3 &lt;span class=&#34;math inline&#34;&gt;\(\} /\)&lt;/span&gt; 0.5.&lt;/p&gt;
&lt;p&gt;We need to compute &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}(X \mid Y=y)\)&lt;/span&gt;. Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m_2 &amp;lt;-  r * s_x / s_y
b_2 &amp;lt;- mu_x - m_2 * mu_y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we get &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}(X \mid Y=y) =\)&lt;/span&gt; 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is to the son average.&lt;/p&gt;
&lt;p&gt;Here is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;galton_heights %&amp;gt;%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = b_1, slope = m_1, col = &amp;quot;blue&amp;quot;) +
  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/05-content_files/figure-html/two-regression-lines-1.png&#34; width=&#34;40%&#34; /&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Load the &lt;code&gt;GaltonFamilies&lt;/code&gt; data from the &lt;strong&gt;HistData&lt;/strong&gt;. The children in each family are listed by gender and then by height. Create a dataset called &lt;code&gt;galton_heights&lt;/code&gt; by picking a male and female at random.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Francis_Galton&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Francis_Galton&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Probability and Statistics</title>
      <link>/content/04-content/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/content/04-content/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#supplemental-readings&#34;&gt;Supplemental Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discrete-probability&#34;&gt;Discrete probability&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#relative-frequency&#34;&gt;Relative frequency&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#notation&#34;&gt;Notation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#probability-distributions&#34;&gt;Probability distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monte-carlo-simulations-for-categorical-data&#34;&gt;Monte Carlo simulations for categorical data&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#setting-the-random-seed&#34;&gt;Setting the random seed&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#with-and-without-replacement&#34;&gt;With and without replacement&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#independence&#34;&gt;Independence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-probabilities&#34;&gt;Conditional probabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#addition-and-multiplication-rules&#34;&gt;Addition and multiplication rules&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#multiplication-rule&#34;&gt;Multiplication rule&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiplication-rule-under-independence&#34;&gt;Multiplication rule under independence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#addition-rule&#34;&gt;Addition rule&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#combinations-and-permutations&#34;&gt;Combinations and permutations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#monte-carlo-example&#34;&gt;Monte Carlo example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examples&#34;&gt;Examples&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#monty-hall-problem&#34;&gt;Monty Hall problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#birthday-problem&#34;&gt;Birthday problem&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#infinity-in-practice&#34;&gt;Infinity in practice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#theoretical-continuous-distributions&#34;&gt;Theoretical continuous distributions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#theoretical-distributions-as-approximations&#34;&gt;Theoretical distributions as approximations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-probability-density&#34;&gt;The probability density&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monte-carlo-simulations-for-continuous-variables&#34;&gt;Monte Carlo simulations for continuous variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-distributions&#34;&gt;Continuous distributions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-variables&#34;&gt;Random variables&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#definition-of-random-variables&#34;&gt;Definition of Random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sampling-models&#34;&gt;Sampling models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-probability-distribution-of-a-random-variable&#34;&gt;The probability distribution of a random variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distributions-versus-probability-distributions&#34;&gt;Distributions versus probability distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#notation-for-random-variables&#34;&gt;Notation for random variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-expected-value-and-standard-error&#34;&gt;The expected value and standard error&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#population-sd-versus-the-sample-sd&#34;&gt;Population SD versus the sample SD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#central-limit-theorem&#34;&gt;Central Limit Theorem&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#how-large-is-large-in-the-central-limit-theorem&#34;&gt;How large is large in the Central Limit Theorem?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#statistical-properties-of-averages&#34;&gt;Statistical properties of averages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#law-of-large-numbers&#34;&gt;Law of large numbers&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#misinterpreting-law-of-averages&#34;&gt;Misinterpreting law of averages&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;supplemental-readings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Supplemental Readings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://hbr.org/2016/11/why-its-so-hard-for-us-to-visualize-uncertainty&#34;&gt;Why It’s So Hard for Us to Visualize Uncertainty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fab fa-youtube&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=0L1tGo-DvD0&#34;&gt;Amanda Cox’s keynote address at the 2017 OpenVis Conf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://eagereyes.org/blog/2017/communicating-uncertainty-when-lives-are-on-the-line&#34;&gt;Communicating Uncertainty When Lives Are on the Line&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://flowingdata.com/2016/11/15/showing-uncertainty-during-the-live-election-forecast/&#34;&gt;Showing uncertainty during the live election forecast&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://flowingdata.com/2017/06/27/trolling-the-uncertainty-dial/&#34;&gt;Trolling the uncertainty dial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Why is uncertainty inherently a major part of data analytics?&lt;/li&gt;
&lt;li&gt;How have past attempts to visualize uncertainty failed?&lt;/li&gt;
&lt;li&gt;What is the right way to visualize election uncertainty?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slides&lt;/h3&gt;
&lt;p&gt;As with last week, today’s lecture will ask you to work with &lt;code&gt;R&lt;/code&gt; during the lecture.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discrete-probability&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discrete probability&lt;/h1&gt;
&lt;p&gt;We start by covering some basic principles related to categorical data. The subset of probability is referred to as &lt;em&gt;discrete probability&lt;/em&gt;. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications. Discrete probability is more useful in card games and therefore we use these as examples.&lt;/p&gt;
&lt;div id=&#34;relative-frequency&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Relative frequency&lt;/h3&gt;
&lt;p&gt;The word probability is used in everyday language. Answering questions about probability is often hard, if not impossible. Here we discuss a mathematical definition of &lt;em&gt;probability&lt;/em&gt; that does permit us to give precise answers to certain questions.&lt;/p&gt;
&lt;p&gt;For example, if I have 2 red beads and 3 blue beads inside an urn&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; (most probability books use this archaic term, so we do too) and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is .4 for red and .6 for blue.&lt;/p&gt;
&lt;p&gt;A more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Notation&lt;/h3&gt;
&lt;p&gt;We use the notation &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(A)\)&lt;/span&gt; to denote the probability of event &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; happening. We use the very general term &lt;em&gt;event&lt;/em&gt; to refer to things that can happen when something occurs by chance. In our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”.&lt;/p&gt;
&lt;p&gt;In data science applications, we will often deal with continuous variables. These events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: &lt;span class=&#34;math inline&#34;&gt;\(X \geq 6\)&lt;/span&gt;. We will see more of these examples later. Here we focus on categorical data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;probability-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Probability distributions&lt;/h3&gt;
&lt;p&gt;If we know the relative frequency of the different categories, defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution.&lt;/p&gt;
&lt;p&gt;If we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided, and 2% Green Party, these proportions define the probability for each group. The probability distribution is:&lt;/p&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Pr(picking a Republican)&lt;/td&gt;
&lt;td&gt;=&lt;/td&gt;
&lt;td&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Pr(picking a Democrat)&lt;/td&gt;
&lt;td&gt;=&lt;/td&gt;
&lt;td&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Pr(picking an undecided)&lt;/td&gt;
&lt;td&gt;=&lt;/td&gt;
&lt;td&gt;0.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Pr(picking a Green)&lt;/td&gt;
&lt;td&gt;=&lt;/td&gt;
&lt;td&gt;0.02&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;monte-carlo-simulations-for-categorical-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo simulations for categorical data&lt;/h2&gt;
&lt;p&gt;Computers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.&lt;/p&gt;
&lt;p&gt;An example is the &lt;code&gt;sample&lt;/code&gt; function in R. We demonstrate its use in the code below. First, we use the function &lt;code&gt;rep&lt;/code&gt; to generate the urn:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beads &amp;lt;- rep(c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), times = c(2,3))
beads&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;red&amp;quot;  &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then use &lt;code&gt;sample&lt;/code&gt; to pick a bead at random:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;blue&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This line of code produces one random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat forever. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to repeating forever. &lt;strong&gt;This is an example of a &lt;em&gt;Monte Carlo&lt;/em&gt; simulation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Much of what mathematical and theoretical statisticians study, which we do not cover in this class, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this lecture, we provide a practical approach to deciding what is “large enough”.&lt;/p&gt;
&lt;p&gt;To perform our first Monte Carlo simulation, we use the &lt;code&gt;replicate&lt;/code&gt; function, which permits us to repeat the same task any number of times. Here, we repeat the random event &lt;span class=&#34;math inline&#34;&gt;\(B =\)&lt;/span&gt; 10,000 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
events &amp;lt;- replicate(B, sample(beads, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use &lt;code&gt;table&lt;/code&gt; to see the distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab &amp;lt;- table(events)
tab&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## events
## blue  red 
## 5952 4048&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and &lt;code&gt;prop.table&lt;/code&gt; gives us the proportions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prop.table(tab)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## events
##   blue    red 
## 0.5952 0.4048&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory, not covered here, tells us that as &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; gets larger, the estimates get closer to 3/5=.6 and 2/5=.4.&lt;/p&gt;
&lt;p&gt;Although this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.&lt;/p&gt;
&lt;div id=&#34;setting-the-random-seed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting the random seed&lt;/h3&gt;
&lt;p&gt;Before we continue, we will briefly explain the following important line of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1986)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Throughout this class, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the class may show a different result than what you obtain when you try to code as shown in the class. This is actually fine since the results are random and change from time to time. However, if you want to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number. Above we set it to 1986. We want to avoid using the same seed everytime. A popular way to pick the seed is the year - month - day. For example, we picked 1986 on December 20, 2018: &lt;span class=&#34;math inline&#34;&gt;\(2018 - 12 - 20 = 1986\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;You can learn more about setting the seed by looking at the documentation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?set.seed&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;with-and-without-replacement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;With and without replacement&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;sample&lt;/code&gt; has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs &lt;em&gt;without replacement&lt;/em&gt;: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;red&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;red&amp;quot;  &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;blue&amp;quot; &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot; &amp;quot;red&amp;quot;  &amp;quot;blue&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(beads, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Error in sample.int(length(x), size, replace, prob) :   cannot take a sample larger than the population when &#39;replace = FALSE&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;However, the &lt;code&gt;sample&lt;/code&gt; function can be used directly, without the use of &lt;code&gt;replicate&lt;/code&gt;, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample &lt;em&gt;with replacement&lt;/em&gt;: return the bead back to the urn after selecting it.
We can tell &lt;code&gt;sample&lt;/code&gt; to do this by changing the &lt;code&gt;replace&lt;/code&gt; argument, which defaults to &lt;code&gt;FALSE&lt;/code&gt;, to &lt;code&gt;replace = TRUE&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;events &amp;lt;- sample(beads, B, replace = TRUE)
prop.table(table(events))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## events
##   blue    red 
## 0.6017 0.3983&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not surprisingly, we get results very similar to those previously obtained with &lt;code&gt;replicate&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;independence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Independence&lt;/h2&gt;
&lt;p&gt;We say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws.&lt;/p&gt;
&lt;p&gt;Many examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, &lt;span class=&#34;math inline&#34;&gt;\(\dots\)&lt;/span&gt;, Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore &lt;strong&gt;not independent&lt;/strong&gt;: the first outcome affected the next one.&lt;/p&gt;
&lt;p&gt;To see an extreme case of non-independent events, consider our example of drawing five beads at random &lt;strong&gt;without&lt;/strong&gt; replacement:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sample(beads, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x[2:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;blue&amp;quot; &amp;quot;red&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;would you still guess blue? Of course not. Now you know that the probability of red is 1 since the only bead left is red. The events are not independent, so the probabilities change.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-probabilities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional probabilities&lt;/h2&gt;
&lt;p&gt;When events are not independent, &lt;em&gt;conditional probabilities&lt;/em&gt; are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(\mbox{Card 2 is a king} \mid \mbox{Card 1 is a king}) = 3/51
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We use the &lt;span class=&#34;math inline&#34;&gt;\(\mid\)&lt;/span&gt; as shorthand for “given that” or “conditional on”.&lt;/p&gt;
&lt;p&gt;When two events, say &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, are independent, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mid B) = \mbox{Pr}(A)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is the mathematical way of saying: the fact that &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; happened does not affect the probability of &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; happening. In fact, this can be considered the mathematical definition of independence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;addition-and-multiplication-rules&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Addition and multiplication rules&lt;/h2&gt;
&lt;div id=&#34;multiplication-rule&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiplication rule&lt;/h3&gt;
&lt;p&gt;If we want to know the probability of two events, say &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, occurring, we can use the multiplication rule:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mbox{ and } B) = \mbox{Pr}(A)\mbox{Pr}(B \mid A)
\]&lt;/span&gt;
Let’s use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose).&lt;/p&gt;
&lt;p&gt;So, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being an Ace and multiply by the probability of drawing a face card or a 10 given that the first was an Ace: &lt;span class=&#34;math inline&#34;&gt;\(1/13 \times 16/51 \approx 0.025\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The multiplication rule also applies to more than two events. We can use induction to expand for more events:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mbox{ and } B \mbox{ and } C) = \mbox{Pr}(A)\mbox{Pr}(B \mid A)\mbox{Pr}(C \mid A \mbox{ and } B)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiplication-rule-under-independence&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multiplication rule under independence&lt;/h3&gt;
&lt;p&gt;When we have independent events, then the multiplication rule becomes simpler:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mbox{ and } B \mbox{ and } C) = \mbox{Pr}(A)\mbox{Pr}(B)\mbox{Pr}(C)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But we have to be very careful before using this since assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence.&lt;/p&gt;
&lt;p&gt;As an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an “expert” to testify that 1/10 men have beards and 1/5 have mustaches, so using the multiplication rule we conclude that only &lt;span class=&#34;math inline&#34;&gt;\(1/10 \times 1/5\)&lt;/span&gt; or 0.02 have both.&lt;/p&gt;
&lt;p&gt;But to multiply like this we need to assume independence! Say the conditional probability of a man having a mustache conditional on him having a beard is .95. So the correct calculation probability is much higher: &lt;span class=&#34;math inline&#34;&gt;\(1/10 \times 95/100 = 0.095\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The multiplication rule also gives us a general formula for computing conditional probabilities:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(B \mid A) = \frac{\mbox{Pr}(A \mbox{ and } B)}{ \mbox{Pr}(A)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To illustrate how we use these formulas and concepts in practice, we will use several examples related to card games.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;addition-rule&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Addition rule&lt;/h3&gt;
&lt;p&gt;The addition rule tells us that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(A \mbox{ or } B) = \mbox{Pr}(A) + \mbox{Pr}(B) - \mbox{Pr}(A \mbox{ and } B)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice so we need to substract one instance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/04-content_files/figure-html/venn-diagram-addition-rule-1.png&#34; width=&#34;35%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;combinations-and-permutations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Combinations and permutations&lt;/h2&gt;
&lt;p&gt;In our very first example, we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue.&lt;/p&gt;
&lt;p&gt;For more complicated cases, the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush” in poker? In a discrete probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers.&lt;/p&gt;
&lt;p&gt;First, let’s construct a deck of cards. For this, we will use the &lt;code&gt;expand.grid&lt;/code&gt; and &lt;code&gt;paste&lt;/code&gt; functions. We use &lt;code&gt;paste&lt;/code&gt; to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;number &amp;lt;- &amp;quot;Three&amp;quot;
suit &amp;lt;- &amp;quot;Hearts&amp;quot;
paste(number, suit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Three Hearts&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;paste&lt;/code&gt; also works on pairs of vectors performing the operation element-wise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste(letters[1:5], as.character(1:5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a 1&amp;quot; &amp;quot;b 2&amp;quot; &amp;quot;c 3&amp;quot; &amp;quot;d 4&amp;quot; &amp;quot;e 5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;expand.grid&lt;/code&gt; gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey, and plaid shirts, all your combinations are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expand.grid(pants = c(&amp;quot;blue&amp;quot;, &amp;quot;black&amp;quot;), shirt = c(&amp;quot;white&amp;quot;, &amp;quot;grey&amp;quot;, &amp;quot;plaid&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   pants shirt
## 1  blue white
## 2 black white
## 3  blue  grey
## 4 black  grey
## 5  blue plaid
## 6 black plaid&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is how we generate a deck of cards:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suits &amp;lt;- c(&amp;quot;Diamonds&amp;quot;, &amp;quot;Clubs&amp;quot;, &amp;quot;Hearts&amp;quot;, &amp;quot;Spades&amp;quot;)
numbers &amp;lt;- c(&amp;quot;Ace&amp;quot;, &amp;quot;Deuce&amp;quot;, &amp;quot;Three&amp;quot;, &amp;quot;Four&amp;quot;, &amp;quot;Five&amp;quot;, &amp;quot;Six&amp;quot;, &amp;quot;Seven&amp;quot;,
             &amp;quot;Eight&amp;quot;, &amp;quot;Nine&amp;quot;, &amp;quot;Ten&amp;quot;, &amp;quot;Jack&amp;quot;, &amp;quot;Queen&amp;quot;, &amp;quot;King&amp;quot;)
deck &amp;lt;- expand.grid(number=numbers, suit=suits)
deck &amp;lt;- paste(deck$number, deck$suit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the deck constructed, we can double check that the probability of a King in the first card is 1/13 by computing the proportion of possible outcomes that satisfy our condition:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kings &amp;lt;- paste(&amp;quot;King&amp;quot;, suits)
mean(deck %in% kings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.07692308&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, how about the conditional probability of the second card being a King given that the first was a King? Earlier, we deduced that if one King is already out of the deck and there are 51 left, then this probability is 3/51. Let’s confirm by listing out all possible outcomes.&lt;/p&gt;
&lt;p&gt;To do this, we can use the &lt;code&gt;permutations&lt;/code&gt; function from the &lt;strong&gt;gtools&lt;/strong&gt; package. For any list of size &lt;code&gt;n&lt;/code&gt;, this function computes all the different combinations we can get when we select &lt;code&gt;r&lt;/code&gt; items. Here are all the ways we can choose two numbers from a list consisting of &lt;code&gt;1,2,3&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gtools)
permutations(3, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    2
## [2,]    1    3
## [3,]    2    1
## [4,]    2    3
## [5,]    3    1
## [6,]    3    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.&lt;/p&gt;
&lt;p&gt;Optionally, we can add a vector. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_phone_numbers &amp;lt;- permutations(10, 7, v = 0:9)
n &amp;lt;- nrow(all_phone_numbers)
index &amp;lt;- sample(n, 5)
all_phone_numbers[index,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]    1    3    8    0    6    7    5
## [2,]    2    9    1    6    4    8    0
## [3,]    5    1    6    0    9    8    2
## [4,]    7    4    6    0    2    8    1
## [5,]    4    6    5    9    2    8    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of using the numbers 1 through 10, the default, it uses what we provided through &lt;code&gt;v&lt;/code&gt;: the digits 0 through 9.&lt;/p&gt;
&lt;p&gt;To compute all possible ways we can choose two cards when the order matters, we type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hands &amp;lt;- permutations(52, 2, v = deck)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a matrix with two columns and 2652 rows. With a matrix we can get the first and second cards like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;first_card &amp;lt;- hands[,1]
second_card &amp;lt;- hands[,2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the cases for which the first hand was a King can be computed like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kings &amp;lt;- paste(&amp;quot;King&amp;quot;, suits)
sum(first_card %in% kings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 204&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get the conditional probability, we compute what fraction of these have a King in the second card:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(first_card%in%kings &amp;amp; second_card%in%kings) / sum(first_card%in%kings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05882353&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is exactly 3/51, as we had already deduced. Notice that the code above is equivalent to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(first_card%in%kings &amp;amp; second_card%in%kings) / mean(first_card%in%kings)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05882353&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which uses &lt;code&gt;mean&lt;/code&gt; instead of &lt;code&gt;sum&lt;/code&gt; and is an R version of:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\mbox{Pr}(A \mbox{ and } B)}{ \mbox{Pr}(A)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a &lt;em&gt;Natural 21&lt;/em&gt; and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the &lt;em&gt;combinations&lt;/em&gt;, not the permutations, since the order does not matter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;combinations(3,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    2
## [2,]    1    3
## [3,]    2    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the second line, the outcome does not include (2,1) because (1,2) already was enumerated. The same applies to (3,1) and (3,2).&lt;/p&gt;
&lt;p&gt;So to compute the probability of a &lt;em&gt;Natural 21&lt;/em&gt; in Blackjack, we can do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aces &amp;lt;- paste(&amp;quot;Ace&amp;quot;, suits)

facecard &amp;lt;- c(&amp;quot;King&amp;quot;, &amp;quot;Queen&amp;quot;, &amp;quot;Jack&amp;quot;, &amp;quot;Ten&amp;quot;)
facecard &amp;lt;- expand.grid(number = facecard, suit = suits)
facecard &amp;lt;- paste(facecard$number, facecard$suit)

hands &amp;lt;- combinations(52, 2, v = deck)
mean(hands[,1] %in% aces &amp;amp; hands[,2] %in% facecard)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04826546&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the last line, we assume the Ace comes first. This is only because we know the way &lt;code&gt;combination&lt;/code&gt; enumerates possibilities and it will list this case first. But to be safe, we could have written this and produced the same answer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean((hands[,1] %in% aces &amp;amp; hands[,2] %in% facecard) |
       (hands[,2] %in% aces &amp;amp; hands[,1] %in% facecard))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04826546&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;monte-carlo-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monte Carlo example&lt;/h3&gt;
&lt;p&gt;Instead of using &lt;code&gt;combinations&lt;/code&gt; to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw two cards without replacements:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hand &amp;lt;- sample(deck, 2)
hand&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Queen Clubs&amp;quot;  &amp;quot;Seven Spades&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say &lt;em&gt;face card&lt;/em&gt;. Now we need to check both possibilities:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(hands[1] %in% aces &amp;amp; hands[2] %in% facecard) |
  (hands[2] %in% aces &amp;amp; hands[1] %in% facecard)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21.&lt;/p&gt;
&lt;p&gt;Let’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blackjack &amp;lt;- function(){
   hand &amp;lt;- sample(deck, 2)
  (hand[1] %in% aces &amp;amp; hand[2] %in% facecard) |
    (hand[2] %in% aces &amp;amp; hand[1] %in% facecard)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we do have to check both possibilities: Ace first or Ace second because we are not using the &lt;code&gt;combinations&lt;/code&gt; function. The function returns &lt;code&gt;TRUE&lt;/code&gt; if we get a 21 and &lt;code&gt;FALSE&lt;/code&gt; otherwise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;blackjack()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can play this game, say, 10,000 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
results &amp;lt;- replicate(B, blackjack())
mean(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0475&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;In this section, we describe two discrete probability popular examples: the Monty Hall problem and the birthday problem. We use R to help illustrate the mathematical concepts.&lt;/p&gt;
&lt;div id=&#34;monty-hall-problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Monty Hall problem&lt;/h3&gt;
&lt;p&gt;In the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and show the contestant there was no prize behind that door. Then he would ask “Do you want to switch doors?” What would you do?&lt;/p&gt;
&lt;p&gt;We can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; or read one on Wikipedia&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.&lt;/p&gt;
&lt;p&gt;Let’s start with the stick strategy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
monty_hall &amp;lt;- function(strategy){
  doors &amp;lt;- as.character(1:3)
  prize &amp;lt;- sample(c(&amp;quot;car&amp;quot;, &amp;quot;goat&amp;quot;, &amp;quot;goat&amp;quot;))
  prize_door &amp;lt;- doors[prize == &amp;quot;car&amp;quot;]
  my_pick  &amp;lt;- sample(doors, 1)
  show &amp;lt;- sample(doors[!doors %in% c(my_pick, prize_door)],1)
  stick &amp;lt;- my_pick
  stick == prize_door
  switch &amp;lt;- doors[!doors%in%c(my_pick, show)]
  choice &amp;lt;- ifelse(strategy == &amp;quot;stick&amp;quot;, stick, switch)
  choice == prize_door
}
stick &amp;lt;- replicate(B, monty_hall(&amp;quot;stick&amp;quot;))
mean(stick)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3416&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;switch &amp;lt;- replicate(B, monty_hall(&amp;quot;switch&amp;quot;))
mean(switch)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6682&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we write the code, we note that the lines starting with &lt;code&gt;my_pick&lt;/code&gt; and &lt;code&gt;show&lt;/code&gt; have no influence on the last logical operation when we stick to our original choice anyway. From this we should realize that the chance is 1 in 3, what we began with. When we switch,
the Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by showing that we are removing a door, &lt;code&gt;show&lt;/code&gt;, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;birthday-problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Birthday problem&lt;/h3&gt;
&lt;p&gt;Suppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.&lt;/p&gt;
&lt;p&gt;First, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 50
bdays &amp;lt;- sample(1:365, n, replace = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check if in this particular set of 50 people we have at least two with the same birthday, we can use the function &lt;code&gt;duplicated&lt;/code&gt;, which returns &lt;code&gt;TRUE&lt;/code&gt; whenever an element of a vector is a duplicate. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;duplicated(c(1,2,3,1,4,3,5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second time 1 and 3 appear, we get a &lt;code&gt;TRUE&lt;/code&gt;. So to check if two birthdays were the same, we simply use the &lt;code&gt;any&lt;/code&gt; and &lt;code&gt;duplicated&lt;/code&gt; functions like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;any(duplicated(bdays))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, we see that it did happen. At least two people had the same birthday.&lt;/p&gt;
&lt;p&gt;To estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 50 birthdays over and over:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
same_birthday &amp;lt;- function(n){
  bdays &amp;lt;- sample(1:365, n, replace=TRUE)
  any(duplicated(bdays))
}
results &amp;lt;- replicate(B, same_birthday(50))
mean(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9691&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Were you expecting the probability to be this high?&lt;/p&gt;
&lt;p&gt;People tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.&lt;/p&gt;
&lt;p&gt;Say we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?&lt;/p&gt;
&lt;p&gt;Let’s create a look-up table. We can quickly create a function to compute this for any group size:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_prob &amp;lt;- function(n, B=10000){
  results &amp;lt;- replicate(B, same_birthday(n))
  mean(results)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the function &lt;code&gt;sapply&lt;/code&gt;, we can perform element-wise operations on any function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- seq(1,60)
prob &amp;lt;- sapply(n, compute_prob)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now make a plot of the estimated probabilities of two people having the same birthday in a group of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
prob &amp;lt;- sapply(n, compute_prob)
qplot(n, prob)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/04-content_files/figure-html/birthday-problem-mc-probabilities-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments.&lt;/p&gt;
&lt;p&gt;To make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule.&lt;/p&gt;
&lt;p&gt;Let’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
1 \times \frac{364}{365}\times\frac{363}{365} \dots \frac{365-n + 1}{365}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can write a function that does this for any number:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exact_prob &amp;lt;- function(n){
  prob_unique &amp;lt;- seq(365,365-n+1)/365
  1 - prod( prob_unique)
}
eprob &amp;lt;- sapply(n, exact_prob)
qplot(n, prob) + geom_line(aes(n, eprob), col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/04-content_files/figure-html/birthday-problem-exact-probabilities-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;infinity-in-practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Infinity in practice&lt;/h2&gt;
&lt;p&gt;The theory described here requires repeating experiments over and over forever. In practice we can’t do this.
In the examples above, we used &lt;span class=&#34;math inline&#34;&gt;\(B=10,000\)&lt;/span&gt; Monte Carlo experiments and it turned out that this provided accurate estimates. The larger this number, the more accurate the estimate becomes until the approximaton is so good that your computer can’t tell the difference. But in more complex calculations, 10,000 may not be nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is, so we won’t know if our Monte Carlo estimate is accurate. We know that the larger &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, the better the approximation. But how big do we need it to be? This is actually a challenging question and answering it often requires advanced theoretical statistics training.&lt;/p&gt;
&lt;p&gt;One practical approach we will describe here is to check for the stability of the estimate. The following is an example with the birthday problem for a group of 25 people.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10^seq(1, 5, len = 100)
compute_prob &amp;lt;- function(B, n=25){
  same_day &amp;lt;- replicate(B, same_birthday(n))
  mean(same_day)
}
prob &amp;lt;- sapply(B, compute_prob)
qplot(log10(B), prob, geom = &amp;quot;line&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/04-content_files/figure-html/monte-carlo-convergence-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this plot, we can see that the values start to stabilize (that is, they vary less than .01) around 1000. Note that the exact probability, which we know in this case, is 0.5686997.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;One ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the probability that the ball will not be cyan?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling &lt;strong&gt;without&lt;/strong&gt; replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling &lt;strong&gt;with&lt;/strong&gt; replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two events &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; are independent if &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(A \mbox{ and } B) = \mbox{Pr}(A) P(B)\)&lt;/span&gt;. Under which situation are the draws independent?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;You don’t replace the draw.&lt;/li&gt;
&lt;li&gt;You replace the draw.&lt;/li&gt;
&lt;li&gt;Neither&lt;/li&gt;
&lt;li&gt;Both&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If you roll a 6-sided die six times, what is the probability of not seeing a 6?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win &lt;strong&gt;at least&lt;/strong&gt; one game?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a Monte Carlo simulation to confirm your answer to the previous problem. Use &lt;code&gt;B &amp;lt;- 10000&lt;/code&gt; simulations. Hint: use the following code to generate the results of the first four games:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;celtic_wins &amp;lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Celtics must win one of these 4 games.&lt;/p&gt;
&lt;ol start=&#34;10&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Two teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Confirm the results of the previous question with a Monte Carlo simulation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Two teams, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;, are playing a seven game series. Team &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is better than team &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; and has a &lt;span class=&#34;math inline&#34;&gt;\(p&amp;gt;0.5\)&lt;/span&gt; chance of winning each game. Given a value &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, the probability of winning the series for the underdog team &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; can be computed with the following function based on a Monte Carlo simulation:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prob_win &amp;lt;- function(p){
  B &amp;lt;- 10000
  result &amp;lt;- replicate(B, {
    b_win &amp;lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p))
    sum(b_win)&amp;gt;=4
  })
  mean(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the function &lt;code&gt;sapply&lt;/code&gt; to compute the probability, call it &lt;code&gt;Pr&lt;/code&gt;, of winning for &lt;code&gt;p &amp;lt;- seq(0.5, 0.95, 0.025)&lt;/code&gt;. Then plot the result.&lt;/p&gt;
&lt;ol start=&#34;13&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Repeat the exercise above, but now keep the probability fixed at &lt;code&gt;p &amp;lt;- 0.75&lt;/code&gt; and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, &lt;code&gt;N &amp;lt;- seq(1, 25, 2)&lt;/code&gt;. Hint: use this function:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prob_win &amp;lt;- function(N, p=0.75){
  B &amp;lt;- 10000
  result &amp;lt;- replicate(B, {
    b_win &amp;lt;- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p))
    sum(b_win)&amp;gt;=(N+1)/2
  })
  mean(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;In previous lectures, we explained why when summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome. For example, if we measure every single person in a very large population of size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; with extremely high precision, since no two people are exactly the same height, we need to assign the proportion &lt;span class=&#34;math inline&#34;&gt;\(1/n\)&lt;/span&gt; to each observed value and attain no useful summary at all. Similarly, when defining probability distributions, it is not useful to assign a very small probability to every single height.&lt;/p&gt;
&lt;p&gt;Just as when using distributions to summarize numeric data, it is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the &lt;em&gt;cumulative distribution function&lt;/em&gt; (CDF).&lt;/p&gt;
&lt;p&gt;We described empirical cumulative distribution function (eCDF) as a basic summary of a list of numeric values. As an example, we earlier defined the height distribution for adult male students. Here, we define the vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to contain these heights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
data(heights)
x &amp;lt;- heights %&amp;gt;% filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;% pull(height)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We defined the empirical distribution function as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;F &amp;lt;- function(a) mean(x&amp;lt;=a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which, for any value &lt;code&gt;a&lt;/code&gt;, gives the proportion of values in the list &lt;code&gt;x&lt;/code&gt; that are smaller or equal than &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Keep in mind that we have not yet introduced probability in the context of CDFs. Let’s do this by asking the following: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 inches. Using the CDF we obtain an answer by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - F(70)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3768473&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once a CDF is defined, we can use this to compute the probability of any subset. For instance, the probability of a student being between height &lt;code&gt;a&lt;/code&gt; and height &lt;code&gt;b&lt;/code&gt; is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;F(b)-F(a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we can compute the probability for any possible event this way, the cumulative probability function defines the probability distribution for picking a height at random from our vector of heights &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theoretical-continuous-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Theoretical continuous distributions&lt;/h2&gt;
&lt;p&gt;The normal distribution is a useful approximation to many naturally occurring distributions, including that of height. The cumulative distribution for the normal distribution is defined by a mathematical formula which in &lt;code&gt;R&lt;/code&gt; can be obtained with the function &lt;code&gt;pnorm&lt;/code&gt;. We say that a random quantity is normally distributed with average &lt;code&gt;m&lt;/code&gt; and standard deviation &lt;code&gt;s&lt;/code&gt; if its probability distribution is defined by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;F(a) = pnorm(a, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is useful because if we are willing to use the normal approximation for, say, height, we don’t need the entire dataset to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- mean(x)
s &amp;lt;- sd(x)
1 - pnorm(70.5, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.371369&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;theoretical-distributions-as-approximations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Theoretical distributions as approximations&lt;/h3&gt;
&lt;p&gt;The normal distribution is derived mathematically: we do not need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/04-content_files/figure-html/plot-of-height-frequencies-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787, which is 177 centimeters. The probability assigned to this height is 0.0012315 or 1 in 812. The probability for 70 inches is much higher at 0.1059113, but does it really make sense to think of the probability of being exactly 70 inches as being different than 69.6850393700787? Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch.&lt;/p&gt;
&lt;p&gt;With continuous distributions, the probability of a singular value is not even defined. For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, we define probabilities for intervals. We thus could ask what is the probability that someone is between 69.5 and 70.5.&lt;/p&gt;
&lt;p&gt;In cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. For example, the normal distribution is useful for approximating the proportion of students reporting values in intervals like the following three:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 68.5) - mean(x &amp;lt;= 67.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.114532&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 69.5) - mean(x &amp;lt;= 68.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1194581&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 70.5) - mean(x &amp;lt;= 69.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1219212&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how close we get with the normal approximation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(68.5, m, s) - pnorm(67.5, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1031077&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(69.5, m, s) - pnorm(68.5, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1097121&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(70.5, m, s) - pnorm(69.5, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1081743&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, the approximation is not as useful for other intervals. For instance, notice how the approximation breaks down when we try to estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 70.9) - mean(x&amp;lt;=70.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02216749&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(70.9, m, s) - pnorm(70.1, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.08359562&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, we call this situation &lt;em&gt;discretization&lt;/em&gt;. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-probability-density&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The probability density&lt;/h3&gt;
&lt;p&gt;For categorical distributions, we can define the probability of a category. For example, a roll of a die, let’s call it &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, can be 1,2,3,4,5 or 6. The probability of 4 is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(X=4) = 1/6
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The CDF can then easily be defined:
&lt;span class=&#34;math display&#34;&gt;\[
F(4) = \mbox{Pr}(X\leq 4) =  \mbox{Pr}(X = 4) +  \mbox{Pr}(X = 3) +  \mbox{Pr}(X = 2) +  \mbox{Pr}(X = 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Although for continuous distributions the probability of a single value &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(X=x)\)&lt;/span&gt; is not defined, there is a theoretical definition that has a similar interpretation. The probability density at &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is defined as the function &lt;span class=&#34;math inline&#34;&gt;\(f(a)\)&lt;/span&gt; such that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
F(a) = \mbox{Pr}(X\leq a) = \int_{-\infty}^a f(x)\, dx
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For those that know calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don’t know calculus, you can think of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; as a curve for which the area under that curve up to the value &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, gives you the probability &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(X\leq a)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - pnorm(76, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03206008&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which mathematically is the grey area below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/04-content_files/figure-html/intergrals-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The curve you see is the probability density for the normal distribution. In R, we get this using the function &lt;code&gt;dnorm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Although it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;monte-carlo-simulations-for-continuous-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Monte Carlo simulations for continuous variables&lt;/h2&gt;
&lt;p&gt;R provides functions to generate normally distributed outcomes. Specifically, the &lt;code&gt;rnorm&lt;/code&gt; function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produces random numbers. Here is an example of how we could generate data that looks like our reported heights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- length(x)
m &amp;lt;- mean(x)
s &amp;lt;- sd(x)
simulated_heights &amp;lt;- rnorm(n, m, s)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not surprisingly, the distribution looks normal:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/04-content_files/figure-html/simulated-heights-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations.&lt;/p&gt;
&lt;p&gt;If, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer in a group of 800 males? The following Monte Carlo simulation helps us answer that question:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10000
tallest &amp;lt;- replicate(B, {
  simulated_data &amp;lt;- rnorm(800, m, s)
  max(simulated_data)
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having a seven footer is quite rare:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(tallest &amp;gt;= 7*12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0172&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the resulting distribution:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/04-content_files/figure-html/simulated-tallest-height-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that it does not look normal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous distributions&lt;/h2&gt;
&lt;p&gt;The normal distribution is not the only useful theoretical distribution. Other continuous distributions that we may encounter are the student-t, Chi-square, exponential, gamma, beta, and beta-binomial. &lt;code&gt;R&lt;/code&gt; provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. &lt;code&gt;R&lt;/code&gt; uses a convention that lets us remember the names, namely using the letters &lt;code&gt;d&lt;/code&gt;, &lt;code&gt;q&lt;/code&gt;, &lt;code&gt;p&lt;/code&gt;, and &lt;code&gt;r&lt;/code&gt; in front of a shorthand for the distribution. We have already seen the functions &lt;code&gt;dnorm&lt;/code&gt;, &lt;code&gt;pnorm&lt;/code&gt;, and &lt;code&gt;rnorm&lt;/code&gt; for the normal distribution. The functions &lt;code&gt;qnorm&lt;/code&gt; gives us the quantiles. We can therefore draw a distribution like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(-4, 4, length.out = 100)
qplot(x, f, geom = &amp;quot;line&amp;quot;, data = data.frame(x, f = dnorm(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the student-t, described later as we move toward hypothesis testing, the shorthand &lt;code&gt;t&lt;/code&gt; is used so the functions are &lt;code&gt;dt&lt;/code&gt; for the density, &lt;code&gt;qt&lt;/code&gt; for the quantiles, &lt;code&gt;pt&lt;/code&gt; for the cumulative distribution function, and &lt;code&gt;rt&lt;/code&gt; for Monte Carlo simulation.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Notice that the answer to the question does not change when you change units. This makes sense since the answer to the question should not be affected by what units we use. In fact, if you look closely, you notice that 61 and 64 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To see the math that explains why the answers to questions 3, 4, and 5 are the same, suppose we have a random variable with average &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and standard error &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. Suppose we ask the probability of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; being smaller or equal to &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;. Remember that, by definition, &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\((a - m)/s\)&lt;/span&gt; standard deviations &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; away from the average &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;. The probability is:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}(X \leq a)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we subtract &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; to both sides and then divide both sides by &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(\frac{X-m}{s} \leq \frac{a-m}{s} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{Pr}\left(Z \leq \frac{a-m}{s} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, no matter the units, the probability of &lt;span class=&#34;math inline&#34;&gt;\(X\leq a\)&lt;/span&gt; is the same as the probability of a standard normal variable being less than &lt;span class=&#34;math inline&#34;&gt;\((a - m)/s\)&lt;/span&gt;. If &lt;code&gt;mu&lt;/code&gt; is the average and &lt;code&gt;sigma&lt;/code&gt; the standard error, which of the following R code would give us the right answer in every situation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;mean(X&amp;lt;=a)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pnorm((a - m)/s)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pnorm((a - m)/s, m, s)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pnorm(a)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use &lt;code&gt;qnorm&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with &lt;code&gt;B=1000&lt;/code&gt; generating 10,000 IQ scores and keeping the highest. Make a histogram.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Random variables&lt;/h1&gt;
&lt;p&gt;In data science, we often deal with data that is affected by chance in some way: the data comes from a random sample, the data is affected by measurement error, or the data measures some outcome that is random in nature. Being able to quantify the uncertainty introduced by randomness is one of the most important jobs of a data analyst. Statistical inference offers a framework, as well as several practical tools, for doing this. The first step is to learn how to mathematically describe random variables.&lt;/p&gt;
&lt;p&gt;In this section, we introduce random variables and their properties starting with their application to games of chance. We then describe some of the events surrounding the financial crisis of 2007-2008&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; using probability theory. This financial crisis was in part caused by underestimating the risk of certain securities&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; sold by financial institutions. Specifically, the risks of mortgage-backed securities (MBS) and collateralized debt obligations (CDO) were grossly underestimated. These assets were sold at prices that assumed most homeowners would make their monthly payments, and the probability of this not occurring was calculated as being low. A combination of factors resulted in many more defaults than were expected, which led to a price crash of these securities. As a consequence, banks lost so much money that they needed government bailouts to avoid closing down completely.&lt;/p&gt;
&lt;div id=&#34;definition-of-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definition of Random variables&lt;/h2&gt;
&lt;p&gt;Random variables are numeric outcomes resulting from random processes. We can easily generate random variables using some of the simple examples we have shown. For example, define &lt;code&gt;X&lt;/code&gt; to be 1 if a bead is blue and red otherwise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beads &amp;lt;- rep( c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;), times = c(2,3))
X &amp;lt;- ifelse(sample(beads, 1) == &amp;quot;blue&amp;quot;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;X&lt;/code&gt; is a random variable: every time we select a new bead the outcome changes randomly. See below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ifelse(sample(beads, 1) == &amp;quot;blue&amp;quot;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ifelse(sample(beads, 1) == &amp;quot;blue&amp;quot;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ifelse(sample(beads, 1) == &amp;quot;blue&amp;quot;, 1, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sometimes it’s 1 and sometimes it’s 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sampling models&lt;/h2&gt;
&lt;p&gt;Many data generation procedures, those that produce the data we study, can be modeled quite well as draws from an urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 code for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing the outcome for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can also be modeled by draws from an urn given the way individuals are assigned into groups: when getting assigned, you draw your group at random. Sampling models are therefore ubiquitous in data science. Casino games offer a plethora of examples of real-world situations in which sampling models are used to answer specific questions. We will therefore start with such examples.&lt;/p&gt;
&lt;p&gt;Suppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play and that the only game you can play on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will pass on installing roulette wheels.&lt;/p&gt;
&lt;p&gt;We are going to define a random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; that will represent the casino’s total winnings. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color &amp;lt;- rep(c(&amp;quot;Black&amp;quot;, &amp;quot;Red&amp;quot;, &amp;quot;Green&amp;quot;), c(18, 18, 2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 1,000 outcomes from 1,000 people playing are independent draws from this urn. If red comes up, the gambler wins and the casino loses a dollar, so we draw a -1. Otherwise, the casino wins a dollar and we draw a 1. To construct our random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, we can use this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
X &amp;lt;- sample(ifelse(color == &amp;quot;Red&amp;quot;, -1, 1),  n, replace = TRUE)
X[1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] -1  1  1 -1 -1 -1  1  1  1  1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining &lt;code&gt;color&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We call this a &lt;strong&gt;sampling model&lt;/strong&gt; since we are modeling the random behavior of roulette with the sampling of draws from an urn. The total winnings &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is simply the sum of these 1,000 independent draws:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
S &amp;lt;- sum(X)
S&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 22&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-probability-distribution-of-a-random-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The probability distribution of a random variable&lt;/h2&gt;
&lt;p&gt;If you run the code above, you see that &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; changes every time. This is, of course, because &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is a &lt;strong&gt;random variable&lt;/strong&gt;. The probability distribution of a random variable tells us the probability of the observed value falling at any given interval. So, for example, if we want to know the probability that we lose money, we are asking the probability that &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is in the interval &lt;span class=&#34;math inline&#34;&gt;\(S&amp;lt;0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that if we can define a cumulative distribution function &lt;span class=&#34;math inline&#34;&gt;\(F(a) = \mbox{Pr}(S\leq a)\)&lt;/span&gt;, then we will be able to answer any question related to the probability of events defined by our random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;, including the event &lt;span class=&#34;math inline&#34;&gt;\(S&amp;lt;0\)&lt;/span&gt;. We call this &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; the random variable’s &lt;em&gt;distribution function&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can estimate the distribution function for the random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people play roulette, over and over, specifically &lt;span class=&#34;math inline&#34;&gt;\(B = 10,000\)&lt;/span&gt; times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
B &amp;lt;- 10000
roulette_winnings &amp;lt;- function(n){
  X &amp;lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
  sum(X)
}
S &amp;lt;- replicate(B, roulette_winnings(n))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can ask the following: in our simulations, how often did we get sums less than or equal to &lt;code&gt;a&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(S &amp;lt;= a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will be a very good approximation of &lt;span class=&#34;math inline&#34;&gt;\(F(a)\)&lt;/span&gt; and we can easily answer the casino’s question: how likely is it that we will lose money? We can see it is quite low:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(S&amp;lt;0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0456&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can visualize the distribution of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; by creating a histogram showing the probability &lt;span class=&#34;math inline&#34;&gt;\(F(b)-F(a)\)&lt;/span&gt; for several intervals &lt;span class=&#34;math inline&#34;&gt;\((a,b]\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/04-content_files/figure-html/normal-approximates-distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the distribution appears to be approximately normal. A qq-plot will confirm that the normal approximation is close to a perfect approximation for this distribution. If, in fact, the distribution is normal, then all we need to define the distribution is the average and the standard deviation. Because we have the original values from which the distribution is created, we can easily compute these with &lt;code&gt;mean(S)&lt;/code&gt; and &lt;code&gt;sd(S)&lt;/code&gt;. The blue curve you see added to the histogram above is a normal density with this average and standard deviation.&lt;/p&gt;
&lt;p&gt;This average and this standard deviation have special names. They are referred to as the &lt;em&gt;expected value&lt;/em&gt; and &lt;em&gt;standard error&lt;/em&gt; of the random variable &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. We will say more about these in the next section.&lt;/p&gt;
&lt;p&gt;Statistical theory provides a way to derive the distribution of random variables defined as independent random draws from an urn. Specifically, in our example above, we can show that &lt;span class=&#34;math inline&#34;&gt;\((S+n)/2\)&lt;/span&gt; follows a binomial distribution. We therefore do not need to run for Monte Carlo simulations to know the probability distribution of &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. We did this for illustrative purposes.&lt;/p&gt;
&lt;p&gt;We can use the function &lt;code&gt;dbinom&lt;/code&gt; and &lt;code&gt;pbinom&lt;/code&gt; to compute the probabilities exactly. For example, to compute &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(S &amp;lt; 0)\)&lt;/span&gt; we note that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{Pr}(S &amp;lt; 0) = \mbox{Pr}((S+n)/2 &amp;lt; (0+n)/2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we can use the &lt;code&gt;pbinom&lt;/code&gt; to compute &lt;span class=&#34;math display&#34;&gt;\[\mbox{Pr}(S \leq 0)\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
pbinom(n/2, size = n, prob = 10/19)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05109794&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because this is a discrete probability function, to get &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(S &amp;lt; 0)\)&lt;/span&gt; rather than &lt;span class=&#34;math inline&#34;&gt;\(\mbox{Pr}(S \leq 0)\)&lt;/span&gt;, we write:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pbinom(n/2-1, size = n, prob = 10/19)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04479591&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the details of the binomial distribution, you can consult any basic probability book or even Wikipedia&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here we do not cover these details. Instead, we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums and averages of draws from any urn: the Central Limit Theorem (CLT).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distributions-versus-probability-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Distributions versus probability distributions&lt;/h2&gt;
&lt;p&gt;Before we continue, let’s make an important distinction and connection between the distribution of a list of numbers and a probability distribution. In the visualization lectures, we described how any list of numbers &lt;span class=&#34;math inline&#34;&gt;\(x_1,\dots,x_n\)&lt;/span&gt; has a distribution. The definition is quite straightforward. We define &lt;span class=&#34;math inline&#34;&gt;\(F(a)\)&lt;/span&gt; as the function that tells us what proportion of the list is less than or equal to &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;. Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. These are defined with a straightforward operation of the vector containing the list of numbers &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- sum(x)/length(x)
s &amp;lt;- sqrt(sum((x - m)^2) / length(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; has a distribution function. To define this, we do not need a list of numbers. It is a theoretical concept. In this case, we define the distribution as the &lt;span class=&#34;math inline&#34;&gt;\(F(a)\)&lt;/span&gt; that answers the question: what is the probability that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is less than or equal to &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;? There is no list of numbers.&lt;/p&gt;
&lt;p&gt;However, if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. In this case, the distribution of that list is the probability distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and the average and standard deviation of that list are the expected value and standard error of the random variable.&lt;/p&gt;
&lt;p&gt;Another way to think about it that does not involve an urn is to run a Monte Carlo simulation and generate a very large list of outcomes of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. These outcomes are a list of numbers. The distribution of this list will be a very good approximation of the probability distribution of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The longer the list, the better the approximation. The average and standard deviation of this list will approximate the expected value and standard error of the random variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notation-for-random-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notation for random variables&lt;/h2&gt;
&lt;p&gt;In statistical textbooks, upper case letters are used to denote random variables and we follow this convention here. Lower case letters are used for observed values. You will see some notation that includes both. For example, you will see events defined as &lt;span class=&#34;math inline&#34;&gt;\(X \leq x\)&lt;/span&gt;. Here &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a random variable, making it a random event, and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is an arbitrary value and not random. So, for example, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; might represent the number on a die roll and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; will represent an actual value we see 1, 2, 3, 4, 5, or 6. So in this case, the probability of &lt;span class=&#34;math inline&#34;&gt;\(X=x\)&lt;/span&gt; is 1/6 regardless of the observed value &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. This notation is a bit strange because, when we ask questions about probability, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is not an observed quantity. Instead, it’s a random quantity that we will see in the future. We can talk about what we expect it to be, what values are probable, but not what it is. But once we have data, we do see a realization of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. So data scientists talk of what could have been after we see what actually happened.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-expected-value-and-standard-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The expected value and standard error&lt;/h2&gt;
&lt;p&gt;We have described sampling models for draws. We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion which we will need to understand how polls work.&lt;/p&gt;
&lt;p&gt;The first important concept to learn is the &lt;em&gt;expected value&lt;/em&gt;.
In statistics books, it is common to use letter &lt;span class=&#34;math inline&#34;&gt;\(\mbox{E}\)&lt;/span&gt; like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{E}[X]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to denote the expected value of the random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take.&lt;/p&gt;
&lt;p&gt;Theoretical statistics provides techniques that facilitate the calculation of expected values in different circumstances. For example, a useful formula tells us that the &lt;em&gt;expected value of a random variable defined by one draw is the average of the numbers in the urn&lt;/em&gt;. In the urn used to model betting on red in roulette, we have 20 one dollars and 18 negative one dollars. The expected value is thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[X] = (20 + -18)/38
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is about 5 cents. It is a bit counterintuitive to say that &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; varies around 0.05, when the only values it takes is 1 and -1. One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- 10^6
x &amp;lt;- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19))
mean(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.05169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In general, if the urn has two possible outcomes, say &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, with proportions &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt; respectively, the average is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{E}[X] = ap + b(1-p)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To see this, notice that if there are &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; beads in the urn, then we have &lt;span class=&#34;math inline&#34;&gt;\(np\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;s and &lt;span class=&#34;math inline&#34;&gt;\(n(1-p)\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;s and because the average is the sum, &lt;span class=&#34;math inline&#34;&gt;\(n\times a \times p + n\times b \times (1-p)\)&lt;/span&gt;, divided by the total &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, we get that the average is &lt;span class=&#34;math inline&#34;&gt;\(ap + b(1-p)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. The first useful fact is that the &lt;em&gt;expected value of the sum of the draws&lt;/em&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{}\mbox{number of draws } \times \mbox{ average of the numbers in the urn}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So if 1,000 people play roulette, the casino expects to win, on average, about 1,000 &lt;span class=&#34;math inline&#34;&gt;\(\times\)&lt;/span&gt; $0.05 = $50. But this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question. The &lt;em&gt;standard error&lt;/em&gt; (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{SE}[X]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to denote the standard error of a random variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If our draws are independent&lt;/strong&gt;, then the &lt;em&gt;standard error of the sum&lt;/em&gt; is given by the equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{\mbox{number of draws }} \times \mbox{ standard deviation of the numbers in the urn}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; with proportions &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((1-p)\)&lt;/span&gt;, respectively, the standard deviation is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mid b - a \mid \sqrt{p(1-p)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So in our roulette example, the standard deviation of the values inside the urn is: &lt;span class=&#34;math inline&#34;&gt;\(\mid 1 - (-1) \mid \sqrt{10/19 \times 9/19}\)&lt;/span&gt; or:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 * sqrt(90)/19&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.998614&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we either get 1 or -1, with 1 slightly favored over -1.&lt;/p&gt;
&lt;p&gt;Using the formula above, the sum of 1,000 people playing has standard error of about $32:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
sqrt(n) * 2 * sqrt(90)/19&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.57895&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet. But we still haven’t answered the question: how likely is it to lose money? Here the CLT will help.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced note&lt;/strong&gt;: Before continuing we should point out that exact probability calculations for the casino winnings can be performed with the binomial distribution. However, here we focus on the CLT, which can be generally applied to sums of random variables in a way that the binomial distribution can’t.&lt;/p&gt;
&lt;div id=&#34;population-sd-versus-the-sample-sd&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Population SD versus the sample SD&lt;/h3&gt;
&lt;p&gt;The standard deviation of a list &lt;code&gt;x&lt;/code&gt; (below we use heights as an example) is defined as the square root of the average of the squared differences:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
x &amp;lt;- heights$height
m &amp;lt;- mean(x)
s &amp;lt;- sqrt(mean((x-m)^2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using mathematical notation we write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mu = \frac{1}{n} \sum_{i=1}^n x_i \\
\sigma =  \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, be aware that the &lt;code&gt;sd&lt;/code&gt; function returns a slightly different result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;identical(s, sd(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s-sd(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.001942661&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is because the &lt;code&gt;sd&lt;/code&gt; function R does not return the &lt;code&gt;sd&lt;/code&gt; of the list, but rather uses a formula that estimates standard deviations of a population from a random sample &lt;span class=&#34;math inline&#34;&gt;\(X_1, \dots, X_N\)&lt;/span&gt; which, for reasons not discussed here, divide the sum of squares by the &lt;span class=&#34;math inline&#34;&gt;\(N-1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\bar{X} = \frac{1}{N} \sum_{i=1}^N X_i, \,\,\,\,
s =  \sqrt{\frac{1}{N-1} \sum_{i=1}^N (X_i - \bar{X})^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You can see that this is the case by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- length(x)
s-sd(x)*sqrt((n-1) / n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For all the theory discussed here, you need to compute the actual standard deviation as defined:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(mean((x-m)^2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So be careful when using the &lt;code&gt;sd&lt;/code&gt; function in R. However, keep in mind that throughout the book we sometimes use the &lt;code&gt;sd&lt;/code&gt; function when we really want the actual SD. This is because when the list size is big, these two are practically equivalent since &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{(N-1)/N} \approx 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;central-limit-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Central Limit Theorem&lt;/h2&gt;
&lt;p&gt;The Central Limit Theorem (CLT) tells us that when the number of draws, also called the &lt;em&gt;sample size&lt;/em&gt;, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.&lt;/p&gt;
&lt;p&gt;Previously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error.&lt;/p&gt;
&lt;p&gt;We previously ran this Monte Carlo simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
B &amp;lt;- 10000
roulette_winnings &amp;lt;- function(n){
  X &amp;lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19))
  sum(X)
}
S &amp;lt;- replicate(B, roulette_winnings(n))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Central Limit Theorem (CLT) tells us that the sum &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is approximated by a normal distribution.
Using the formulas above, we know that the expected value and standard error are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n * (20-18)/38&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 52.63158&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(n) * 2 * sqrt(90)/19&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.57895&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The theoretical values above match those obtained with the Monte Carlo simulation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(S)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 52.2242&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(S)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31.65508&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mu &amp;lt;- n * (20-18)/38
se &amp;lt;-  sqrt(n) * 2 * sqrt(90)/19
pnorm(0, mu, se)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04779035&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is also in very good agreement with our Monte Carlo result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(S &amp;lt; 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0458&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;how-large-is-large-in-the-central-limit-theorem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How large is large in the Central Limit Theorem?&lt;/h3&gt;
&lt;p&gt;The CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need much larger sample sizes.&lt;/p&gt;
&lt;p&gt;By way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate.&lt;/p&gt;
&lt;p&gt;You can examine the properties of the Poisson distribution using &lt;code&gt;dpois&lt;/code&gt; and &lt;code&gt;ppois&lt;/code&gt;. You can generate random variables following this distribution with &lt;code&gt;rpois&lt;/code&gt;. However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-properties-of-averages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Statistical properties of averages&lt;/h2&gt;
&lt;p&gt;There are several useful mathematical results that we used above and often employ when working with data. We list them below.&lt;/p&gt;
&lt;p&gt;1. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[X_1+X_2+\dots+X_n] =  \mbox{E}[X_1] + \mbox{E}[X_2]+\dots+\mbox{E}[X_n]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; are independent draws from the urn, then they all have the same expected value. Let’s call it &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and thus:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[X_1+X_2+\dots+X_n]=  n\mu
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which is another way of writing the result we show above for the sum of draws.&lt;/p&gt;
&lt;p&gt;2. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[aX] =  a\times\mbox{E}[X]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; again:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{E}[(X_1+X_2+\dots+X_n) / n]=   \mbox{E}[X_1+X_2+\dots+X_n] / n = n\mu/n = \mu
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;3. The square of the standard error of the sum of &lt;strong&gt;independent&lt;/strong&gt; random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SE}[X_1+X_2+\dots+X_n] = \sqrt{\mbox{SE}[X_1]^2 + \mbox{SE}[X_2]^2+\dots+\mbox{SE}[X_n]^2  }
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The square of the standard error is referred to as the &lt;em&gt;variance&lt;/em&gt; in statistical textbooks. Note that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.&lt;/p&gt;
&lt;p&gt;4. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation:
&lt;span class=&#34;math display&#34;&gt;\[
\mbox{SE}[aX] =  a \times \mbox{SE}[X]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To see why this is intuitive, again think of units.&lt;/p&gt;
&lt;p&gt;A consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; (the number of draws), call it &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mbox{SE}[(X_1+X_2+\dots+X_n) / n] &amp;amp;=   \mbox{SE}[X_1+X_2+\dots+X_n]/n \\
&amp;amp;= \sqrt{\mbox{SE}[X_1]^2+\mbox{SE}[X_2]^2+\dots+\mbox{SE}[X_n]^2}/n \\
&amp;amp;= \sqrt{\sigma^2+\sigma^2+\dots+\sigma^2}/n\\
&amp;amp;= \sqrt{n\sigma^2}/n\\
&amp;amp;= \sigma / \sqrt{n}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;5. If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a normally distributed random variable, then if &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; are non-random constants, &lt;span class=&#34;math inline&#34;&gt;\(aX + b\)&lt;/span&gt; is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, then shifting the center by &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that statistical textbooks use the Greek letters &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to denote the expected value and standard error, respectively. This is because &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the Greek letter for &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt;, the first letter of &lt;em&gt;mean&lt;/em&gt;, which is another term used for expected value. Similarly, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is the Greek letter for &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, the first letter of standard error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;law-of-large-numbers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Law of large numbers&lt;/h2&gt;
&lt;p&gt;An important implication of the final result is that the standard error of the average becomes smaller and smaller as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; grows larger. When &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages.&lt;/p&gt;
&lt;div id=&#34;misinterpreting-law-of-averages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Misinterpreting law of averages&lt;/h3&gt;
&lt;p&gt;The law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. These events are independent so the chance of a coin landing heads is 50% regardless of the previous 5. This is also the case for the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses.&lt;/p&gt;
&lt;p&gt;Another funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Urn_problem&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Urn_problem&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem&#34; class=&#34;uri&#34;&gt;https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Monty_Hall_problem&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Monty_Hall_problem&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Security_(finance)&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/w/index.php?title=Security_(finance)&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Binomial_distribution&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/w/index.php?title=Binomial_distribution&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/w/index.php?title=Poisson_distribution&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/w/index.php?title=Poisson_distribution&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizations in Practice</title>
      <link>/content/03-content/</link>
      <pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/content/03-content/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#required-reading&#34;&gt;Required Reading&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#supplemental-readings&#34;&gt;Supplemental Readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#for-fun&#34;&gt;For “Fun”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gapminder&#34;&gt;Data visualization in practice&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-new-insights-on-poverty&#34;&gt;Case study: new insights on poverty&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-the-data&#34;&gt;Exploring the Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#scatterplots&#34;&gt;Scatterplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#faceting&#34;&gt;Faceting&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#facet_wrap&#34;&gt;&lt;code&gt;facet_wrap&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-scales-for-better-comparisons&#34;&gt;Fixed scales for better comparisons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#time-series-plots&#34;&gt;Time series plots&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#labels-instead-of-legends&#34;&gt;Labels instead of legends&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-transformations&#34;&gt;Data transformations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#log-transformation&#34;&gt;Log transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#which-base&#34;&gt;Which base?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transform-the-values-or-the-scale&#34;&gt;Transform the values or the scale?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualizing-multimodal-distributions&#34;&gt;Visualizing multimodal distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-multiple-distributions-with-boxplots-and-ridge-plots&#34;&gt;Comparing multiple distributions with boxplots and ridge plots&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#boxplots&#34;&gt;Boxplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ridge-plots&#34;&gt;Ridge plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example-1970-versus-2010-income-distributions&#34;&gt;Example: 1970 versus 2010 income distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#accessing-computed-variables&#34;&gt;Accessing computed variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weighted-densities&#34;&gt;Weighted densities&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-ecological-fallacy-and-importance-of-showing-the-data&#34;&gt;The ecological fallacy and importance of showing the data&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#logit&#34;&gt;Logistic transformation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#show-the-data&#34;&gt;Show the data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vaccines&#34;&gt;Case study: vaccines and infectious diseases&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;required-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Required Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;supplemental-readings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Supplemental Readings&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://serialmentor.com/dataviz/visualizing-amounts.html&#34;&gt;Chapter 6&lt;/a&gt; in Claus Wilke, &lt;em&gt;Fundamentals of Data Visualization&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;[@Wilke:2018]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; Chapter 6 in Alberto Cairo, &lt;em&gt;The Truthful Art&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;[@Cairo:2016]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-book&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://serialmentor.com/dataviz/visualizing-proportions.html&#34;&gt;Chapter 10&lt;/a&gt; in Claus Wilke, &lt;em&gt;Fundamentals of Data Visualization&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;[@Wilke:2018]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://eagereyes.org/blog/2008/engaging-readers-with-square-pie-waffle-charts&#34;&gt;Engaging Readers with Square Pie/Waffle Charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://eagereyes.org/techniques/pie-charts&#34;&gt;Understanding Pie Charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://flowingdata.com/2016/07/15/square-pie-chart-beats-out-the-rest-in-perception-study/&#34;&gt;Square pie chart beats out the rest in perception study&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fab fa-twitter&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://twitter.com/janinegibson/status/1244519429825802240&#34;&gt;Twitter thread&lt;/a&gt; from John Burn-Murdoch on why the &lt;em&gt;Financial Times&lt;/em&gt; uses log scales in their COVID-19 tracking charts&lt;/li&gt;
&lt;li&gt;&lt;i class=&#34;fab fa-twitter&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1238914490772701185&#34;&gt;Tweet&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/jburnmurdoch/status/1242904596856614912&#34;&gt;Twitter thread&lt;/a&gt; from John Burn-Murdoch on why the &lt;em&gt;Financial Times&lt;/em&gt; doesn’t use population-adjusted numbers in their COVID-19 tracking charts&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;for-fun&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;For “Fun”&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i class=&#34;fas fa-external-link-square-alt&#34;&gt;&lt;/i&gt; &lt;a href=&#34;https://robjhyndman.com/hyndsight/logratios-covid19/&#34;&gt;See how to create your own COVID-19 tracking chart with R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;How do these types of visualizations help or hinder our search for truth in data?&lt;/li&gt;
&lt;li&gt;What is the appropriate visualization technique for the pandemic? Should we use population-adjusted numbers? Why or why not?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slides&lt;/h3&gt;
&lt;p&gt;As with last week, today’s lecture will ask you to work with real data during the lecture. Moreover, we will again avoid slides entirely. To follow along, please have &lt;code&gt;R&lt;/code&gt; open throughout lecture and work through this reading on your own time to lock in some of the key concepts.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;gapminder&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data visualization in practice&lt;/h1&gt;
&lt;p&gt;In this chapter, we will demonstrate how relatively simple &lt;strong&gt;ggplot2&lt;/strong&gt; code can create insightful and aesthetically pleasing plots. As motivation we will create plots that help us better understand trends in world health and economics. We will implement what we learned in previous sections of the class and learn how to augment the code to perfect the plots. As we go through our case study, we will describe relevant general data visualization principles and learn concepts such as &lt;em&gt;faceting&lt;/em&gt;, &lt;em&gt;time series plots&lt;/em&gt;, &lt;em&gt;transformations&lt;/em&gt;, and &lt;em&gt;ridge plots&lt;/em&gt;.&lt;/p&gt;
&lt;div id=&#34;case-study-new-insights-on-poverty&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: new insights on poverty&lt;/h2&gt;
&lt;p&gt;Hans Rosling&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; was the co-founder of the Gapminder Foundation&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world. The organization uses data to show how actual trends in health and economics contradict the narratives that emanate from sensationalist media coverage of catastrophes, tragedies, and other unfortunate events. As stated in the Gapminder Foundation’s website:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Journalists and lobbyists tell dramatic stories. That’s their job. They tell stories about extraordinary events and unusual people. The piles of dramatic stories pile up in peoples’ minds into an over-dramatic worldview and strong negative stress feelings: “The world is getting worse!”, “It’s we vs. them!”, “Other people are strange!”, “The population just keeps growing!” and “Nobody cares!”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hans Rosling conveyed actual data-based trends in a dramatic way of his own, using effective data visualization. This section is based on two talks that exemplify this approach to education: [New Insights on Poverty]&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; and The Best Stats You’ve Ever Seen&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Specifically, in this section, we use data to attempt to answer the following two questions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Is it a fair characterization of today’s world to say it is divided into western rich nations and the developing world in Africa, Asia, and Latin America?&lt;/li&gt;
&lt;li&gt;Has income inequality across countries worsened during the last 40 years?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To answer these questions, we will be using the &lt;code&gt;gapminder&lt;/code&gt; dataset provided in &lt;strong&gt;dslabs&lt;/strong&gt;. This dataset was created using a number of spreadsheets available from the Gapminder Foundation. You can access the table like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
library(ggrepel)
data(gapminder)
gapminder %&amp;gt;% as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10,545 x 9
##    country  year infant_mortality life_expectancy fertility population      gdp
##    &amp;lt;fct&amp;gt;   &amp;lt;int&amp;gt;            &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 Albania  1960            115.             62.9      6.19    1636054 NA      
##  2 Algeria  1960            148.             47.5      7.65   11124892  1.38e10
##  3 Angola   1960            208              36.0      7.32    5270844 NA      
##  4 Antigu…  1960             NA              63.0      4.43      54681 NA      
##  5 Argent…  1960             59.9            65.4      3.11   20619075  1.08e11
##  6 Armenia  1960             NA              66.9      4.55    1867396 NA      
##  7 Aruba    1960             NA              65.7      4.82      54208 NA      
##  8 Austra…  1960             20.3            70.9      3.45   10292328  9.67e10
##  9 Austria  1960             37.3            68.8      2.7     7065525  5.24e10
## 10 Azerba…  1960             NA              61.3      5.57    3897889 NA      
## # … with 10,535 more rows, and 2 more variables: continent &amp;lt;fct&amp;gt;, region &amp;lt;fct&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exploring-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploring the Data&lt;/h3&gt;
&lt;p&gt;Taking an exercise from the &lt;em&gt;New Insights on Poverty&lt;/em&gt; video, we start by testing our knowledge regarding differences in child mortality across different countries. For each of the six pairs of countries below, which country do you think had the highest child mortality rates in 2015? Which pairs do you think are most similar?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sri Lanka or Turkey&lt;/li&gt;
&lt;li&gt;Poland or South Korea&lt;/li&gt;
&lt;li&gt;Malaysia or Russia&lt;/li&gt;
&lt;li&gt;Pakistan or Vietnam&lt;/li&gt;
&lt;li&gt;Thailand or South Africa&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When answering these questions without data, the non-European countries are typically picked as having higher child mortality rates: Sri Lanka over Turkey, South Korea over Poland, and Malaysia over Russia. It is also common to assume that countries considered to be part of the developing world: Pakistan, Vietnam, Thailand, and South Africa, have similarly high mortality rates.&lt;/p&gt;
&lt;p&gt;To answer these questions &lt;strong&gt;with data&lt;/strong&gt;, we can use &lt;strong&gt;dplyr&lt;/strong&gt;. For example, for the first comparison we see that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year == 2015 &amp;amp; country %in% c(&amp;quot;Sri Lanka&amp;quot;,&amp;quot;Turkey&amp;quot;)) %&amp;gt;%
  select(country, infant_mortality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     country infant_mortality
## 1 Sri Lanka              8.4
## 2    Turkey             11.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Turkey has the higher infant mortality rate.&lt;/p&gt;
&lt;p&gt;We can use this code on all comparisons and find the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## New names:
## * country -&amp;gt; country...1
## * infant_mortality -&amp;gt; infant_mortality...2
## * country -&amp;gt; country...3
## * infant_mortality -&amp;gt; infant_mortality...4&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
country
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
infant mortality
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
country
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
infant mortality
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Sri Lanka
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Turkey
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
11.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Poland
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Korea
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Malaysia
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Russia
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pakistan
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65.8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Vietnam
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
17.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Thailand
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10.5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
South Africa
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
33.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that the European countries on this list have higher child mortality rates: Poland has a higher rate than South Korea, and Russia has a higher rate than Malaysia. We also see that Pakistan has a much higher rate than Vietnam, and South Africa has a much higher rate than Thailand. It turns out that when Hans Rosling gave this quiz to educated groups of people, the average score was less than 2.5 out of 5, worse than what they would have obtained had they guessed randomly. This implies that more than ignorant, we are misinformed. In this chapter we see how data visualization helps inform us.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;scatterplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scatterplots&lt;/h2&gt;
&lt;p&gt;The reason for this stems from the preconceived notion that the world is divided into two groups: the western world (Western Europe and North America), characterized by long life spans and small families, versus the developing world (Africa, Asia, and Latin America) characterized by short life spans and large families. But do the data support this dichotomous view?&lt;/p&gt;
&lt;p&gt;The necessary data to answer this question is also available in our &lt;code&gt;gapminder&lt;/code&gt; table. Using our newly learned data visualization skills, we will be able to tackle this challenge.&lt;/p&gt;
&lt;p&gt;In order to analyze this world view, our first plot is a scatterplot of life expectancy versus fertility rates (average number of children per woman). We start by looking at data from about 50 years ago, when perhaps this view was first cemented in our minds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(gapminder, year == 1962) %&amp;gt;%
  ggplot(aes(fertility, life_expectancy)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/fertility-versus-life-expectancy-1962-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most points fall into two distinct categories:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Life expectancy around 70 years and 3 or fewer children per family.&lt;/li&gt;
&lt;li&gt;Life expectancy lower than 65 years and more than 5 children per family.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To confirm that indeed these countries are from the regions we expect, we can use color to represent continent.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(gapminder, year == 1962) %&amp;gt;%
  ggplot( aes(fertility, life_expectancy, color = continent)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/fertility-versus-life-expectancy-1962-with-color-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In 1962, the notion of “the West versus developing world” view was grounded in some reality. Is this still the case 50 years later? How might visualizations help us learn something? Before continuing, make a note of your prior beliefs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;faceting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Faceting&lt;/h2&gt;
&lt;p&gt;We could easily plot the 2012 data in the same way we did for 1962. To make comparisons, however, side by side plots are preferable. In &lt;strong&gt;ggplot2&lt;/strong&gt;, we can achieve this by &lt;em&gt;faceting&lt;/em&gt; variables: we stratify the data by some variable and make the same plot for each strata.&lt;/p&gt;
&lt;p&gt;To achieve faceting, we add a layer with the function &lt;code&gt;facet_grid&lt;/code&gt;, which automatically separates the plots. This function lets you facet by up to two variables using columns to represent one variable and rows to represent the other. The function expects the row and column variables to be separated by a &lt;code&gt;~&lt;/code&gt;. Here is an example of a scatterplot with &lt;code&gt;facet_grid&lt;/code&gt; added as the last layer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(gapminder, year%in%c(1962, 2012)) %&amp;gt;%
  ggplot(aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_grid(continent~year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/fertility-versus-life-expectancy-facet-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see a plot for each continent/year pair. However, this is just an example and more than what we want, which is simply to compare 1962 and 2012. In this case, there is just one variable and we use &lt;code&gt;.&lt;/code&gt; to let facet know that we are not using one of the variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(gapminder, year%in%c(1962, 2012)) %&amp;gt;%
  ggplot(aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_grid(. ~ year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/fertility-versus-life-expectancy-two-years-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot clearly shows that the majority of countries have moved from the &lt;em&gt;developing world&lt;/em&gt; cluster to the &lt;em&gt;western world&lt;/em&gt; one. In 2012, the western versus developing world view no longer makes sense. This is particularly clear when comparing Europe to Asia, the latter of which includes several countries that have made great improvements.&lt;/p&gt;
&lt;div id=&#34;facet_wrap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;facet_wrap&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;To explore how this transformation happened through the years, we can make the plot for several years. For example, we can add 1970, 1980, 1990, and 2000. If we do this, we will not want all the plots on the same row, the default behavior of &lt;code&gt;facet_grid&lt;/code&gt;, since they will become too thin to show the data. Instead, we will want to use multiple rows and columns. The function &lt;code&gt;facet_wrap&lt;/code&gt; permits us to do this by automatically wrapping the series of plots so that each display has viewable dimensions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;years &amp;lt;- c(1962, 1980, 1990, 2000, 2012)
continents &amp;lt;- c(&amp;quot;Europe&amp;quot;, &amp;quot;Asia&amp;quot;)
gapminder %&amp;gt;%
  filter(year %in% years &amp;amp; continent %in% continents) %&amp;gt;%
  ggplot( aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_wrap(~year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/fertility-versus-life-expectancy-five-years-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot clearly shows how most Asian countries have improved at a much faster rate than European ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fixed-scales-for-better-comparisons&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fixed scales for better comparisons&lt;/h3&gt;
&lt;p&gt;The default choice of the range of the axes is important. When not using &lt;code&gt;facet&lt;/code&gt;, this range is determined by the data shown in the plot. When using &lt;code&gt;facet&lt;/code&gt;, this range is determined by the data shown in all plots and therefore kept fixed across plots. This makes comparisons across plots much easier. For example, in the above plot, we can see that life expectancy has increased and the fertility has decreased across most countries. We see this because the cloud of points moves. This is not the case if we adjust the scales:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(gapminder, year%in%c(1962, 2012)) %&amp;gt;%
  ggplot(aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_wrap(. ~ year, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/facet-without-fixed-scales-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the plot above, we have to pay special attention to the range to notice that the plot on the right has a larger life expectancy.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;time-series-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Time series plots&lt;/h2&gt;
&lt;p&gt;The visualizations above effectively illustrate that data no longer supports the western versus developing world view. Once we see these plots, new questions emerge. For example, which countries are improving more and which ones less? Was the improvement constant during the last 50 years or was it more accelerated during certain periods? For a closer look that may help answer these questions, we introduce &lt;em&gt;time series plots&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Time series plots have time in the x-axis and an outcome or measurement of interest on the y-axis. For example, here is a trend plot of United States fertility rates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(country == &amp;quot;United States&amp;quot;) %&amp;gt;%
  ggplot(aes(year, fertility)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/fertility-time-series-plot-points-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the trend is not linear at all. Instead there is sharp drop during the 1960s and 1970s to below 2. Then the trend comes back to 2 and stabilizes during the 1990s.&lt;/p&gt;
&lt;p&gt;When the points are regularly and densely spaced, as they are here, we create curves by joining the points with lines, to convey that these data are from a single series, here a country. To do this, we use the &lt;code&gt;geom_line&lt;/code&gt; function instead of &lt;code&gt;geom_point&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(country == &amp;quot;United States&amp;quot;) %&amp;gt;%
  ggplot(aes(year, fertility)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/fertility-time-series-plot-curve-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is particularly helpful when we look at two countries. If we subset the data to include two countries, one from Europe and one from Asia, then adapt the code above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;countries &amp;lt;- c(&amp;quot;South Korea&amp;quot;,&amp;quot;Germany&amp;quot;)

gapminder %&amp;gt;% filter(country %in% countries) %&amp;gt;%
  ggplot(aes(year,fertility)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/wrong-time-series-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, this is &lt;strong&gt;not&lt;/strong&gt; the plot that we want. Rather than a line for each country, the points for both countries are joined. This is actually expected since we have not told &lt;code&gt;ggplot&lt;/code&gt; anything about wanting two separate lines. To let &lt;code&gt;ggplot&lt;/code&gt; know that there are two curves that need to be made separately, we assign each point to a &lt;code&gt;group&lt;/code&gt;, one for each country:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;countries &amp;lt;- c(&amp;quot;South Korea&amp;quot;,&amp;quot;Germany&amp;quot;)

gapminder %&amp;gt;% filter(country %in% countries &amp;amp; !is.na(fertility)) %&amp;gt;%
  ggplot(aes(year, fertility, group = country)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/time-series-two-curves-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But which line goes with which country? We can assign colors to make this distinction.
A useful side-effect of using the &lt;code&gt;color&lt;/code&gt; argument to assign different colors to the different countries is that the data is automatically grouped:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;countries &amp;lt;- c(&amp;quot;South Korea&amp;quot;,&amp;quot;Germany&amp;quot;)

gapminder %&amp;gt;% filter(country %in% countries &amp;amp; !is.na(fertility)) %&amp;gt;%
  ggplot(aes(year,fertility, col = country)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/fertility-time-series-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot clearly shows how South Korea’s fertility rate dropped drastically during the 1960s and 1970s, and by 1990 had a similar rate to that of Germany.&lt;/p&gt;
&lt;div id=&#34;labels-instead-of-legends&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Labels instead of legends&lt;/h3&gt;
&lt;p&gt;For trend plots we recommend labeling the lines rather than using legends since the viewer can quickly see which line is which country. This suggestion actually applies to most plots: labeling is usually preferred over legends.&lt;/p&gt;
&lt;p&gt;We demonstrate how we can do this using the life expectancy data. We define a data table with the label locations and then use a second mapping just for these labels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels &amp;lt;- data.frame(country = countries, x = c(1975,1965), y = c(60,72))

gapminder %&amp;gt;%
  filter(country %in% countries) %&amp;gt;%
  ggplot(aes(year, life_expectancy, col = country)) +
  geom_line() +
  geom_text(data = labels, aes(x, y, label = country), size = 5) +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/labels-better-than-legends-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot clearly shows how an improvement in life expectancy followed the drops in fertility rates. In 1960, Germans lived 15 years longer than South Koreans, although by 2010 the gap is completely closed. It exemplifies the improvement that many non-western countries have achieved in the last 40 years.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-transformations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data transformations&lt;/h2&gt;
&lt;p&gt;We now shift our attention to the second question related to the commonly held notion that wealth distribution across the world has become worse during the last decades. When general audiences are asked if poor countries have become poorer and rich countries become richer, the majority answers yes. By using stratification, histograms, smooth densities, and boxplots, we will be able to understand if this is in fact the case. First we learn how transformations can sometimes help provide more informative summaries and plots.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;gapminder&lt;/code&gt; data table includes a column with the countries’ gross domestic product (GDP). GDP measures the market value of goods and services produced by a country in a year. The GDP per person is often used as a rough summary of a country’s wealth. Here we divide this quantity by 365 to obtain the more interpretable measure &lt;em&gt;dollars per day&lt;/em&gt;. Using current US dollars as a unit, a person surviving on an income of less than $2 a day is defined to be living in &lt;em&gt;absolute poverty&lt;/em&gt;. We add this variable to the data table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder &amp;lt;- gapminder %&amp;gt;%  mutate(dollars_per_day = gdp/population/365)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The GDP values are adjusted for inflation and represent current US dollars, so these values are meant to be comparable across the years. Of course, these are country averages and within each country there is much variability. All the graphs and insights described below relate to country averages and not to individuals.&lt;/p&gt;
&lt;div id=&#34;log-transformation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Log transformation&lt;/h3&gt;
&lt;p&gt;Here is a histogram of per day incomes from 1970:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;past_year &amp;lt;- 1970
gapminder %&amp;gt;%
  filter(year == past_year &amp;amp; !is.na(gdp)) %&amp;gt;%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/dollars-per-day-distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We use the &lt;code&gt;color = &#34;black&#34;&lt;/code&gt; argument to draw a boundary and clearly distinguish the bins.&lt;/p&gt;
&lt;p&gt;In this plot, we see that for the majority of countries, averages are below $10 a day.&lt;/p&gt;
&lt;p&gt;However, the majority of the x-axis is dedicated to the &lt;code&gt;35&lt;/code&gt; countries with averages above $10. So the plot is not very informative about countries with values below $10 a day.&lt;/p&gt;
&lt;p&gt;It might be more informative to quickly be able to see how many countries have average daily incomes of about $1 (extremely poor), $2 (very poor), $4 (poor), $8 (middle), $16 (well off), $32 (rich), $64 (very rich) per day. These changes are multiplicative and log transformations convert multiplicative changes into additive ones: when using base 2, a doubling of a value turns into an increase by 1.&lt;/p&gt;
&lt;p&gt;Here is the distribution if we apply a log base 2 transform:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year == past_year &amp;amp; !is.na(gdp)) %&amp;gt;%
  ggplot(aes(log2(dollars_per_day))) +
  geom_histogram(binwidth = 1, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/dollars-per-day-distribution-log-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In a way this provides a &lt;em&gt;close-up&lt;/em&gt; of the mid to lower income countries.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;which-base&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Which base?&lt;/h3&gt;
&lt;p&gt;In the case above, we used base 2 in the log transformations. Other common choices are base &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{e}\)&lt;/span&gt; (the natural log) and base 10.&lt;/p&gt;
&lt;p&gt;In general, we do not recommend using the natural log for data exploration and visualization. This is because while &lt;span class=&#34;math inline&#34;&gt;\(2^2, 2^3, 2^4, \dots\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(10^2, 10^3, \dots\)&lt;/span&gt; are easy to compute in our heads, the same is not true for &lt;span class=&#34;math inline&#34;&gt;\(\mathrm{e}^2, \mathrm{e}^3, \dots\)&lt;/span&gt;, so the scale is not intuitive or easy to interpret.&lt;/p&gt;
&lt;p&gt;In the dollars per day example, we used base 2 instead of base 10 because the resulting range is easier to interpret. The range of the values being plotted is 0.3269426, 48.8852142.&lt;/p&gt;
&lt;p&gt;In base 10, this turns into a range that includes very few integers: just 0 and 1.
With base two, our range includes -2, -1, 0, 1, 2, 3, 4, and 5. It is easier to compute &lt;span class=&#34;math inline&#34;&gt;\(2^x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(10^x\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is an integer and between -10 and 10, so we prefer to have smaller integers in the scale. Another consequence of a limited range is that choosing the binwidth is more challenging. With log base 2, we know that a binwidth of 1 will translate to a bin with range &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(2x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For an example in which base 10 makes more sense, consider population sizes. A log base 10 is preferable since the range for these is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(gapminder, year == past_year) %&amp;gt;%
  summarize(min = min(population), max = max(population))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     min       max
## 1 46075 808510713&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the histogram of the transformed values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year == past_year) %&amp;gt;%
  ggplot(aes(log10(population))) +
  geom_histogram(binwidth = 0.5, color = &amp;quot;black&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/population-histogram-log10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above, we quickly see that country populations range between ten thousand and ten billion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transform-the-values-or-the-scale&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Transform the values or the scale?&lt;/h3&gt;
&lt;p&gt;There are two ways we can use log transformations in plots. We can log the values before plotting them or use log scales in the axes. Both approaches are useful and have different strengths. If we log the data, we can more easily interpret intermediate values in the scale. For example, if we see:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;----1----x----2--------3----&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;for log transformed data, we know that the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is about 1.5. If the scales are logged:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;----1----x----10------100---&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;then, to determine &lt;code&gt;x&lt;/code&gt;, we need to compute &lt;span class=&#34;math inline&#34;&gt;\(10^{1.5}\)&lt;/span&gt;, which is not easy to do in our heads. The advantage of using logged scales is that we see the original values on the axes. However, the advantage of showing logged scales is that the original values are displayed in the plot, which are easier to interpret. For example, we would see “32 dollars a day” instead of “5 log base 2 dollars a day”.&lt;/p&gt;
&lt;p&gt;As we learned earlier, if we want to scale the axis with logs, we can use the &lt;code&gt;scale_x_continuous&lt;/code&gt; function. Instead of logging the values first, we apply this layer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year == past_year &amp;amp; !is.na(gdp)) %&amp;gt;%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, color = &amp;quot;black&amp;quot;) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/dollars-per-day-log-scale-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the log base 10 transformation has its own function: &lt;code&gt;scale_x_log10()&lt;/code&gt;, but currently base 2 does not, although we could easily define our own.&lt;/p&gt;
&lt;p&gt;There are other transformations available through the &lt;code&gt;trans&lt;/code&gt; argument. As we learn later on, the square root (&lt;code&gt;sqrt&lt;/code&gt;) transformation is useful when considering counts. The logistic transformation (&lt;code&gt;logit&lt;/code&gt;) is useful when plotting proportions between 0 and 1. The &lt;code&gt;reverse&lt;/code&gt; transformation is useful when we want smaller values to be on the right or on top.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-multimodal-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing multimodal distributions&lt;/h2&gt;
&lt;p&gt;In the histogram above we see two &lt;em&gt;bumps&lt;/em&gt;: one at about 4 and another at about 32. In statistics these bumps are sometimes referred to as &lt;em&gt;modes&lt;/em&gt;. The mode of a distribution is the value with the highest frequency. The mode of the normal distribution is the average. When a distribution, like the one above, doesn’t monotonically decrease from the mode, we call the locations where it goes up and down again &lt;em&gt;local modes&lt;/em&gt; and say that the distribution has &lt;em&gt;multiple modes&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The histogram above suggests that the 1970 country income distribution has two modes: one at about 2 dollars per day (1 in the log 2 scale) and another at about 32 dollars per day (5 in the log 2 scale). This &lt;em&gt;bimodality&lt;/em&gt; is consistent with a dichotomous world made up of countries with average incomes less than $8 (3 in the log 2 scale) a day and countries above that.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-multiple-distributions-with-boxplots-and-ridge-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing multiple distributions with boxplots and ridge plots&lt;/h2&gt;
&lt;p&gt;A histogram showed us that the 1970 income distribution values show a dichotomy. However, the histogram does not show us if the two groups of countries are &lt;em&gt;west&lt;/em&gt; versus the &lt;em&gt;developing&lt;/em&gt; world.&lt;/p&gt;
&lt;p&gt;Let’s start by quickly examining the data by region. We reorder the regions by the median value and use a log scale.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year == past_year &amp;amp; !is.na(gdp)) %&amp;gt;%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %&amp;gt;%
  ggplot(aes(dollars_per_day, region)) +
  geom_point() +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can already see that there is indeed a “west versus the rest” dichotomy: we see two clear groups, with the rich group composed of North America, Northern and Western Europe, New Zealand and Australia. We define groups based on this observation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder &amp;lt;- gapminder %&amp;gt;%
  mutate(group = case_when(
    region %in% c(&amp;quot;Western Europe&amp;quot;, &amp;quot;Northern Europe&amp;quot;,&amp;quot;Southern Europe&amp;quot;,
                    &amp;quot;Northern America&amp;quot;,
                  &amp;quot;Australia and New Zealand&amp;quot;) ~ &amp;quot;West&amp;quot;,
    region %in% c(&amp;quot;Eastern Asia&amp;quot;, &amp;quot;South-Eastern Asia&amp;quot;) ~ &amp;quot;East Asia&amp;quot;,
    region %in% c(&amp;quot;Caribbean&amp;quot;, &amp;quot;Central America&amp;quot;,
                  &amp;quot;South America&amp;quot;) ~ &amp;quot;Latin America&amp;quot;,
    continent == &amp;quot;Africa&amp;quot; &amp;amp;
      region != &amp;quot;Northern Africa&amp;quot; ~ &amp;quot;Sub-Saharan&amp;quot;,
    TRUE ~ &amp;quot;Others&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We turn this &lt;code&gt;group&lt;/code&gt; variable into a factor to control the order of the levels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder &amp;lt;- gapminder %&amp;gt;%
  mutate(group = factor(group, levels = c(&amp;quot;Others&amp;quot;, &amp;quot;Latin America&amp;quot;,
                                          &amp;quot;East Asia&amp;quot;, &amp;quot;Sub-Saharan&amp;quot;,
                                          &amp;quot;West&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the next section we demonstrate how to visualize and compare distributions across groups.&lt;/p&gt;
&lt;div id=&#34;boxplots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Boxplots&lt;/h3&gt;
&lt;p&gt;The exploratory data analysis above has revealed two characteristics about average income distribution in 1970. Using a histogram, we found a bimodal distribution with the modes relating to poor and rich countries. We now want to compare the distribution across these five groups to confirm the “west versus the rest” dichotomy. The number of points in each category is large enough that a summary plot may be useful. We could generate five histograms or five density plots, but it may be more practical to have all the visual summaries in one plot. We therefore start by stacking boxplots next to each other. Note that we add the layer &lt;code&gt;theme(axis.text.x = element_text(angle = 90, hjust = 1))&lt;/code&gt; to turn the group labels vertical, since they do not fit if we show them horizontally, and remove the axis label to make space.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- gapminder %&amp;gt;%
  filter(year == past_year &amp;amp; !is.na(gdp)) %&amp;gt;%
  ggplot(aes(group, dollars_per_day)) +
  geom_boxplot() +
  scale_y_continuous(trans = &amp;quot;log2&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/dollars-per-day-boxplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Boxplots have the limitation that by summarizing the data into five numbers, we might miss important characteristics of the data. One way to avoid this is by showing the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_point(alpha = 0.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/dollars-per-day-boxplot-with-data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ridge-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ridge plots&lt;/h3&gt;
&lt;p&gt;Showing each individual point does not always reveal important characteristics of the distribution. Although not the case here, when the number of data points is so large that there is over-plotting, showing the data can be counterproductive. Boxplots help with this by providing a five-number summary, but this has limitations too. For example, boxplots will not permit us to discover bimodal distributions. To see this, note that the two plots below are summarizing the same dataset:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/boxplot-dont-show-bimodal-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In cases in which we are concerned that the boxplot summary is too simplistic, we can show stacked smooth densities or histograms. We refer to these as &lt;em&gt;ridge plots&lt;/em&gt;. Because we are used to visualizing densities with values in the x-axis, we stack them vertically. Also, because more space is needed in this approach, it is convenient to overlay them. The package &lt;strong&gt;ggridges&lt;/strong&gt; provides a convenient function for doing this. Here is the income data shown above with boxplots but with a &lt;em&gt;ridge plot&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggridges)
p &amp;lt;- gapminder %&amp;gt;%
  filter(year == past_year &amp;amp; !is.na(dollars_per_day)) %&amp;gt;%
  ggplot(aes(dollars_per_day, group)) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;)
p  + geom_density_ridges()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/ridge-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that we have to invert the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; used for the boxplot. A useful &lt;code&gt;geom_density_ridges&lt;/code&gt; parameter is &lt;code&gt;scale&lt;/code&gt;, which lets you determine the amount of overlap, with &lt;code&gt;scale = 1&lt;/code&gt; meaning no overlap and larger values resulting in more overlap.&lt;/p&gt;
&lt;p&gt;If the number of data points is small enough, we can add them to the ridge plot using the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_density_ridges(jittered_points = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/ridge-plot-with-points-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By default, the height of the points is jittered and should not be interpreted in any way. To show data points, but without using jitter we can use the following code to add what is referred to as a &lt;em&gt;rug representation&lt;/em&gt; of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_density_ridges(jittered_points = TRUE,
                        position = position_points_jitter(height = 0),
                        point_shape = &amp;#39;|&amp;#39;, point_size = 3,
                        point_alpha = 1, alpha = 0.7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/ridge-plot-with-rug-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1970-versus-2010-income-distributions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example: 1970 versus 2010 income distributions&lt;/h3&gt;
&lt;p&gt;Data exploration clearly shows that in 1970 there was a “west versus the rest” dichotomy. But does this dichotomy persist? Let’s use &lt;code&gt;facet_grid&lt;/code&gt; see how the distributions have changed. To start, we will focus on two groups: the west and the rest. We make four histograms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;past_year &amp;lt;- 1970
present_year &amp;lt;- 2010
years &amp;lt;- c(past_year, present_year)
gapminder %&amp;gt;%
  filter(year %in% years &amp;amp; !is.na(gdp)) %&amp;gt;%
  mutate(west = ifelse(group == &amp;quot;West&amp;quot;, &amp;quot;West&amp;quot;, &amp;quot;Developing&amp;quot;)) %&amp;gt;%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, color = &amp;quot;black&amp;quot;) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;) +
  facet_grid(year ~ west)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-hist-west-v-developing-two-years-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Before we interpret the findings of this plot, we notice that there are more countries represented in the 2010 histograms than in 1970: the total counts are larger. One reason for this is that several countries were founded after 1970. For example, the Soviet Union divided into several countries during the 1990s. Another reason is that data was available for more countries in 2010.&lt;/p&gt;
&lt;p&gt;We remake the plots using only countries with data available for both years. In the data wrangling part of this class (after Thanksgiving), we will learn &lt;strong&gt;tidyverse&lt;/strong&gt; tools that permit us to write efficient code for this, but here we can use simple code using the &lt;code&gt;intersect&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country_list_1 &amp;lt;- gapminder %&amp;gt;%
  filter(year == past_year &amp;amp; !is.na(dollars_per_day)) %&amp;gt;%
  pull(country)

country_list_2 &amp;lt;- gapminder %&amp;gt;%
  filter(year == present_year &amp;amp; !is.na(dollars_per_day)) %&amp;gt;%
  pull(country)

country_list &amp;lt;- intersect(country_list_1, country_list_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These 108 account for
86% of the world population, so this subset should be representative.&lt;/p&gt;
&lt;p&gt;Let’s remake the plot, but only for this subset by simply adding &lt;code&gt;country %in% country_list&lt;/code&gt; to the &lt;code&gt;filter&lt;/code&gt; function:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-histogram-west-v-devel-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now see that the rich countries have become a bit richer, but percentage-wise, the poor countries appear to have improved more. In particular, we see that the proportion of &lt;em&gt;developing&lt;/em&gt; countries earning more than $16 a day increased substantially.&lt;/p&gt;
&lt;p&gt;To see which specific regions improved the most, we can remake the boxplots we made above, but now adding the year 2010 and then using facet to compare the two years.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year %in% years &amp;amp; country %in% country_list) %&amp;gt;%
  ggplot(aes(group, dollars_per_day)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_y_continuous(trans = &amp;quot;log2&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) +
  facet_grid(. ~ year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-histogram-by-region-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, we pause to introduce another powerful &lt;strong&gt;ggplot2&lt;/strong&gt; feature. Because we want to compare each region before and after, it would be convenient to have the 1970 boxplot next to the 2010 boxplot for each region. In general, comparisons are easier when data are plotted next to each other.&lt;/p&gt;
&lt;p&gt;So instead of faceting, we keep the data from each year together and ask to color (or fill) them depending on the year. Note that groups are automatically separated by year and each pair of boxplots drawn next to each other. Because year is a number, we turn it into a factor since &lt;strong&gt;ggplot2&lt;/strong&gt; automatically assigns a color to each category of a factor. Note that we have to convert the year columns from numeric to factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year %in% years &amp;amp; country %in% country_list) %&amp;gt;%
  mutate(year = factor(year)) %&amp;gt;%
  ggplot(aes(group, dollars_per_day, fill = year)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_y_continuous(trans = &amp;quot;log2&amp;quot;) +
  xlab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-histogram-west-v-devel-by-year-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we point out that if what we are most interested in is comparing before and after values, it might make more sense to plot the percentage increases. We are still not ready to learn to code this, but here is what the plot would look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-west-v-devel-before-after-ratio-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The previous data exploration suggested that the income gap between rich and poor countries has narrowed considerably during the last 40 years.
We used a series of histograms and boxplots to see this. We suggest a succinct way to convey this message with just one plot.&lt;/p&gt;
&lt;p&gt;Let’s start by noting that density plots for income distribution in 1970 and 2010 deliver the message that the gap is closing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year %in% years &amp;amp; country %in% country_list) %&amp;gt;%
  ggplot(aes(dollars_per_day)) +
  geom_density(fill = &amp;quot;grey&amp;quot;) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;) +
  facet_grid(. ~ year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-smooth-density-by-year-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the 1970 plot, we see two clear modes: poor and rich countries. In 2010, it appears that some of the poor countries have shifted towards the right, closing the gap.&lt;/p&gt;
&lt;p&gt;The next message we need to convey is that the reason for this change in distribution is that several poor countries became richer, rather than some rich countries becoming poorer. To do this, we can assign a color to the groups we identified during data exploration.&lt;/p&gt;
&lt;p&gt;However, we first need to learn how to make these smooth densities in a way that preserves information on the number of countries in each group. To understand why we need this, note the discrepancy in the size of each group:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Developing
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
West
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
87
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;But when we overlay two densities, the default is to have the area represented by each distribution add up to 1, regardless of the size of each group:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year %in% years &amp;amp; country %in% country_list) %&amp;gt;%
  mutate(group = ifelse(group == &amp;quot;West&amp;quot;, &amp;quot;West&amp;quot;, &amp;quot;Developing&amp;quot;)) %&amp;gt;%
  ggplot(aes(dollars_per_day, fill = group)) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;) +
  geom_density(alpha = 0.2) +
  facet_grid(year ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-smooth-density-by-year-west-v-developing-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This makes it appear as if there are the same number of countries in each group. To change this, we will need to learn to access computed variables with &lt;code&gt;geom_density&lt;/code&gt; function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accessing-computed-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Accessing computed variables&lt;/h3&gt;
&lt;p&gt;To have the areas of these densities be proportional to the size of the groups, we can simply multiply the y-axis values by the size of the group. From the &lt;code&gt;geom_density&lt;/code&gt; help file, we see that the functions compute a variable called &lt;code&gt;count&lt;/code&gt; that does exactly this. We want this variable to be on the y-axis rather than the density.&lt;/p&gt;
&lt;p&gt;In &lt;strong&gt;ggplot2&lt;/strong&gt;, we access these variables by surrounding the name with two dots. We will therefore use the following mapping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aes(x = dollars_per_day, y = ..count..)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now create the desired plot by simply changing the mapping in the previous code chunk. We will also expand the limits of the x-axis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- gapminder %&amp;gt;%
  filter(year %in% years &amp;amp; country %in% country_list) %&amp;gt;%
  mutate(group = ifelse(group == &amp;quot;West&amp;quot;, &amp;quot;West&amp;quot;, &amp;quot;Developing&amp;quot;)) %&amp;gt;%
  ggplot(aes(dollars_per_day, y = ..count.., fill = group)) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;, limit = c(0.125, 300))

p + geom_density(alpha = 0.2) +
  facet_grid(year ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-smooth-density-counts-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we want the densities to be smoother, we use the &lt;code&gt;bw&lt;/code&gt; argument so that the same bandwidth is used in each density. We selected 0.75 after trying out several values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-smooth-density-counts-by-year-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot now shows what is happening very clearly. The developing world distribution is changing. A third mode appears consisting of the countries that most narrowed the gap.&lt;/p&gt;
&lt;p&gt;To visualize if any of the groups defined above are driving this we can quickly make a ridge plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
  filter(year %in% years &amp;amp; !is.na(dollars_per_day)) %&amp;gt;%
  ggplot(aes(dollars_per_day, group)) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;) +
  geom_density_ridges(adjust = 1.5) +
  facet_grid(. ~ year)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/ridge-plot-income-five-regions-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way to achieve this is by stacking the densities on top of each other:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gapminder %&amp;gt;%
    filter(year %in% years &amp;amp; country %in% country_list) %&amp;gt;%
  group_by(year) %&amp;gt;%
  mutate(weight = population/sum(population)*2) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(dollars_per_day, fill = group)) +
  scale_x_continuous(trans = &amp;quot;log2&amp;quot;, limit = c(0.125, 300)) +
  geom_density(alpha = 0.2, bw = 0.75, position = &amp;quot;stack&amp;quot;) +
  facet_grid(year ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-smooth-density-counts-by-region-and-year-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we can clearly see how the distributions for East Asia, Latin America, and others shift markedly to the right. While Sub-Saharan Africa remains stagnant.&lt;/p&gt;
&lt;p&gt;Notice that we order the levels of the group so that the West’s density is plotted first, then Sub-Saharan Africa. Having the two extremes plotted first allows us to see the remaining bimodality better.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;weighted-densities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Weighted densities&lt;/h3&gt;
&lt;p&gt;As a final point, we note that these distributions weigh every country the same. So if most of the population is improving, but living in a very large country, such as China, we might not appreciate this. We can actually weight the smooth densities using the &lt;code&gt;weight&lt;/code&gt; mapping argument. The plot then looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/income-smooth-density-counts-by-region-year-weighted-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This particular figure shows very clearly how the income distribution gap is closing with most of the poor remaining in Sub-Saharan Africa.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-ecological-fallacy-and-importance-of-showing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The ecological fallacy and importance of showing the data&lt;/h2&gt;
&lt;p&gt;Throughout this section, we have been comparing regions of the world. We have seen that, on average, some regions do better than others. In this section, we focus on describing the importance of variability within the groups when examining the relationship between a country’s infant mortality rates and average income.&lt;/p&gt;
&lt;p&gt;We define a few more regions and compare the averages across regions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/ecological-fallacy-averages-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The relationship between these two variables is almost perfectly linear and the graph shows a dramatic difference. While in the West less than 0.5% of infants die, in Sub-Saharan Africa the rate is higher than 6%!&lt;/p&gt;
&lt;p&gt;Note that the plot uses a new transformation, the logistic transformation.&lt;/p&gt;
&lt;div id=&#34;logit&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logistic transformation&lt;/h3&gt;
&lt;p&gt;The logistic or logit transformation for a proportion or rate &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(p) = \log \left( \frac{p}{1-p} \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is a proportion or probability, the quantity that is being logged, &lt;span class=&#34;math inline&#34;&gt;\(p/(1-p)\)&lt;/span&gt;, is called the &lt;em&gt;odds&lt;/em&gt;. In this case &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the proportion of infants that survived. The odds tell us how many more infants are expected to survive than to die. The log transformation makes this symmetric. If the rates are the same, then the log odds is 0. Fold increases or decreases turn into positive and negative increments, respectively.&lt;/p&gt;
&lt;p&gt;This scale is useful when we want to highlight differences near 0 or 1. For survival rates this is important because a survival rate of 90% is unacceptable, while a survival of 99% is relatively good. We would much prefer a survival rate closer to 99.9%. We want our scale to highlight these difference and the logit does this. Note that 99.9/0.1 is about 10 times bigger than 99/1 which is about 10 times larger than 90/10. By using the log, these fold changes turn into constant increases.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;show-the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Show the data&lt;/h3&gt;
&lt;p&gt;Now, back to our plot. Based on the plot above, do we conclude that a country with a low income is destined to have low survival rate? Do we conclude that survival rates in Sub-Saharan Africa are all lower than in Southern Asia, which in turn are lower than in the Pacific Islands, and so on?&lt;/p&gt;
&lt;p&gt;Jumping to this conclusion based on a plot showing averages is referred to as the &lt;em&gt;ecological fallacy&lt;/em&gt;. The almost perfect relationship between survival rates and income is only observed for the averages at the region level. Once we show all the data, we see a somewhat more complicated story:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/ecological-fallacy-all-data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Specifically, we see that there is a large amount of variability. We see that countries from the same regions can be quite different and that countries with the same income can have different survival rates. For example, while on average Sub-Saharan Africa had the worse health and economic outcomes, there is wide variability within that group. Mauritius and Botswana are doing better than Angola and Sierra Leone, with Mauritius comparable to Western countries.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vaccines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: vaccines and infectious diseases&lt;/h2&gt;
&lt;p&gt;Vaccines have helped save millions of lives. In the 19th century, before herd immunization was achieved through vaccination programs, deaths from infectious diseases, such as smallpox and polio, were common. However, today vaccination programs have become somewhat controversial despite all the scientific evidence for their importance.&lt;/p&gt;
&lt;p&gt;The controversy started with a paper&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; published in 1988 and led by Andrew Wakefield claiming
there was a link between the administration of the measles, mumps, and rubella (MMR) vaccine and the appearance of autism and bowel disease.
Despite much scientific evidence contradicting this finding, sensationalist media reports and fear-mongering from conspiracy theorists led parts of the public into believing that vaccines were harmful. As a result, many parents ceased to vaccinate their children. This dangerous practice can be potentially disastrous given that the Centers for Disease Control (CDC) estimates that vaccinations will prevent more than 21 million hospitalizations and 732,000 deaths among children born in the last 20 years (see Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;).
The 1988 paper has since been retracted and Andrew Wakefield was eventually “struck off the UK medical register, with a statement identifying deliberate falsification in the research published in The Lancet, and was thereby barred from practicing medicine in the UK.” (source: Wikipedia&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;). Yet misconceptions persist, in part due to self-proclaimed activists who continue to disseminate misinformation about vaccines.&lt;/p&gt;
&lt;p&gt;Effective communication of data is a strong antidote to misinformation and fear-mongering. Earlier we used an example provided by a Wall Street Journal article&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; showing data related to the impact of vaccines on battling infectious diseases. Here we reconstruct that example.&lt;/p&gt;
&lt;p&gt;The data used for these plots were collected, organized, and distributed by the Tycho Project&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;. They include weekly reported counts for seven diseases from 1928 to 2011, from all fifty states. The yearly totals are helpfully included in the &lt;strong&gt;dslabs&lt;/strong&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(RColorBrewer)
library(dslabs)
data(us_contagious_diseases)
names(us_contagious_diseases)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;disease&amp;quot;         &amp;quot;state&amp;quot;           &amp;quot;year&amp;quot;            &amp;quot;weeks_reporting&amp;quot;
## [5] &amp;quot;count&amp;quot;           &amp;quot;population&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a temporary object &lt;code&gt;dat&lt;/code&gt; that stores only the measles data, includes a per 100,000 rate, orders states by average value of disease and removes Alaska and Hawaii since they only became states in the late 1950s. Note that there is a &lt;code&gt;weeks_reporting&lt;/code&gt; column that tells us for how many weeks of the year data was reported. We have to adjust for that value when computing the rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_disease &amp;lt;- &amp;quot;Measles&amp;quot;
dat &amp;lt;- us_contagious_diseases %&amp;gt;%
  filter(!state%in%c(&amp;quot;Hawaii&amp;quot;,&amp;quot;Alaska&amp;quot;) &amp;amp; disease == the_disease) %&amp;gt;%
  mutate(rate = count / population * 10000 * 52 / weeks_reporting) %&amp;gt;%
  mutate(state = reorder(state, rate))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now easily plot disease rates per year. Here are the measles data from California:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% filter(state == &amp;quot;California&amp;quot; &amp;amp; !is.na(rate)) %&amp;gt;%
  ggplot(aes(year, rate)) +
  geom_line() +
  ylab(&amp;quot;Cases per 10,000&amp;quot;)  +
  geom_vline(xintercept=1963, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/california-measles-time-series-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We add a vertical line at 1963 since this is when the vaccine was introduced [Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 (the yellow book). p. 250. ISBN 9780199948505].&lt;/p&gt;
&lt;p&gt;Now can we show data for all states in one plot? We have three variables to show: year, state, and rate. In the WSJ figure, they use the x-axis for year, the y-axis for state, and color hue to represent rates. However, the color scale they use, which goes from yellow to blue to green to orange to red, can be improved.&lt;/p&gt;
&lt;p&gt;In our example, we want to use a sequential palette since there is no meaningful center, just low and high rates.&lt;/p&gt;
&lt;p&gt;We use the geometry &lt;code&gt;geom_tile&lt;/code&gt; to tile the region with colors representing disease rates. We use a square root transformation to avoid having the really high counts dominate the plot. Notice that missing values are shown in grey. Note that once a disease was pretty much eradicated, some states stopped reporting cases all together. This is why we see so much grey after 1980.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% ggplot(aes(year, state, fill = rate)) +
  geom_tile(color = &amp;quot;grey50&amp;quot;) +
  scale_x_continuous(expand=c(0,0)) +
  scale_fill_gradientn(colors = brewer.pal(9, &amp;quot;Reds&amp;quot;), trans = &amp;quot;sqrt&amp;quot;) +
  geom_vline(xintercept=1963, col = &amp;quot;blue&amp;quot;) +
  theme_minimal() +
  theme(panel.grid = element_blank(),
        legend.position=&amp;quot;bottom&amp;quot;,
        text = element_text(size = 8)) +
  ggtitle(the_disease) +
  ylab(&amp;quot;&amp;quot;) + xlab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/vaccines-plot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot makes a very striking argument for the contribution of vaccines. However, one limitation of this plot is that it uses color to represent quantity, which we earlier explained makes it harder to know exactly how high values are going. Position and lengths are better cues. If we are willing to lose state information, we can make a version of the plot that shows the values with position. We can also show the average for the US, which we compute like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avg &amp;lt;- us_contagious_diseases %&amp;gt;%
  filter(disease==the_disease) %&amp;gt;% group_by(year) %&amp;gt;%
  summarize(us_rate = sum(count, na.rm = TRUE) /
              sum(population, na.rm = TRUE) * 10000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to make the plot we simply use the &lt;code&gt;geom_line&lt;/code&gt; geometry:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  filter(!is.na(rate)) %&amp;gt;%
    ggplot() +
  geom_line(aes(year, rate, group = state),  color = &amp;quot;grey50&amp;quot;,
            show.legend = FALSE, alpha = 0.2, size = 1) +
  geom_line(mapping = aes(year, us_rate),  data = avg, size = 1) +
  scale_y_continuous(trans = &amp;quot;sqrt&amp;quot;, breaks = c(5, 25, 125, 300)) +
  ggtitle(&amp;quot;Cases per 10,000 by state&amp;quot;) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;&amp;quot;) +
  geom_text(data = data.frame(x = 1955, y = 50),
            mapping = aes(x, y, label=&amp;quot;US average&amp;quot;),
            color=&amp;quot;black&amp;quot;) +
  geom_vline(xintercept=1963, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/03-content_files/figure-html/time-series-vaccines-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In theory, we could use color to represent the categorical value state, but it is hard to pick 50 distinct colors.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Reproduce the image plot we previously made but for smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now reproduce the time series plot we previously made, but this time following the instructions of the previous question for smallpox.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For the state of California, make a time series plot showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now do the same for the rates for the US. Hint: compute the US rate by using summarize: the total divided by total population.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Hans_Rosling&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Hans_Rosling&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.gapminder.org/&#34; class=&#34;uri&#34;&gt;http://www.gapminder.org/&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&#34; class=&#34;uri&#34;&gt;https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract&#34; class=&#34;uri&#34;&gt;http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm&#34; class=&#34;uri&#34;&gt;https://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Andrew_Wakefield&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Andrew_Wakefield&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://graphics.wsj.com/infectious-diseases-and-vaccines/&#34; class=&#34;uri&#34;&gt;http://graphics.wsj.com/infectious-diseases-and-vaccines/&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://www.tycho.pitt.edu/&#34; class=&#34;uri&#34;&gt;http://www.tycho.pitt.edu/&lt;/a&gt;&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Effective Visualizations</title>
      <link>/content/02-content/</link>
      <pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/content/02-content/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#readings&#34;&gt;Readings&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-questions&#34;&gt;Guiding Questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distributions&#34;&gt;Visualizing data distributions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#variable-types&#34;&gt;Variable types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-describing-student-heights&#34;&gt;Case study: describing student heights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#distribution-function&#34;&gt;Distribution function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cdf-intro&#34;&gt;Cumulative distribution functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histograms&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#smoothed-density&#34;&gt;Smoothed density&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interpreting-the-y-axis&#34;&gt;Interpreting the y-axis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#densities-permit-stratification&#34;&gt;Densities permit stratification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normal-distribution&#34;&gt;The normal distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standard-units&#34;&gt;Standard units&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quantile-quantile-plots&#34;&gt;Quantile-quantile plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#percentiles&#34;&gt;Percentiles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boxplots&#34;&gt;Boxplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stratification&#34;&gt;Stratification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#student-height-cont&#34;&gt;Case study: describing student heights (continued)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-geometries&#34;&gt;ggplot2 geometries&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#barplots&#34;&gt;Barplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#histograms-1&#34;&gt;Histograms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-plots&#34;&gt;Density plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#boxplots-1&#34;&gt;Boxplots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qq-plots&#34;&gt;QQ-plots&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-visualization-principles&#34;&gt;Data visualization principles&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#encoding-data-using-visual-cues&#34;&gt;Encoding data using visual cues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#know-when-to-include-0&#34;&gt;Know when to include 0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#do-not-distort-quantities&#34;&gt;Do not distort quantities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#order-categories-by-a-meaningful-value&#34;&gt;Order categories by a meaningful value&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#show-the-data&#34;&gt;Show the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ease-comparisons&#34;&gt;Ease comparisons&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#use-common-axes&#34;&gt;Use common axes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes&#34;&gt;Align plots vertically to see horizontal changes and horizontally to see vertical changes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#consider-transformations&#34;&gt;Consider transformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visual-cues-to-be-compared-should-be-adjacent&#34;&gt;Visual cues to be compared should be adjacent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#use-color&#34;&gt;Use color&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#think-of-the-color-blind&#34;&gt;Think of the color blind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plots-for-two-variables&#34;&gt;Plots for two variables&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#slope-charts&#34;&gt;Slope charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bland-altman-plot&#34;&gt;Bland-Altman plot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#encoding-a-third-variable&#34;&gt;Encoding a third variable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#avoid-pseudo-three-dimensional-plots&#34;&gt;Avoid pseudo-three-dimensional plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#avoid-too-many-significant-digits&#34;&gt;Avoid too many significant digits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#know-your-audience&#34;&gt;Know your audience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Readings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Questions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Why do we create visualizations? What types of data are best suited for visuals?&lt;/li&gt;
&lt;li&gt;How do we best visualize the variability in our data?&lt;/li&gt;
&lt;li&gt;What makes a visual compelling?&lt;/li&gt;
&lt;li&gt;What are the worst visuals? Which of these are most frequently used? Why?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slides&lt;/h3&gt;
&lt;p&gt;As with last week’s content, the technical aspects of this lecture will be explored in greater detail in the Thursday practical lecture. Today, we will focus on some principles. We will also avoid slides entirely.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The greatest value of a picture is when it forces us to notice what we never expected to see.” – John Tukey&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Today’s lecture will ask you to touch real data during the lecture. Please download the following dataset and load it into &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/projects/data/ames.csv&#34;&gt;&lt;i class=&#34;fas fa-file-csv&#34;&gt;&lt;/i&gt; &lt;code&gt;Ames.csv&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This dataset is from houses in Ames, Iowa. (Thrilling!) We will use this dataset during the lecture to illustrate some of the points discussed below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;distributions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualizing data distributions&lt;/h1&gt;
&lt;p&gt;Throughout your education, you may have noticed that numerical data is often summarized with the &lt;em&gt;average&lt;/em&gt; value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the &lt;em&gt;standard deviation&lt;/em&gt;. For example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list?&lt;/p&gt;
&lt;p&gt;Our first data visualization building block is learning to summarize lists of factors or numeric vectors—the two primary data types that we encounter in data analytics. More often than not, the best way to share or explore this summary is through data visualization. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as a distribution, there are several data visualization techniques to effectively relay this information.&lt;/p&gt;
&lt;p&gt;In this section, we first discuss properties of a variety of distributions and how to visualize distributions using a motivating example of student heights. We then discuss some principles of data visualizations more broadly.&lt;/p&gt;
&lt;div id=&#34;variable-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variable types&lt;/h2&gt;
&lt;p&gt;We will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.&lt;/p&gt;
&lt;p&gt;When each entry in a vector comes from one of a small number of groups, we refer to the data as &lt;em&gt;categorical data&lt;/em&gt;. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as &lt;em&gt;ordinal&lt;/em&gt; data. In psychology, a number of different terms are used for this same idea.&lt;/p&gt;
&lt;p&gt;Examples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as population sizes, are discrete because they have to be integers—that’s how we count.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;case-study-describing-student-heights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: describing student heights&lt;/h2&gt;
&lt;p&gt;Here we consider an artificial problem to help us illustrate the underlying concepts.&lt;/p&gt;
&lt;p&gt;Pretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the &lt;code&gt;heights&lt;/code&gt; data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
data(heights)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distribution-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Distribution function&lt;/h2&gt;
&lt;p&gt;It turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.&lt;/p&gt;
&lt;p&gt;The most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##    Female      Male 
## 0.2266667 0.7733333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This two-category &lt;em&gt;frequency table&lt;/em&gt; is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% group_by(region) %&amp;gt;%
  summarize(n = n()) %&amp;gt;%
  mutate(Proportion = n/sum(n),
         region = reorder(region, Proportion)) %&amp;gt;%
  ggplot(aes(x=region, y=Proportion, fill=region)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, show.legend = FALSE) +
  xlab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/state-region-distribution-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cdf-intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cumulative distribution functions&lt;/h2&gt;
&lt;p&gt;Numerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of &lt;code&gt;68.503937007874&lt;/code&gt; inches and only one student reported a height &lt;code&gt;68.8976377952756&lt;/code&gt; inches. We assume that they converted from 174 and 175 centimeters, respectively.&lt;/p&gt;
&lt;p&gt;Statistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; for all possible values of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;. This function is called the cumulative distribution function (CDF). In statistics, the following notation is used:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ F(a) = \mbox{Pr}(x \leq a) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here is a plot of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; for the male height data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/ecdf-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar to what the frequency table does for categorical data, the CDF
defines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since &lt;span class=&#34;math inline&#34;&gt;\(F(66)=\)&lt;/span&gt; 0.1637931, or that 84% of the values are below 72, since &lt;span class=&#34;math inline&#34;&gt;\(F(72)=\)&lt;/span&gt; 0.841133,
and so on. In fact, we can report the proportion of values between any two heights, say &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, by computing &lt;span class=&#34;math inline&#34;&gt;\(F(b) - F(a)\)&lt;/span&gt;. This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.&lt;/p&gt;
&lt;p&gt;A final note: because CDFs can be defined mathematically—and absent any data—the word &lt;em&gt;empirical&lt;/em&gt; is added to make the distinction when data is used. We therefore use the term empirical CDF (eCDF).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;histograms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Histograms&lt;/h2&gt;
&lt;p&gt;Although the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? I doubt you can figure these out from glancing at the plot above. Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret.&lt;/p&gt;
&lt;p&gt;The simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: &lt;span class=&#34;math inline&#34;&gt;\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/height-histogram-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we send this plot to some uninformed reader, she will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, this reader could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.&lt;/p&gt;
&lt;p&gt;What information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;smoothed-density&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Smoothed density&lt;/h2&gt;
&lt;p&gt;Smooth density plots are aesthetically more appealing than histograms. Here is what a smooth density plot looks like for our heights data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/example-of-smoothed-density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to &lt;em&gt;density&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To understand the smooth densities, we have to understand &lt;em&gt;estimates&lt;/em&gt;, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool.&lt;/p&gt;
&lt;p&gt;The main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.&lt;/p&gt;
&lt;p&gt;However, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/simulated-data-histogram-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/simulated-data-histogram-2-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/simulated-density-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, back to reality. We don’t have millions of measurements. In this concrete example, we have 812 and we can’t make a histogram with very small bins.&lt;/p&gt;
&lt;p&gt;We therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots (loosely) demonstrate the steps that the computer goes through to ultimately create a smooth density:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/smooth-density-2-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, remember that &lt;em&gt;smooth&lt;/em&gt; is a relative term. We can actually control the &lt;em&gt;smoothness&lt;/em&gt; of the curve that defines the smooth density through an option in the function that computes the smooth density curve. Here are two examples using different degrees of smoothness on the same histogram:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/densities-different-smoothness-1.png&#34; width=&#34;100%&#34; /&gt;
We need to make this choice with care as the resulting visualizations can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. In the case of height, we really do have reason to believe that the proportion of people with similar heights should be the same. For example, the proportion that is 72 inches should be more similar to the proportion that is 71 than to the proportion that is 78 or 65. This implies that the curve should be pretty smooth; that is, the curve should look more like the example on the right than on the left.&lt;/p&gt;
&lt;p&gt;While the histogram is an assumption-free summary, the smoothed density is based on some assumptions.&lt;/p&gt;
&lt;div id=&#34;interpreting-the-y-axis&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpreting the y-axis&lt;/h3&gt;
&lt;p&gt;Note that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/area-under-curve-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The proportion of this area is about
0.3,
meaning that about
30%
of male heights are between 65 and 68 inches.&lt;/p&gt;
&lt;p&gt;By understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/example-of-smoothed-density-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;densities-permit-stratification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Densities permit stratification&lt;/h3&gt;
&lt;p&gt;As a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities make it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/two-densities-one-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the right argument, &lt;code&gt;ggplot&lt;/code&gt; automatically shades the intersecting region with a different color. We will show examples of &lt;strong&gt;ggplot2&lt;/strong&gt; code in the coming Example later this week.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The normal distribution&lt;/h2&gt;
&lt;p&gt;Histograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.&lt;/p&gt;
&lt;p&gt;The normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.&lt;/p&gt;
&lt;p&gt;Rather than using data, the normal distribution is defined with a mathematical formula. For any interval &lt;span class=&#34;math inline&#34;&gt;\((a,b)\)&lt;/span&gt;, the proportion of values in that interval can be computed using this formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mbox{Pr}(a &amp;lt; x &amp;lt; b) = \int_a^b \frac{1}{\sqrt{2\pi}s} e^{-\frac{1}{2}\left( \frac{x-m}{s} \right)^2} \, dx\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. The rest of the symbols in the formula represent the interval ends that we determine, &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and known mathematical constants &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;. These two parameters, &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, are referred to as the &lt;em&gt;average&lt;/em&gt; (also called the &lt;em&gt;mean&lt;/em&gt;) and the &lt;em&gt;standard deviation&lt;/em&gt; (SD) of the distribution, respectively.&lt;/p&gt;
&lt;p&gt;The distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/normal-distribution-density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.&lt;/p&gt;
&lt;p&gt;For a list of numbers contained in a vector &lt;code&gt;x&lt;/code&gt;, the average is defined as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- sum(x) / length(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the SD is defined as:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- sqrt(sum((x-mu)^2) / length(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which can be interpreted as the average distance between values and their average.&lt;/p&gt;
&lt;p&gt;Let’s compute the values for the height for males which we will store in the object &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- heights$sex == &amp;quot;Male&amp;quot;
x &amp;lt;- heights$height[index]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pre-built functions &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sd&lt;/code&gt; (note that for reasons explained in Section &lt;a href=&#34;#data-driven-model&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;, &lt;code&gt;sd&lt;/code&gt; divides by &lt;code&gt;length(x)-1&lt;/code&gt; rather than &lt;code&gt;length(x)&lt;/code&gt;) can be used here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- mean(x)
s &amp;lt;- sd(x)
c(average = m, sd = s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   average        sd 
## 69.314755  3.611024&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/data-and-normal-densities-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-units&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard units&lt;/h2&gt;
&lt;p&gt;For data that is approximately normally distributed, it is convenient to think in terms of &lt;em&gt;standard units&lt;/em&gt;. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value &lt;code&gt;x&lt;/code&gt; from a vector &lt;code&gt;X&lt;/code&gt;, we define the value of &lt;code&gt;x&lt;/code&gt; in standard units as &lt;code&gt;z = (x - m)/s&lt;/code&gt; with &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;s&lt;/code&gt; the average and standard deviation of &lt;code&gt;X&lt;/code&gt;, respectively. Why is this convenient?&lt;/p&gt;
&lt;p&gt;First look back at the formula for the normal distribution and note that what is being exponentiated is &lt;span class=&#34;math inline&#34;&gt;\(-z^2/2\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; equivalent to &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in standard units. Because the maximum of &lt;span class=&#34;math inline&#34;&gt;\(e^{-z^2/2}\)&lt;/span&gt; is when &lt;span class=&#34;math inline&#34;&gt;\(z=0\)&lt;/span&gt;, this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since &lt;span class=&#34;math inline&#34;&gt;\(- z^2/2\)&lt;/span&gt; is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (&lt;span class=&#34;math inline&#34;&gt;\(z=0\)&lt;/span&gt;), one of the largest (&lt;span class=&#34;math inline&#34;&gt;\(z \approx 2\)&lt;/span&gt;), one of the smallest (&lt;span class=&#34;math inline&#34;&gt;\(z \approx -2\)&lt;/span&gt;), or an extremely rare occurrence (&lt;span class=&#34;math inline&#34;&gt;\(z &amp;gt; 3\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(z &amp;lt; -3\)&lt;/span&gt;). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.&lt;/p&gt;
&lt;p&gt;In R, we can obtain standard units using the function &lt;code&gt;scale&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- scale(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to see how many men are within 2 SDs from the average, we simply type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(abs(z) &amp;lt; 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9495074&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-quantile-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quantile-quantile plots&lt;/h2&gt;
&lt;p&gt;A systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot).&lt;/p&gt;
&lt;p&gt;First let’s define the theoretical quantiles for the normal distribution. In statistics books we use the symbol &lt;span class=&#34;math inline&#34;&gt;\(\Phi(x)\)&lt;/span&gt; to define the function that gives us the probability of a standard normal distribution being smaller than &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. So, for example, &lt;span class=&#34;math inline&#34;&gt;\(\Phi(-1.96) = 0.025\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Phi(1.96) = 0.975\)&lt;/span&gt;. In R, we can evaluate &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; using the &lt;code&gt;pnorm&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pnorm(-1.96)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0249979&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The inverse function &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1}(x)\)&lt;/span&gt; gives us the &lt;em&gt;theoretical quantiles&lt;/em&gt; for the normal distribution. So, for example, &lt;span class=&#34;math inline&#34;&gt;\(\Phi^{-1}(0.975) = 1.96\)&lt;/span&gt;. In R, we can evaluate the inverse of &lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt; using the &lt;code&gt;qnorm&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qnorm(0.975)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.959964&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution. We can do this using the &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sd&lt;/code&gt; arguments in the &lt;code&gt;pnorm&lt;/code&gt; and &lt;code&gt;qnorm&lt;/code&gt; function. For example, we can use &lt;code&gt;qnorm&lt;/code&gt; to determine quantiles of a distribution with a specific average and standard deviation&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qnorm(0.975, mean = 5, sd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8.919928&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the normal distribution, all the calculations related to quantiles are done without data, thus the name &lt;em&gt;theoretical quantiles&lt;/em&gt;. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we can define the quantile associated with any proportion &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; as the &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; for which the proportion of values below &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. Using R code, we can define &lt;code&gt;q&lt;/code&gt; as the value for which &lt;code&gt;mean(x &amp;lt;= q) = p&lt;/code&gt;. Notice that not all &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; have a &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; for which the proportion is exactly &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;. There are several ways of defining the best &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; as discussed in the help for the &lt;code&gt;quantile&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;To give a quick example, for the male heights data, we have that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(x &amp;lt;= 69.5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5147783&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So about 50% are shorter or equal to 69 inches. This implies that if &lt;span class=&#34;math inline&#34;&gt;\(p=0.50\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(q=69.5\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Define a vector of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; proportions &lt;span class=&#34;math inline&#34;&gt;\(p_1, p_2, \dots, p_m\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Define a vector of quantiles &lt;span class=&#34;math inline&#34;&gt;\(q_1, \dots, q_m\)&lt;/span&gt; for your data for the proportions &lt;span class=&#34;math inline&#34;&gt;\(p_1, \dots, p_m\)&lt;/span&gt;. We refer to these as the &lt;em&gt;sample quantiles&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Define a vector of theoretical quantiles for the proportions &lt;span class=&#34;math inline&#34;&gt;\(p_1, \dots, p_m\)&lt;/span&gt; for a normal distribution with the same average and standard deviation as the data.&lt;/li&gt;
&lt;li&gt;Plot the sample quantiles versus the theoretical quantiles.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s construct a QQ-plot using R code. Start by defining the vector of proportions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- seq(0.05, 0.95, 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain the quantiles from the data, we can use the &lt;code&gt;quantile&lt;/code&gt; function like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_quantiles &amp;lt;- quantile(x, p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the &lt;code&gt;qnorm&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theoretical_quantiles &amp;lt;- qnorm(p, mean = mean(x), sd = sd(x))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see if they match or not, we plot them against each other and draw the identity line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(theoretical_quantiles, sample_quantiles) + geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/qqplot-original-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice that this code becomes much cleaner if we use standard units:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_quantiles &amp;lt;- quantile(z, p)
theoretical_quantiles &amp;lt;- qnorm(p)
qplot(theoretical_quantiles, sample_quantiles) + geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code is included to help describe QQ-plots. However, in practice it is easier to use the &lt;strong&gt;ggplot2&lt;/strong&gt; code described in Section &lt;a href=&#34;#other-geometries&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% filter(sex == &amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(sample = scale(height))) +
  geom_qq() +
  geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While for the illustration above we used 20 quantiles, the default from the &lt;code&gt;geom_qq&lt;/code&gt; function is to use as many quantiles as data points.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;percentiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Percentiles&lt;/h2&gt;
&lt;p&gt;Before we move on, let’s define some terms that are commonly used in exploratory data analysis.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Percentiles&lt;/em&gt; are special cases of &lt;em&gt;quantiles&lt;/em&gt; that are commonly used. The percentiles are the quantiles you obtain when setting the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(0.01, 0.02, ..., 0.99\)&lt;/span&gt;. We call, for example, the case of &lt;span class=&#34;math inline&#34;&gt;\(p=0.25\)&lt;/span&gt; the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the &lt;em&gt;median&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For the normal distribution the &lt;em&gt;median&lt;/em&gt; and average are the same, but this is generally not the case.&lt;/p&gt;
&lt;p&gt;Another special case that receives a name are the &lt;em&gt;quartiles&lt;/em&gt;, which are obtained when setting &lt;span class=&#34;math inline&#34;&gt;\(p=0.25,0.50\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(0.75\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boxplots&lt;/h2&gt;
&lt;p&gt;To introduce boxplots we will go back to the US murder data.
Suppose we want to summarize the murder rate distribution. Using the data visualization technique we have learned, we can quickly see that the normal approximation does not apply here:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/hist-qqplot-non-normal-data-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case, the histogram above or a smooth density plot would serve as a relatively succinct summary.&lt;/p&gt;
&lt;p&gt;Now suppose those used to receiving just two numbers as summaries ask us for a more compact numerical summary.&lt;/p&gt;
&lt;p&gt;Here, some of our wise predecessors have offered their advice. In short, the standard methodology is to provide a five-number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). Further, ignore &lt;em&gt;outliers&lt;/em&gt; when computing the range and instead plot these as independent points.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Finally, plot these numbers as a “box” with “whiskers” like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/first-boxplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;with the box defined by the 25% and 75% percentile and the whiskers showing the range. The distance between these two is called the &lt;em&gt;interquartile&lt;/em&gt; range. The median is shown with a horizontal line. Today, we call these &lt;em&gt;boxplots&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;From just this simple plot, we know that the median is about 2.5, that the distribution is not symmetric, and that the range is 0 to 5 for the great majority of states with two exceptions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stratification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stratification&lt;/h2&gt;
&lt;p&gt;In data analysis we often divide observations into groups based on the values of one or more variables associated with those observations. For example in the next section we divide the height values into groups based on a sex variable: females and males. We call this procedure &lt;em&gt;stratification&lt;/em&gt; and refer to the resulting groups as &lt;em&gt;strata&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Stratification is common in data visualization because we are often interested in how the distribution of variables differs across different subgroups. We will see several examples throughout this part of the book. We will revisit the concept of stratification when we learn regression in Chapter &lt;a href=&#34;#regression&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; and in the Machine Learning part of the book.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;student-height-cont&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: describing student heights (continued)&lt;/h2&gt;
&lt;p&gt;Using the histogram, density plots, and QQ-plots, we have become convinced that the male height data is well approximated with a normal distribution. In this case, we report back to ET a very succinct summary: male heights follow a normal distribution with an average of 69.3 inches and a SD of 3.6 inches. With this information, ET will have a good idea of what to expect when he meets our male students.
However, to provide a complete picture we need to also provide a summary of the female heights.&lt;/p&gt;
&lt;p&gt;We learned that boxplots are useful when we want to quickly compare two or more distributions. Here are the heights for men and women:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/female-male-boxplots-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. But does the normal approximation also work for the female height data collected by the survey? We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/histogram-qqplot-female-heights-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see something we did not see for the males: the density plot has a second “bump”. Also, the QQ-plot shows that the highest points tend to be taller than expected by the normal distribution. Finally, we also see five points in the QQ-plot that suggest shorter than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights.&lt;/p&gt;
&lt;p&gt;However, go back and read Tukey’s quote. We have noticed what we didn’t expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, &lt;code&gt;FEMALE&lt;/code&gt; was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data.&lt;/p&gt;
&lt;p&gt;Regarding the five smallest values, note that these values are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  top_n(5, desc(height)) %&amp;gt;%
  pull(height)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51 53 55 52 52&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because these are reported heights, a possibility is that the student meant to enter &lt;code&gt;5&#39;1&#34;&lt;/code&gt;, &lt;code&gt;5&#39;2&#34;&lt;/code&gt;, &lt;code&gt;5&#39;3&#34;&lt;/code&gt; or &lt;code&gt;5&#39;5&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Define variables containing the heights of males and females like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(heights)
male &amp;lt;- heights$height[heights$sex == &amp;quot;Male&amp;quot;]
female &amp;lt;- heights$height[heights$sex == &amp;quot;Female&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many measurements do we have for each?&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Suppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing &lt;code&gt;female_percentiles&lt;/code&gt; and &lt;code&gt;male_percentiles&lt;/code&gt; with the 10th, 30th, 50th, 70th, &amp;amp; 90th percentiles for each sex. Then create a data frame with these two as columns.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Study the following boxplots showing population sizes by country:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/boxplot-exercise-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Which continent has the country with the biggest population size?&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What continent has the largest median population size?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is median population size for Africa to the nearest million?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What proportion of countries in Europe have populations below 14 million?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If we use a log transformation, which continent shown above has the largest interquartile range?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Load the height data set and create a vector &lt;code&gt;x&lt;/code&gt; with just the male heights:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(heights)
x &amp;lt;- heights$height[heights$sex==&amp;quot;Male&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and &lt;code&gt;mean&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;9&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Suppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. Hint: start by computing the average and standard deviation. Then use the &lt;code&gt;pnorm&lt;/code&gt; function to predict the proportions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Notice that the approximation calculated in question nine is very close to the exact calculation in the first question. Now perform the same task for more extreme values. Compare the exact calculation and the normal approximation for the interval (79,81]. How many times bigger is the actual proportion than the approximation?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Approximate the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches. Using this approximation, estimate the proportion of adult men that are 7 feet tall or taller, referred to as &lt;em&gt;seven footers&lt;/em&gt;. Hint: use the &lt;code&gt;pnorm&lt;/code&gt; function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are about 1 billion men between the ages of 18 and 40 in the world. Use your answer to the previous question to estimate how many of these men (18-40 year olds) are seven feet tall or taller in the world?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher. Using the answer to the previous two questions, what proportion of the world’s 18-to-40-year-old &lt;em&gt;seven footers&lt;/em&gt; are in the NBA?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat the calculations performed in the previous question for Lebron James’ height: 6 feet 8 inches. There are about 150 players that are at least that tall.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In answering the previous questions, we found that it is not at all rare for a seven footer to become an NBA player. What would be a fair critique of our calculations:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Practice and talent are what make a great basketball player, not height.&lt;/li&gt;
&lt;li&gt;The normal approximation is not appropriate for heights.&lt;/li&gt;
&lt;li&gt;As seen in question 10, the normal approximation tends to underestimate the extreme values. It’s possible that there are more seven footers than we predicted.&lt;/li&gt;
&lt;li&gt;As seen in question 10, the normal approximation tends to overestimate the extreme values. It’s possible that there are fewer seven footers than we predicted.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;other-geometries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ggplot2 geometries&lt;/h2&gt;
&lt;p&gt;Alhough we haven’t gone into detain about the &lt;strong&gt;ggplot2&lt;/strong&gt; package for data visualization, we now will briefly discuss some of the geometries involved in the plots above. We will discuss &lt;strong&gt;ggplot2&lt;/strong&gt; in (excruciating) detail later this week. For now, we will briefly demonstrate how to generate plots related to distributions.&lt;/p&gt;
&lt;div id=&#34;barplots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Barplots&lt;/h3&gt;
&lt;p&gt;To generate a barplot we can use the &lt;code&gt;geom_bar&lt;/code&gt; geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% ggplot(aes(region)) + geom_bar()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/barplot-geom-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(murders)
tab &amp;lt;- murders %&amp;gt;%
  count(region) %&amp;gt;%
  mutate(proportion = n/sum(n))
tab&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          region  n proportion
## 1     Northeast  9  0.1764706
## 2         South 17  0.3333333
## 3 North Central 12  0.2352941
## 4          West 13  0.2549020&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We no longer want &lt;code&gt;geom_bar&lt;/code&gt; to count, but rather just plot a bar to the height provided by the &lt;code&gt;proportion&lt;/code&gt; variable. For this we need to provide &lt;code&gt;x&lt;/code&gt; (the categories) and &lt;code&gt;y&lt;/code&gt; (the values) and use the &lt;code&gt;stat=&#34;identity&#34;&lt;/code&gt; option.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab %&amp;gt;% ggplot(aes(region, proportion)) + geom_bar(stat = &amp;quot;identity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/region-freq-barplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;histograms-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Histograms&lt;/h3&gt;
&lt;p&gt;To generate histograms we use &lt;code&gt;geom_histogram&lt;/code&gt;. By looking at the help file for this function, we learn that the only required argument is &lt;code&gt;x&lt;/code&gt;, the variable for which we will construct a histogram. We dropped the &lt;code&gt;x&lt;/code&gt; because we know it is the first argument.
The code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we run the code above, it gives us a message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;stat_bin()&lt;/code&gt; using &lt;code&gt;bins = 30&lt;/code&gt;. Pick better value with
&lt;code&gt;binwidth&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We previously used a bin size of 1 inch, so the code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_histogram(binwidth = 1, fill = &amp;quot;blue&amp;quot;, col = &amp;quot;black&amp;quot;) +
  xlab(&amp;quot;Male heights in inches&amp;quot;) +
  ggtitle(&amp;quot;Histogram&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/height-histogram-geom-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density plots&lt;/h3&gt;
&lt;p&gt;To create a smooth density, we use the &lt;code&gt;geom_density&lt;/code&gt;. To make a smooth density plot with the data previously shown as a histogram we can use this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To fill in with color, we can use the &lt;code&gt;fill&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  ggplot(aes(height)) +
  geom_density(fill=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/ggplot-density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To change the smoothness of the density, we use the &lt;code&gt;adjust&lt;/code&gt; argument to multiply the default value by that &lt;code&gt;adjust&lt;/code&gt;. For example, if we want the bandwidth to be twice as big we use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) +
  geom_density(fill=&amp;quot;blue&amp;quot;, adjust = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplots-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Boxplots&lt;/h3&gt;
&lt;p&gt;The geometry for boxplot is &lt;code&gt;geom_boxplot&lt;/code&gt;. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments &lt;code&gt;x&lt;/code&gt; as the categories, and &lt;code&gt;y&lt;/code&gt; as the values.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/female-male-boxplots-geom-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;qq-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;QQ-plots&lt;/h3&gt;
&lt;p&gt;For qq-plots we use the &lt;code&gt;geom_qq&lt;/code&gt; geometry. From the help file, we learn that we need to specify the &lt;code&gt;sample&lt;/code&gt; (we will learn about samples in a later bit of the course). Here is the qqplot for men heights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(sample = height)) +
  geom_qq()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/ggplot-qq-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;By default, the sample variable is compared to a normal distribution with average 0 and standard deviation 1. To change this, we use the &lt;code&gt;dparams&lt;/code&gt; arguments based on the help file. Adding an identity line is as simple as assigning another layer. For straight lines, we use the &lt;code&gt;geom_abline&lt;/code&gt; function. The default line is the identity line (slope = 1, intercept = 0).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;params &amp;lt;- heights %&amp;gt;% filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  summarize(mean = mean(height), sd = sd(height))

heights %&amp;gt;% filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(sample = height)) +
  geom_qq(dparams = params) +
  geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another option here is to scale the data first and then make a qqplot against the standard normal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex==&amp;quot;Male&amp;quot;) %&amp;gt;%
  ggplot(aes(sample = scale(height))) +
  geom_qq() +
  geom_abline()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-visualization-principles&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data visualization principles&lt;/h1&gt;
&lt;p&gt;We have already provided some rules to follow as we created plots for our examples. Here, we aim to provide some general principles we can use as a guide for effective data visualization. Much of this section is based on a talk by Karl Broman&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; titled “Creating Effective Figures and Tables”&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; and includes some of the figures which were made with code that Karl makes available on his GitHub repository&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;, as well as class notes from Peter Aldhous’ Introduction to Data Visualization course&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;. Following Karl’s approach, we show some examples of plot styles we should avoid, explain how to improve them, and use these as motivation for a list of principles. We compare and contrast plots that follow these principles to those that don’t.&lt;/p&gt;
&lt;p&gt;The principles are mostly based on research related to how humans detect patterns and make visual comparisons. The preferred approaches are those that best fit the way our brains process visual information. When deciding on a visualization approach, it is also important to keep our goal in mind. We may be comparing a viewable number of quantities, describing distributions for categories or numeric values, comparing the data from two groups, or describing the relationship between two variables. As a final note, we want to emphasize that for a data scientist it is important to adapt and optimize graphs to the audience. For example, an exploratory plot made for ourselves will be different than a chart intended to communicate a finding to a general audience.&lt;/p&gt;
&lt;p&gt;As with the discussion above, we will be using these libraries—note the addition of &lt;strong&gt;gridExtra&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(dslabs)
library(gridExtra)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;encoding-data-using-visual-cues&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Encoding data using visual cues&lt;/h2&gt;
&lt;p&gt;We start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue.&lt;/p&gt;
&lt;p&gt;To illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/piechart-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. The donut chart is an example of a plot that uses only area:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/donutchart-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In fact, the &lt;code&gt;pie&lt;/code&gt; R function help file states that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Browser
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2000
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
2015
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Opera
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Safari
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
22
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Firefox
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
21
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Chrome
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IE
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
27
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/two-barplots-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.&lt;/p&gt;
&lt;p&gt;If for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/excel-barplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;know-when-to-include-0&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Know when to include 0&lt;/h2&gt;
&lt;p&gt;When using barplots, it is misinformative not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example used by Peter Aldhous in this lecture: &lt;a href=&#34;http://paldhous.github.io/ucb/2016/dataviz/week2.html&#34;&gt;http://paldhous.github.io/ucb/2016/dataviz/week2.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./02-content_files/class2_8.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: Fox News, via Media Matters&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;From the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/barplot-from-zero-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is another example, described in detail in a Flowing Data blog post:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./02-content_files/Bush-cuts.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: Fox News, via Flowing Data&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;This plot makes a 13% increase look like a five fold change. Here is the appropriate plot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/barplot-from-zero-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./02-content_files/venezuela-election.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source:
Venezolana de Televisión via Pakistan Today&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt; and Diego Mariano.)&lt;/p&gt;
&lt;p&gt;Here is the appropriate plot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/barplot-from-zero-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/points-plot-not-from-zero-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do-not-distort-quantities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Do not distort quantities&lt;/h2&gt;
&lt;p&gt;During President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./02-content_files/state-of-the-union.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Source: The 2011 State of the Union Address&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Judging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gdp &amp;lt;- c(14.6, 5.7, 5.3, 3.3, 2.5)
gdp_data &amp;lt;- data.frame(Country = rep(c(&amp;quot;United States&amp;quot;, &amp;quot;China&amp;quot;, &amp;quot;Japan&amp;quot;, &amp;quot;Germany&amp;quot;, &amp;quot;France&amp;quot;),2),
           y = factor(rep(c(&amp;quot;Radius&amp;quot;,&amp;quot;Area&amp;quot;),each=5), levels = c(&amp;quot;Radius&amp;quot;, &amp;quot;Area&amp;quot;)),
           GDP= c(gdp^2/min(gdp^2), gdp/min(gdp))) %&amp;gt;%
   mutate(Country = reorder(Country, GDP))
gdp_data %&amp;gt;%
  ggplot(aes(Country, y, size = GDP)) +
  geom_point(show.legend = FALSE, color = &amp;quot;blue&amp;quot;) +
  scale_size(range = c(2,25)) +
  coord_flip() + ylab(&amp;quot;&amp;quot;) + xlab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/area-not-radius-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not surprisingly, &lt;strong&gt;ggplot2&lt;/strong&gt; defaults to using area rather than
radius. Of course, in this case, we really should not be using area at all since we can use position and length:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gdp_data %&amp;gt;%
  filter(y == &amp;quot;Area&amp;quot;) %&amp;gt;%
  ggplot(aes(Country, GDP)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;, width = 0.5) +
  ylab(&amp;quot;GDP in trillions of US dollars&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/barplot-better-than-area-1.png&#34; width=&#34;50%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;order-categories-by-a-meaningful-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Order categories by a meaningful value&lt;/h2&gt;
&lt;p&gt;When one of the axes is used to show categories, as is done in barplots, the default &lt;strong&gt;ggplot2&lt;/strong&gt; behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.&lt;/p&gt;
&lt;p&gt;We previously learned how to use the &lt;code&gt;reorder&lt;/code&gt; function, which helps us achieve this goal.
To appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(murders)
p1 &amp;lt;- murders %&amp;gt;% mutate(murder_rate = total / population * 100000) %&amp;gt;%
  ggplot(aes(state, murder_rate)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 8))  +
  xlab(&amp;quot;&amp;quot;)

p2 &amp;lt;- murders %&amp;gt;% mutate(murder_rate = total / population * 100000) %&amp;gt;%
  mutate(state = reorder(state, murder_rate)) %&amp;gt;%
  ggplot(aes(state, murder_rate)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 8))  +
  xlab(&amp;quot;&amp;quot;)

grid.arrange(p1, p2, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/do-not-order-alphabetically-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can make the second plot like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(murders)
murders %&amp;gt;% mutate(murder_rate = total / population * 100000) %&amp;gt;%
  mutate(state = reorder(state, murder_rate)) %&amp;gt;%
  ggplot(aes(state, murder_rate)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  coord_flip() +
  theme(axis.text.y = element_text(size = 6)) +
  xlab(&amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;reorder&lt;/code&gt; function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/reorder-boxplot-example-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first orders the regions alphabetically, while the second orders them by the group’s median.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;show-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Show the data&lt;/h2&gt;
&lt;p&gt;We have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups.&lt;/p&gt;
&lt;p&gt;To motivate our first principle, “show the data”, we go back to our artificial example of describing heights to a person who is unaware of some basic facts about the population of interest (and is otherwise unsophisticated). This time let’s assume that this person is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors.&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; The plot looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/show-data-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.&lt;/p&gt;
&lt;p&gt;This brings us to our first principle: show the data. This simple &lt;strong&gt;ggplot2&lt;/strong&gt; code already generates a more informative plot than the barplot by simply showing all the data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(sex, height)) +
  geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/show-data-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.&lt;/p&gt;
&lt;p&gt;The first is to add &lt;em&gt;jitter&lt;/em&gt;, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using &lt;em&gt;alpha blending&lt;/em&gt;: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(sex, height)) +
  geom_jitter(width = 0.1, alpha = 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/show-points-with-jitter-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ease-comparisons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ease comparisons&lt;/h2&gt;
&lt;div id=&#34;use-common-axes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use common axes&lt;/h3&gt;
&lt;p&gt;Since there are so many points, it is more effective to show distributions rather than individual points. We therefore show histograms for each group:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/common-axes-histograms-wrong-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, from this plot it is not immediately obvious that males are, on average, taller than females. We have to look carefully to notice that the x-axis has a higher range of values in the male histogram. An important principle here is to &lt;strong&gt;keep the axes the same&lt;/strong&gt; when comparing data across two plots. Below we see how the comparison becomes easier:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/common-axes-histograms-right-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Align plots vertically to see horizontal changes and horizontally to see vertical changes&lt;/h3&gt;
&lt;p&gt;In these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/common-axes-histograms-right-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  ggplot(aes(height, ..density..)) +
  geom_histogram(binwidth = 1, color=&amp;quot;black&amp;quot;) +
  facet_grid(sex~.)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This plot makes it much easier to notice that men are, on average, taller.&lt;/p&gt;
&lt;p&gt;If , we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our &lt;em&gt;show the data&lt;/em&gt; principle, we then overlay all the data points:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/boxplot-with-points-with-jitter-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; heights %&amp;gt;%
  ggplot(aes(sex, height)) +
  geom_boxplot(coef=3) +
  geom_jitter(width = 0.1, alpha = 0.2) +
  ylab(&amp;quot;Height in inches&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now contrast and compare these three plots, based on exactly the same data:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/show-the-data-comparison-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;consider-transformations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Consider transformations&lt;/h3&gt;
&lt;p&gt;We have motivated the use of the log transformation in cases where the changes are multiplicative. Population size was an example in which we found a log transformation to yield a more informative transformation.&lt;/p&gt;
&lt;p&gt;The combination of an incorrectly chosen barplot and a failure to use a log transformation when one is merited can be particularly distorting. As an example, consider this barplot showing the average population sizes for each continent in 2015:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/no-transformations-wrong-use-of-barplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this plot, one would conclude that countries in Asia are much more populous than in other continents. Following the &lt;em&gt;show the data&lt;/em&gt; principle, we quickly notice that this is due to two very large countries, which we assume are India and China:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/no-transformation-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Using a log transformation here provides a much more informative plot. We compare the original barplot to a boxplot using the log scale transformation for the y-axis:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/correct-transformation-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the new plot, we realize that countries in Africa actually have a larger median population size than those in Asia.&lt;/p&gt;
&lt;p&gt;Other transformations you should consider are the logistic transformation (&lt;code&gt;logit&lt;/code&gt;), useful to better see fold changes in odds, and the square root transformation (&lt;code&gt;sqrt&lt;/code&gt;), useful for count data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visual-cues-to-be-compared-should-be-adjacent&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visual cues to be compared should be adjacent&lt;/h3&gt;
&lt;p&gt;For each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/boxplots-not-adjacent-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The default in &lt;strong&gt;ggplot2&lt;/strong&gt; is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/boxplot-adjacent-comps-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-color&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Use color&lt;/h3&gt;
&lt;p&gt;The comparison becomes even easier to make if we use color to denote the two things we want to compare:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/boxplot-adjacent-comps-with-color-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;think-of-the-color-blind&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Think of the color blind&lt;/h2&gt;
&lt;p&gt;About 10% of the population is color blind. Unfortunately, the default colors used in &lt;strong&gt;ggplot2&lt;/strong&gt; are not optimal for this group. However, &lt;strong&gt;ggplot2&lt;/strong&gt; does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: &lt;a href=&#34;http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette&#34;&gt;http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_blind_friendly_cols &amp;lt;-
  c(&amp;quot;#999999&amp;quot;, &amp;quot;#E69F00&amp;quot;, &amp;quot;#56B4E9&amp;quot;, &amp;quot;#009E73&amp;quot;,
    &amp;quot;#F0E442&amp;quot;, &amp;quot;#0072B2&amp;quot;, &amp;quot;#D55E00&amp;quot;, &amp;quot;#CC79A7&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the colors
&lt;img src=&#34;/content/02-content_files/figure-html/color-blind-friendly-colors-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are several resources that can help you select colors, for example this one: &lt;a href=&#34;http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/&#34;&gt;http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plots-for-two-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plots for two variables&lt;/h2&gt;
&lt;p&gt;In general, you should use scatterplots to visualize the relationship between two variables.
In every single instance in which we have examined the relationship between two variables, including total murders versus population size, life expectancy versus fertility rates, and infant mortality versus income, we have used scatterplots. This is the plot we generally recommend. However, there are some exceptions and we describe two alternative plots here: the &lt;em&gt;slope chart&lt;/em&gt; and the &lt;em&gt;Bland-Altman plot&lt;/em&gt;.&lt;/p&gt;
&lt;div id=&#34;slope-charts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Slope charts&lt;/h3&gt;
&lt;p&gt;One exception where another type of plot may be more informative is when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a &lt;em&gt;slope chart&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;There is no geometry for slope charts in &lt;strong&gt;ggplot2&lt;/strong&gt;, but we can construct one using &lt;code&gt;geom_line&lt;/code&gt;. We need to do some tinkering to add labels. Below is an example comparing 2010 to 2015 for large western countries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;west &amp;lt;- c(&amp;quot;Western Europe&amp;quot;,&amp;quot;Northern Europe&amp;quot;,&amp;quot;Southern Europe&amp;quot;,
          &amp;quot;Northern America&amp;quot;,&amp;quot;Australia and New Zealand&amp;quot;)

dat &amp;lt;- gapminder %&amp;gt;%
  filter(year%in% c(2010, 2015) &amp;amp; region %in% west &amp;amp;
           !is.na(life_expectancy) &amp;amp; population &amp;gt; 10^7)

dat %&amp;gt;%
  mutate(location = ifelse(year == 2010, 1, 2),
         location = ifelse(year == 2015 &amp;amp;
                             country %in% c(&amp;quot;United Kingdom&amp;quot;, &amp;quot;Portugal&amp;quot;),
                           location+0.22, location),
         hjust = ifelse(year == 2010, 1, 0)) %&amp;gt;%
  mutate(year = as.factor(year)) %&amp;gt;%
  ggplot(aes(year, life_expectancy, group = country)) +
  geom_line(aes(color = country), show.legend = FALSE) +
  geom_text(aes(x = location, label = country, hjust = hjust),
            show.legend = FALSE) +
  xlab(&amp;quot;&amp;quot;) + ylab(&amp;quot;Life Expectancy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/slope-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/scatter-plot-instead-of-slope-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the scatterplot, we have followed the principle &lt;em&gt;use common axes&lt;/em&gt; since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bland-altman-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bland-Altman plot&lt;/h3&gt;
&lt;p&gt;Since we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also known as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggrepel)
dat %&amp;gt;%
  mutate(year = paste0(&amp;quot;life_expectancy_&amp;quot;, year)) %&amp;gt;%
  select(country, year, life_expectancy) %&amp;gt;%
  spread(year, life_expectancy) %&amp;gt;%
  mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2,
         difference = life_expectancy_2015 - life_expectancy_2010) %&amp;gt;%
  ggplot(aes(average, difference, label = country)) +
  geom_point() +
  geom_text_repel() +
  geom_abline(lty = 2) +
  xlab(&amp;quot;Average of 2010 and 2015&amp;quot;) +
  ylab(&amp;quot;Difference between 2015 and 2010&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/bland-altman-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;encoding-a-third-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Encoding a third variable&lt;/h2&gt;
&lt;p&gt;An earlier scatterplot showed the relationship between infant survival and average income. Below is a version of this plot that encodes three variables: OPEC membership, region, and population.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/encoding-third-variable-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We encode categorical variables with color and shape. These shapes can be controlled with &lt;code&gt;shape&lt;/code&gt; argument. Below are the shapes available for use in R. For the last five, the color goes inside.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/available-shapes-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For continuous variables, we can use color, intensity, or size. We now show an example of how we do this with a case study.&lt;/p&gt;
&lt;p&gt;When selecting colors to quantify a numeric variable, we choose between two options: sequential and diverging. Sequential colors are suited for data that goes from high to low. High values are clearly distinguished from low values. Here are some examples offered by the package &lt;code&gt;RColorBrewer&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RColorBrewer)
display.brewer.all(type=&amp;quot;seq&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/r-color-brewer-seq-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Diverging colors are used to represent values that diverge from a center. We put equal emphasis on both ends of the data range: higher than the center and lower than the center. An example of when we would use a divergent pattern would be if we were to show height in standard deviations away from the average. Here are some examples of divergent patterns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RColorBrewer)
display.brewer.all(type=&amp;quot;div&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/r-color-brewer-div-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;avoid-pseudo-three-dimensional-plots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Avoid pseudo-three-dimensional plots&lt;/h2&gt;
&lt;p&gt;The figure below, taken from the scientific literature&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;,
shows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./02-content_files/fig8b.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Image courtesy of Karl Broman)&lt;/p&gt;
&lt;p&gt;Humans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##First read data
url &amp;lt;- &amp;quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig8dat.csv&amp;quot;
dat &amp;lt;- read.csv(url)

##Now make alternative plot
dat %&amp;gt;% gather(drug, survival, -log.dose) %&amp;gt;%
  mutate(drug = gsub(&amp;quot;Drug.&amp;quot;,&amp;quot;&amp;quot;,drug)) %&amp;gt;%
  ggplot(aes(log.dose, survival, color = drug)) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/colors-for-different-lines-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how much easier it is to determine the survival values.&lt;/p&gt;
&lt;p&gt;Pseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/./02-content_files/fig1e.png&#34; /&gt;
&lt;img src=&#34;/./02-content_files/fig2d.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;(Images courtesy of Karl Broman)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;avoid-too-many-significant-digits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Avoid too many significant digits&lt;/h2&gt;
&lt;p&gt;By default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
state
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Measles
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Pertussis
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Polio
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1940
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.8826320
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.3397861
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8266512
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13.9124205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.7467350
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.9742639
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1960
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.1386471
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2640419
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9767889
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1980
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3743467
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0515466
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
state
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
year
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Measles
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Pertussis
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Polio
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1940
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1950
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1960
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1980
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Useful ways to change the number of significant digits or to round numbers are &lt;code&gt;signif&lt;/code&gt; and &lt;code&gt;round&lt;/code&gt;. You can define the number of significant digits globally by setting options like this: &lt;code&gt;options(digits = 3)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Another principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
state
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
disease
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1940
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1950
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1960
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1970
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1980
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Measles
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
37.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
13.9
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
14.1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Pertussis
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
California
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Polio
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;know-your-audience&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Know your audience&lt;/h2&gt;
&lt;p&gt;Graphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot.&lt;/p&gt;
&lt;p&gt;As a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For these exercises, we will be using the vaccines data in the &lt;strong&gt;dslabs&lt;/strong&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(us_contagious_diseases)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Pie charts are appropriate:&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;When we want to display percentages.&lt;/li&gt;
&lt;li&gt;When &lt;strong&gt;ggplot2&lt;/strong&gt; is not available.&lt;/li&gt;
&lt;li&gt;When I am in a bakery.&lt;/li&gt;
&lt;li&gt;Never. Barplots and tables are always better.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;What is the problem with the plot below:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/baplot-not-from-zero-exercises-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The values are wrong. The final vote was 306 to 232.&lt;/li&gt;
&lt;li&gt;The axis does not start at 0. Judging by the length, it appears Trump received 3 times as many votes when, in fact, it was about 30% more.&lt;/li&gt;
&lt;li&gt;The colors should be the same.&lt;/li&gt;
&lt;li&gt;Percentages should be shown as a pie chart.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Take a look at the following two plots. They show the same information: 1928 rates of measles across the 50 states.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/measels-exercise-1.png&#34; width=&#34;672&#34; /&gt;
Which plot is easier to read if you are interested in determining which are the best and worst states in terms of rates, and why?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;They provide the same information, so they are both equally as good.&lt;/li&gt;
&lt;li&gt;The plot on the right is better because it orders the states alphabetically.&lt;/li&gt;
&lt;li&gt;The plot on the right is better because alphabetical order has nothing to do with the disease and by ordering according to actual rate, we quickly see the states with most and least rates.&lt;/li&gt;
&lt;li&gt;Both plots should be a pie chart.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To make the plot on the left, we have to reorder the levels of the states’ variables.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- us_contagious_diseases %&amp;gt;%
  filter(year == 1967 &amp;amp; disease==&amp;quot;Measles&amp;quot; &amp;amp; !is.na(population)) %&amp;gt;%
  mutate(rate = count / population * 10000 * 52 / weeks_reporting)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note what happens when we make a barplot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% ggplot(aes(state, rate)) +
  geom_bar(stat=&amp;quot;identity&amp;quot;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/barplot-plot-exercise-example-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Define these objects:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;state &amp;lt;- dat$state
rate &amp;lt;- dat$count/dat$population*10000*52/dat$weeks_reporting&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Redefine the &lt;code&gt;state&lt;/code&gt; object so that the levels are re-ordered. Print the new object &lt;code&gt;state&lt;/code&gt; and its levels so you can see that the vector is not re-ordered by the levels.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Now with one line of code, define the &lt;code&gt;dat&lt;/code&gt; table as done above, but change the use mutate to create a rate variable and re-order the state variable so that the levels are re-ordered by this variable. Then make a barplot using the code above, but for this new &lt;code&gt;dat&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Say we are interested in comparing gun homicide rates across regions of the US. We see this plot:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)
murders %&amp;gt;% mutate(rate = total/population*100000) %&amp;gt;%
group_by(region) %&amp;gt;%
summarize(avg = mean(rate)) %&amp;gt;%
mutate(region = factor(region)) %&amp;gt;%
ggplot(aes(region, avg)) +
geom_bar(stat=&amp;quot;identity&amp;quot;) +
ylab(&amp;quot;Murder Rate Average&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/us-murders-barplot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and decide to move to a state in the western region. What is the main problem with this interpretation?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The categories are ordered alphabetically.&lt;/li&gt;
&lt;li&gt;The graph does not show standarad errors.&lt;/li&gt;
&lt;li&gt;It does not show all the data. We do not see the variability within a region and it’s possible that the safest states are not in the West.&lt;/li&gt;
&lt;li&gt;The Northeast has the lowest average.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Make a boxplot of the murder rates defined as&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;murders&amp;quot;)
murders %&amp;gt;% mutate(rate = total/population*100000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;by region, showing all the points and ordering the regions by their median rate.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The plots below show three continuous variables.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/pseudo-3d-exercise-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The line &lt;span class=&#34;math inline&#34;&gt;\(x=2\)&lt;/span&gt; appears to separate the points. But it is actually not the case, which we can see by plotting the data in a couple of two-dimensional points.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/02-content_files/figure-html/pseud-3d-exercise-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Why is this happening?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Humans are not good at reading pseudo-3D plots.&lt;/li&gt;
&lt;li&gt;There must be an error in the code.&lt;/li&gt;
&lt;li&gt;The colors confuse us.&lt;/li&gt;
&lt;li&gt;Scatterplots should not be used to compare two variables when we have access to 3.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Keep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;We provide a detailed explanation of outliers later.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://kbroman.org/&#34; class=&#34;uri&#34;&gt;http://kbroman.org/&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf&#34; class=&#34;uri&#34;&gt;https://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf&lt;/a&gt;&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/kbroman/Talk_Graphs&#34; class=&#34;uri&#34;&gt;https://github.com/kbroman/Talk_Graphs&lt;/a&gt;&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://paldhous.github.io/ucb/2016/dataviz/index.html&#34; class=&#34;uri&#34;&gt;http://paldhous.github.io/ucb/2016/dataviz/index.html&lt;/a&gt;&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507&#34; class=&#34;uri&#34;&gt;http://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507&lt;/a&gt;&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/&#34; class=&#34;uri&#34;&gt;http://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/&lt;/a&gt;&lt;a href=&#34;#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote&#34; class=&#34;uri&#34;&gt;https://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote&lt;/a&gt;&lt;a href=&#34;#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kl2g40GoRxg&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=kl2g40GoRxg&lt;/a&gt;&lt;a href=&#34;#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;If you’re unfamiliar, standard errors are defined later in the course—do not confuse them with the standard deviation of the data.&lt;a href=&#34;#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://projecteuclid.org/download/pdf_1/euclid.ss/1177010488&#34; class=&#34;uri&#34;&gt;https://projecteuclid.org/download/pdf_1/euclid.ss/1177010488&lt;/a&gt;&lt;a href=&#34;#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to the tidyverse</title>
      <link>/content/01-content/</link>
      <pubDate>Tue, 08 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/content/01-content/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#readings&#34;&gt;Readings&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-question&#34;&gt;Guiding Question&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidyverse&#34;&gt;The tidyverse&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tidy-data&#34;&gt;Tidy data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#manipulating-data-frames&#34;&gt;Manipulating data frames&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#adding-a-column-with-mutate&#34;&gt;Adding a column with &lt;code&gt;mutate&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subsetting-with-filter&#34;&gt;Subsetting with &lt;code&gt;filter&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selecting-columns-with-select&#34;&gt;Selecting columns with &lt;code&gt;select&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-pipe&#34;&gt;The pipe: &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summarizing-data&#34;&gt;Summarizing data&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#summarize&#34;&gt;&lt;code&gt;summarize&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pull&#34;&gt;&lt;code&gt;pull&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#group-by&#34;&gt;Group then summarize with &lt;code&gt;group_by&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sorting-data-frames&#34;&gt;Sorting data frames&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#nested-sorting&#34;&gt;Nested sorting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-top-n&#34;&gt;The top &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tibbles&#34;&gt;Tibbles&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tibbles-display-better&#34;&gt;Tibbles display better&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subsets-of-tibbles-are-tibbles&#34;&gt;Subsets of tibbles are tibbles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tibbles-can-have-complex-entries&#34;&gt;Tibbles can have complex entries&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tibbles-can-be-grouped&#34;&gt;Tibbles can be grouped&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#create-a-tibble-using-tibble-instead-of-data.frame&#34;&gt;Create a tibble using &lt;code&gt;tibble&lt;/code&gt; instead of &lt;code&gt;data.frame&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-dot-operator&#34;&gt;The dot operator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#do&#34;&gt;&lt;code&gt;do&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-purrr-package&#34;&gt;The &lt;strong&gt;purrr&lt;/strong&gt; package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidyverse-conditionals&#34;&gt;Tidyverse conditionals&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case_when&#34;&gt;&lt;code&gt;case_when&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#between&#34;&gt;&lt;code&gt;between&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#videos&#34;&gt;Videos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Readings&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This page.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://forms.gle/SCgNC9N3PLw6rMea9&#34;&gt;Link to survey about teams.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chapter 1 of Introduction to Statistical Learning, available &lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf&#34;&gt;here.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Question&lt;/h3&gt;
&lt;p&gt;For future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why do we want tidy data?&lt;/li&gt;
&lt;li&gt;What are the challenges associated with shaping things into a tidy format?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Slides&lt;/h2&gt;
&lt;p&gt;The slides below are a much broader overview about goals for the class and thinking about relationships that we might explore with data. The technical aspects of this lecture will be explored in greater detail in the Thursday practical lecture.&lt;/p&gt;
&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;slide-tabs&#34; role=&#34;tablist&#34;&gt;
&lt;li class=&#34;nav-item&#34;&gt;
&lt;a class=&#34;nav-link active&#34; id=&#34;overview-tab&#34; data-toggle=&#34;tab&#34; href=&#34;#overview&#34; role=&#34;tab&#34; aria-controls=&#34;overview&#34; aria-selected=&#34;true&#34;&gt;Overview&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;slide-tabs&#34; class=&#34;tab-content&#34;&gt;
&lt;div id=&#34;overview&#34; class=&#34;tab-pane fade show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;overview-tab&#34;&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;/slides/01-slides.html#Overview&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This week’s content is split into two “halves”: the critical data manipulation information contained below and a more-entertaining discussion of visualization included in the Exercises.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tidyverse&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The tidyverse&lt;/h1&gt;
&lt;p&gt;In the first weeks’ content, we demonstrated how to manipulate vectors by reordering and subsetting them through indexing. However, once we start more advanced analyses, the preferred unit for data storage is not the vector but the data frame. In this lecture, we learn to work directly with data frames, which greatly facilitate the organization of information. We will be using data frames for the majority of this class and you will use them for the majority of your data science life (however long that might be). We will focus on a specific data format referred to as &lt;em&gt;tidy&lt;/em&gt; and on specific collection of packages that are particularly helpful for working with &lt;em&gt;tidy&lt;/em&gt; data referred to as the &lt;em&gt;tidyverse&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can load all the tidyverse packages at once by installing and loading the &lt;strong&gt;tidyverse&lt;/strong&gt; package:&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will learn how to implement the tidyverse approach throughout the book, but before delving into the details, in this chapter we introduce some of the most widely used tidyverse functionality, starting with the &lt;strong&gt;dplyr&lt;/strong&gt; package for manipulating data frames and the &lt;strong&gt;purrr&lt;/strong&gt; package for working with functions. Note that the tidyverse also includes a graphing package, &lt;strong&gt;ggplot2&lt;/strong&gt;, which we introduce later in Chapter &lt;a href=&#34;#ggplot2&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; in the Data Visualization part of the book; the &lt;strong&gt;readr&lt;/strong&gt; package discussed in Chapter &lt;a href=&#34;#importing-data&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;; and many others. In this chapter, we first introduce the concept of &lt;em&gt;tidy data&lt;/em&gt; and then demonstrate how we use the tidyverse to work with data frames in this format.&lt;/p&gt;
&lt;div id=&#34;tidy-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidy data&lt;/h2&gt;
&lt;p&gt;We say that a data table is in &lt;em&gt;tidy&lt;/em&gt; format if each row represents one observation and columns represent the different variables available for each of these observations. The &lt;code&gt;murders&lt;/code&gt; dataset is an example of a tidy data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total
## 1    Alabama  AL  South    4779736   135
## 2     Alaska  AK   West     710231    19
## 3    Arizona  AZ   West    6392017   232
## 4   Arkansas  AR  South    2915918    93
## 5 California  CA   West   37253956  1257
## 6   Colorado  CO   West    5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.&lt;/p&gt;
&lt;p&gt;To see how the same information can be provided in different formats, consider the following example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;gapminder&amp;quot;)
tidy_data &amp;lt;- gapminder %&amp;gt;%
  filter(country %in% c(&amp;quot;South Korea&amp;quot;, &amp;quot;Germany&amp;quot;) &amp;amp; !is.na(fertility)) %&amp;gt;%
  select(country, year, fertility)
head(tidy_data, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       country year fertility
## 1     Germany 1960      2.41
## 2 South Korea 1960      6.16
## 3     Germany 1961      2.44
## 4 South Korea 1961      5.99
## 5     Germany 1962      2.47
## 6 South Korea 1962      5.79&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the &lt;strong&gt;dslabs&lt;/strong&gt; package. Originally, the data was in the following format:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       country 1960 1961 1962
## 1     Germany 2.41 2.44 2.47
## 2 South Korea 6.16 5.99 5.79&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into &lt;code&gt;tidy&lt;/code&gt; format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.&lt;/p&gt;
&lt;p&gt;Although not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Examine the built-in dataset &lt;code&gt;co2&lt;/code&gt;. Which of the following is true:&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;co2&lt;/code&gt; is tidy data: it has one year for each row.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;co2&lt;/code&gt; is not tidy: we need at least one column with a character vector.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;co2&lt;/code&gt; is not tidy: it is a matrix instead of a data frame.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;co2&lt;/code&gt; is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Examine the built-in dataset &lt;code&gt;ChickWeight&lt;/code&gt;. Which of the following is true:&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;ChickWeight&lt;/code&gt; is not tidy: each chick has more than one row.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ChickWeight&lt;/code&gt; is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ChickWeight&lt;/code&gt; is not tidy: we are missing the year column.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ChickWeight&lt;/code&gt; is tidy: it is stored in a data frame.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Examine the built-in dataset &lt;code&gt;BOD&lt;/code&gt;. Which of the following is true:&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;BOD&lt;/code&gt; is not tidy: it only has six rows.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BOD&lt;/code&gt; is not tidy: the first column is just an index.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BOD&lt;/code&gt; is tidy: each row is an observation with two values (time and demand)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BOD&lt;/code&gt; is tidy: all small datasets are tidy by definition.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which of the following built-in datasets is tidy (you can pick more than one):&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;BJsales&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;EuStockMarkets&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DNase&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Formaldehyde&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Orange&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UCBAdmissions&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;manipulating-data-frames&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manipulating data frames&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;dplyr&lt;/strong&gt; package from the &lt;strong&gt;tidyverse&lt;/strong&gt; introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use &lt;code&gt;mutate&lt;/code&gt;. To filter the data table to a subset of rows, we use &lt;code&gt;filter&lt;/code&gt;. Finally, to subset the data by selecting specific columns, we use &lt;code&gt;select&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;adding-a-column-with-mutate&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding a column with &lt;code&gt;mutate&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;We want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function &lt;code&gt;mutate&lt;/code&gt; takes the data frame as a first argument and the name and values of the variable as a second argument using the convention &lt;code&gt;name = values&lt;/code&gt;. So, to add murder rates, we use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)
murders &amp;lt;- mutate(murders, rate = total / population * 100000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that here we used &lt;code&gt;total&lt;/code&gt; and &lt;code&gt;population&lt;/code&gt; inside the function, which are objects that are &lt;strong&gt;not&lt;/strong&gt; defined in our workspace. But why don’t we get an error?&lt;/p&gt;
&lt;p&gt;This is one of &lt;strong&gt;dplyr&lt;/strong&gt;’s main features. Functions in this package, such as &lt;code&gt;mutate&lt;/code&gt;, know to look for variables in the data frame provided in the first argument. In the call to mutate above, &lt;code&gt;total&lt;/code&gt; will have the values in &lt;code&gt;murders$total&lt;/code&gt;. This approach makes the code much more readable.&lt;/p&gt;
&lt;p&gt;We can see that the new column is added:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total     rate
## 1    Alabama  AL  South    4779736   135 2.824424
## 2     Alaska  AK   West     710231    19 2.675186
## 3    Arizona  AZ   West    6392017   232 3.629527
## 4   Arkansas  AR  South    2915918    93 3.189390
## 5 California  CA   West   37253956  1257 3.374138
## 6   Colorado  CO   West    5029196    65 1.292453&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although we have overwritten the original &lt;code&gt;murders&lt;/code&gt; object, this does not change the object that loaded with &lt;code&gt;data(murders)&lt;/code&gt;. If we load the &lt;code&gt;murders&lt;/code&gt; data again, the original will overwrite our mutated version.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsetting-with-filter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting with &lt;code&gt;filter&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Now suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the &lt;code&gt;filter&lt;/code&gt; function, which takes the data table as the first argument and then the conditional statement as the second. Like &lt;code&gt;mutate&lt;/code&gt;, we can use the unquoted variable names from &lt;code&gt;murders&lt;/code&gt; inside the function and it will know we mean the columns and not objects in the workspace.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, rate &amp;lt;= 0.71)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state abb        region population total      rate
## 1        Hawaii  HI          West    1360301     7 0.5145920
## 2          Iowa  IA North Central    3046355    21 0.6893484
## 3 New Hampshire  NH     Northeast    1316470     5 0.3798036
## 4  North Dakota  ND North Central     672591     4 0.5947151
## 5       Vermont  VT     Northeast     625741     2 0.3196211&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-columns-with-select&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Selecting columns with &lt;code&gt;select&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Although our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the &lt;strong&gt;dplyr&lt;/strong&gt; &lt;code&gt;select&lt;/code&gt; function. In the code below we select three columns, assign this to a new object and then filter the new object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_table &amp;lt;- select(murders, state, region, rate)
filter(new_table, rate &amp;lt;= 0.71)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state        region      rate
## 1        Hawaii          West 0.5145920
## 2          Iowa North Central 0.6893484
## 3 New Hampshire     Northeast 0.3798036
## 4  North Dakota North Central 0.5947151
## 5       Vermont     Northeast 0.3196211&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the call to &lt;code&gt;select&lt;/code&gt;, the first argument &lt;code&gt;murders&lt;/code&gt; is an object, but &lt;code&gt;state&lt;/code&gt;, &lt;code&gt;region&lt;/code&gt;, and &lt;code&gt;rate&lt;/code&gt; are variable names.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the &lt;strong&gt;dplyr&lt;/strong&gt; package and the murders dataset.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can add columns using the &lt;strong&gt;dplyr&lt;/strong&gt; function &lt;code&gt;mutate&lt;/code&gt;. This function is aware of the column names and inside the function you can call them unquoted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders &amp;lt;- mutate(murders, population_in_millions = population / 10^6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can write &lt;code&gt;population&lt;/code&gt; rather than &lt;code&gt;murders$population&lt;/code&gt;. The function &lt;code&gt;mutate&lt;/code&gt; knows we are grabbing columns from &lt;code&gt;murders&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Use the function &lt;code&gt;mutate&lt;/code&gt; to add a murders column named &lt;code&gt;rate&lt;/code&gt; with the per 100,000 murder rate as in the example code above. Make sure you redefine &lt;code&gt;murders&lt;/code&gt; as done in the example code above ( murders &amp;lt;- [your code]) so we can keep using this variable.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;If &lt;code&gt;rank(x)&lt;/code&gt; gives you the ranks of &lt;code&gt;x&lt;/code&gt; from lowest to highest, &lt;code&gt;rank(-x)&lt;/code&gt; gives you the ranks from highest to lowest. Use the function &lt;code&gt;mutate&lt;/code&gt; to add a column &lt;code&gt;rank&lt;/code&gt; containing the rank, from highest to lowest murder rate. Make sure you redefine &lt;code&gt;murders&lt;/code&gt; so we can keep using this variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;With &lt;strong&gt;dplyr&lt;/strong&gt;, we can use &lt;code&gt;select&lt;/code&gt; to show only certain columns. For example, with this code we would only show the states and population sizes:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;select(murders, state, population) %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use &lt;code&gt;select&lt;/code&gt; to show the state names and abbreviations in &lt;code&gt;murders&lt;/code&gt;. Do not redefine &lt;code&gt;murders&lt;/code&gt;, just show the results.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The &lt;strong&gt;dplyr&lt;/strong&gt; function &lt;code&gt;filter&lt;/code&gt; is used to choose specific rows of the data frame to keep. Unlike &lt;code&gt;select&lt;/code&gt; which is for columns, &lt;code&gt;filter&lt;/code&gt; is for rows. For example, you can show just the New York row like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, state == &amp;quot;New York&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can use other logical vectors to filter rows.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;filter&lt;/code&gt; to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the &lt;code&gt;rank&lt;/code&gt; column.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can remove rows using the &lt;code&gt;!=&lt;/code&gt; operator. For example, to remove Florida, we would do this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;no_florida &amp;lt;- filter(murders, state != &amp;quot;Florida&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a new data frame called &lt;code&gt;no_south&lt;/code&gt; that removes states from the South region. How many states are in this category? You can use the function &lt;code&gt;nrow&lt;/code&gt; for this.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We can also use &lt;code&gt;%in%&lt;/code&gt; to filter with &lt;strong&gt;dplyr&lt;/strong&gt;. You can therefore see the data from New York and Texas like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, state %in% c(&amp;quot;New York&amp;quot;, &amp;quot;Texas&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Create a new data frame called &lt;code&gt;murders_nw&lt;/code&gt; with only the states from the Northeast and the West. How many states are in this category?&lt;/p&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Suppose you want to live in the Northeast or West &lt;strong&gt;and&lt;/strong&gt; want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with &lt;code&gt;filter&lt;/code&gt;. Here is an example in which we filter to keep only small states in the Northeast region.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, population &amp;lt; 5000000 &amp;amp; region == &amp;quot;Northeast&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Make sure &lt;code&gt;murders&lt;/code&gt; has been defined with &lt;code&gt;rate&lt;/code&gt; and &lt;code&gt;rank&lt;/code&gt; and still has all states. Create a table called &lt;code&gt;my_states&lt;/code&gt; that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use &lt;code&gt;select&lt;/code&gt; to show only the state name, the rate, and the rank.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The pipe: &lt;code&gt;%&amp;gt;%&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;With &lt;strong&gt;dplyr&lt;/strong&gt; we can perform a series of operations, for example &lt;code&gt;select&lt;/code&gt; and then &lt;code&gt;filter&lt;/code&gt;, by sending the results of one function to another using what is called the &lt;em&gt;pipe operator&lt;/em&gt;: &lt;code&gt;%&amp;gt;%&lt;/code&gt;. Some details are included below.&lt;/p&gt;
&lt;p&gt;We wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object &lt;code&gt;new_table&lt;/code&gt;. In &lt;strong&gt;dplyr&lt;/strong&gt; we can write code that looks more like a description of what we want to do without intermediate objects:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \mbox{original data }
\rightarrow \mbox{ select }
\rightarrow \mbox{ filter } \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For such an operation, we can use the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;. The code looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% select(state, region, rate) %&amp;gt;% filter(rate &amp;lt;= 0.71)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state        region      rate
## 1        Hawaii          West 0.5145920
## 2          Iowa North Central 0.6893484
## 3 New Hampshire     Northeast 0.3798036
## 4  North Dakota North Central 0.5947151
## 5       Vermont     Northeast 0.3196211&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This line of code is equivalent to the two lines of code above. What is going on here?&lt;/p&gt;
&lt;p&gt;In general, the pipe &lt;em&gt;sends&lt;/em&gt; the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;16 %&amp;gt;% sqrt()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can continue to pipe values along:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;16 %&amp;gt;% sqrt() %&amp;gt;% log2()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above statement is equivalent to &lt;code&gt;log2(sqrt(16))&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Remember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;16 %&amp;gt;% sqrt() %&amp;gt;% log(base = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore, when using the pipe with data frames and &lt;strong&gt;dplyr&lt;/strong&gt;, we no longer need to specify the required first argument since the &lt;strong&gt;dplyr&lt;/strong&gt; functions we have described all take the data as the first argument. In the code we wrote:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% select(state, region, rate) %&amp;gt;% filter(rate &amp;lt;= 0.71)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;murders&lt;/code&gt; is the first argument of the &lt;code&gt;select&lt;/code&gt; function, and the new data frame (formerly &lt;code&gt;new_table&lt;/code&gt;) is the first argument of the &lt;code&gt;filter&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Note that the pipe works well with functions where the first argument is the input data. Functions in &lt;strong&gt;tidyverse&lt;/strong&gt; packages like &lt;strong&gt;dplyr&lt;/strong&gt; have this format and can be used easily with the pipe.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders &amp;lt;- mutate(murders, rate =  total / population * 100000,
                  rank = rank(-rate))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the solution to the previous exercise, we did the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_states &amp;lt;- filter(murders, region %in% c(&amp;quot;Northeast&amp;quot;, &amp;quot;West&amp;quot;) &amp;amp;
                      rate &amp;lt; 1)

select(my_states, state, rate, rank)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; permits us to perform both operations sequentially without having to define an intermediate variable &lt;code&gt;my_states&lt;/code&gt;. We therefore could have mutated and selected in the same line like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mutate(murders, rate =  total / population * 100000,
       rank = rank(-rate)) %&amp;gt;%
  select(state, rate, rank)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that &lt;code&gt;select&lt;/code&gt; no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the &lt;code&gt;%&amp;gt;%&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Repeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; to do this in just one line.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Reset &lt;code&gt;murders&lt;/code&gt; to the original table by using &lt;code&gt;data(murders)&lt;/code&gt;. Use a pipe to create a new data frame called &lt;code&gt;my_states&lt;/code&gt; that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three &lt;code&gt;%&amp;gt;%&lt;/code&gt;. The code should look something like this:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_states &amp;lt;- murders %&amp;gt;%
  mutate SOMETHING %&amp;gt;%
  filter SOMETHING %&amp;gt;%
  select SOMETHING&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summarizing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summarizing data&lt;/h2&gt;
&lt;p&gt;An important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new &lt;strong&gt;dplyr&lt;/strong&gt; verbs that make these computations easier: &lt;code&gt;summarize&lt;/code&gt; and &lt;code&gt;group_by&lt;/code&gt;. We learn to access resulting values using the &lt;code&gt;pull&lt;/code&gt; function.&lt;/p&gt;
&lt;div id=&#34;summarize&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;summarize&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;summarize&lt;/code&gt; function in &lt;strong&gt;dplyr&lt;/strong&gt; provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The &lt;code&gt;heights&lt;/code&gt; dataset includes heights and sex reported by students in an in-class survey.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(dslabs)
data(heights)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following code computes the average and standard deviation for females:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  summarize(average = mean(height), standard_deviation = sd(height))
s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    average standard_deviation
## 1 64.93942           3.760656&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use &lt;code&gt;average&lt;/code&gt; and &lt;code&gt;standard_deviation&lt;/code&gt;, but we could have used other names just the same.&lt;/p&gt;
&lt;p&gt;Because the resulting table stored in &lt;code&gt;s&lt;/code&gt; is a data frame, we can access the components with the accessor &lt;code&gt;$&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s$average&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 64.93942&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s$standard_deviation&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.760656&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with most other &lt;strong&gt;dplyr&lt;/strong&gt; functions, &lt;code&gt;summarize&lt;/code&gt; is aware of the variable names and we can use them directly. So when inside the call to the &lt;code&gt;summarize&lt;/code&gt; function we write &lt;code&gt;mean(height)&lt;/code&gt;, the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  summarize(median = median(height), minimum = min(height),
            maximum = max(height))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     median minimum maximum
## 1 64.98031      51      79&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can obtain these three values with just one line using the &lt;code&gt;quantile&lt;/code&gt; function: for example, &lt;code&gt;quantile(x, c(0,0.5,1))&lt;/code&gt; returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector &lt;code&gt;x&lt;/code&gt;. However, if we attempt to use a function like this that returns two or more values inside &lt;code&gt;summarize&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  summarize(range = quantile(height, c(0, 0.5, 1)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we will receive an error: &lt;code&gt;Error: expecting result of length one, got : 2&lt;/code&gt;. With the function &lt;code&gt;summarize&lt;/code&gt;, we can only call functions that return a single value. In Section &lt;a href=&#34;#do&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;, we will learn how to deal with functions that return more than one value.&lt;/p&gt;
&lt;p&gt;For another example of how we can use the &lt;code&gt;summarize&lt;/code&gt; function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used &lt;strong&gt;dplyr&lt;/strong&gt; to add a murder rate column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders &amp;lt;- murders %&amp;gt;% mutate(rate = total/population*100000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember that the US murder rate is &lt;strong&gt;not&lt;/strong&gt; the average of the state murder rates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summarize(murders, mean(rate))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mean(rate)
## 1   2.779125&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_murder_rate &amp;lt;- murders %&amp;gt;%
  summarize(rate = sum(total) / sum(population) * 100000)
us_murder_rate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       rate
## 1 3.034555&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This computation counts larger states proportionally to their size which results in a larger value.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pull&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;pull&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;us_murder_rate&lt;/code&gt; object defined above represents just one number. Yet we are storing it in a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(us_murder_rate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;since, as most &lt;strong&gt;dplyr&lt;/strong&gt; functions, &lt;code&gt;summarize&lt;/code&gt; always returns a data frame.&lt;/p&gt;
&lt;p&gt;This might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the &lt;code&gt;pull&lt;/code&gt; function. To understand what we mean take a look at this line of code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_murder_rate %&amp;gt;% pull(rate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.034555&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns the value in the &lt;code&gt;rate&lt;/code&gt; column of &lt;code&gt;us_murder_rate&lt;/code&gt; making it equivalent to &lt;code&gt;us_murder_rate$rate&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To get a number from the original data table with one line of code we can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;us_murder_rate &amp;lt;- murders %&amp;gt;%
  summarize(rate = sum(total) / sum(population) * 100000) %&amp;gt;%
  pull(rate)

us_murder_rate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.034555&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is now a numeric:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(us_murder_rate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;group-by&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Group then summarize with &lt;code&gt;group_by&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;A common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The &lt;code&gt;group_by&lt;/code&gt; function helps us do this.&lt;/p&gt;
&lt;p&gt;If we type this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;% group_by(sex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,050 x 2
## # Groups:   sex [2]
##    sex    height
##    &amp;lt;fct&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 Male       75
##  2 Male       70
##  3 Male       68
##  4 Male       74
##  5 Male       61
##  6 Female     65
##  7 Female     66
##  8 Female     62
##  9 Female     66
## 10 Male       67
## # … with 1,040 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result does not look very different from &lt;code&gt;heights&lt;/code&gt;, except we see &lt;code&gt;Groups: sex [2]&lt;/code&gt; when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a &lt;em&gt;grouped data frame&lt;/em&gt;, and &lt;strong&gt;dplyr&lt;/strong&gt; functions, in particular &lt;code&gt;summarize&lt;/code&gt;, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  summarize(average = mean(height), standard_deviation = sd(height))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   sex    average standard_deviation
##   &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt;
## 1 Female    64.9               3.76
## 2 Male      69.3               3.61&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;summarize&lt;/code&gt; function applies the summarization to each group separately.&lt;/p&gt;
&lt;p&gt;For another example, let’s compute the median murder rate in the four regions of the country:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  group_by(region) %&amp;gt;%
  summarize(median_rate = median(rate))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   region        median_rate
##   &amp;lt;fct&amp;gt;               &amp;lt;dbl&amp;gt;
## 1 Northeast            1.80
## 2 South                3.40
## 3 North Central        1.97
## 4 West                 1.29&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting-data-frames&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sorting data frames&lt;/h2&gt;
&lt;p&gt;When examining a dataset, it is often convenient to sort the table by the different columns. We know about the &lt;code&gt;order&lt;/code&gt; and &lt;code&gt;sort&lt;/code&gt; function, but for ordering entire tables, the &lt;strong&gt;dplyr&lt;/strong&gt; function &lt;code&gt;arrange&lt;/code&gt; is useful. For example, here we order the states by population size:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  arrange(population) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  state abb        region population total       rate
## 1              Wyoming  WY          West     563626     5  0.8871131
## 2 District of Columbia  DC         South     601723    99 16.4527532
## 3              Vermont  VT     Northeast     625741     2  0.3196211
## 4         North Dakota  ND North Central     672591     4  0.5947151
## 5               Alaska  AK          West     710231    19  2.6751860
## 6         South Dakota  SD North Central     814180     8  0.9825837&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;arrange&lt;/code&gt; we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by &lt;code&gt;rate&lt;/code&gt; instead:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  arrange(rate) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state abb        region population total      rate
## 1       Vermont  VT     Northeast     625741     2 0.3196211
## 2 New Hampshire  NH     Northeast    1316470     5 0.3798036
## 3        Hawaii  HI          West    1360301     7 0.5145920
## 4  North Dakota  ND North Central     672591     4 0.5947151
## 5          Iowa  IA North Central    3046355    21 0.6893484
## 6         Idaho  ID          West    1567582    12 0.7655102&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the default behavior is to order in ascending order. In &lt;strong&gt;dplyr&lt;/strong&gt;, the function &lt;code&gt;desc&lt;/code&gt; transforms a vector so that it is in descending order. To sort the table in descending order, we can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  arrange(desc(rate))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;nested-sorting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Nested sorting&lt;/h3&gt;
&lt;p&gt;If we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by &lt;code&gt;region&lt;/code&gt;, then within region we order by murder rate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  arrange(region, rate) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           state abb    region population total      rate
## 1       Vermont  VT Northeast     625741     2 0.3196211
## 2 New Hampshire  NH Northeast    1316470     5 0.3798036
## 3         Maine  ME Northeast    1328361    11 0.8280881
## 4  Rhode Island  RI Northeast    1052567    16 1.5200933
## 5 Massachusetts  MA Northeast    6547629   118 1.8021791
## 6      New York  NY Northeast   19378102   517 2.6679599&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-top-n&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The top &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;In the code above, we have used the function &lt;code&gt;head&lt;/code&gt; to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the &lt;code&gt;top_n&lt;/code&gt; function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% top_n(5, rate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  state abb        region population total      rate
## 1 District of Columbia  DC         South     601723    99 16.452753
## 2            Louisiana  LA         South    4533372   351  7.742581
## 3             Maryland  MD         South    5773552   293  5.074866
## 4             Missouri  MO North Central    5988927   321  5.359892
## 5       South Carolina  SC         South    4625364   207  4.475323&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that rows are not sorted by &lt;code&gt;rate&lt;/code&gt;, only filtered. If we want to sort, we need to use &lt;code&gt;arrange&lt;/code&gt;.
Note that if the third argument is left blank, &lt;code&gt;top_n&lt;/code&gt; filters by the last column.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the &lt;strong&gt;NHANES&lt;/strong&gt; package. Once you install the &lt;strong&gt;NHANES&lt;/strong&gt; package, you can load the data like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(NHANES)
data(NHANES)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;NHANES&lt;/strong&gt; data has many missing values. The &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;sd&lt;/code&gt; functions in R will return &lt;code&gt;NA&lt;/code&gt; if any of the entries of the input vector is an &lt;code&gt;NA&lt;/code&gt;. Here is an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(na_example)
mean(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To ignore the &lt;code&gt;NA&lt;/code&gt;s we can use the &lt;code&gt;na.rm&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(na_example, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.301754&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(na_example, na.rm = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.22338&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now explore the NHANES data.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. &lt;code&gt;AgeDecade&lt;/code&gt; is a categorical variable with these ages. Note that the category is coded like &#34; 20-29&#34;, with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the &lt;code&gt;BPSysAve&lt;/code&gt; variable? Save it to a variable called &lt;code&gt;ref&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hint: Use &lt;code&gt;filter&lt;/code&gt; and &lt;code&gt;summarize&lt;/code&gt; and use the &lt;code&gt;na.rm = TRUE&lt;/code&gt; argument when computing the average and standard deviation. You can also filter the NA values using &lt;code&gt;filter&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Using a pipe, assign the average to a numeric variable &lt;code&gt;ref_avg&lt;/code&gt;. Hint: Use the code similar to above and then &lt;code&gt;pull&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now report the min and max values for the same group.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by &lt;code&gt;AgeDecade&lt;/code&gt;. Hint: rather than filtering by age and gender, filter by &lt;code&gt;Gender&lt;/code&gt; and then use &lt;code&gt;group_by&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeat exercise 4 for males.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can actually combine both summaries for exercises 4 and 5 into one line of code. This is because &lt;code&gt;group_by&lt;/code&gt; permits us to group by more than one variable. Obtain one big summary table using &lt;code&gt;group_by(AgeDecade, Gender)&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For males between the ages of 40-49, compare systolic blood pressure across race as reported in the &lt;code&gt;Race1&lt;/code&gt; variable. Order the resulting table from lowest to highest average systolic blood pressure.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tibbles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tibbles&lt;/h2&gt;
&lt;p&gt;Tidy data must be stored in data frames. We introduced the data frame in Section &lt;a href=&#34;#data-frames&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; and have been using the &lt;code&gt;murders&lt;/code&gt; data frame throughout the book. In Section &lt;a href=&#34;#group-by&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we introduced the &lt;code&gt;group_by&lt;/code&gt; function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% group_by(region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 51 x 6
## # Groups:   region [4]
##    state                abb   region    population total  rate
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt; &amp;lt;fct&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 Alabama              AL    South        4779736   135  2.82
##  2 Alaska               AK    West          710231    19  2.68
##  3 Arizona              AZ    West         6392017   232  3.63
##  4 Arkansas             AR    South        2915918    93  3.19
##  5 California           CA    West        37253956  1257  3.37
##  6 Colorado             CO    West         5029196    65  1.29
##  7 Connecticut          CT    Northeast    3574097    97  2.71
##  8 Delaware             DE    South         897934    38  4.23
##  9 District of Columbia DC    South         601723    99 16.5 
## 10 Florida              FL    South       19687653   669  3.40
## # … with 41 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that there are no columns with this information. But, if you look closely at the output above, you see the line &lt;code&gt;A tibble&lt;/code&gt; followd by dimensions. We can learn the class of the returned object using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;% group_by(region) %&amp;gt;% class()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;grouped_df&amp;quot; &amp;quot;tbl_df&amp;quot;     &amp;quot;tbl&amp;quot;        &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;tbl&lt;/code&gt;, pronounced tibble, is a special kind of data frame. The functions &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarize&lt;/code&gt; always return this type of data frame. The &lt;code&gt;group_by&lt;/code&gt; function returns a special kind of &lt;code&gt;tbl&lt;/code&gt;, the &lt;code&gt;grouped_df&lt;/code&gt;. We will say more about these later. For consistency, the &lt;strong&gt;dplyr&lt;/strong&gt; manipulation verbs (&lt;code&gt;select&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;mutate&lt;/code&gt;, and &lt;code&gt;arrange&lt;/code&gt;) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble. For example, in Chapter &lt;a href=&#34;#importing-data&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we will see that tidyverse functions used to import data create tibbles.&lt;/p&gt;
&lt;p&gt;Tibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.&lt;/p&gt;
&lt;div id=&#34;tibbles-display-better&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tibbles display better&lt;/h3&gt;
&lt;p&gt;The print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing &lt;code&gt;murders&lt;/code&gt; and the output of murders if we convert it to a tibble. We can do this using &lt;code&gt;as_tibble(murders)&lt;/code&gt;. If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsets-of-tibbles-are-tibbles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsets of tibbles are tibbles&lt;/h3&gt;
&lt;p&gt;If you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders[,4])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is not a data frame. With tibbles this does not happen:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(as_tibble(murders)[,4])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;tbl_df&amp;quot;     &amp;quot;tbl&amp;quot;        &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is useful in the tidyverse since functions require data frames as input.&lt;/p&gt;
&lt;p&gt;With tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor &lt;code&gt;$&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(as_tibble(murders)$population)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write &lt;code&gt;Population&lt;/code&gt; instead of &lt;code&gt;population&lt;/code&gt; this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$Population&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;returns a &lt;code&gt;NULL&lt;/code&gt; with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_tibble(murders)$Population&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Unknown or uninitialised column: `Population`.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tibbles-can-have-complex-entries&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tibbles can have complex entries&lt;/h3&gt;
&lt;p&gt;While data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(id = c(1, 2, 3), func = c(mean, median, sd))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##      id func  
##   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;
## 1     1 &amp;lt;fn&amp;gt;  
## 2     2 &amp;lt;fn&amp;gt;  
## 3     3 &amp;lt;fn&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tibbles-can-be-grouped&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tibbles can be grouped&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;group_by&lt;/code&gt; returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the &lt;code&gt;summarize&lt;/code&gt; function, are aware of the group information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-tibble-using-tibble-instead-of-data.frame&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create a tibble using &lt;code&gt;tibble&lt;/code&gt; instead of &lt;code&gt;data.frame&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;It is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the &lt;code&gt;tibble&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grades &amp;lt;- tibble(names = c(&amp;quot;John&amp;quot;, &amp;quot;Juan&amp;quot;, &amp;quot;Jean&amp;quot;, &amp;quot;Yao&amp;quot;),
                     exam_1 = c(95, 80, 90, 85),
                     exam_2 = c(90, 85, 85, 90))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that base R (without packages loaded) has a function with a very similar name, &lt;code&gt;data.frame&lt;/code&gt;, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default &lt;code&gt;data.frame&lt;/code&gt; coerces characters into factors without providing a warning or message:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grades &amp;lt;- data.frame(names = c(&amp;quot;John&amp;quot;, &amp;quot;Juan&amp;quot;, &amp;quot;Jean&amp;quot;, &amp;quot;Yao&amp;quot;),
                     exam_1 = c(95, 80, 90, 85),
                     exam_2 = c(90, 85, 85, 90))
class(grades$names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To avoid this, we use the rather cumbersome argument &lt;code&gt;stringsAsFactors&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grades &amp;lt;- data.frame(names = c(&amp;quot;John&amp;quot;, &amp;quot;Juan&amp;quot;, &amp;quot;Jean&amp;quot;, &amp;quot;Yao&amp;quot;),
                     exam_1 = c(95, 80, 90, 85),
                     exam_2 = c(90, 85, 85, 90),
                     stringsAsFactors = FALSE)
class(grades$names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To convert a regular data frame to a tibble, you can use the &lt;code&gt;as_tibble&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as_tibble(grades) %&amp;gt;% class()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;tbl_df&amp;quot;     &amp;quot;tbl&amp;quot;        &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-dot-operator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The dot operator&lt;/h2&gt;
&lt;p&gt;One of the advantages of using the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab_1 &amp;lt;- filter(murders, region == &amp;quot;South&amp;quot;)
tab_2 &amp;lt;- mutate(tab_1, rate = total / population * 10^5)
rates &amp;lt;- tab_2$rate
median(rates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.398069&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can avoid defining any new intermediate objects by instead typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(murders, region == &amp;quot;South&amp;quot;) %&amp;gt;%
  mutate(rate = total / population * 10^5) %&amp;gt;%
  summarize(median = median(rate)) %&amp;gt;%
  pull(median)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.398069&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the &lt;code&gt;pull&lt;/code&gt; function was not available and we wanted to access &lt;code&gt;tab_2$rate&lt;/code&gt;? What data frame name would we use? The answer is the dot operator.&lt;/p&gt;
&lt;p&gt;For example to access the rate vector without the &lt;code&gt;pull&lt;/code&gt; function we could use&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rates &amp;lt;-   filter(murders, region == &amp;quot;South&amp;quot;) %&amp;gt;%
  mutate(rate = total / population * 10^5) %&amp;gt;%
  .$rate
median(rates)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.398069&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the next section, we will see other instances in which using the &lt;code&gt;.&lt;/code&gt; is useful.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;do&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;do&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt;, tidyverse functions consistently return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The &lt;code&gt;quantile&lt;/code&gt; function is an example we described in Section &lt;a href=&#34;#summarize&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;. The &lt;code&gt;do&lt;/code&gt; function serves as a bridge between R functions such as &lt;code&gt;quantile&lt;/code&gt; and the tidyverse. The &lt;code&gt;do&lt;/code&gt; function understands grouped tibbles and always returns a data frame.&lt;/p&gt;
&lt;p&gt;In Section &lt;a href=&#34;#summarize&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt;, we noted that if we attempt to use &lt;code&gt;quantile&lt;/code&gt; to obtain the min, median and max in one call, we will receive an error: &lt;code&gt;Error: expecting result of length one, got : 2&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(heights)
heights %&amp;gt;%
  filter(sex == &amp;quot;Female&amp;quot;) %&amp;gt;%
  summarize(range = quantile(height, c(0, 0.5, 1)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can use the &lt;code&gt;do&lt;/code&gt; function to fix this.&lt;/p&gt;
&lt;p&gt;First we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_summary &amp;lt;- function(dat){
  x &amp;lt;- quantile(dat$height, c(0, 0.5, 1))
  tibble(min = x[1], median = x[2], max = x[3])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now apply the function to the heights dataset to obtain the summaries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  my_summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##     min median   max
##   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1    50   68.5  82.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this is not what we want. We want a summary for each sex and the code returned just one summary. This is because &lt;code&gt;my_summary&lt;/code&gt; is not part of the tidyverse and does not know how to handled grouped tibbles. &lt;code&gt;do&lt;/code&gt; makes this connection:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  do(my_summary(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
## # Groups:   sex [2]
##   sex      min median   max
##   &amp;lt;fct&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 Female    51   65.0  79  
## 2 Male      50   69    82.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that here we need to use the dot operator. The tibble created by &lt;code&gt;group_by&lt;/code&gt; is piped to &lt;code&gt;do&lt;/code&gt;. Within the call to &lt;code&gt;do&lt;/code&gt;, the name of this tibble is &lt;code&gt;.&lt;/code&gt; and we want to send it to &lt;code&gt;my_summary&lt;/code&gt;. If you do not use the dot, then &lt;code&gt;my_summary&lt;/code&gt; has &lt;em&gt;no argument&lt;/em&gt; and returns an error telling us that &lt;code&gt;argument &#34;dat&#34;&lt;/code&gt; is missing. You can see the error by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  do(my_summary())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you do not use the parenthesis, then the function is not executed and instead &lt;code&gt;do&lt;/code&gt; tries to return the function. This gives an error because &lt;code&gt;do&lt;/code&gt; must always return a data frame. You can see the error by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;heights %&amp;gt;%
  group_by(sex) %&amp;gt;%
  do(my_summary)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-purrr-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;strong&gt;purrr&lt;/strong&gt; package&lt;/h2&gt;
&lt;p&gt;In Section &lt;a href=&#34;#vectorization&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we learned about the &lt;code&gt;sapply&lt;/code&gt; function, which permitted us to apply the same function to each element of a vector. We constructed a function and used &lt;code&gt;sapply&lt;/code&gt; to compute the sum of the first &lt;code&gt;n&lt;/code&gt; integers for several values of &lt;code&gt;n&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_s_n &amp;lt;- function(n){
  x &amp;lt;- 1:n
  sum(x)
}
n &amp;lt;- 1:25
s_n &amp;lt;- sapply(n, compute_s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The &lt;strong&gt;purrr&lt;/strong&gt; package includes functions similar to &lt;code&gt;sapply&lt;/code&gt; but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, &lt;code&gt;sapply&lt;/code&gt; can return several different object types; for example, we might expect a numeric result from a line of code, but &lt;code&gt;sapply&lt;/code&gt; might convert our result to character under some circumstances. &lt;strong&gt;purrr&lt;/strong&gt; functions will never do this: they will return objects of a specified type or return an error if this is not possible.&lt;/p&gt;
&lt;p&gt;The first &lt;strong&gt;purrr&lt;/strong&gt; function we will learn is &lt;code&gt;map&lt;/code&gt;, which works very similar to &lt;code&gt;sapply&lt;/code&gt; but always, without exception, returns a list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)
s_n &amp;lt;- map(n, compute_s_n)
class(s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want a numeric vector, we can instead use &lt;code&gt;map_dbl&lt;/code&gt; which always returns a vector of numeric values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_n &amp;lt;- map_dbl(n, compute_s_n)
class(s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This produces the same results as the &lt;code&gt;sapply&lt;/code&gt; call shown above.&lt;/p&gt;
&lt;p&gt;A particularly useful &lt;strong&gt;purrr&lt;/strong&gt; function for interacting with the rest of the tidyverse is &lt;code&gt;map_df&lt;/code&gt;, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a &lt;code&gt;Argument 1 must have names&lt;/code&gt; error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_n &amp;lt;- map_df(n, compute_s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to change the function to make this work:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_s_n &amp;lt;- function(n){
  x &amp;lt;- 1:n
  tibble(sum = sum(x))
}
s_n &amp;lt;- map_df(n, compute_s_n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;purrr&lt;/strong&gt; package provides much more functionality not covered here. For more details you can consult &lt;a href=&#34;https://jennybc.github.io/purrr-tutorial/&#34;&gt;this online resource&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidyverse-conditionals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidyverse conditionals&lt;/h2&gt;
&lt;p&gt;A typical data analysis will often involve one or more conditional operations. In Section &lt;a href=&#34;#conditionals&#34;&gt;&lt;strong&gt;??&lt;/strong&gt;&lt;/a&gt; we described the &lt;code&gt;ifelse&lt;/code&gt; function, which we will use extensively in this book. In this section we present two &lt;strong&gt;dplyr&lt;/strong&gt; functions that provide further functionality for performing conditional operations.&lt;/p&gt;
&lt;div id=&#34;case_when&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;case_when&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;case_when&lt;/code&gt; function is useful for vectorizing conditional statements. It is similar to &lt;code&gt;ifelse&lt;/code&gt; but can output any number of values, as opposed to just &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;. Here is an example splitting numbers into negative, positive, and 0:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(-2, -1, 0, 1, 2)
case_when(x &amp;lt; 0 ~ &amp;quot;Negative&amp;quot;,
          x &amp;gt; 0 ~ &amp;quot;Positive&amp;quot;,
          TRUE  ~ &amp;quot;Zero&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Negative&amp;quot; &amp;quot;Negative&amp;quot; &amp;quot;Zero&amp;quot;     &amp;quot;Positive&amp;quot; &amp;quot;Positive&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: &lt;em&gt;New England&lt;/em&gt;, &lt;em&gt;West Coast&lt;/em&gt;, &lt;em&gt;South&lt;/em&gt;, and &lt;em&gt;other&lt;/em&gt;. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign &lt;em&gt;other&lt;/em&gt;. Here is how we use &lt;code&gt;case_when&lt;/code&gt; to do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders %&amp;gt;%
  mutate(group = case_when(
    abb %in% c(&amp;quot;ME&amp;quot;, &amp;quot;NH&amp;quot;, &amp;quot;VT&amp;quot;, &amp;quot;MA&amp;quot;, &amp;quot;RI&amp;quot;, &amp;quot;CT&amp;quot;) ~ &amp;quot;New England&amp;quot;,
    abb %in% c(&amp;quot;WA&amp;quot;, &amp;quot;OR&amp;quot;, &amp;quot;CA&amp;quot;) ~ &amp;quot;West Coast&amp;quot;,
    region == &amp;quot;South&amp;quot; ~ &amp;quot;South&amp;quot;,
    TRUE ~ &amp;quot;Other&amp;quot;)) %&amp;gt;%
  group_by(group) %&amp;gt;%
  summarize(rate = sum(total) / sum(population) * 10^5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   group        rate
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 New England  1.72
## 2 Other        2.71
## 3 South        3.63
## 4 West Coast   2.90&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;between&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;between&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;A common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector &lt;code&gt;x&lt;/code&gt; are between &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; we can type&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;gt;= a &amp;amp; x &amp;lt;= b&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this can become cumbersome, especially within the tidyverse approach. The &lt;code&gt;between&lt;/code&gt; function performs the same operation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;between(x, a, b)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the &lt;code&gt;murders&lt;/code&gt; dataset. Which of the following is true?&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; is in tidy format and is stored in a tibble.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; is in tidy format and is stored in a data frame.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; is not in tidy format and is stored in a tibble.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;murders&lt;/code&gt; is not in tidy format and is stored in a data frame.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use &lt;code&gt;as_tibble&lt;/code&gt; to convert the &lt;code&gt;murders&lt;/code&gt; data table into a tibble and save it in an object called &lt;code&gt;murders_tibble&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;group_by&lt;/code&gt; function to convert &lt;code&gt;murders&lt;/code&gt; into a tibble that is grouped by region.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write tidyverse code that is equivalent to this code:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(mean(log(murders$population)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Write it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with &lt;code&gt;murders %&amp;gt;%&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Use the &lt;code&gt;map_df&lt;/code&gt; to create a data frame with three columns named &lt;code&gt;n&lt;/code&gt;, &lt;code&gt;s_n&lt;/code&gt;, and &lt;code&gt;s_n_2&lt;/code&gt;. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; the row number.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;videos&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Videos&lt;/h1&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/D6WqHA8TDWQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;If you have not installed this package already, you must use &lt;code&gt;install.packages(&#34;tidyverse&#34;)&lt;/code&gt; prior to the &lt;code&gt;library()&lt;/code&gt; call you see below.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Welcome Back to R</title>
      <link>/content/00-content/</link>
      <pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/content/00-content/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#readings&#34;&gt;Readings&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#guiding-question&#34;&gt;Guiding Question&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-basics&#34;&gt;&lt;code&gt;R&lt;/code&gt; basics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#case-study-us-homicides-by-firearm&#34;&gt;Case study: US homicides by firearm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-very-basics&#34;&gt;The (very) basics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#objects&#34;&gt;Objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-workspace&#34;&gt;The workspace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#functions&#34;&gt;Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-prebuilt-objects&#34;&gt;Other prebuilt objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variable-names&#34;&gt;Variable names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#saving-your-workspace&#34;&gt;Saving your workspace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivating-scripts&#34;&gt;Motivating scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#commenting-your-code&#34;&gt;Commenting your code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-types&#34;&gt;Data types&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-frames&#34;&gt;Data frames&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#examining-an-object&#34;&gt;Examining an object&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-accessor&#34;&gt;The accessor: &lt;code&gt;$&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors-numerics-characters-and-logical&#34;&gt;Vectors: numerics, characters, and logical&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#factors&#34;&gt;Factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lists&#34;&gt;Lists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrices&#34;&gt;Matrices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vectors&#34;&gt;Vectors&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#creating-vectors&#34;&gt;Creating vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#names&#34;&gt;Names&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sequences&#34;&gt;Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#subsetting&#34;&gt;Subsetting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coercion&#34;&gt;Coercion&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#not-availables-na&#34;&gt;Not availables (NA)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sorting&#34;&gt;Sorting&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sort&#34;&gt;&lt;code&gt;sort&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#order&#34;&gt;&lt;code&gt;order&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#max-and-which.max&#34;&gt;&lt;code&gt;max&lt;/code&gt; and &lt;code&gt;which.max&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rank&#34;&gt;&lt;code&gt;rank&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#beware-of-recycling&#34;&gt;Beware of recycling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#vector-arithmetics&#34;&gt;Vector arithmetics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rescaling-a-vector&#34;&gt;Rescaling a vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-vectors&#34;&gt;Two vectors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#indexing&#34;&gt;Indexing&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#subsetting-with-logicals&#34;&gt;Subsetting with logicals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logical-operators&#34;&gt;Logical operators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#which&#34;&gt;&lt;code&gt;which&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#match&#34;&gt;&lt;code&gt;match&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#in&#34;&gt;&lt;code&gt;%in%&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#videos&#34;&gt;Videos&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Readings&lt;/h2&gt;
&lt;p&gt;As noted in the syllabus, your readings will be assigned each week in this area. For this initial week, please read the course content. &lt;strong&gt;Read closely&lt;/strong&gt; the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;/syllabus/&#34;&gt;syllabus&lt;/a&gt;, &lt;a href=&#34;/content/&#34;&gt;content&lt;/a&gt;, &lt;a href=&#34;/example/&#34;&gt;examples&lt;/a&gt;, and &lt;a href=&#34;/lab/&#34;&gt;labs&lt;/a&gt; pages for this class.&lt;/li&gt;
&lt;li&gt;This page. Yes, the whole thing.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;guiding-question&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Guiding Question&lt;/h3&gt;
&lt;p&gt;For future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do you remember anything about &lt;code&gt;R&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;What are the different data types in &lt;code&gt;R&lt;/code&gt;?&lt;/li&gt;
&lt;li&gt;How do you index specific elements of a vector? Why might you want to do that?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;slides&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Slides&lt;/h2&gt;
&lt;p&gt;There are no direct slides for this lesson; the content below is intended to serve as a refresher for those who may have forgotten some of the basics of &lt;code&gt;R&lt;/code&gt;. This content will form the first week of the course and will include the Example 0. The slides below cover introductory material from the first lecture; some kinks to be worked out.&lt;/p&gt;
&lt;ul class=&#34;nav nav-tabs&#34; id=&#34;slide-tabs&#34; role=&#34;tablist&#34;&gt;
&lt;li class=&#34;nav-item&#34;&gt;
&lt;a class=&#34;nav-link active&#34; id=&#34;introduction-tab&#34; data-toggle=&#34;tab&#34; href=&#34;#introduction&#34; role=&#34;tab&#34; aria-controls=&#34;introduction&#34; aria-selected=&#34;true&#34;&gt;Introduction&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;slide-tabs&#34; class=&#34;tab-content&#34;&gt;
&lt;div id=&#34;introduction&#34; class=&#34;tab-pane fade show active&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;introduction-tab&#34;&gt;
&lt;div class=&#34;embed-responsive embed-responsive-16by9&#34;&gt;
&lt;iframe class=&#34;embed-responsive-item&#34; src=&#34;/slides/00-slides.html#Introduction&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;ALERT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The course content below should be considered a prerequisite for success. For those concerned about basics of &lt;code&gt;R&lt;/code&gt;, you absolutely must read this content and attempt the coding exercises. If you struggle to follow the content, please contact the professor or TA.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;r-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;code&gt;R&lt;/code&gt; basics&lt;/h1&gt;
&lt;p&gt;In this class, we will be using &lt;code&gt;R&lt;/code&gt; software environment for all our analyses. You will learn &lt;code&gt;R&lt;/code&gt; and data analysis techniques simultaneously. To follow along you will therefore need access to &lt;code&gt;R&lt;/code&gt;. We also recommend the use of an &lt;em&gt;integrated development environment&lt;/em&gt; (IDE), such as RStudio, to save your work.
Note that it is common for a course or workshop to offer access to an &lt;code&gt;R&lt;/code&gt; environment and an IDE through your web browser, as done by RStudio cloud&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. If you have access to such a resource, you don’t need to install &lt;code&gt;R&lt;/code&gt; and RStudio. However, if you intend on becoming a practicing data analyst, we highly recommend installing these tools on your computer&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;. This is not hard.&lt;/p&gt;
&lt;p&gt;Both &lt;code&gt;R&lt;/code&gt; and RStudio are free and available online.&lt;/p&gt;
&lt;div id=&#34;case-study-us-homicides-by-firearm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Case study: US homicides by firearm&lt;/h2&gt;
&lt;p&gt;Imagine you live in Europe (if only!) and are offered a job in a US company with many locations in every state. It is a great job, but headlines such as &lt;strong&gt;US Gun Homicide Rate Higher Than Other Developed Countries&lt;/strong&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; have you worried. Fox News runs a scary looking graphic, and charts like the one below only add to that concern:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/00-content_files/figure-html/murder-rate-example-1-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;!--(Source:
[Ma’ayan Rosenzweigh/ABC News](https://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/), Data from UNODC Homicide Statistics) --&gt;
&lt;p&gt;Or even worse, this version from &lt;a href=&#34;https://everytownresearch.org&#34;&gt;everytown.org&lt;/a&gt;:
&lt;img src=&#34;/content/00-content_files/figure-html/murder-rate-example-2-1.png&#34; width=&#34;70%&#34; /&gt;
&lt;!--(Source  [everytown.org](https://everytownresearch.org))--&gt;&lt;/p&gt;
&lt;p&gt;But then you remember that (1) this is a hypothetical exercise; (2) you’ll take literally any job at this point; and (3) Geographic diversity matters – the United States is a large and diverse country with 50 very different states (plus the District of Columbia and some lovely territories).&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/00-content_files/figure-html/us-murders-by-state-map-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;California, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/content/00-content_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a relatively simple and straightforward problem in social science: you have options of where to live, and want to determine the safety of the various states. Your “research” is clearly policy-relevant: you will eventually have to live somewhere. We will begin to tackle the problem by examining data related to gun homicides in the US during 2010 using &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced &lt;code&gt;R&lt;/code&gt; skills. Ideally, this is a refresher. However, we are aware that your preparation in previously courses varies greatly from student to student. Moreover, we want you to be aware that the usefulness of some of these early building blocks may not be immediately obvious. Later in the class you will appreciate having these skills. Mastery will be rewarded both in this class and (of course) in life.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-very-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The (very) basics&lt;/h2&gt;
&lt;p&gt;Before we get started with the motivating dataset, we need to cover the very basics of &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;objects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Objects&lt;/h3&gt;
&lt;p&gt;Suppose a relatively math unsavvy student asks us for help solving several quadratic equations of the form &lt;span class=&#34;math inline&#34;&gt;\(ax^2+bx+c = 0\)&lt;/span&gt;. You—a savvy student—recall that the quadratic formula gives us the solutions:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{-b - \sqrt{b^2 - 4ac}}{2a}\,\, \mbox{ and } \frac{-b + \sqrt{b^2 - 4ac}}{2a}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which of course depend on the values of &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. That is, the quadratic equation represents a &lt;em&gt;function&lt;/em&gt; with three &lt;em&gt;arguments&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;One advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve &lt;span class=&#34;math inline&#34;&gt;\(x^2 + x -1 = 0\)&lt;/span&gt;, then we define:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 1
b &amp;lt;- 1
c &amp;lt;- -1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which stores the values for later use. We use &lt;code&gt;&amp;lt;-&lt;/code&gt; to assign values to the variables.&lt;/p&gt;
&lt;p&gt;We can also assign values using &lt;code&gt;=&lt;/code&gt; instead of &lt;code&gt;&amp;lt;-&lt;/code&gt;, but we recommend against using &lt;code&gt;=&lt;/code&gt; to avoid confusion.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Copy and paste the code above into your console to define the three variables. Note that &lt;code&gt;R&lt;/code&gt; does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Throughout these written notes, you’ll have the most success if you continue to copy code into your own console.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To see the value stored in a variable, we simply ask &lt;code&gt;R&lt;/code&gt; to evaluate &lt;code&gt;a&lt;/code&gt; and it shows the stored value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A more explicit way to ask &lt;code&gt;R&lt;/code&gt; to show us the value stored in &lt;code&gt;a&lt;/code&gt; is using &lt;code&gt;print&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use the term &lt;em&gt;object&lt;/em&gt; to describe stuff that is stored in &lt;code&gt;R&lt;/code&gt;. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-workspace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The workspace&lt;/h3&gt;
&lt;p&gt;As we define objects in the console, we are actually changing the &lt;em&gt;workspace&lt;/em&gt;. You can see all the variables saved in your workspace by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ls()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot;          &amp;quot;b&amp;quot;          &amp;quot;c&amp;quot;          &amp;quot;dat&amp;quot;        &amp;quot;murders&amp;quot;   
## [6] &amp;quot;sections&amp;quot;   &amp;quot;slide_tabs&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Note that one of &lt;em&gt;my&lt;/em&gt; variables listed above comes from generating the graphs above). In RStudio, the &lt;em&gt;Environment&lt;/em&gt; tab shows the values:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/rstudio-environment.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We should see &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt;. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type &lt;code&gt;x&lt;/code&gt; you will receive the following message: &lt;code&gt;Error: object &#39;x&#39; not found&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.618034&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(-b - sqrt(b^2 - 4*a*c) ) / ( 2*a )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1.618034&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;p&gt;Once you define variables, the data analysis process can usually be described as a series of &lt;em&gt;functions&lt;/em&gt; applied to the data. &lt;code&gt;R&lt;/code&gt; includes several zillion predefined functions and most of the analysis pipelines we construct make extensive use of the built-in functions. But &lt;code&gt;R&lt;/code&gt;’s power comes from its scalability. We have access to (nearly) infinite functions via &lt;code&gt;install.packages&lt;/code&gt; and &lt;code&gt;library&lt;/code&gt;. As we go through the course, we will carefully note new functions we bring to each problem. For now, though, we will stick to the basics.&lt;/p&gt;
&lt;p&gt;Note that you’ve used a function already: you used the function &lt;code&gt;sqrt&lt;/code&gt; to solve the quadratic equation above. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.&lt;/p&gt;
&lt;p&gt;In general, we need to use parentheses to evaluate a function. If you type &lt;code&gt;ls&lt;/code&gt;, the function is not evaluated and instead &lt;code&gt;R&lt;/code&gt; shows you the code that defines the function. If you type &lt;code&gt;ls()&lt;/code&gt; the function is evaluated and, as seen above, we see objects in the workspace.&lt;/p&gt;
&lt;p&gt;Unlike &lt;code&gt;ls&lt;/code&gt;, most functions require one or more &lt;em&gt;arguments&lt;/em&gt;. Below is an example of how we assign an object to the argument of the function &lt;code&gt;log&lt;/code&gt;. Remember that we earlier defined &lt;code&gt;a&lt;/code&gt; to be 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.079442&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find out what the function expects and what it does by reviewing the very useful manuals included in &lt;code&gt;R&lt;/code&gt;. You can get help by using the &lt;code&gt;help&lt;/code&gt; function like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(&amp;quot;log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For most functions, we can also use this shorthand:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?log&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The help page will show you what arguments the function is expecting. For example, &lt;code&gt;log&lt;/code&gt; needs &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;base&lt;/code&gt; to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with &lt;code&gt;=&lt;/code&gt;. Defining these is optional.&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; For example, the base of the function &lt;code&gt;log&lt;/code&gt; defaults to &lt;code&gt;base = exp(1)&lt;/code&gt;—that is, &lt;code&gt;log&lt;/code&gt; evaluates the natural log by default.&lt;/p&gt;
&lt;p&gt;If you want a quick look at the arguments without opening the help system, you can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;args(log)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (x, base = exp(1)) 
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can change the default values by simply assigning another object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(8, base = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we have not been specifying the argument &lt;code&gt;x&lt;/code&gt; as such:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(x = 8, base = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code works, but we can save ourselves some typing: if no argument name is used, &lt;code&gt;R&lt;/code&gt; assumes you are entering arguments in the order shown in the help file or by &lt;code&gt;args&lt;/code&gt;. So by not using the names, it assumes the arguments are &lt;code&gt;x&lt;/code&gt; followed by &lt;code&gt;base&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(8,2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If using the arguments’ names, then we can include them in whatever order we want:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log(base = 2, x = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To specify arguments, we must use &lt;code&gt;=&lt;/code&gt;, and cannot use &lt;code&gt;&amp;lt;-&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;2 ^ 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the arithmetic operators by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(&amp;quot;+&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?&amp;quot;+&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the relational operators by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;help(&amp;quot;&amp;gt;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?&amp;quot;&amp;gt;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-prebuilt-objects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other prebuilt objects&lt;/h3&gt;
&lt;p&gt;There are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;co2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; will show you Mauna Loa atmospheric &lt;span class=&#34;math inline&#34;&gt;\(CO^2\)&lt;/span&gt; concentration data.&lt;/p&gt;
&lt;p&gt;Other prebuilt objects are mathematical quantities, such as the constant &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pi&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.141593&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Inf+1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] Inf&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;variable-names&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variable names&lt;/h3&gt;
&lt;p&gt;We have used the letters &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt; as variable names, but variable names can be almost anything. Some basic rules in &lt;code&gt;R&lt;/code&gt; are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in &lt;code&gt;R&lt;/code&gt;. For example, don’t name one of your variables &lt;code&gt;install.packages&lt;/code&gt; by typing something like &lt;code&gt;install.packages &amp;lt;- 2&lt;/code&gt;. Usually, &lt;code&gt;R&lt;/code&gt; is smart enough to prevent you from doing such nonsense, but it’s important to develop good habits.&lt;/p&gt;
&lt;p&gt;A nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solution_1 &amp;lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a)
solution_2 &amp;lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For more advice, we highly recommend studying (Hadley Wickham’s style guide)[&lt;a href=&#34;http://adv-r.had.co.nz/Style.html&#34; class=&#34;uri&#34;&gt;http://adv-r.had.co.nz/Style.html&lt;/a&gt;].&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;saving-your-workspace&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Saving your workspace&lt;/h3&gt;
&lt;p&gt;Values remain in the workspace until you end your session or erase them with the function &lt;code&gt;rm&lt;/code&gt;. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.&lt;/p&gt;
&lt;p&gt;We actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function &lt;code&gt;save&lt;/code&gt; or &lt;code&gt;save.image&lt;/code&gt;. To load, use the function &lt;code&gt;load&lt;/code&gt;. When saving a workspace, we recommend the suffix &lt;code&gt;rda&lt;/code&gt; or &lt;code&gt;RData&lt;/code&gt;. In RStudio, you can also do this by navigating to the &lt;em&gt;Session&lt;/em&gt; tab and choosing &lt;em&gt;Save Workspace as&lt;/em&gt;. You can later load it using the &lt;em&gt;Load Workspace&lt;/em&gt; options in the same tab.
You can read the help pages on &lt;code&gt;save&lt;/code&gt;, &lt;code&gt;save.image&lt;/code&gt;, and &lt;code&gt;load&lt;/code&gt; to learn more.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivating-scripts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Motivating scripts&lt;/h3&gt;
&lt;p&gt;To solve another equation such as &lt;span class=&#34;math inline&#34;&gt;\(3x^2 + 2x -1\)&lt;/span&gt;, we can copy and paste the code above and then redefine the variables and recompute the solution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 3
b &amp;lt;- 2
c &amp;lt;- -1
(-b + sqrt(b^2 - 4*a*c)) / (2*a)
(-b - sqrt(b^2 - 4*a*c)) / (2*a)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;commenting-your-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Commenting your code&lt;/h3&gt;
&lt;p&gt;If a line of &lt;code&gt;R&lt;/code&gt; code starts with the symbol &lt;code&gt;#&lt;/code&gt;, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Code to compute solution to quadratic equation of the form ax^2 + bx + c
## define the variables
a &amp;lt;- 3
b &amp;lt;- 2
c &amp;lt;- -1

## now compute the solution
(-b + sqrt(b^2 - 4*a*c)) / (2*a)
(-b - sqrt(b^2 - 4*a*c)) / (2*a)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What is the sum of the first 100 positive integers? The formula for the sum of integers &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(n(n+1)/2\)&lt;/span&gt;. Define &lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt; and then use &lt;code&gt;R&lt;/code&gt; to compute the sum of &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; through &lt;span class=&#34;math inline&#34;&gt;\(100\)&lt;/span&gt; using the formula. What is the sum?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the same formula to compute the sum of the integers from 1 through 1,000.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Look at the result of typing the following code into R:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 1000
x &amp;lt;- seq(1, n)
sum(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on the result, what do you think the functions &lt;code&gt;seq&lt;/code&gt; and &lt;code&gt;sum&lt;/code&gt; do? You can use &lt;code&gt;help&lt;/code&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;sum&lt;/code&gt; creates a list of numbers and &lt;code&gt;seq&lt;/code&gt; adds them up.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;seq&lt;/code&gt; creates a list of numbers and &lt;code&gt;sum&lt;/code&gt; adds them up.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;seq&lt;/code&gt; creates a random list and &lt;code&gt;sum&lt;/code&gt; computes the sum of 1 through 1,000.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sum&lt;/code&gt; always returns the same number.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type &lt;code&gt;sqrt(4)&lt;/code&gt;, we evaluate the &lt;code&gt;sqrt&lt;/code&gt; function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Which of the following will always return the numeric value stored in &lt;code&gt;x&lt;/code&gt;? You can try out examples and use the help system if you want.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;code&gt;log(10^x)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;log10(x^10)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;log(exp(x))&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;exp(log(x, base = 2))&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-types&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data types&lt;/h2&gt;
&lt;p&gt;Variables in &lt;code&gt;R&lt;/code&gt; can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function &lt;code&gt;class&lt;/code&gt; helps us determine what type of object we have:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 2
class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To work efficiently in R, it is important to learn the different types of variables and what we can do with these.&lt;/p&gt;
&lt;div id=&#34;data-frames&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data frames&lt;/h3&gt;
&lt;p&gt;Up to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in &lt;code&gt;R&lt;/code&gt; is in a &lt;em&gt;data frame&lt;/em&gt;. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.&lt;/p&gt;
&lt;p&gt;A large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the &lt;strong&gt;dslabs&lt;/strong&gt; library and loading the &lt;code&gt;murders&lt;/code&gt; dataset using the &lt;code&gt;data&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see that this is in fact a data frame, we type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;examining-an-object&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Examining an object&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;str&lt;/code&gt; is useful for finding out more about the structure of an object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    51 obs. of  5 variables:
## $ state : chr &amp;quot;Alabama&amp;quot; &amp;quot;Alaska&amp;quot; &amp;quot;Arizona&amp;quot; &amp;quot;Arkansas&amp;quot; ...
## $ abb : chr &amp;quot;AL&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;AR&amp;quot; ...
## $ region : Factor w/ 4 levels &amp;quot;Northeast&amp;quot;,&amp;quot;South&amp;quot;,..: 2 4 4 2 4 4 1 2 2 2 ...
## $ population: num 4779736 710231 6392017 2915918 37253956 ...
## $ total : num 135 19 232 93 1257 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function &lt;code&gt;head&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        state abb region population total
## 1    Alabama  AL  South    4779736   135
## 2     Alaska  AK   West     710231    19
## 3    Arizona  AZ   West    6392017   232
## 4   Arkansas  AR  South    2915918    93
## 5 California  CA   West   37253956  1257
## 6   Colorado  CO   West    5029196    65&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this dataset, each state is considered an observation and five variables are reported for each state.&lt;/p&gt;
&lt;p&gt;Before we go any further in answering our original question about different states, let’s learn more about the components of this object.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-accessor&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The accessor: &lt;code&gt;$&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;For our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator &lt;code&gt;$&lt;/code&gt; in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$population&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097   897934
##  [9]   601723 19687653  9920000  1360301  1567582 12830632  6483802  3046355
## [17]  2853118  4339367  4533372  1328361  5773552  6547629  9883640  5303925
## [25]  2967297  5988927   989415  1826341  2700551  1316470  8791894  2059179
## [33] 19378102  9535483   672591 11536504  3751351  3831074 12702379  1052567
## [41]  4625364   814180  6346105 25145561  2763885   625741  8001024  6724540
## [49]  1852994  5686986   563626&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But how did we know to use &lt;code&gt;population&lt;/code&gt;? Previously, by applying the function &lt;code&gt;str&lt;/code&gt; to the object &lt;code&gt;murders&lt;/code&gt;, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;state&amp;quot;      &amp;quot;abb&amp;quot;        &amp;quot;region&amp;quot;     &amp;quot;population&amp;quot; &amp;quot;total&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to know that the order of the entries in &lt;code&gt;murders$population&lt;/code&gt; preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tip&lt;/strong&gt;: &lt;code&gt;R&lt;/code&gt; comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing &lt;code&gt;murders$p&lt;/code&gt; then hitting the &lt;kbd&gt;tab&lt;/kbd&gt; key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors-numerics-characters-and-logical&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vectors: numerics, characters, and logical&lt;/h3&gt;
&lt;p&gt;The object &lt;code&gt;murders$population&lt;/code&gt; is not one number but several. We call these types of objects &lt;em&gt;vectors&lt;/em&gt;. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function &lt;code&gt;length&lt;/code&gt; tells you how many entries are in the vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pop &amp;lt;- murders$population
length(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 51&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This particular vector is &lt;em&gt;numeric&lt;/em&gt; since population sizes are numbers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(pop)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In a numeric vector, every entry must be a number.&lt;/p&gt;
&lt;p&gt;To store character strings, vectors can also be of class &lt;em&gt;character&lt;/em&gt;. For example, the state names are characters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders$state)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with numeric vectors, all entries in a character vector need to be a character.&lt;/p&gt;
&lt;p&gt;Another important type of vectors are &lt;em&gt;logical vectors&lt;/em&gt;. These must be either &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;z &amp;lt;- 3 == 2
z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(z)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;logical&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the &lt;code&gt;==&lt;/code&gt; is a relational operator asking if 3 is equal to 2. In &lt;code&gt;R&lt;/code&gt;, if you just use one &lt;code&gt;=&lt;/code&gt;, you actually assign a variable, but if you use two &lt;code&gt;==&lt;/code&gt; you test for equality. Yet another reason to avoid assigning via &lt;code&gt;=&lt;/code&gt;… it can get confusing and typos can really mess things up.&lt;/p&gt;
&lt;p&gt;You can see the other &lt;em&gt;relational operators&lt;/em&gt; by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?Comparison&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In future sections, you will see how useful relational operators can be.&lt;/p&gt;
&lt;p&gt;We discuss more important features of vectors after the next set of exercises.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Mathematically, the values in &lt;code&gt;pop&lt;/code&gt; are integers and there is an integer class in &lt;code&gt;R&lt;/code&gt;. However, by default, numbers are assigned class numeric even when they are round integers. For example, &lt;code&gt;class(1)&lt;/code&gt; returns numeric. You can turn them into class integer with the &lt;code&gt;as.integer()&lt;/code&gt; function or by adding an &lt;code&gt;L&lt;/code&gt; like this: &lt;code&gt;1L&lt;/code&gt;. Note the class by typing: &lt;code&gt;class(1L)&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;factors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Factors&lt;/h3&gt;
&lt;p&gt;In the &lt;code&gt;murders&lt;/code&gt; dataset, we might expect the region to also be a character vector. However, it is not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders$region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a &lt;em&gt;factor&lt;/em&gt;. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the &lt;code&gt;levels&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(murders$region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Northeast&amp;quot;     &amp;quot;South&amp;quot;         &amp;quot;North Central&amp;quot; &amp;quot;West&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the background, &lt;code&gt;R&lt;/code&gt; stores these &lt;em&gt;levels&lt;/em&gt; as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. It is also useful for computational reasons we’ll explore later.&lt;/p&gt;
&lt;p&gt;Note that the levels have an order that is different from the order of appearance in the factor object. The default in &lt;code&gt;R&lt;/code&gt; is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the &lt;code&gt;levels&lt;/code&gt; argument when creating the factor with the &lt;code&gt;factor&lt;/code&gt; function. For example, in the murders dataset regions are ordered from east to west. The function &lt;code&gt;reorder&lt;/code&gt; lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.&lt;/p&gt;
&lt;p&gt;Suppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the &lt;code&gt;reorder&lt;/code&gt; and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;region &amp;lt;- murders$region
value &amp;lt;- murders$total
region &amp;lt;- reorder(region, value, FUN = sum)
levels(region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Northeast&amp;quot;     &amp;quot;North Central&amp;quot; &amp;quot;West&amp;quot;          &amp;quot;South&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new order is in agreement with the fact that the Northeast has the least murders and the South has the most.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lists&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lists&lt;/h3&gt;
&lt;p&gt;Data frames are a special case of &lt;em&gt;lists&lt;/em&gt;. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $name
## [1] &amp;quot;John Doe&amp;quot;
## 
## $student_id
## [1] 1234
## 
## $grades
## [1] 95 82 91 97 93
## 
## $final_grade
## [1] &amp;quot;A&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(record)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As with data frames, you can extract the components of a list with the accessor &lt;code&gt;$&lt;/code&gt;. In fact, data frames are a type of list.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record$student_id&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also use double square brackets (&lt;code&gt;[[&lt;/code&gt;) like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record[[&amp;quot;student_id&amp;quot;]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should get used to the fact that in &lt;code&gt;R&lt;/code&gt; there are often several ways to do the same thing. such as accessing entries.&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You might also encounter lists without variable names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;John Doe&amp;quot;
## 
## [[2]]
## [1] 1234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If a list does not have names, you cannot extract the elements with &lt;code&gt;$&lt;/code&gt;, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;record2[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;John Doe&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We won’t be using lists until later, but you might encounter one in your own exploration of &lt;code&gt;R&lt;/code&gt;. For this reason, we show you some basics here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matrices&lt;/h3&gt;
&lt;p&gt;Matrices are another type of object that are common in &lt;code&gt;R&lt;/code&gt;. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.&lt;/p&gt;
&lt;p&gt;Yet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this class, but much of what happens in the background when you perform a data analysis involves matrices. We describe them briefly here since some of the functions we will learn return matrices.&lt;/p&gt;
&lt;p&gt;We can define a matrix using the &lt;code&gt;matrix&lt;/code&gt; function. We need to specify the number of rows and columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat &amp;lt;- matrix(1:12, 4, 3)
mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    5    9
## [2,]    2    6   10
## [3,]    3    7   11
## [4,]    4    8   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can access specific entries in a matrix using square brackets (&lt;code&gt;[&lt;/code&gt;). If you want the second row, third column, you use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[2, 3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want the entire second row, you leave the column spot empty:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[2, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  2  6 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that this returns a vector, not a matrix.&lt;/p&gt;
&lt;p&gt;Similarly, if you want the entire third column, you leave the row spot empty:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[, 3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  9 10 11 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is also a vector, not a matrix.&lt;/p&gt;
&lt;p&gt;You can access more than one column or more than one row if you like. This will give you a new matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[, 2:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    5    9
## [2,]    6   10
## [3,]    7   11
## [4,]    8   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can subset both rows and columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat[1:2, 2:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    5    9
## [2,]    6   10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can convert matrices into data frames using the function &lt;code&gt;as.data.frame&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.data.frame(mat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   V1 V2 V3
## 1  1  5  9
## 2  2  6 10
## 3  3  7 11
## 4  4  8 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use single square brackets (&lt;code&gt;[&lt;/code&gt;) to access rows and columns of a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;murders&amp;quot;)
murders[25, 1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Mississippi&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders[2:3, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     state abb region population total
## 2  Alaska  AK   West     710231    19
## 3 Arizona  AZ   West    6392017   232&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load the US murders dataset.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the function &lt;code&gt;str&lt;/code&gt; to examine the structure of the &lt;code&gt;murders&lt;/code&gt; object. Which of the following best describes the variables represented in this data frame?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The 51 states.&lt;/li&gt;
&lt;li&gt;The murder rates for all 50 states and DC.&lt;/li&gt;
&lt;li&gt;The state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;str&lt;/code&gt; shows no relevant information.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;What are the column names used by the data frame for these five variables?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the accessor &lt;code&gt;$&lt;/code&gt; to extract the state abbreviations and assign them to the object &lt;code&gt;a&lt;/code&gt;. What is the class of this object?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the square brackets to extract the state abbreviations and assign them to the object &lt;code&gt;b&lt;/code&gt;. Use the &lt;code&gt;identical&lt;/code&gt; function to determine if &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are the same.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We saw that the &lt;code&gt;region&lt;/code&gt; column stores a factor. You can corroborate this by typing:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(murders$region)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With one line of code, use the function &lt;code&gt;levels&lt;/code&gt; and &lt;code&gt;length&lt;/code&gt; to determine the number of regions defined by this dataset.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The function &lt;code&gt;table&lt;/code&gt; takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vectors&lt;/h2&gt;
&lt;p&gt;In R, the most basic objects available to store data are &lt;em&gt;vectors&lt;/em&gt;. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.&lt;/p&gt;
&lt;div id=&#34;creating-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating vectors&lt;/h3&gt;
&lt;p&gt;We can create vectors using the function &lt;code&gt;c&lt;/code&gt;, which stands for &lt;em&gt;concatenate&lt;/em&gt;. We use &lt;code&gt;c&lt;/code&gt; to concatenate entries in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(380, 124, 818)
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 380 124 818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country &amp;lt;- c(&amp;quot;italy&amp;quot;, &amp;quot;canada&amp;quot;, &amp;quot;egypt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;code&gt;R&lt;/code&gt; you can also use single quotes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country &amp;lt;- c(&amp;#39;italy&amp;#39;, &amp;#39;canada&amp;#39;, &amp;#39;egypt&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But be careful not to confuse the single quote ’ with the &lt;em&gt;back quote&lt;/em&gt;, which shares a keyboard key with &lt;kbd&gt;~&lt;/kbd&gt;.&lt;/p&gt;
&lt;p&gt;By now you should know that if you type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country &amp;lt;- c(italy, canada, egypt)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;you receive an error because the variables &lt;code&gt;italy&lt;/code&gt;, &lt;code&gt;canada&lt;/code&gt;, and &lt;code&gt;egypt&lt;/code&gt; are not defined. If we do not use the quotes, &lt;code&gt;R&lt;/code&gt; looks for variables with those names and returns an error.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;names&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Names&lt;/h3&gt;
&lt;p&gt;Sometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(italy = 380, canada = 124, egypt = 818)
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada  egypt 
##    380    124    818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The object &lt;code&gt;codes&lt;/code&gt; continues to be a numeric vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(codes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but with names:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(codes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;italy&amp;quot;  &amp;quot;canada&amp;quot; &amp;quot;egypt&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the use of strings without quotes looks confusing, know that you can use the quotes as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(&amp;quot;italy&amp;quot; = 380, &amp;quot;canada&amp;quot; = 124, &amp;quot;egypt&amp;quot; = 818)
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada  egypt 
##    380    124    818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is no difference between this function call and the previous one. This is one of the many ways in which &lt;code&gt;R&lt;/code&gt; is quirky compared to other languages.&lt;/p&gt;
&lt;p&gt;We can also assign names using the &lt;code&gt;names&lt;/code&gt; functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes &amp;lt;- c(380, 124, 818)
country &amp;lt;- c(&amp;quot;italy&amp;quot;,&amp;quot;canada&amp;quot;,&amp;quot;egypt&amp;quot;)
names(codes) &amp;lt;- country
codes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada  egypt 
##    380    124    818&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sequences&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sequences&lt;/h3&gt;
&lt;p&gt;Another useful function for creating vectors generates sequences:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 10, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 3 5 7 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want consecutive integers, we can use the following shorthand:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we use these functions, &lt;code&gt;R&lt;/code&gt; produces integers, not numerics, because they are typically used to index something:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, if we create a sequence including non-integers, the class changes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(seq(1, 10, 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;subsetting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting&lt;/h3&gt;
&lt;p&gt;We use square brackets to access specific elements of a vector. For the vector &lt;code&gt;codes&lt;/code&gt; we defined above, we can access the second element using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## canada 
##    124&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can get more than one entry by using a multi-entry vector as an index:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[c(1,3)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## italy egypt 
##   380   818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The sequences defined above are particularly useful if we want to access, say, the first two elements:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  italy canada 
##    380    124&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the elements have names, we can also access the entries using these names. Below are two examples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[&amp;quot;canada&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## canada 
##    124&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codes[c(&amp;quot;egypt&amp;quot;,&amp;quot;italy&amp;quot;)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## egypt italy 
##   818   380&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;coercion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coercion&lt;/h2&gt;
&lt;p&gt;In general, &lt;em&gt;coercion&lt;/em&gt; is an attempt by &lt;code&gt;R&lt;/code&gt; to be flexible with data types. When an entry does not match the expected, some of the prebuilt &lt;code&gt;R&lt;/code&gt; functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand &lt;em&gt;coercion&lt;/em&gt; can drive programmers crazy when attempting to code in &lt;code&gt;R&lt;/code&gt; since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.&lt;/p&gt;
&lt;p&gt;We said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(1, &amp;quot;canada&amp;quot;, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But we don’t get one, not even a warning! What happened? Look at &lt;code&gt;x&lt;/code&gt; and its class:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1&amp;quot;      &amp;quot;canada&amp;quot; &amp;quot;3&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R &lt;em&gt;coerced&lt;/em&gt; the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings &lt;code&gt;&#34;1&#34;&lt;/code&gt; and “&lt;code&gt;3&lt;/code&gt;”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;R also offers functions to change from one type to another. For example, you can turn numbers into characters with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:5
y &amp;lt;- as.character(x)
y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; &amp;quot;5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can turn it back with &lt;code&gt;as.numeric&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is actually quite useful since datasets that include numbers as character strings are common.&lt;/p&gt;
&lt;div id=&#34;not-availables-na&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Not availables (NA)&lt;/h3&gt;
&lt;p&gt;This “topic” seems to be wholly unappreciated and it has been our experience that students often panic when encountering an &lt;code&gt;NA&lt;/code&gt;. This often happens when a function tries to coerce one type to another and encounters an impossible case. In such circumstances, &lt;code&gt;R&lt;/code&gt; usually gives us a warning and turns the entry into a special value called an &lt;code&gt;NA&lt;/code&gt; (for “not available”). For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(&amp;quot;1&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;3&amp;quot;)
as.numeric(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1 NA  3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R does not have any guesses for what number you want when you type &lt;code&gt;b&lt;/code&gt;, so it does not try.&lt;/p&gt;
&lt;p&gt;While coercion is a common case leading to &lt;code&gt;NA&lt;/code&gt;s, you’ll see them in nearly every real-world dataset. Most often, you will encounter the &lt;code&gt;NA&lt;/code&gt;s as a stand-in for missing data. Again, this a common problem in real-world datasets and you need to be aware that it will come up.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sorting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sorting&lt;/h2&gt;
&lt;p&gt;Now that we have mastered some basic &lt;code&gt;R&lt;/code&gt; knowledge (ha!), let’s try to gain some insights into the safety of different states in the context of gun murders.&lt;/p&gt;
&lt;div id=&#34;sort&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;sort&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Say we want to rank the states from least to most gun murders. The function &lt;code&gt;sort&lt;/code&gt; sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)
sort(murders$total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]    2    4    5    5    7    8   11   12   12   16   19   21   22   27   32
## [16]   36   38   53   63   65   67   84   93   93   97   97   99  111  116  118
## [31]  120  135  142  207  219  232  246  250  286  293  310  321  351  364  376
## [46]  413  457  517  669  805 1257&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;order&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;order&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;order&lt;/code&gt; is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(31, 4, 15, 92, 65)
sort(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  4 15 31 65 92&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rather than sort the input vector, the function &lt;code&gt;order&lt;/code&gt; returns the index that sorts input vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- order(x)
x[index]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  4 15 31 65 92&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the same output as that returned by &lt;code&gt;sort(x)&lt;/code&gt;. If we look at this index, we see why it works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31  4 15 92 65&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;order(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2 3 1 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second entry of &lt;code&gt;x&lt;/code&gt; is the smallest, so &lt;code&gt;order(x)&lt;/code&gt; starts with &lt;code&gt;2&lt;/code&gt;. The next smallest is the third entry, so the second entry is &lt;code&gt;3&lt;/code&gt; and so on.&lt;/p&gt;
&lt;p&gt;How does this help us order the states by murders? First, remember that the entries of vectors you access with &lt;code&gt;$&lt;/code&gt; follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$state[1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Alabama&amp;quot;    &amp;quot;Alaska&amp;quot;     &amp;quot;Arizona&amp;quot;    &amp;quot;Arkansas&amp;quot;   &amp;quot;California&amp;quot;
## [6] &amp;quot;Colorado&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$abb[1:6]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;AL&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;AR&amp;quot; &amp;quot;CA&amp;quot; &amp;quot;CO&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- order(murders$total)
murders$abb[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;VT&amp;quot; &amp;quot;ND&amp;quot; &amp;quot;NH&amp;quot; &amp;quot;WY&amp;quot; &amp;quot;HI&amp;quot; &amp;quot;SD&amp;quot; &amp;quot;ME&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;MT&amp;quot; &amp;quot;RI&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;IA&amp;quot; &amp;quot;UT&amp;quot; &amp;quot;WV&amp;quot; &amp;quot;NE&amp;quot;
## [16] &amp;quot;OR&amp;quot; &amp;quot;DE&amp;quot; &amp;quot;MN&amp;quot; &amp;quot;KS&amp;quot; &amp;quot;CO&amp;quot; &amp;quot;NM&amp;quot; &amp;quot;NV&amp;quot; &amp;quot;AR&amp;quot; &amp;quot;WA&amp;quot; &amp;quot;CT&amp;quot; &amp;quot;WI&amp;quot; &amp;quot;DC&amp;quot; &amp;quot;OK&amp;quot; &amp;quot;KY&amp;quot; &amp;quot;MA&amp;quot;
## [31] &amp;quot;MS&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;SC&amp;quot; &amp;quot;TN&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;NJ&amp;quot; &amp;quot;VA&amp;quot; &amp;quot;NC&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;OH&amp;quot; &amp;quot;MO&amp;quot; &amp;quot;LA&amp;quot; &amp;quot;IL&amp;quot; &amp;quot;GA&amp;quot;
## [46] &amp;quot;MI&amp;quot; &amp;quot;PA&amp;quot; &amp;quot;NY&amp;quot; &amp;quot;FL&amp;quot; &amp;quot;TX&amp;quot; &amp;quot;CA&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to the above, California had the most murders.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;max-and-which.max&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;max&lt;/code&gt; and &lt;code&gt;which.max&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If we are only interested in the entry with the largest value, we can use &lt;code&gt;max&lt;/code&gt; for the value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max(murders$total)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1257&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and &lt;code&gt;which.max&lt;/code&gt; for the index of the largest value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;i_max &amp;lt;- which.max(murders$total)
murders$state[i_max]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;California&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the minimum, we can use &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;which.min&lt;/code&gt; in the same way.&lt;/p&gt;
&lt;p&gt;Does this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: &lt;code&gt;rank&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rank&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;rank&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Although not as frequently used as &lt;code&gt;order&lt;/code&gt; and &lt;code&gt;sort&lt;/code&gt;, the function &lt;code&gt;rank&lt;/code&gt; is also related to order and can be useful.
For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(31, 4, 15, 92, 65)
rank(x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 1 2 5 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To summarize, let’s look at the results of the three functions we have introduced:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;webshot&amp;#39;:
##   method        from    
##   print.webshot webshot2&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
original
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sort
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
order
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
rank
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
31
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
65
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
92
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;beware-of-recycling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Beware of recycling&lt;/h3&gt;
&lt;p&gt;Another common source of unnoticed errors in &lt;code&gt;R&lt;/code&gt; is the use of &lt;em&gt;recycling&lt;/em&gt;. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- c(1,2,3)
y &amp;lt;- c(10, 20, 30, 40, 50, 60, 70)
x+y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in x + y: longer object length is not a multiple of shorter object
## length&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11 22 33 41 52 63 71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do get a warning, but no error. For the output, &lt;code&gt;R&lt;/code&gt; has recycled the numbers in &lt;code&gt;x&lt;/code&gt;. Notice the last digit of numbers in the output.&lt;/p&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For these exercises we will use the US murders dataset. Make sure you load it prior to starting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;$&lt;/code&gt; operator to access the population size data and store it as the object &lt;code&gt;pop&lt;/code&gt;. Then use the &lt;code&gt;sort&lt;/code&gt; function to redefine &lt;code&gt;pop&lt;/code&gt; so that it is sorted. Finally, use the &lt;code&gt;[&lt;/code&gt; operator to report the smallest population size.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use &lt;code&gt;order&lt;/code&gt; instead of &lt;code&gt;sort&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can actually perform the same operation as in the previous exercise using the function &lt;code&gt;which.min&lt;/code&gt;. Write one line of code that does this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable &lt;code&gt;states&lt;/code&gt; to be the state names from the &lt;code&gt;murders&lt;/code&gt; data frame. Report the name of the state with the smallest population.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You can create a data frame using the &lt;code&gt;data.frame&lt;/code&gt; function. Here is a quick example:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp &amp;lt;- c(35, 88, 42, 84, 81, 30)
city &amp;lt;- c(&amp;quot;Beijing&amp;quot;, &amp;quot;Lagos&amp;quot;, &amp;quot;Paris&amp;quot;, &amp;quot;Rio de Janeiro&amp;quot;,
          &amp;quot;San Juan&amp;quot;, &amp;quot;Toronto&amp;quot;)
city_temps &amp;lt;- data.frame(name = city, temperature = temp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the &lt;code&gt;rank&lt;/code&gt; function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called &lt;code&gt;ranks&lt;/code&gt;, then create a data frame with the state name and its rank. Call the data frame &lt;code&gt;my_df&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Repeat the previous exercise, but this time order &lt;code&gt;my_df&lt;/code&gt; so that the states are ordered from least populous to most populous. Hint: create an object &lt;code&gt;ind&lt;/code&gt; that stores the indexes needed to order the population values. Then use the bracket operator &lt;code&gt;[&lt;/code&gt; to re-order each column in the data frame.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;na_example&lt;/code&gt; vector represents a series of counts. You can quickly examine the object using:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;na_example&amp;quot;)
str(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  int [1:1000] 2 1 3 2 1 3 1 4 3 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, when we compute the average with the function &lt;code&gt;mean&lt;/code&gt;, we obtain an &lt;code&gt;NA&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(na_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;is.na&lt;/code&gt; function returns a logical vector that tells us which entries are &lt;code&gt;NA&lt;/code&gt;. Assign this logical vector to an object called &lt;code&gt;ind&lt;/code&gt; and determine how many &lt;code&gt;NA&lt;/code&gt;s does &lt;code&gt;na_example&lt;/code&gt; have.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Now compute the average again, but only for the entries that are not &lt;code&gt;NA&lt;/code&gt;. Hint: remember the &lt;code&gt;!&lt;/code&gt; operator.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;vector-arithmetics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vector arithmetics&lt;/h2&gt;
&lt;p&gt;California had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)
murders$state[which.max(murders$population)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;California&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;with over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of &lt;code&gt;R&lt;/code&gt; come in handy.&lt;/p&gt;
&lt;div id=&#34;rescaling-a-vector&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Rescaling a vector&lt;/h3&gt;
&lt;p&gt;In R, arithmetic operations on vectors occur &lt;em&gt;element-wise&lt;/em&gt;. For a quick example, suppose we have height in inches:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inches &amp;lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and want to convert to centimeters. Notice what happens when we multiply &lt;code&gt;inches&lt;/code&gt; by 2.54:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inches * 2.54&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inches - 69&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0 -7 -3  1  1  4 -2  4 -2  1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;two-vectors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Two vectors&lt;/h3&gt;
&lt;p&gt;If we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{pmatrix}
a\\
b\\
c\\
d
\end{pmatrix}
+
\begin{pmatrix}
e\\
f\\
g\\
h
\end{pmatrix}
=
\begin{pmatrix}
a +e\\
b + f\\
c + g\\
d + h
\end{pmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The same holds for other mathematical operations, such as &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This implies that to compute the murder rates we can simply type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murder_rate &amp;lt;- murders$total / murders$population * 100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$abb[order(murder_rate)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;VT&amp;quot; &amp;quot;NH&amp;quot; &amp;quot;HI&amp;quot; &amp;quot;ND&amp;quot; &amp;quot;IA&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;UT&amp;quot; &amp;quot;ME&amp;quot; &amp;quot;WY&amp;quot; &amp;quot;OR&amp;quot; &amp;quot;SD&amp;quot; &amp;quot;MN&amp;quot; &amp;quot;MT&amp;quot; &amp;quot;CO&amp;quot; &amp;quot;WA&amp;quot;
## [16] &amp;quot;WV&amp;quot; &amp;quot;RI&amp;quot; &amp;quot;WI&amp;quot; &amp;quot;NE&amp;quot; &amp;quot;MA&amp;quot; &amp;quot;IN&amp;quot; &amp;quot;KS&amp;quot; &amp;quot;NY&amp;quot; &amp;quot;KY&amp;quot; &amp;quot;AK&amp;quot; &amp;quot;OH&amp;quot; &amp;quot;CT&amp;quot; &amp;quot;NJ&amp;quot; &amp;quot;AL&amp;quot; &amp;quot;IL&amp;quot;
## [31] &amp;quot;OK&amp;quot; &amp;quot;NC&amp;quot; &amp;quot;NV&amp;quot; &amp;quot;VA&amp;quot; &amp;quot;AR&amp;quot; &amp;quot;TX&amp;quot; &amp;quot;NM&amp;quot; &amp;quot;CA&amp;quot; &amp;quot;FL&amp;quot; &amp;quot;TN&amp;quot; &amp;quot;PA&amp;quot; &amp;quot;AZ&amp;quot; &amp;quot;GA&amp;quot; &amp;quot;MS&amp;quot; &amp;quot;MI&amp;quot;
## [46] &amp;quot;DE&amp;quot; &amp;quot;SC&amp;quot; &amp;quot;MD&amp;quot; &amp;quot;MO&amp;quot; &amp;quot;LA&amp;quot; &amp;quot;DC&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;TRY IT&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Previously we created this data frame:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp &amp;lt;- c(35, 88, 42, 84, 81, 30)
city &amp;lt;- c(&amp;quot;Beijing&amp;quot;, &amp;quot;Lagos&amp;quot;, &amp;quot;Paris&amp;quot;, &amp;quot;Rio de Janeiro&amp;quot;,
          &amp;quot;San Juan&amp;quot;, &amp;quot;Toronto&amp;quot;)
city_temps &amp;lt;- data.frame(name = city, temperature = temp)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is &lt;span class=&#34;math inline&#34;&gt;\(C = \frac{5}{9} \times (F - 32)\)&lt;/span&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Write code to compute the following sum &lt;span class=&#34;math inline&#34;&gt;\(1+1/2^2 + 1/3^2 + \dots 1/100^2\)&lt;/span&gt;? &lt;em&gt;Hint:&lt;/em&gt; thanks to Euler, we know it should be close to &lt;span class=&#34;math inline&#34;&gt;\(\pi^2/6\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the per 100,000 murder rate for each state and store it in the object &lt;code&gt;murder_rate&lt;/code&gt;. Then compute the average murder rate for the US using the function &lt;code&gt;mean&lt;/code&gt;. What is the average?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;indexing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Indexing&lt;/h2&gt;
&lt;p&gt;Indexing is a boring name for an important tool. &lt;code&gt;R&lt;/code&gt; provides a powerful and convenient way of referencing specific elements of vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(&amp;quot;murders&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;subsetting-with-logicals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Subsetting with logicals&lt;/h3&gt;
&lt;p&gt;We have now calculated the murder rate using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murder_rate &amp;lt;- murders$total / murders$population * 100000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Imagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of &lt;code&gt;R&lt;/code&gt; is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- murder_rate &amp;lt; 0.71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we instead want to know if a value is less or equal, we can use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- murder_rate &amp;lt;= 0.71&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we get back a logical vector with &lt;code&gt;TRUE&lt;/code&gt; for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murders$state[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hawaii&amp;quot;        &amp;quot;Iowa&amp;quot;          &amp;quot;New Hampshire&amp;quot; &amp;quot;North Dakota&amp;quot; 
## [5] &amp;quot;Vermont&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to count how many are TRUE, the function &lt;code&gt;sum&lt;/code&gt; returns the sum of the entries of a vector and logical vectors get &lt;em&gt;coerced&lt;/em&gt; to numeric with &lt;code&gt;TRUE&lt;/code&gt; coded as 1 and &lt;code&gt;FALSE&lt;/code&gt; as 0. Thus we can count the states using:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(ind)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;logical-operators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Logical operators&lt;/h3&gt;
&lt;p&gt;Suppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator &lt;em&gt;and&lt;/em&gt;, which in &lt;code&gt;R&lt;/code&gt; is represented with &lt;code&gt;&amp;amp;&lt;/code&gt;. This operation results in &lt;code&gt;TRUE&lt;/code&gt; only when both logicals are &lt;code&gt;TRUE&lt;/code&gt;. To see this, consider this example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TRUE &amp;amp; TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;TRUE &amp;amp; FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FALSE &amp;amp; FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For our example, we can form two logicals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;west &amp;lt;- murders$region == &amp;quot;West&amp;quot;
safe &amp;lt;- murder_rate &amp;lt;= 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we can use the &lt;code&gt;&amp;amp;&lt;/code&gt; to get a vector of logicals that tells us which states satisfy both conditions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- safe &amp;amp; west
murders$state[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hawaii&amp;quot;  &amp;quot;Idaho&amp;quot;   &amp;quot;Oregon&amp;quot;  &amp;quot;Utah&amp;quot;    &amp;quot;Wyoming&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;which&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;which&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Suppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function &lt;code&gt;which&lt;/code&gt; tells us which entries of a logical vector are TRUE. So we can type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- which(murders$state == &amp;quot;California&amp;quot;)
murder_rate[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.374138&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;match&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;match&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function &lt;code&gt;match&lt;/code&gt;. This function tells us which indexes of a second vector match each of the entries of a first vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ind &amp;lt;- match(c(&amp;quot;New York&amp;quot;, &amp;quot;Florida&amp;quot;, &amp;quot;Texas&amp;quot;), murders$state)
ind&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 33 10 44&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can look at the murder rates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;murder_rate[ind]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.667960 3.398069 3.201360&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;in&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;%in%&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function &lt;code&gt;%in%&lt;/code&gt;. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(&amp;quot;Boston&amp;quot;, &amp;quot;Dakota&amp;quot;, &amp;quot;Washington&amp;quot;) %in% murders$state&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we will be using &lt;code&gt;%in%&lt;/code&gt; often throughout the book.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: There is a connection between &lt;code&gt;match&lt;/code&gt; and &lt;code&gt;%in%&lt;/code&gt; through &lt;code&gt;which&lt;/code&gt;. To see this, notice that the following two lines produce the same index (although in different order):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;match(c(&amp;quot;New York&amp;quot;, &amp;quot;Florida&amp;quot;, &amp;quot;Texas&amp;quot;), murders$state)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 33 10 44&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;which(murders$state%in%c(&amp;quot;New York&amp;quot;, &amp;quot;Florida&amp;quot;, &amp;quot;Texas&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10 33 44&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;fyi&#34;&gt;
&lt;p&gt;&lt;strong&gt;EXERCISES&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start by loading the library and data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dslabs)
data(murders)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Compute the per 100,000 murder rate for each state and store it in an object called &lt;code&gt;murder_rate&lt;/code&gt;. Then use logical operators to create a logical vector named &lt;code&gt;low&lt;/code&gt; that tells us which entries of &lt;code&gt;murder_rate&lt;/code&gt; are lower than 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now use the results from the previous exercise and the function &lt;code&gt;which&lt;/code&gt; to determine the indices of &lt;code&gt;murder_rate&lt;/code&gt; associated with values lower than 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the results from the previous exercise to report the names of the states with murder rates lower than 1.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector &lt;code&gt;low&lt;/code&gt; and the logical operator &lt;code&gt;&amp;amp;&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of &lt;code&gt;murders$abb&lt;/code&gt; that match the three abbreviations, then use the &lt;code&gt;[&lt;/code&gt; operator to extract the states.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the &lt;code&gt;%in%&lt;/code&gt; operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Extend the code you used in exercise 7 to report the one entry that is &lt;strong&gt;not&lt;/strong&gt; an actual abbreviation. Hint: use the &lt;code&gt;!&lt;/code&gt; operator, which turns &lt;code&gt;FALSE&lt;/code&gt; into &lt;code&gt;TRUE&lt;/code&gt; and vice versa, then &lt;code&gt;which&lt;/code&gt; to obtain an index.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;videos&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Videos&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/AIbvhxZKOl4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://rstudio.cloud&#34; class=&#34;uri&#34;&gt;https://rstudio.cloud&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://rafalab.github.io/dsbook/installing-r-rstudio.html&#34; class=&#34;uri&#34;&gt;https://rafalab.github.io/dsbook/installing-r-rstudio.html&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/&#34; class=&#34;uri&#34;&gt;http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/&lt;/a&gt;&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;I’m especially partial to Puerto Rico.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;This is, without a doubt, my least favorite aspect of &lt;code&gt;R&lt;/code&gt;. I’d even venture to call it stupid. The logic behind this pesky &lt;code&gt;&amp;lt;-&lt;/code&gt; is a total mystery to me, but there &lt;em&gt;is&lt;/em&gt; logic to avoiding &lt;code&gt;=&lt;/code&gt;. But, you do you.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;This equals sign is the reasons we assign values with &lt;code&gt;&amp;lt;-&lt;/code&gt;; then when arguments of a function are assigned values, we don’t end up with multiple equals signs. But… who cares.&lt;a href=&#34;#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;Whether you view this as a feature or a bug is a good indicator whether you’ll enjoy working with &lt;code&gt;R&lt;/code&gt;.&lt;a href=&#34;#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
