<!DOCTYPE html>
<html lang="en-us" 
      xmlns:og="http://ogp.me/ns#" 
      xmlns:fb="https://www.facebook.com/2008/fbml">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Ben Bushong">

  
  
  
    
  
  <meta name="description" content="Required Reading  Guiding Questions  Introduction to Linear Regression  Case study: is height hereditary? The correlation coefficient  Sample correlation is a random variable Correlation is not always a useful summary  Conditional expectations The regression line  Regression improves precision Bivariate normal distribution (advanced) Variance explained Warning: there are two regression lines     Required Reading  This page.  Guiding Questions  How can we express a simple relationship between variables?">

  
  <link rel="alternate" hreflang="en-us" href="https://ssc442.netlify.app/content/06-content/">

  


  
  
  
  <meta name="theme-color" content="#18453B">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans+Condensed:400,400i,700,700i%7COverpass:400,400i,700,700i&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu5cb5fae69b6b75ece00896ca20e5499a_29457_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu5cb5fae69b6b75ece00896ca20e5499a_29457_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://ssc442.netlify.app/content/06-content/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Data Analytics">
  <meta property="og:url" content="https://ssc442.netlify.app/content/06-content/">
  <meta property="og:title" content="Introduction to Regression | Data Analytics">
  <meta property="og:description" content="Required Reading  Guiding Questions  Introduction to Linear Regression  Case study: is height hereditary? The correlation coefficient  Sample correlation is a random variable Correlation is not always a useful summary  Conditional expectations The regression line  Regression improves precision Bivariate normal distribution (advanced) Variance explained Warning: there are two regression lines     Required Reading  This page.  Guiding Questions  How can we express a simple relationship between variables?"><meta property="og:image" content="https://ssc442.netlify.app/img/social-image.png">
  <meta property="twitter:image" content="https://ssc442.netlify.app/img/social-image.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2021-10-05T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2021-11-02T11:41:59-04:00">
  

  



  


  


  <link rel="shortcut icon" href="https://ssc442.netlify.app/favicon.ico" />
  <link rel="apple-touch-icon-precomposed" sizes="57x57" href="https://ssc442.netlify.app/img/apple-touch-icon-57x57.png" />
  <link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://ssc442.netlify.app/img/apple-touch-icon-114x114.png" />
  <link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://ssc442.netlify.app/img/apple-touch-icon-72x72.png" />
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://ssc442.netlify.app/img/apple-touch-icon-144x144.png" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="https://ssc442.netlify.app/img/apple-touch-icon-120x120.png" />
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="https://ssc442.netlify.app/img/apple-touch-icon-152x152.png" />
  <link rel="icon" type="image/png" href="https://ssc442.netlify.app/img/favicon-32x32.png" sizes="32x32" />
  <link rel="icon" type="image/png" href="https://ssc442.netlify.app/img/favicon-16x16.png" sizes="16x16" />
  <meta name="application-name" content="SSC 442: Data Analytics" />
  <meta name="msapplication-TileColor" content="#FFFFFF" />
  <meta name="msapplication-TileImage" content="https://ssc442.netlify.app/img/mstile-144x144.png" />


  <title>Introduction to Regression | Data Analytics</title>

</head>


<body id="top" data-spy="scroll" data-offset="70"
    data-target="#TableOfContents"
    >

    <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


    







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Data Analytics</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Data Analytics</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/syllabus/"><span>Syllabus</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/schedule/"><span>Schedule</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/content/"><span>Content</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/example/"><span>Examples</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/assignment/"><span>Assignments</span></a>
        </li>

        
        

        

        
        
        
          
            
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="https://join.slack.com/t/ssc442/shared_invite/zt-v3n7r8hh-TjwMzpDLBSywvVYwzftqXA" target="_blank" rel="noopener"><span><i class="fab fa-slack"></i></span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      

      

      

    </ul>

  </div>
</nav>


    

<div class="container-fluid docs">
    <div class="row flex-xl-nowrap">
        <div class="col-12 col-md-3 col-xl-2 docs-sidebar">
            





  
    
  




<form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-3" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <span><i class="fas fa-bars"></i></span>
  </button>

  
</form>

<nav class="collapse docs-links" id="docs-nav">
  

  
  
  
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/content/">Readings, lectures, and videos</a>

  </div>
  
  <div class="docs-toc-item">
    <a class="docs-toc-link" href="/content/10-content/">Course content</a>
    <ul class="nav docs-sidenav">
      
      <li >
        <a href="/content/10-content/">10: Nonparametric Regression</a>
      </li>
      
      <li >
        <a href="/content/01-content/">1: Introduction to R</a>
      </li>
      
      <li >
        <a href="/content/02-content/">2: The Tidyverse</a>
      </li>
      
      <li >
        <a href="/content/03-content/">3: Visualizations I</a>
      </li>
      
      <li >
        <a href="/content/04-content/">4: Visualizations III</a>
      </li>
      
      <li >
        <a href="/content/05-content/">5: Uncertainty</a>
      </li>
      
      <li class="active">
        <a href="/content/06-content/">6: Linear Regression I</a>
      </li>
      
      <li >
        <a href="/content/07-content/">7: Linear Regression II</a>
      </li>
      
      <li >
        <a href="/content/08-content/">8: Linear Regression III</a>
      </li>
      
      <li >
        <a href="/content/09-content/">9: Bias vs. Variance</a>
      </li>
      
      <li >
        <a href="/content/11-content/">11: Classification</a>
      </li>
      
      <li >
        <a href="/content/12-content/">12: Data Wrangling</a>
      </li>
      
      <li >
        <a href="/content/13-content/">13: Geospatial with R</a>
      </li>
      
      <li >
        <a href="/content/14-content/">14: Text as Data</a>
      </li>
      
    </ul>
    

  </div>
  
  
</nav>

        </div>

        

        <main class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 docs-content" role="main">

            <article class="article">

                <div class="docs-article-container">
                    <h1>Introduction to Regression</h1>

                    

                    
                    <div class="due-date p-2 mb-3 bg-content text-white">
                        Content for Tuesday, October 5, 2021
                    </div>
                    

                    

                    <div class="article-style">
                        
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#required-reading">Required Reading</a>
<ul>
<li><a href="#guiding-questions">Guiding Questions</a></li>
</ul></li>
<li><a href="#regression">Introduction to Linear Regression</a>
<ul>
<li><a href="#case-study-is-height-hereditary">Case study: is height hereditary?</a></li>
<li><a href="#corr-coef">The correlation coefficient</a>
<ul>
<li><a href="#sample-correlation-is-a-random-variable">Sample correlation is a random variable</a></li>
<li><a href="#correlation-is-not-always-a-useful-summary">Correlation is not always a useful summary</a></li>
</ul></li>
<li><a href="#conditional-expectation">Conditional expectations</a></li>
<li><a href="#the-regression-line">The regression line</a>
<ul>
<li><a href="#regression-improves-precision">Regression improves precision</a></li>
<li><a href="#bivariate-normal-distribution-advanced">Bivariate normal distribution (advanced)</a></li>
<li><a href="#variance-explained">Variance explained</a></li>
<li><a href="#warning-there-are-two-regression-lines">Warning: there are two regression lines</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<div id="required-reading" class="section level2">
<h2>Required Reading</h2>
<ul>
<li>This page.</li>
</ul>
<!-- ### Supplemental Readings -->
<!-- - Coming soon. -->
<div id="guiding-questions" class="section level3">
<h3>Guiding Questions</h3>
<ul>
<li>How can we express a simple relationship between variables?</li>
<li>What is a geometric intuition behind “linear regression”?</li>
<li>How might we visualize a linear regression?</li>
</ul>
<div class="fyi">
<p>Today’s lecture will ask you to touch real data during the lecture. Please download the following dataset and load it into <code>R</code>.</p>
<ul>
<li><a href="/projects/data/ames.csv"><i class="fas fa-file-csv"></i> <code>Ames.csv</code></a></li>
</ul>
<p>This dataset is from houses in Ames, Iowa. (Thrilling!) We will use this dataset during the lecture to illustrate some of the points discussed below.</p>
</div>
</div>
</div>
<div id="regression" class="section level1">
<h1>Introduction to Linear Regression</h1>
<p>Up to this point, this class has focused mainly on single variables. However, in data analytics applications, it is very common to be interested in the <strong>relationship</strong> between two or more variables. For instance, in the coming days we will use a data-driven approach that examines the relationship between player statistics and success to guide the building of a baseball team with a limited budget. Before delving into this more complex example, we introduce necessary concepts needed to understand regression using a simpler illustration. We actually use the dataset from which regression was born.</p>
<p>The example is from genetics. Francis Galton<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity. While doing this, he developed the concepts of correlation and regression, as well as a connection to pairs of data that follow a normal distribution. Of course, at the time this data was collected our knowledge of genetics was quite limited compared to what we know today. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height? The technique he developed to answer this question, regression, can also be applied to our baseball question. Regression can be applied in many other circumstances as well.</p>
<p><strong>Historical note</strong>: Galton made important contributions to statistics and genetics, but he was also one of the first proponents of eugenics, a scientifically flawed philosophical movement favored by many biologists of Galton’s time but with horrific historical consequences. These consequences still reverberate to this day, and form the basis for much of the Western world’s racist policies. You can read more about it here: <a href="https://pged.org/history-eugenics-and-genetics/">https://pged.org/history-eugenics-and-genetics/</a>.</p>
<div id="case-study-is-height-hereditary" class="section level2">
<h2>Case study: is height hereditary?</h2>
<p>We have access to Galton’s family height data through the <strong>HistData</strong> package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:</p>
<pre class="r"><code>library(tidyverse)
library(HistData)
data(&quot;GaltonFamilies&quot;)

set.seed(1983)
galton_heights &lt;- GaltonFamilies %&gt;%
  filter(gender == &quot;male&quot;) %&gt;%
  group_by(family) %&gt;%
  sample_n(1) %&gt;%
  ungroup() %&gt;%
  select(father, childHeight) %&gt;%
  rename(son = childHeight)</code></pre>
<p>In the exercises, we will look at other relationships including mothers and daughters.</p>
<p>Suppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:</p>
<pre class="r"><code>galton_heights %&gt;%
  summarize(mean(father), sd(father), mean(son), sd(son))</code></pre>
<pre><code>## # A tibble: 1 × 4
##   `mean(father)` `sd(father)` `mean(son)` `sd(son)`
##            &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;
## 1           69.1         2.55        69.2      2.71</code></pre>
<p>However, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.</p>
<pre class="r"><code>galton_heights %&gt;% ggplot(aes(father, son)) +
  geom_point(alpha = 0.5)</code></pre>
<p><img src="/content/06-content_files/figure-html/scatterplot-1.png" width="40%" /></p>
<p>We will learn that the correlation coefficient is an informative summary of how two variables move together and then see how this can be used to predict one variable using the other.</p>
</div>
<div id="corr-coef" class="section level2">
<h2>The correlation coefficient</h2>
<p>The correlation coefficient is defined for a list of pairs <span class="math inline">\((x_1, y_1), \dots, (x_n,y_n)\)</span> as the average of the product of the standardized values:</p>
<p><span class="math display">\[
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)
\]</span>
with <span class="math inline">\(\mu_x, \mu_y\)</span> the averages of <span class="math inline">\(x_1,\dots, x_n\)</span> and <span class="math inline">\(y_1, \dots, y_n\)</span>, respectively, and <span class="math inline">\(\sigma_x, \sigma_y\)</span> the standard deviations. The Greek letter <span class="math inline">\(\rho\)</span> is commonly used in statistics books to denote the correlation. The Greek letter for <span class="math inline">\(r\)</span>, <span class="math inline">\(\rho\)</span>, because it is the first letter of regression. Soon we learn about the connection between correlation and regression. We can represent the formula above with R code using:</p>
<pre class="r"><code>rho &lt;- mean(scale(x) * scale(y))</code></pre>
<p>To understand why this equation does in fact summarize how two variables move together, consider the <span class="math inline">\(i\)</span>-th entry of <span class="math inline">\(x\)</span> is <span class="math inline">\(\left( \frac{x_i-\mu_x}{\sigma_x} \right)\)</span> SDs away from the average. Similarly, the <span class="math inline">\(y_i\)</span> that is paired with <span class="math inline">\(x_i\)</span>, is <span class="math inline">\(\left( \frac{y_1-\mu_y}{\sigma_y} \right)\)</span> SDs away from the average <span class="math inline">\(y\)</span>. If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are unrelated, the product <span class="math inline">\(\left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)\)</span> will be positive ( <span class="math inline">\(+ \times +\)</span> and <span class="math inline">\(- \times -\)</span> ) as often as negative (<span class="math inline">\(+ \times -\)</span> and <span class="math inline">\(- \times +\)</span>) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products ( <span class="math inline">\(+ \times +\)</span> and <span class="math inline">\(- \times -\)</span>) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation.</p>
<p>The correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:</p>
<p><span class="math display">\[
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)^2 =
\frac{1}{\sigma_x^2} \frac{1}{n} \sum_{i=1}^n \left( x_i-\mu_x \right)^2 =
\frac{1}{\sigma_x^2} \sigma^2_x =
1
\]</span></p>
<p>A similar derivation, but with <span class="math inline">\(x\)</span> and its exact opposite, proves the correlation has to be bigger or equal to -1.</p>
<p>For other pairs, the correlation is in between -1 and 1. The correlation between father and son’s heights is about 0.5:</p>
<pre class="r"><code>galton_heights %&gt;% summarize(r = cor(father, son)) %&gt;% pull(r)</code></pre>
<pre><code>## [1] 0.4334102</code></pre>
<p>To see what data looks like for different values of <span class="math inline">\(\rho\)</span>, here are six examples of pairs with correlations ranging from -0.9 to 0.99:</p>
<p><img src="/content/06-content_files/figure-html/what-correlation-looks-like-1.png" width="672" /></p>
<div id="sample-correlation-is-a-random-variable" class="section level3">
<h3>Sample correlation is a random variable</h3>
<p>Before we continue connecting correlation to regression, let’s remind ourselves about random variability.</p>
<p>In most data science applications, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest but rather for a random sample. As with the average and standard deviation, the <em>sample correlation</em> is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.</p>
<p>By way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:</p>
<pre class="r"><code>R &lt;- sample_n(galton_heights, 25, replace = TRUE) %&gt;%
  summarize(r = cor(father, son)) %&gt;% pull(r)</code></pre>
<p><code>R</code> is a random variable. We can run a Monte Carlo simulation to see its distribution:</p>
<pre class="r"><code>B &lt;- 1000
N &lt;- 25
R &lt;- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %&gt;%
    summarize(r=cor(father, son)) %&gt;%
    pull(r)
})
qplot(R, geom = &quot;histogram&quot;, binwidth = 0.05, color = I(&quot;black&quot;))</code></pre>
<p><img src="/content/06-content_files/figure-html/sample-correlation-distribution-1.png" width="672" /></p>
<p>We see that the expected value of <code>R</code> is the population correlation:</p>
<pre class="r"><code>mean(R)</code></pre>
<pre><code>## [1] 0.4307393</code></pre>
<p>and that it has a relatively high standard error relative to the range of values <code>R</code> can take:</p>
<pre class="r"><code>sd(R)</code></pre>
<pre><code>## [1] 0.1609393</code></pre>
<p>So, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.</p>
<p>Also, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough <span class="math inline">\(N\)</span>, the distribution of <code>R</code> is approximately normal with expected value <span class="math inline">\(\rho\)</span>. The standard deviation, which is somewhat complex to derive, is <span class="math inline">\(\sqrt{\frac{1-r^2}{N-2}}\)</span>.</p>
<p>In our example, <span class="math inline">\(N=25\)</span> does not seem to be large enough to make the approximation a good one:</p>
<pre class="r"><code>ggplot(aes(sample=R), data = data.frame(R)) +
  stat_qq() +
  geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))</code></pre>
<p><img src="/content/06-content_files/figure-html/small-sample-correlation-not-normal-1.png" width="40%" /></p>
<p>If you increase <span class="math inline">\(N\)</span>, you will see the distribution converging to normal.</p>
</div>
<div id="correlation-is-not-always-a-useful-summary" class="section level3">
<h3>Correlation is not always a useful summary</h3>
<p>Correlation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:</p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="/content/06-content_files/figure-html/ascombe-quartet-1.png" width="672" /></p>
<p>Correlation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction.</p>
</div>
</div>
<div id="conditional-expectation" class="section level2">
<h2>Conditional expectations</h2>
<p>Suppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?</p>
<p>It turns out that if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.</p>
<p>In general, we call this approach <em>conditioning</em>. The general idea is that we stratify a population into groups and compute summaries in each group. To provide a mathematical description of conditioning, consider we have a population of pairs of values <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span>, for example all father and son heights in England. In the previous week’s content, we learned that if you take a random pair <span class="math inline">\((X,Y)\)</span>, the expected value and best predictor of <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mbox{E}(Y) = \mu_y\)</span>, the population average <span class="math inline">\(1/n\sum_{i=1}^n y_i\)</span>. However, we are no longer interested in the general population, instead we are interested in only the subset of a population with a specific <span class="math inline">\(x_i\)</span> value, 72 inches in our example. This subset of the population, is also a population and thus the same principles and properties we have learned apply. The <span class="math inline">\(y_i\)</span> in the subpopulation have a distribution, referred to as the <em>conditional distribution</em>, and this distribution has an expected value referred to as the <em>conditional expectation</em>. In our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches. The statistical notation for the conditional expectation is</p>
<p><span class="math display">\[
\mbox{E}(Y \mid X = x)
\]</span></p>
<p>with <span class="math inline">\(x\)</span> representing the fixed value that defines that subset, for example 72 inches. Similarly, we denote the standard deviation of the strata with</p>
<p><span class="math display">\[
\mbox{SD}(Y \mid X = x) = \sqrt{\mbox{Var}(Y \mid X = x)}
\]</span></p>
<p>Because the conditional expectation <span class="math inline">\(E(Y\mid X=x)\)</span> is the best predictor for the random variable <span class="math inline">\(Y\)</span> for an individual in the strata defined by <span class="math inline">\(X=x\)</span>, many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.</p>
<p>In the example we have been considering, we are interested in computing the average son height <em>conditioned</em> on the father being 72 inches tall. We want to estimate <span class="math inline">\(E(Y|X=72)\)</span> using the sample collected by Galton. We previously learned that the sample average is the preferred approach to estimating the population average. However, a challenge when using this approach to estimating conditional expectations is that for continuous data we don’t have many data points matching exactly one value in our sample. For example, we have only:</p>
<pre class="r"><code>sum(galton_heights$father == 72)</code></pre>
<pre><code>## [1] 8</code></pre>
<p>fathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:</p>
<pre class="r"><code>sum(galton_heights$father == 72.5)</code></pre>
<pre><code>## [1] 1</code></pre>
<p>A practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of <span class="math inline">\(x\)</span>. In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:</p>
<pre class="r"><code>conditional_avg &lt;- galton_heights %&gt;%
  filter(round(father) == 72) %&gt;%
  summarize(avg = mean(son)) %&gt;%
  pull(avg)
conditional_avg</code></pre>
<pre><code>## [1] 70.5</code></pre>
<p>Note that a 72-inch father is taller than average – specifically, 72 - 69.1/2.5 =
1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son. The sons of 72-inch fathers have <em>regressed</em> some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later lecture, this is not a coincidence.</p>
<p>If we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:</p>
<pre class="r"><code>galton_heights %&gt;% mutate(father_strata = factor(round(father))) %&gt;%
  ggplot(aes(father_strata, son)) +
  geom_boxplot() +
  geom_point()</code></pre>
<p><img src="/content/06-content_files/figure-html/boxplot-1-1.png" width="40%" /></p>
<p>Not surprisingly, the centers of the groups are increasing with height.
Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:</p>
<p><img src="/content/06-content_files/figure-html/conditional-averages-follow-line-1.png" width="40%" /></p>
<p>The fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the <em>regression line</em>, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line.</p>
</div>
<div id="the-regression-line" class="section level2">
<h2>The regression line</h2>
<p>If we are predicting a random variable <span class="math inline">\(Y\)</span> knowing the value of another <span class="math inline">\(X=x\)</span> using a regression line, then we predict that for every standard deviation, <span class="math inline">\(\sigma_X\)</span>, that <span class="math inline">\(x\)</span> increases above the average <span class="math inline">\(\mu_X\)</span>, <span class="math inline">\(Y\)</span> increase <span class="math inline">\(\rho\)</span> standard deviations <span class="math inline">\(\sigma_Y\)</span> above the average <span class="math inline">\(\mu_Y\)</span> with <span class="math inline">\(\rho\)</span> the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The formula for the regression is therefore:</p>
<p><span class="math display">\[
\left( \frac{Y-\mu_Y}{\sigma_Y} \right) = \rho \left( \frac{x-\mu_X}{\sigma_X} \right)
\]</span></p>
<p>We can rewrite it like this:</p>
<p><span class="math display">\[
Y = \mu_Y + \rho \left( \frac{x-\mu_X}{\sigma_X} \right) \sigma_Y
\]</span></p>
<p>If there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use <span class="math inline">\(x\)</span> at all for the prediction and simply predict the average <span class="math inline">\(\mu_Y\)</span>. For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.</p>
<p>Note that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, <span class="math inline">\(x\)</span>, is to the average of the <span class="math inline">\(x\)</span>s. This is why we call it <em>regression</em>: the son regresses to the average height. In fact, the title of Galton’s paper was: <em>Regression toward mediocrity in hereditary stature</em>. To add regression lines to plots, we will need the above formula in the form:</p>
<p><span class="math display">\[
y= b + mx \mbox{ with slope } m = \rho \frac{\sigma_y}{\sigma_x} \mbox{ and intercept } b=\mu_y - m \mu_x
\]</span></p>
<p>Here we add the regression line to the original data:</p>
<pre class="r"><code>mu_x &lt;- mean(galton_heights$father)
mu_y &lt;- mean(galton_heights$son)
s_x &lt;- sd(galton_heights$father)
s_y &lt;- sd(galton_heights$son)
r &lt;- cor(galton_heights$father, galton_heights$son)

galton_heights %&gt;%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x)</code></pre>
<p><img src="/content/06-content_files/figure-html/regression-line-1.png" width="40%" /></p>
<p>The regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation <span class="math inline">\(\rho\)</span>. You can make same plot, but using standard units like this:</p>
<pre class="r"><code>galton_heights %&gt;%
  ggplot(aes(scale(father), scale(son))) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = r)</code></pre>
<div id="regression-improves-precision" class="section level3">
<h3>Regression improves precision</h3>
<p>Let’s compare the two approaches to prediction that we have presented:</p>
<ol style="list-style-type: decimal">
<li>Round fathers’ heights to closest inch, stratify, and then take the average.</li>
<li>Compute the regression line and use it to predict.</li>
</ol>
<p>We use a Monte Carlo simulation sampling <span class="math inline">\(N=50\)</span> families:</p>
<pre class="r"><code>B &lt;- 1000
N &lt;- 50

set.seed(1983)
conditional_avg &lt;- replicate(B, {
  dat &lt;- sample_n(galton_heights, N)
  dat %&gt;% filter(round(father) == 72) %&gt;%
    summarize(avg = mean(son)) %&gt;%
    pull(avg)
  })

regression_prediction &lt;- replicate(B, {
  dat &lt;- sample_n(galton_heights, N)
  mu_x &lt;- mean(dat$father)
  mu_y &lt;- mean(dat$son)
  s_x &lt;- sd(dat$father)
  s_y &lt;- sd(dat$son)
  r &lt;- cor(dat$father, dat$son)
  mu_y + r*(72 - mu_x)/s_x*s_y
})</code></pre>
<p>Although the expected value of these two random variables is about the same:</p>
<pre class="r"><code>mean(conditional_avg, na.rm = TRUE)</code></pre>
<pre><code>## [1] 70.49368</code></pre>
<pre class="r"><code>mean(regression_prediction)</code></pre>
<pre><code>## [1] 70.50941</code></pre>
<p>The standard error for the regression prediction is substantially smaller:</p>
<pre class="r"><code>sd(conditional_avg, na.rm = TRUE)</code></pre>
<pre><code>## [1] 0.9635814</code></pre>
<pre class="r"><code>sd(regression_prediction)</code></pre>
<pre><code>## [1] 0.4520833</code></pre>
<p>The regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use <code>na.rm=TRUE</code>. The regression always uses all the data.</p>
<p>So why not always use the regression for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The justification, which we include in the next section, is somewhat more advanced than the rest of this reading.</p>
</div>
<div id="bivariate-normal-distribution-advanced" class="section level3">
<h3>Bivariate normal distribution (advanced)</h3>
<p>Correlation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.</p>
<p>The main way we motivate the use of correlation involves what is called the <em>bivariate normal distribution</em>.</p>
<p>When a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. These ovals can be thin (high correlation) or circle-shaped (no correlation).</p>
<!--
<img src="/content/06-content_files/figure-html/bivariate-ovals-1.png" width="672" />
-->
<p>A more technical way to define the bivariate normal distribution is the following: if <span class="math inline">\(X\)</span> is a normally distributed random variable, <span class="math inline">\(Y\)</span> is also a normally distributed random variable, and the conditional distribution of <span class="math inline">\(Y\)</span> for any <span class="math inline">\(X=x\)</span> is approximately normal, then the pair is approximately bivariate normal.</p>
<p>If we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:</p>
<pre class="r"><code>galton_heights %&gt;%
  mutate(z_father = round((father - mean(father)) / sd(father))) %&gt;%
  filter(z_father %in% -2:2) %&gt;%
  ggplot() +
  stat_qq(aes(sample = son)) +
  facet_wrap( ~ z_father)</code></pre>
<p><img src="/content/06-content_files/figure-html/qqnorm-of-strata-1.png" width="672" /></p>
<p>Now we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of <span class="math inline">\(x\)</span>, the expected value of the <span class="math inline">\(Y\)</span> in pairs for which <span class="math inline">\(X=x\)</span> is:</p>
<p><span class="math display">\[
\mbox{E}(Y | X=x) = \mu_Y +  \rho \frac{X-\mu_X}{\sigma_X}\sigma_Y
\]</span></p>
<p>This is the regression line, with slope <span class="math display">\[\rho \frac{\sigma_Y}{\sigma_X}\]</span> and intercept <span class="math inline">\(\mu_y - m\mu_X\)</span>. It is equivalent to the regression equation we showed earlier which can be written like this:</p>
<p><span class="math display">\[
\frac{\mbox{E}(Y \mid X=x)  - \mu_Y}{\sigma_Y} = \rho \frac{x-\mu_X}{\sigma_X}
\]</span></p>
<p>This implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.</p>
<p>In summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of <span class="math inline">\(Y\)</span> given we know the value of <span class="math inline">\(X\)</span>, is given by the regression line.</p>
</div>
<div id="variance-explained" class="section level3">
<h3>Variance explained</h3>
<p>The bivariate normal theory also tells us that the standard deviation of the <em>conditional</em> distribution described above is:</p>
<p><span class="math display">\[
\mbox{SD}(Y \mid X=x ) = \sigma_Y \sqrt{1-\rho^2}
\]</span></p>
<p>To see why this is intuitive, notice that without conditioning, <span class="math inline">\(\mbox{SD}(Y) = \sigma_Y\)</span>, we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.</p>
<p>Specifically, it is reduced to <span class="math inline">\(\sqrt{1-\rho^2} = \sqrt{1 - 0.25}\)</span> = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.</p>
<p>The statement “<span class="math inline">\(X\)</span> explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by <span class="math inline">\(1-\rho^2\)</span>, so we say that <span class="math inline">\(X\)</span> explains <span class="math inline">\(1- (1-\rho^2)=\rho^2\)</span> (the correlation squared) of the variance.</p>
<p>But it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.</p>
</div>
<div id="warning-there-are-two-regression-lines" class="section level3">
<h3>Warning: there are two regression lines</h3>
<p>We computed a regression line to predict the son’s height from father’s height. We used these calculations:</p>
<pre class="r"><code>mu_x &lt;- mean(galton_heights$father)
mu_y &lt;- mean(galton_heights$son)
s_x &lt;- sd(galton_heights$father)
s_y &lt;- sd(galton_heights$son)
r &lt;- cor(galton_heights$father, galton_heights$son)
m_1 &lt;-  r * s_y / s_x
b_1 &lt;- mu_y - m_1*mu_x</code></pre>
<p>which gives us the function <span class="math inline">\(\mbox{E}(Y\mid X=x) =\)</span> 37.3 + 0.46 <span class="math inline">\(x\)</span>.</p>
<p>What if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function:
<span class="math inline">\(x = \{ \mbox{E}(Y\mid X=x) -\)</span> 37.3 <span class="math inline">\(\} /\)</span> 0.5.</p>
<p>We need to compute <span class="math inline">\(\mbox{E}(X \mid Y=y)\)</span>. Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:</p>
<pre class="r"><code>m_2 &lt;-  r * s_x / s_y
b_2 &lt;- mu_x - m_2 * mu_y</code></pre>
<p>So we get <span class="math inline">\(\mbox{E}(X \mid Y=y) =\)</span> 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights <span class="math inline">\(y\)</span> is to the son average.</p>
<p>Here is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:</p>
<pre class="r"><code>galton_heights %&gt;%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = b_1, slope = m_1, col = &quot;blue&quot;) +
  geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = &quot;red&quot;)</code></pre>
<p><img src="/content/06-content_files/figure-html/two-regression-lines-1.png" width="40%" /></p>
<div class="fyi">
<p><strong>TRY IT</strong></p>
<ol style="list-style-type: decimal">
<li><p>Load the <code>GaltonFamilies</code> data from the <strong>HistData</strong>. The children in each family are listed by gender and then by height. Create a dataset called <code>galton_heights</code> by picking a male and female at random.</p></li>
<li><p>Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.</p></li>
<li><p>Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.</p></li>
</ol>
</div>
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Francis_Galton" class="uri">https://en.wikipedia.org/wiki/Francis_Galton</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>

                    </div>

                    



                    
                </div>

                <div class="body-footer">
                    <p>Last updated on November 2, 2021</p>

                    


                    

                </div>

            </article>

            <footer>
    <hr>

    <div class="row course-info">
        <div class="col-md-7">
            <p>
                <strong>SSC 442: Data Analytics (Fall Semester 2021)</strong><br>

                <a href="https://www.msu.edu" target="_blank" rel="noopener">Michigan State University</a> &emsp;&emsp;
                <a href="https://socialscience.msu.edu/" target="_blank" rel="noopener">College of Social Science</a>
            </p>

            <p>
                <a href="https://www.benbushong.com" target="_blank" rel="noopener"><i class="fas fa-user"></i>
                    Prof. Ben Bushong</a> &emsp;&emsp;
                <a href="mailto:bbushong@msu.edu"><i class="fas fa-envelope"></i>
                    bbushong@msu.edu</a>
            </p>

            <p>
                <i class="far fa-calendar-alt"></i> Tuesday and Thursday &emsp;&emsp;
                <i class="far fa-clock"></i> 12:40pm - 2:00pm 
                
            </p>
        </div>

        <div class="col-md-5 credits">
            <p>All content licensed under a <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" rel="noopener">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.</p>
            
            <p>Content <i class="fab fa-creative-commons"></i> 2021 <a href="https://www.benbushong.com" target="_blank" rel="noopener"></a></p>
        
            <p>This site created with the <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> in <a href="https://bookdown.org/yihui/blogdown/" target="_blank" rel="noopener">blogdown</a> and <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a>. </p>
            
            <p><a href="https://github.com/" target="_blank" rel="noopener"><i class="fab fa-github"></i> View the source at GitHub.</a></p>
        </div>
    </div>
</footer>


        </main>
    </div>
</div>

        

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.1/anchor.min.js" integrity="sha256-pB/deHc9CGfFpJRjC43imB29Rse8tak+5eXqntO94ck=" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.6f7ce8be710290b8c431bbc97f405d15.js"></script>

    



    
    

    
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>

</html>
