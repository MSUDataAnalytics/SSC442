[{"authors":["Ben"],"categories":null,"content":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron. Prior to coming to Michigan State University, I worked with the U.S. Army to help soldiers become more psychologically resilient.\nI hold a Ph.D. in Social Science (Economics) from the California Institute of Technology (Caltech), and a B.S. in Economics from the University of Oregon.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1592849563,"objectID":"bf008f22d9b0754cde4f6972811c28b7","permalink":"/authors/ben/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ben/","section":"authors","summary":"I am an Assistant Professor at Michigan State University in the Department of Economics and a faculty affiliate in the Social Science Data Analytics Program. Prior to coming to MSU, I was a Postdoctoral Research Fellow at Harvard University and a Visiting Scholar at Harvard Business School. My research focuses on the intersection of psychology and economics \u0026ndash; also known as behavioral economics \u0026ndash; and has appeared in the American Economic Review and Neuron.","tags":null,"title":"Ben Bushong","type":"authors"},{"authors":null,"categories":null,"content":" In these (modestly) secret pages, I’ve included some resources for those who read the syllabus closely.\nIf you’re stuck with anything or want help with, say, using markdown, you’ll find some basic guidance here. Additionally, there are links throughout to outside resources.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1594409288,"objectID":"8939c748f3090c6f91bdac5d32db55ec","permalink":"/resource/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/","section":"resource","summary":"In these (modestly) secret pages, I’ve included some resources for those who read the syllabus closely.\nIf you’re stuck with anything or want help with, say, using markdown, you’ll find some basic guidance here. Additionally, there are links throughout to outside resources.","tags":null,"title":"Helpful resources","type":"docs"},{"authors":null,"categories":null,"content":" Each week has a set of required readings that you should complete before coming to the (online) Tuesday lecture. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due).\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts. These questions can serve as helpful starting places for your thinking; they are not representative of the totality of the content and are not intended to be limiting. You should not try to respond to all of these (or any of them if you don’t want to)—they’ll just help you know what to look for and think about as you read.\nEvery weekly session also has some collection of YouTube videos (recordings of the lecture) that are associated with each week. Again, these lectures are inherently different from the written content, and you should consider them as high-level overviews of the written content. I am not replicating the written text as-is and (especially if you’re struggling) you should engage with lectures and written materials in equal measure. The lecture slides are HTML files made with the R package xaringan. For each of the weekly pages, you will see buttons for opening the presentation in a new tab.1.\n View all slides in new window\nThe slides are also embedded on each page. You can click in the slides and navigate through them with ← and →. If you type ? (or shift + /) while viewing the slides you can see a list of slide-specific commands (like f for fullscreen or p for presenter mode if you want to see my notes).\n I aspire to include a link for downloading a PDF of the slides in case you want to print them or store them on your computer. However, this seems… ambitious. As of right now, I have not finished this.↩︎\n   ","date":1598918400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1598465283,"objectID":"1413f960ec974b9863bc45d887efa8bd","permalink":"/content/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/content/","section":"content","summary":"Each week has a set of required readings that you should complete before coming to the (online) Tuesday lecture. That is, you should complete the reading, attend Tuesday class, then do the associated “exercises” (contained within the reading) before Thursday. You will be working each week’s lab between Thursday afternoon and Monday at 11:59 PM (when the labs are due).\nThe course content is structured as follows. For each topic, we begin with a set of questions that might guide your reading and help frame your thoughts.","tags":null,"title":"Readings, lectures, and videos","type":"docs"},{"authors":null,"categories":null,"content":"  Weekly Writings Labs Projects Final project   This course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nEngage with the readings and lecture materials Regularly use R (aka engage daily or almost every day in some way)  Each type of assignment in this class helps with one of these strategies.\nWeekly Writings To encourage you to actively engage with the course content, you will write a ≈150 word memorandum about the reading or lecture each week. That’s fairly short: there are ≈250 words on a typical double-spaced page in Microsoft Word (500 when single-spaced). You must complete a total of twelve of these; there are more than twelve weeks in the course, so you have some flexibility.1 Your actual prompt will be assigned in class, so you must login each day to ensure you get these assignments. To keep you on your toes, we will vary whether these are assigned on Tuesdays or Thursdays.\nYou can do a lot of different things with this memo: discuss something you learned from the course content, write about the best or worst data visualization you saw recently, connect the course content to your own work, etc. These reflections let you explore and answer some of the key questions of this course, including:\n When is a link correlational vs causal? How can we still make useful statements about non-causal things? Why do we visualize data? What makes a great data analysis? What makes a bad analysis? How do you choose which kind of analysis method to use? What is the role of the data structure in choosing an analysis? Can we be flexible?  The course content for each day will also include a set of questions specific to that topic. You do not have to answer all (or any) of these questions. That would be impossible. They exist to guide your thinking and to make complex reading more digestible. The specific topic for each week will be assigned in class. (We can’t emphasize this enough.)\nThe TA will grade these mini-exercises using a very simple system:\n ✔+: (11.5 points (115%) in gradebook) Work shows phenomenal thought and engagement with the course content. We will not assign these often. ✔: (10 points (100%) in gradebook) Work is thoughtful, well-written, and shows engagement with the course content. This is the expected level of performance. ✔−: (5 points (50%) in gradebook) Work is hastily composed, too short, and/or only cursorily engages with the course content. This grade signals that you need to improve next time. I will hopefully not assign these often.  (There is an implicit 0 above for work that is not turned in on-time). Notice that this is essentially a pass/fail or completion-based system. We’re not grading your writing ability; we’re not counting the exact number of words you’re writing; and we’re not looking for encyclopedic citations of every single reading to prove that you did indeed read everything. We are looking for thoughtful engagement. Read the material, engage with the work and you’ll get a ✓.\nYou will turn these reflections in via D2L. You will write them using R Markdown and must knit your work to a PDF document (this will be what you turn in).\n Labs Each week of the course has fully annotated examples of code that teach and demonstrate how to do specific tasks in R. However, without practicing these principles and making graphics on your own, you won’t remember what you learn.\n Practice, uh, makes, whatever.\nBen Bushong\n For example, to practice working with ggplot2 and making data-based graphics, you will complete a brief set of exercises over a few class sessions. These exercises will have 1–3 short tasks that are directly related to the topic for the week. You need to show that you made a good faith effort to work each question. There will also be a final question which requires significantly more thought and work. This will be where you get to show some creativity and stretch your abilities. Overall, labs will be graded the same check system:\n ✔+: (17.5 points (115%) in gradebook) Exercises are complete. Every task was attempted and answered, and most answers are correct. Knitted document is clean and easy to follow. Work on the final problem shows creativity or is otherwise exceptional. We will not assign these often. ✔: (15 points (100%) in gradebook) Exercises are complete and most answers are correct. This is the expected level of performance. ✔−: (7.5 points (50%) in gradebook) Exercises are less than 70% complete and/or most answers are incorrect. This indicates that you need to improve next time. We will hopefully not assign these often, but subpar work can expect a ✔−.  Note that this is also essentially a pass/fail system. As noted in the syllabus, we are not grading your coding ability. We are not checking each line of code to make sure it produces some exact final figure, and we do not expect perfection. Also note that a ✓ does not require 100% success. You will sometimes get stuck with weird errors that you can’t solve, or the demands of pandemic living might occasionally become overwhelming. We are looking for good faith effort. Try hard, engage with the task, and you’ll get a ✓.\nYou may work together on the labs, but you must turn in your own answers. You will turn these labs in via D2L. You will write them using R Markdown and must knit your work to a PDF document.\n Projects To give you practice with the data and design principles you’ll learn in this class, you will complete two projects en route to the overarching final project of the course. Both these mini projects and the final project must be completed in groups.\nThe two (mini) projects are checkpoints to ensure you’re working on your project seriously. They will be graded using a check system:\n ✔+: (55 points (≈115%) in gradebook) Project is phenomenally well-designed and uses advanced R techniques. The project uncovers an important story that is not readily apparent from just looking at the raw data. I will not assign these often. ✔: (50 points (100%) in gradebook) Project is fine, follows most design principles, answers a question from the data, and uses R correctly. This is the expected level of performance. ✔−: (25 points (50%) in gradebook) Project is missing large components, is poorly designed, does not answer a relevant question, and/or uses R incorrectly. This indicates that you need to improve next time. I will hopefully not assign these often.  Because these mini projects give you practice for the final project, we will provide you with substantial feedback on your design and code.\n Final project At the end of the course, you will demonstrate your skills by completing a final project. Complete details for the final project (including past examples of excellent projects) are here. In brief, the final project has the following elements:\nYou must find existing data to analyze.2 Aggregating data from multiple sources is encouraged, but is not required.  You must visualize (at least) three interesting features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.3  You must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.4  You must write your analysis as if presenting to a C-suite executive. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.  There is no final exam. This project is your final exam.\nThe project will not be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:\nTechnical skills: Was the project easy? Does it showcase mastery of data analysis? Visual design: Was the information smartly conveyed and usable? Was it beautiful? Analytic design: Was the analysis appropriate? Was it sensible, given the dataset? Story: Did we learn something?  If you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.\n  Note that sometimes the writing takes the form of a very short coding assignment.↩︎\n Note that existing is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged.↩︎\n Pie charts of any kind will result in a 25% grade deduction.↩︎\n This is an extremely dumb idea for a number of reasons. Moreover, it’s worth mentioning that sports data, while rich, can be overwhelming due to its sheer magnitude and the variety of approaches that can be applied. Use with caution.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1600175601,"objectID":"3aa23ffb1eb3dedbe4d8a9c2165e2c58","permalink":"/assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/","section":"assignment","summary":"Weekly Writings Labs Projects Final project   This course is the capstone of the Data Analytics Minor in the College of Social Science. Accordingly, you should—fingers crossed—enjoy data analysis. You will get the most of out this class if you:\nEngage with the readings and lecture materials Regularly use R (aka engage daily or almost every day in some way)  Each type of assignment in this class helps with one of these strategies.","tags":null,"title":"Assignments and Evaluations","type":"docs"},{"authors":null,"categories":null,"content":" This section contains the content covered in Thursday lectures and some annotated R code that you can use as a reference for creating your own work. The intention is that in the Content section, you will sequentially build up your understanding of R and data analytics; here, you can see how all the pieces work together.\nVisit this section after you have finished the readings in the Content section and any supplemental lecture videos.\nMany of the examples also contain videos of me live-coding so you can see what it looks like to work with R in real time.1 Hopefully, you’ll find it useful to watch the practice of coding. You’ll also notice me make all sorts of errors. This is normal. If you’re finding yourself making lots of errors and generally struggling to get your code to run, start by working with the final code and reverse-engineer the solution you want.\n I might edit these videos for length and because I like to talk to myself when I’m programming. Also, too much swearing.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1598465283,"objectID":"00e8826988eea7dfc8b8047b4c0184ce","permalink":"/example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/","section":"example","summary":"This section contains the content covered in Thursday lectures and some annotated R code that you can use as a reference for creating your own work. The intention is that in the Content section, you will sequentially build up your understanding of R and data analytics; here, you can see how all the pieces work together.\nVisit this section after you have finished the readings in the Content section and any supplemental lecture videos.","tags":null,"title":"Practical Content","type":"docs"},{"authors":null,"categories":null,"content":"  Install R Install RStudio Install tidyverse Install tinytex   As mentioned in the syllabus, you will do all of your work in this class with the open source programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—–R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.\nHopefully you’re well-versed in dealing with these things, but if you’re lost, here’s how you install the required software for the course.\nInstall R First you need to install R itself (the engine).\nGo to the CRAN (Collective R Archive Network)1 website: https://cran.r-project.org/\n Click on “Download R for XXX”, where XXX is either Mac or Windows:\n If you use macOS, scroll down to the first .pkg file in the list of files (in this picture, it’s R-4.0.0.pkg; as of right now, the current version is also 4.0.0) and download it.\n If you use Windows, click “base” (or click on the bolded “install R for the first time” link) and download it.\n  Double click on the downloaded file (check your Downloads folder). Click yes through all the prompts to install like any other program.\n If you use macOS, download and install XQuartz. You do not need to do this on Windows.\n   Install RStudio Next, you need to install RStudio, the nicer graphical user interface (GUI) for R (the dashboard). Once R and RStudio are both installed, you can ignore R and only use RStudio. RStudio will use R automatically and you won’t ever have to interact with it directly.\nGo to the free download location on RStudio’s website: https://www.rstudio.com/products/rstudio/download/#download\n The website should automatically detect your operating system (macOS or Windows) and show a big download button for it:\nIf not, scroll down a little to the large table and choose the version of RStudio that matches your operating system.\n Double click on the downloaded file (again, check your Downloads folder). Click yes through all the prompts to install like any other program.\n  Double click on RStudio to run it (check your applications folder or start menu).\n Install tidyverse R packages are easy to install with RStudio. Select the packages panel, click on “Install,” type the name of the package you want to install, and press enter.\nThis can sometimes be tedious when you’re installing lots of packages, though. The tidyverse, for instance, consists of dozens of packages (including the ever-present ggplot2) that all work together. Rather than install each individually, you can install a single magical package and get them all at the same time.\nGo to the packages panel in RStudio, click on “Install,” type “tidyverse”, and press enter. You’ll see a bunch of output in the RStudio console as all the tidyverse packages are installed.\nNotice also that RStudio will generate a line of code for you and run it: install.packages(\"tidyverse\"). You can also just paste and run this instead of using the packages panel. Hopefully you’ve experienced installing packages before now; if not, consider this a crash course!\n Install tinytex When you knit to PDF, R uses a special scientific typesetting program named LaTeX.2\nLaTeX is neat and makes pretty documents, but it’s a huge program—the macOS version, for instance, is nearly 4 GB. To make life easier, there’s an R package named tinytex that installs a minimal LaTeX program and that automatically deals with differences between macOS and Windows.\nHere’s how to install tinytex so you can knit to pretty PDFs:\nUse the Packages in panel in RStudio to install tinytex like you did above with tidyverse. Alternatively, run install.packages(\"tinytex\") in the console. Run tinytex::install_tinytex() in the console. Wait for a bit while R downloads and installs everything you need. The end! You should now be able to knit to PDF.    It’s a goofy name, but CRAN is where most R packages—and R itself—lives.↩︎\n Pronounced “lay-tek” for those who are correct; or “lah-tex” to those who love goofy nerdy pronunciation. Technically speaking, the x is the “ch” sound in “Bach”, but most people just say it as “k”. While either saying “lay” or “lah” is correct, “layteks” is frowned upon because it clearly shows you’re not cool.↩︎\n   ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594409288,"objectID":"efb59c0882a965443ffcbafa3cd27ca6","permalink":"/resource/install/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/resource/install/","section":"resource","summary":"Install R Install RStudio Install tidyverse Install tinytex   As mentioned in the syllabus, you will do all of your work in this class with the open source programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—–R handles all the calculations and the actual statistics, while RStudio provides a nice interface for running R code.","tags":null,"title":"Installing R, RStudio, tidyverse, and tinytex","type":"docs"},{"authors":null,"categories":null,"content":"  Programming basics Conditional expressions Defining functions Namespaces For-loops Vectorization and functionals    NOTE:\nThis lab is not due for credit. Nevertheless, if you do not have a firm grasp of R, you should work through both the entirety of this section and the exercises at the end. There are a number of tricks that will come up over and over.\n Programming basics We teach R because it greatly facilitates data analysis, the main topic of this book. By coding in R, we can efficiently perform exploratory data analysis, build data analysis pipelines, and prepare data visualization to communicate results. However, R is not just a data analysis environment but a programming language. Advanced R programmers can develop complex packages and even improve R itself, but we do not cover advanced programming in this book. Nonetheless, in this section, we introduce three key programming concepts: conditional expressions, for-loops, and functions. These are not just key building blocks for advanced programming, but are sometimes useful during data analysis. We also note that there are several functions that are widely used to program in R but that we will not cover in this book. These include split, cut, do.call, and Reduce, as well as the data.table package. These are worth learning if you plan to become an expert R programmer.\nConditional expressions Conditional expressions are one of the basic features of programming. They are used for what is called flow control. The most common conditional expression is the if-else statement. In R, we can actually perform quite a bit of data analysis without conditionals. However, they do come up occasionally, and you will need them once you start writing your own functions and packages.\nHere is a very simple example showing the general structure of an if-else statement. The basic idea is to print the reciprocal of a unless a is 0:\na \u0026lt;- 0 if(a!=0){ print(1/a) } else{ print(\u0026quot;No reciprocal for 0.\u0026quot;) } ## [1] \u0026quot;No reciprocal for 0.\u0026quot; Let’s look at one more example using the US murders data frame:\nlibrary(dslabs) data(murders) murder_rate \u0026lt;- murders$total / murders$population*100000 Here is a very simple example that tells us which states, if any, have a murder rate lower than 0.5 per 100,000. The if statement protects us from the case in which no state satisfies the condition.\nind \u0026lt;- which.min(murder_rate) if(murder_rate[ind] \u0026lt; 0.5){ print(murders$state[ind]) } else{ print(\u0026quot;No state has murder rate that low\u0026quot;) } ## [1] \u0026quot;Vermont\u0026quot; If we try it again with a rate of 0.25, we get a different answer:\nif(murder_rate[ind] \u0026lt; 0.25){ print(murders$state[ind]) } else{ print(\u0026quot;No state has a murder rate that low.\u0026quot;) } ## [1] \u0026quot;No state has a murder rate that low.\u0026quot; A related function that is very useful is ifelse. This function takes three arguments: a logical and two possible answers. If the logical is TRUE, the value in the second argument is returned and if FALSE, the value in the third argument is returned. Here is an example:\na \u0026lt;- 0 ifelse(a \u0026gt; 0, 1/a, NA) ## [1] NA The function is particularly useful because it works on vectors. It examines each entry of the logical vector and returns elements from the vector provided in the second argument, if the entry is TRUE, or elements from the vector provided in the third argument, if the entry is FALSE.\na \u0026lt;- c(0, 1, 2, -4, 5) result \u0026lt;- ifelse(a \u0026gt; 0, 1/a, NA) This table helps us see what happened:    a  is_a_positive  answer1  answer2  result      0  FALSE  Inf  NA  NA    1  TRUE  1.00  NA  1.0    2  TRUE  0.50  NA  0.5    -4  FALSE  -0.25  NA  NA    5  TRUE  0.20  NA  0.2     Here is an example of how this function can be readily used to replace all the missing values in a vector with zeros:\ndata(na_example) no_nas \u0026lt;- ifelse(is.na(na_example), 0, na_example) sum(is.na(no_nas)) ## [1] 0 Two other useful functions are any and all. The any function takes a vector of logicals and returns TRUE if any of the entries is TRUE. The all function takes a vector of logicals and returns TRUE if all of the entries are TRUE. Here is an example:\nz \u0026lt;- c(TRUE, TRUE, FALSE) any(z) ## [1] TRUE all(z) ## [1] FALSE  Defining functions As you become more experienced, you will find yourself needing to perform the same operations over and over. A simple example is computing averages. We can compute the average of a vector x using the sum and length functions: sum(x)/length(x). Because we do this repeatedly, it is much more efficient to write a function that performs this operation. This particular operation is so common that someone already wrote the mean function and it is included in base R. However, you will encounter situations in which the function does not already exist, so R permits you to write your own. A simple version of a function that computes the average can be defined like this:\navg \u0026lt;- function(x){ s \u0026lt;- sum(x) n \u0026lt;- length(x) s/n } Now avg is a function that computes the mean:\nx \u0026lt;- 1:100 identical(mean(x), avg(x)) ## [1] TRUE Notice that variables defined inside a function are not saved in the workspace. So while we use s and n when we call avg, the values are created and changed only during the call. Here is an illustrative example:\ns \u0026lt;- 3 avg(1:10) ## [1] 5.5 s ## [1] 3 Note how s is still 3 after we call avg.\nIn general, functions are objects, so we assign them to variable names with \u0026lt;-. The function function tells R you are about to define a function. The general form of a function definition looks like this:\nmy_function \u0026lt;- function(VARIABLE_NAME){ perform operations on VARIABLE_NAME and calculate VALUE VALUE } The functions you define can have multiple arguments as well as default values. For example, we can define a function that computes either the arithmetic or geometric average depending on a user defined variable like this:\navg \u0026lt;- function(x, arithmetic = TRUE){ n \u0026lt;- length(x) ifelse(arithmetic, sum(x)/n, prod(x)^(1/n)) } We will learn more about how to create functions through experience as we face more complex tasks.\n Namespaces Once you start becoming more of an R expert user, you will likely need to load several add-on packages for some of your analysis. Once you start doing this, it is likely that two packages use the same name for two different functions. And often these functions do completely different things. In fact, you have already encountered this because both dplyr and the R-base stats package define a filter function. There are five other examples in dplyr. We know this because when we first load dplyr we see the following message:\nThe following objects are masked from ‘package:stats’: filter, lag The following objects are masked from ‘package:base’: intersect, setdiff, setequal, union So what does R do when we type filter? Does it use the dplyr function or the stats function? From our previous work we know it uses the dplyr one. But what if we want to use the stats version?\nThese functions live in different namespaces. R will follow a certain order when searching for a function in these namespaces. You can see the order by typing:\nsearch() The first entry in this list is the global environment which includes all the objects you define.\nSo what if we want to use the stats filter instead of the dplyr filter but dplyr appears first in the search list? You can force the use of a specific namespace by using double colons (::) like this:\nstats::filter If we want to be absolutely sure that we use the dplyr filter, we can use\ndplyr::filter Also note that if we want to use a function in a package without loading the entire package, we can use the double colon as well.\nFor more on this more advanced topic we recommend the R packages book1.\n For-loops If we had to write this section in a single sentence, it would be: Don’t use for-loops. Looping is intuitive, but R is designed to provide more computationally efficient solutions. For-loops should be considered a quick-and-dirty way to get an answer. But, hey, you live your own life. Below we provide a brief overview to for-looping.\nThe formula for the sum of the series \\(1+2+\\dots+n\\) is \\(n(n+1)/2\\). What if we weren’t sure that was the right function? How could we check? Using what we learned about functions we can create one that computes the \\(S_n\\):\ncompute_s_n \u0026lt;- function(n){ x \u0026lt;- 1:n sum(x) } How can we compute \\(S_n\\) for various values of \\(n\\), say \\(n=1,\\dots,25\\)? Do we write 25 lines of code calling compute_s_n? No, that is what for-loops are for in programming. In this case, we are performing exactly the same task over and over, and the only thing that is changing is the value of \\(n\\). For-loops let us define the range that our variable takes (in our example \\(n=1,\\dots,10\\)), then change the value and evaluate expression as you loop.\nPerhaps the simplest example of a for-loop is this useless piece of code:\nfor(i in 1:5){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 Here is the for-loop we would write for our \\(S_n\\) example:\nm \u0026lt;- 25 s_n \u0026lt;- vector(length = m) # create an empty vector for(n in 1:m){ s_n[n] \u0026lt;- compute_s_n(n) } In each iteration \\(n=1\\), \\(n=2\\), etc…, we compute \\(S_n\\) and store it in the \\(n\\)th entry of s_n.\nNow we can create a plot to search for a pattern:\nn \u0026lt;- 1:m plot(n, s_n) If you noticed that it appears to be a quadratic, you are on the right track because the formula is \\(n(n+1)/2\\). --\n Vectorization and functionals Although for-loops are an important concept to understand, in R we rarely use them. As you learn more R, you will realize that vectorization is preferred over for-loops since it results in shorter and clearer code. (It’s also vastly more efficient computationally, which can matter as your data grows.) A vectorized function is a function that will apply the same operation on each of the vectors.\nx \u0026lt;- 1:10 sqrt(x) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 y \u0026lt;- 1:10 x*y ## [1] 1 4 9 16 25 36 49 64 81 100 To make this calculation, there is no need for for-loops. However, not all functions work this way. For instance, the function we just wrote, compute_s_n, does not work element-wise since it is expecting a scalar. This piece of code does not run the function on each entry of n:\nn \u0026lt;- 1:25 compute_s_n(n) Functionals are functions that help us apply the same function to each entry in a vector, matrix, data frame, or list. Here we cover the functional that operates on numeric, logical, and character vectors: sapply.\nThe function sapply permits us to perform element-wise operations on any function. Here is how it works:\nx \u0026lt;- 1:10 sapply(x, sqrt) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 Each element of x is passed on to the function sqrt and the result is returned. These results are concatenated. In this case, the result is a vector of the same length as the original x. This implies that the for-loop above can be written as follows:\nn \u0026lt;- 1:25 s_n \u0026lt;- sapply(n, compute_s_n) Other functionals are apply, lapply, tapply, mapply, vapply, and replicate. We mostly use sapply, apply, and replicate in this book, but we recommend familiarizing yourselves with the others as they can be very useful.\nEXERCISES\nWhat will this conditional expression return?  x \u0026lt;- c(1,2,-3,4) if(all(x\u0026gt;0)){ print(\u0026quot;All Postives\u0026quot;) } else{ print(\u0026quot;Not all positives\u0026quot;) } Which of the following expressions is always FALSE when at least one entry of a logical vector x is TRUE?  all(x) any(x) any(!x) all(!x)  The function nchar tells you how many characters long a character vector is. Write a line of code that assigns to the object new_names the state abbreviation when the state name is longer than 8 characters.\n Create a function sum_n that for any given value, say \\(n\\), computes the sum of the integers from 1 to n (inclusive). Use the function to determine the sum of integers from 1 to 5,000.\n Create a function altman_plot that takes two arguments, x and y, and plots the difference against the sum.\n After running the code below, what is the value of x?\n  x \u0026lt;- 3 my_func \u0026lt;- function(y){ x \u0026lt;- 5 y+5 } Write a function compute_s_n that for any given \\(n\\) computes the sum \\(S_n = 1^2 + 2^2 + 3^2 + \\dots n^2\\). Report the value of the sum when \\(n=10\\).\n Define an empty numerical vector s_n of size 25 using s_n \u0026lt;- vector(\"numeric\", 25) and store in the results of \\(S_1, S_2, \\dots S_{25}\\) using a for-loop.\n Repeat exercise 8, but this time use sapply.\n Repeat exercise 8, but this time use map_dbl.\n Plot \\(S_n\\) versus \\(n\\). Use points defined by \\(n=1,\\dots,25\\).\n Confirm that the formula for this sum is \\(S_n= n(n+1)(2n+1)/6\\).\n      http://r-pkgs.had.co.nz/namespace.html↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599753513,"objectID":"5e86e029830987df59b0fed9d67636a4","permalink":"/assignment/00-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/00-assignment/","section":"assignment","summary":"Programming basics Conditional expressions Defining functions Namespaces For-loops Vectorization and functionals    NOTE:\nThis lab is not due for credit. Nevertheless, if you do not have a firm grasp of R, you should work through both the entirety of this section and the exercises at the end. There are a number of tricks that will come up over and over.\n Programming basics We teach R because it greatly facilitates data analysis, the main topic of this book.","tags":null,"title":"Programming Basics in R","type":"docs"},{"authors":null,"categories":null,"content":"  Introduction to Examples Getting started with R and RStudio The R console Scripts RStudio The panes Key bindings Running commands while editing scripts  Installing R packages    Introduction to Examples Examples in this class are designed to be presented in-class. Accordingly, the notes here are not comprehensive. Instead, they are intended to guide students through\nI’m also aware that my writing is dry and lifeless. If you’re reading this online without the advantage of seeing it in person, don’t worry—I’ll be “funnier” in class.1\nGetting started with R and RStudio R is not a programming language like C or Java. It was not created by software engineers for software development. Instead, it was developed by statisticians as an interactive environment for data analysis. You can read the full history in the paper A Brief History of S2. The interactivity is an indispensable feature in data science because, as you will soon learn, the ability to quickly explore data is a necessity for success in this field. However, like in other programming languages, you can save your work as scripts that can be easily executed at any moment. These scripts serve as a record of the analysis you performed, a key feature that facilitates reproducible work. If you are an expert programmer, you should not expect R to follow the conventions you are used—assuming this will leave you disappointed. If you are patient, you will come to appreciate the unequal power of R when it comes to data analysis and data visualization.\nOther attractive features of R are:\nR is free and open source3. It runs on all major platforms: Windows, Mac OS, UNIX/Linux. Scripts and data objects can be shared seamlessly across platforms. There is a large, growing, and active community of R users and, as a result, there are numerous resources for learning and asking questions4 5 6. It is easy for others to contribute add-ons which enables developers to share software implementations of new data science methodologies. The latest methods and tools are developed in R for a wide variety of disciplines and since social science is so broad, R is one of the few tools that spans the varied social sciences.   The R console Interactive data analysis usually occurs on the R console that executes commands as you type them. There are several ways to gain access to an R console. One way is to simply start R on your computer. The console looks something like this:\nAs a quick example, try using the console to calculate a 15% tip on a meal that cost $19.71:7\n0.15 * 19.71  ## [1] 2.9565 Note that in this course (at least, on most browsers), grey boxes are used to show R code typed into the R console. The symbol ## is used to denote what the R console outputs.\n Scripts One of the great advantages of R over point-and-click analysis software is that you can save your work as scripts. You can edit and save these scripts using a text editor. The material in this course was developed using the interactive integrated development environment (IDE) RStudio8. RStudio includes an editor with many R specific features, a console to execute your code, and other useful panes, including one to show figures.\nMost web-based R consoles also provide a pane to edit scripts, but not all permit you to save the scripts for later use. On the upper-right part of this webpage you’ll see a little button with the R logo. You can access a web-based console there.\n RStudio RStudio will be our launching pad for data science projects. It not only provides an editor for us to create and edit our scripts but also provides many other useful tools. In this section, we go over some of the basics.\nThe panes When you start RStudio for the first time, you will see three panes. The left pane shows the R console. On the right, the top pane includes tabs such as Environment and History, while the bottom pane shows five tabs: File, Plots, Packages, Help, and Viewer (these tabs may change in new versions). You can click on each tab to move across the different features.\nTo start a new script, you can click on File, then New File, then R Script.\nThis starts a new pane on the left and it is here where you can start writing your script.\n Key bindings Many tasks we perform with the mouse can be achieved with a combination of key strokes instead. These keyboard versions for performing tasks are referred to as key bindings. For example, we just showed how to use the mouse to start a new script, but you can also use a key binding: Ctrl+Shift+N on Windows and command+shift+N on the Mac.\nAlthough in this tutorial we often show how to use the mouse, we highly recommend that you memorize key bindings for the operations you use most. RStudio provides a useful cheat sheet with the most widely used commands. You might want to keep this handy so you can look up key-bindings when you find yourself performing repetitive point-and-clicking.\n Running commands while editing scripts There are many editors specifically made for coding. These are useful because color and indentation are automatically added to make code more readable. RStudio is one of these editors, and it was specifically developed for R. One of the main advantages provided by RStudio over other editors is that we can test our code easily as we edit our scripts. Below we show an example.\nLet’s start by opening a new script as we did before. A next step is to give the script a name. We can do this through the editor by saving the current new unnamed script. To do this, click on the save icon or use the key binding Ctrl+S on Windows and command+S on the Mac.\nWhen you ask for the document to be saved for the first time, RStudio will prompt you for a name. A good convention is to use a descriptive name, with lower case letters, no spaces, only hyphens to separate words, and then followed by the suffix .R. We will call this script my-first-script.R.\nNow we are ready to start editing our first script. The first lines of code in an R script are dedicated to loading the libraries we will use. Another useful RStudio feature is that once we type library() it starts auto-completing with libraries that we have installed. Note what happens when we type library(ti):\nAnother feature you may have noticed is that when you type library( the second parenthesis is automatically added. This will help you avoid one of the most common errors in coding: forgetting to close a parenthesis.\nNow we can continue to write code. As an example, we will make a graph showing murder totals versus population totals by state. Once you are done writing the code needed to make this plot, you can try it out by executing the code. To do this, click on the Run button on the upper right side of the editing pane. You can also use the key binding: Ctrl+Shift+Enter on Windows or command+shift+return on the Mac.\nOnce you run the code, you will see it appear in the R console and, in this case, the generated plot appears in the plots console. Note that the plot console has a useful interface that permits you to click back and forward across different plots, zoom in to the plot, or save the plots as files.\nTo run one line at a time instead of the entire script, you can use Control-Enter on Windows and command-return on the Mac.\nSETUP TIP\nChange the option Save workspace to .RData on exit to Never and uncheck the Restore .RData into workspace at start. By default, when you exit R saves all the objects you have created into a file called .RData. This is done so that when you restart the session in the same folder, it will load these objects. I find that this causes confusion especially when sharing code with colleagues or peers.\n   Installing R packages The functionality provided by a fresh install of R is only a small fraction of what is possible. In fact, we refer to what you get after your first install as base R. The extra functionality comes from add-ons available from developers. There are currently hundreds of these available from CRAN and many others shared via other repositories such as GitHub. However, because not everybody needs all available functionality, R instead makes different components available via packages. R makes it very easy to install packages from within R. For example, to install the dslabs package, which we use to share datasets and code related to this book, you would type:\ninstall.packages(\u0026quot;dslabs\u0026quot;) In RStudio, you can navigate to the Tools tab and select install packages. We can then load the package into our R sessions using the library function:\nlibrary(dslabs) As you go through this book, you will see that we load packages without installing them. This is because once you install a package, it remains installed and only needs to be loaded with library. The package remains loaded until we quit the R session. If you try to load a package and get an error, it probably means you need to install it first.\nWe can install more than one package at once by feeding a character vector to this function:\ninstall.packages(c(\u0026quot;tidyverse\u0026quot;, \u0026quot;dslabs\u0026quot;)) One advantage of using RStudio is that it auto-completes package names once you start typing, which is helpful when you do not remember the exact spelling of the package. Once you select your package, we recommend selecting all the defaults. Note that installing tidyverse actually installs several packages. This commonly occurs when a package has dependencies, or uses functions from other packages. When you load a package using library, you also load its dependencies.\nOnce packages are installed, you can load them into R and you do not need to install them again, unless you install a fresh version of R. Remember packages are installed in R not RStudio.\nIt is helpful to keep a list of all the packages you need for your work in a script because if you need to perform a fresh install of R, you can re-install all your packages by simply running a script.\nYou can see all the packages you have installed using the following function:\ninstalled.packages() As we move through this course, we will constantly be adding to our toolbox of packages. Accordingly, you will need to keep track to ensure you have the requisite package for any given lecture.\n   Comments from previous classes indicate that I am not, in fact, funny.↩︎\n https://pdfs.semanticscholar.org/9b48/46f192aa37ca122cfabb1ed1b59866d8bfda.pdf↩︎\n https://opensource.org/history↩︎\n https://stats.stackexchange.com/questions/138/free-resources-for-learning-r↩︎\n https://www.r-project.org/help.html↩︎\n https://stackoverflow.com/documentation/r/topics↩︎\n But probably tip more than 15%. Times are tough, man.↩︎\n https://www.rstudio.com/↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594409288,"objectID":"bbf45ee74dc37731d7fd26186d3a77a6","permalink":"/example/00-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/00-example/","section":"example","summary":"Introduction to Examples Getting started with R and RStudio The R console Scripts RStudio The panes Key bindings Running commands while editing scripts  Installing R packages    Introduction to Examples Examples in this class are designed to be presented in-class. Accordingly, the notes here are not comprehensive. Instead, they are intended to guide students through\nI’m also aware that my writing is dry and lifeless. If you’re reading this online without the advantage of seeing it in person, don’t worry—I’ll be “funnier” in class.","tags":null,"title":"Working with R and RStudio","type":"docs"},{"authors":null,"categories":null,"content":"  Using ggplot2 How to use ggplot2 – the too-fast and wholly unclear recipe  Mappings Link Data to Things You See The Recipe Mapping Aesthetics vs Setting them    NOTE\nYou must turn in a PDF document of your R markdown code. Submit this to D2L by 11:59 PM on Monday.\n Our primary tool for data visualization in the course will be ggplot. Technically, we’re using ggplot2; the o.g. version lacked some of the modern features of its big brother. ggplot2 implements the grammar of graphics, a coherent and relatively straightforward system for describing and building graphs. With ggplot2, you can do more faster by learning one system and applying it in many places. Other languages provide more specific tools, but require you to learn a different tool for each application. In this class, we’ll dig into a single package for our visuals.\nUsing ggplot2 In order to get our hands dirty, we will first have to load ggplot2. To do this, and to access the datasets, help pages, and functions that we will use in this assignment, we will load the so-called tidyverse by running this code:\nlibrary(tidyverse) If you run this code and get an error message “there is no package called ‘tidyverse’”, you’ll need to first install it, then run library() once again. To install packages in R, we utilize the simple function install.packages(). In this case, we would write:\ninstall.packages(\u0026quot;tidyverse\u0026quot;) library(tidyverse) Once we’re up and running, we’re ready to dive into some basic exercises. ggplot2 works by specifying the connections between the variables in the data and the colors, points, and shapes you see on the screen. These logical connections are called aesthetic mappings or simply aesthetics.\nHow to use ggplot2 – the too-fast and wholly unclear recipe  data =: Define what your data is. For instance, below we’ll use the mpg data frame found in ggplot2 (by using ggplot2::mpg). As a reminder, a data frame is a rectangular collection of variables (in the columns) and observations (in the rows). This structure of data is often called a “table” but we’ll try to use terms slightly more precisely. The mpg data frame contains observations collected by the US Environmental Protection Agency on 38 different models of car.\n mapping = aes(...): How to map the variables in the data to aesthetics\n Axes, size of points, intensities of colors, which colors, shape of points, lines/points  Then say what type of plot you want:\n boxplot, scatterplot, histogram, … these are called ‘geoms’ in ggplot’s grammar, such as geom_point() giving scatter plots   library(ggplot2) ... + geom_point() # Produces scatterplots ... + geom_bar() # Bar plots .... + geom_boxplot() # boxplots ... # You link these steps by literally adding them together with + as we’ll see.\nTry it: What other types of plots are there? Try to find several more geom_ functions.\n  Mappings Link Data to Things You See library(gapminder) library(ggplot2) gapminder ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point() Above we’ve loaded a different dataset and have started to explore a particular relationship. Before putting in this code yourself, try to intuit what might be going on.\nAny ideas?\nHere’s a breakdown of everything that happens after the p\u0026lt;- ggplot() call:\n data = gapminder tells ggplot to use gapminder dataset, so if variable names are mentioned, they should be looked up in gapminder mapping = aes(...) shows that the mapping is a function call. There is a deeper logic to this that I will disucss below, but it’s easiest to simply accept that this is how you write it. Put another way, the mapping = aes(...) argument links variables to things you will see on the plot. aes(x = gdpPercap, y = lifeExp) maps the GDP data onto x, which is a known aesthetic (the x-coordinate) and life expectancy data onto x  x and y are predefined names that are used by ggplot and friends   Exercise 1:\nLet’s return to the mpg data. Among the variables in mpg are:\n displ, a car’s engine size, in litres.\n hwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance.\n  Generate a scatterplot between these two variables. Does it capture the intuitive relationship you expected? What happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n It turns out there’s a reason for doing all of this:\n “The greatest value of a picture is when it forces us to notice what we never expected to see.”\" — John Tukey\n In the plot you made above, one group of points seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars?\nLet’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular).\nYou can add a third variable, like class, to a two dimensional scatterplot by mapping it to an aesthetic. An aesthetic is a visual property of the objects in your plot. Aesthetics include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its aesthetic properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe aesthetic properties. Thus, we are interested in exploring class as a level.\nYou can convey information about your data by mapping the aesthetics in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. To map an aesthetic to a variable, associate the name of the aesthetic to the name of the variable inside aes(). ggplot2 will automatically assign a unique level of the aesthetic (here a unique color) to each unique value of the variable, a process known as scaling. ggplot2 will also add a legend that explains which levels correspond to which values.\nExercise 2:\nUsing your previous scatterplot of displ and hwy, map the colors of your points to the class variable to reveal the class of each car. What conclusions can we make?\n Let’s explore our previously saved p in greater detail. As with Exercise 1, we’ll add a layer. This says how some data gets turned into concrete visual aspects.\np + geom_point() p + geom_smooth() Note: Both of the above geom’s use the same mapping, where the x-axis represents gdpPercap and the y-axis represents lifeExp. You can find this yourself with some ease. But the first one maps the data to individual points, the other one maps it to a smooth line with error ranges.\nWe get a message that tells us that geom_smooth() is using the method = ‘gam’, so presumably we can use other methods. Let’s see if we can figure out which other methods there are.\n?geom_smooth p + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ...) p + geom_point() + geom_smooth() + geom_smooth(method = ...) + geom_smooth(method = ..., color = \u0026quot;red\u0026quot;) You may start to see why ggplot2’s way of breaking up tasks is quite powerful: the geometric objects can all reuse the same mapping of data to aesthetics, yet the results are quite different. And if we want later geoms to use different mappings, then we can override them – but it isn’t necessary.\nConsider the output we’ve explored thus far. One potential issue lurking in the data is that most of it is bunched to the left. If we instead used a logarithmic scale, we should be able to spread the data out better.\np + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) + scale_x_log10() Try it: Describe what the scale_x_log10() does. Why is it a more evenly distributed cloud of points now? (2-3 sentences.)\nNice. We’re starting to get somewhere. But, you might notice that the x-axis now has scientific notation. Let’s change that.\nlibrary(scales) p + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) + scale_x_log10(labels = scales::dollar) p + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) + scale_x_log10(labels = scales::...) Try it: What does the dollar() call do? How can you find other ways of relabeling the scales when using scale_x_log10()?\n?dollar()  The Recipe Tell the ggplot() function what our data is. Tell ggplot() what relationships we want to see. For convenience we will put the results of the first two steps in an object called p. Tell ggplot how we want to see the relationships in our data. Layer on geoms as needed, by adding them on the p object one at a time. Use some additional functions to adjust scales, labels, tickmarks, titles.   e.g. scale_, labs(), and guides() functions  As you start to run more R code, you’re likely to run into problems. Don’t worry — it happens to everyone. I have been writing code in numerous languages for years, and every day I still write code that doesn’t work. Sadly, R is particularly persnickity, and its error messages are often opaque.\nStart by carefully comparing the code that you’re running to the code in these notes. R is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every \" is paired with another \". Sometimes you’ll run the code and nothing happens. Check the left-hand of your console: if it’s a +, it means that R doesn’t think you’ve typed a complete expression and it’s waiting for you to finish it. In this case, it’s usually easy to start from scratch again by pressing ESCAPE to abort processing the current command.\nOne common problem when creating ggplot2 graphics is to put the + in the wrong place: it has to come at the end of the line, not the start.\nMapping Aesthetics vs Setting them p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = \u0026#39;yellow\u0026#39;)) p + geom_point() + scale_x_log10() This is interesting (or annoying): the points are not yellow. How can we tell ggplot to draw yellow points?\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, ...)) p + geom_point(...) + scale_x_log10() Try it: describe in your words what is going on. One way to avoid such mistakes is to read arguments inside aes(\u0026lt;property\u0026gt; = \u0026lt;variable\u0026gt;)as the property  in the graph is determined by the data in .\nTry it: Write the above sentence for the original call aes(x = gdpPercap, y = lifeExp, color = 'yellow').\nAesthetics convey information about a variable in the dataset, whereas setting the color of all points to yellow conveys no information about the dataset - it changes the appearance of the plot in a way that is independent of the underlying data.\nRemember: color = 'yellow' and aes(color = 'yellow') are very different, and the second makes usually no sense, as 'yellow' is treated as data.\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point() + geom_smooth(color = \u0026quot;orange\u0026quot;, se = FALSE, size = 8, method = \u0026quot;lm\u0026quot;) + scale_x_log10() Try it: Write down what all those arguments in geom_smooth(...) do.\np + geom_point(alpha = 0.3) + geom_smooth(method = \u0026quot;gam\u0026quot;) + scale_x_log10(labels = scales::dollar) + labs(x = \u0026quot;GDP Per Capita\u0026quot;, y = \u0026quot;Life Expectancy in Years\u0026quot;, title = \u0026quot;Economic Growth and Life Expectancy\u0026quot;, subtitle = \u0026quot;Data Points are country-years\u0026quot;, caption = \u0026quot;Source: Gapminder\u0026quot;) Coloring by continent:\nlibrary(scales) p \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent, fill = continent)) p + geom_point() p + geom_point() + scale_x_log10(labels = dollar) p + geom_point() + scale_x_log10(labels = dollar) + geom_smooth() Try it: What does fill = continent do? What do you think about the match of colors between lines and error bands?\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point(mapping = aes(color = continent)) + geom_smooth() + scale_x_log10() Try it: Notice how the above code leads to a single smooth line, not one per continent. Why?\nTry it: What is bad about the following example, assuming the graph is the one we want? Think about why you should set aesthetics at the top level rather than at the individual geometry level if that’s your intent.\np \u0026lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) p + geom_point(mapping = aes(color = continent)) + geom_smooth(mapping = aes(color = continent, fill = continent)) + scale_x_log10() + geom_smooth(mapping = aes(color = continent), method = \u0026quot;gam\u0026quot;) Exercise 3:\nGenerate two new plots with data = gapminder (note: you’ll need to install the package by the same name if you have not already). Label the axes and the header with clear, easy to understand language. In a few sentences, describe what you’ve visualized and why.\nNote that this is your first foray into ggplot2; accordingly, you should ry to make sure that you do not bite off more than you can chew. We will improve and refine our abilities as we progress through the semester.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600098198,"objectID":"94cae82c16c517ab19420570e5d8c2ad","permalink":"/assignment/01-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/01-assignment/","section":"assignment","summary":"Using ggplot2 How to use ggplot2 – the too-fast and wholly unclear recipe  Mappings Link Data to Things You See The Recipe Mapping Aesthetics vs Setting them    NOTE\nYou must turn in a PDF document of your R markdown code. Submit this to D2L by 11:59 PM on Monday.\n Our primary tool for data visualization in the course will be ggplot. Technically, we’re using ggplot2; the o.","tags":null,"title":"Basics of ggplot","type":"docs"},{"authors":null,"categories":null,"content":"  Introduction to data visualization Code Video    Introduction to data visualization Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:\nlibrary(dslabs) data(murders) head(murders) ## state abb region population total ## 1 Alabama AL South 4779736 135 ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232 ## 4 Arkansas AR South 2915918 93 ## 5 California CA West 37253956 1257 ## 6 Colorado CO West 5029196 65 What do you learn from staring at this table? Even though it is a relatively straightforward table, we can’t learn anything. For starters, it is grossly abbreviated, though you could scroll through. In doing so, how quickly might you be able to determine which states have the largest populations? Which states have the smallest? How populous is a typical state? Is there a relationship between population size and total murders? How do murder rates vary across regions of the country? For most folks, it is quite difficult to extract this information just by looking at the numbers. In contrast, the answer to the questions above are readily available from examining this plot:\nlibrary(tidyverse) library(ggthemes) library(ggrepel) r \u0026lt;- murders %\u0026gt;% summarize(pop=sum(population), tot=sum(total)) %\u0026gt;% mutate(rate = tot/pop*10^6) %\u0026gt;% pull(rate) murders %\u0026gt;% ggplot(aes(x = population/10^6, y = total, label = abb)) + geom_abline(intercept = log10(r), lty=2, col=\u0026quot;darkgrey\u0026quot;) + geom_point(aes(color=region), size = 3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(\u0026quot;Populations in millions (log scale)\u0026quot;) + ylab(\u0026quot;Total number of murders (log scale)\u0026quot;) + ggtitle(\u0026quot;US Gun Murders in 2010\u0026quot;) + scale_color_discrete(name=\u0026quot;Region\u0026quot;) + theme_economist_white() We are reminded of the saying: “A picture is worth a thousand words”. Data visualization provides a powerful way to communicate a data-driven finding. In some cases, the visualization is so convincing that no follow-up analysis is required. You should consider visualization the most potent tool in your data analytics arsenal.\nThe growing availability of informative datasets and software tools has led to increased reliance on data visualizations across many industries, academia, and government. A salient example is news organizations, which are increasingly embracing data journalism and including effective infographics as part of their reporting.\nA particularly salient example—given the current state of the world—is a Wall Street Journal article1 showing data related to the impact of vaccines on battling infectious diseases. One of the graphs shows measles cases by US state through the years with a vertical line demonstrating when the vaccine was introduced.\n(Source: Wall Street Journal)\nAnother striking example comes from a New York Times chart2, which summarizes scores from the NYC Regents Exams. As described in the article3, these scores are collected for several reasons, including to determine if a student graduates from high school. In New York City you need a 65 to pass. The distribution of the test scores forces us to notice something somewhat problematic:\n(Source: New York Times via Amanda Cox)\nThe most common test score is the minimum passing grade, with very few scores just below the threshold. This unexpected result is consistent with students close to passing having their scores bumped up.\nThis is an example of how data visualization can lead to discoveries which would otherwise be missed if we simply subjected the data to a battery of data analysis tools or procedures. Data visualization is the strongest tool of what we call exploratory data analysis (EDA). John W. Tukey4, considered the father of EDA, once said,\n  “The greatest value of a picture is when it forces us to notice what we never expected to see.”\n  Many widely used data analysis tools were initiated by discoveries made via EDA. EDA is perhaps the most important part of data analysis, yet it is one that is often overlooked.\nData visualization is also now pervasive in philanthropic and educational organizations. In the talks New Insights on Poverty5 and The Best Stats You’ve Ever Seen6, Hans Rosling forces us to notice the unexpected with a series of plots related to world health and economics. In his videos, he uses animated graphs to show us how the world is changing and how old narratives are no longer true.\nIt is also important to note that mistakes, biases, systematic errors and other unexpected problems often lead to data that should be handled with care. Failure to discover these problems can give rise to flawed analyses and false discoveries. As an example, consider that measurement devices sometimes fail and that most data analysis procedures are not designed to detect these. Yet these data analysis procedures will still give you an answer. The fact that it can be difficult or impossible to notice an error just from the reported results makes data visualization particularly important.\nToday, we will discuss the basics of data visualization and exploratory data analysis. We will use the ggplot2 package to code. To learn the very basics, we will start with a somewhat artificial example: heights reported by students. Then we will cover the two examples mentioned above: 1) world health and economics and 2) infectious disease trends in the United States.\nOf course, there is much more to data visualization than what we cover here. The following are references for those who wish to learn more:\n ER Tufte (1983) The visual display of quantitative information. Graphics Press. ER Tufte (1990) Envisioning information. Graphics Press. ER Tufte (1997) Visual explanations. Graphics Press. WS Cleveland (1993) Visualizing data. Hobart Press. WS Cleveland (1994) The elements of graphing data. CRC Press. A Gelman, C Pasarica, R Dodhia (2002) Let’s practice what we preach: Turning tables into graphs. The American Statistician 56:121-130. NB Robbins (2004) Creating more effective graphs. Wiley. A Cairo (2013) The functional art: An introduction to information graphics and visualization. New Riders. N Yau (2013) Data points: Visualization that means something. Wiley.  We also do not cover interactive graphics, a topic that is both too advanced for this course and too unweildy. Some useful resources for those interested in learning more can be found below, and you are encouraged to draw inspiration from those websites in your projects:\n https://shiny.rstudio.com/ https://d3js.org/  Code Some of the code from today’s class will be available below after the class.\n Video Video from today’s class will be available below after the class.\n   http://graphics.wsj.com/infectious-diseases-and-vaccines/?mc_cid=711ddeb86e↩︎\n http://graphics8.nytimes.com/images/2011/02/19/nyregion/19schoolsch/19schoolsch-popup.gif↩︎\n https://www.nytimes.com/2011/02/19/nyregion/19schools.html↩︎\n https://en.wikipedia.org/wiki/John_Tukey↩︎\n https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩︎\n https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599753603,"objectID":"059bb398e999a9d10b388c3df2b5644f","permalink":"/example/01-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/01-example/","section":"example","summary":"Introduction to data visualization Code Video    Introduction to data visualization Looking at the numbers and character strings that define a dataset is rarely useful. To convince yourself, print and stare at the US murders data table:\nlibrary(dslabs) data(murders) head(murders) ## state abb region population total ## 1 Alabama AL South 4779736 135 ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232 ## 4 Arkansas AR South 2915918 93 ## 5 California CA West 37253956 1257 ## 6 Colorado CO West 5029196 65 What do you learn from staring at this table?","tags":null,"title":"Introduction to Visualization","type":"docs"},{"authors":null,"categories":null,"content":"  Preliminaries Background R Markdown  Turning everything in   NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, September 21.\n Preliminaries As always, we will first have to load ggplot2. To do this, we will load the tidyverse by running this code:\nlibrary(tidyverse)  Background The New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order. They’ve provided an interactive map here where you can see the different projects. There’s also a link there to download the complete dataset.\nFor this exercise, you’re going to use this data to visualize the amounts or proportions of different types of essential projects in the five boroughs of New York City (Brooklyn, Manhattan, the Bronx, Queens, and Staten Island).\nAs you hopefully figured out by now, you’ll be doing all your R work in R Markdown. You can use an RStudio Project to keep your files well organized (either on your computer or on RStudio.cloud), but this is optional. If you decide to do so, either create a new project for this exercise only, or make a project for all your work in this class.\nYou’ll need to download one CSV file and put it somewhere on your computer or upload it to RStudio.cloud—preferably in a folder named data in your project folder. You can download the data from the DOB’s map, or use this link to get it directly:\n  EssentialConstruction.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and use it to begin your lab this week. Note: skip this step at your own peril.\n  02-lab.Rmd  R Markdown (We learned after the first assignment the following.) Many of you have not worked with R Markdown before. That’s okay—we’ll teach you. Importantly, there are resources here to help.\nWriting regular text with R Markdown follows the rules of Markdown. You can make lists; different-size headers, etc. This should be relatively straightfoward; consult the resouces for more information.\nYou’ll also need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\nExercise 1: Essential pandemic construction\nMake the following plots and briefly explain what they show. Note that the included .Rmd file above provides some intial guidance.\nShow the count or proportion of approved projects by borough using a bar chart.\n Show the count or proportion of approved projects by category using a lollipop chart\n Show the proportion of approved projects by borough and category simultaneously using a heatmap\n  You don’t need to make these super fancy, but if you’re feeling brave, experiment with adding a labs() layer or changing fill colors with scale_fill_manual() or with palettes.\nBonus\nOverlay the data from Part 1 above onto a map of NYC. For double bonus, color the boroughs.\n   Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. If you haven’t already install tinytex) to ensure that works. Upload the PDF file to D2L.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600175601,"objectID":"b6a0ce80cabafe6d7d9f272294abfa85","permalink":"/assignment/02-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/02-assignment/","section":"assignment","summary":"Preliminaries Background R Markdown  Turning everything in   NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, September 21.\n Preliminaries As always, we will first have to load ggplot2. To do this, we will load the tidyverse by running this code:\nlibrary(tidyverse)  Background The New York City Department of Buildings (DOB) maintains a list of construction sites that have been categorized as “essential” during the city’s shelter-in-place pandemic order.","tags":null,"title":"Applying ggplot2 to Real Data","type":"docs"},{"authors":null,"categories":null,"content":"  ggplot2 The components of a graph ggplot objects Geometries Aesthetic mappings Layers Tinkering with arguments  Global versus local aesthetic mappings Scales Labels and titles Categories as colors Annotation, shapes, and adjustments Add-on packages Putting it all together Quick plots with qplot Grids of plots    ggplot2 Exploratory data visualization is perhaps the greatest strength of R. One can quickly go from idea to data to plot with a unique balance of flexibility and ease. For example, Excel may be easier than R for some plots, but it is nowhere near as flexible. D3.js may be more flexible and powerful than R, but it takes much longer to generate a plot. One of the reasons we use R is its incredible flexibility and ease.\nThroughout this course, we will be creating plots using the ggplot21 package.\nlibrary(dplyr) library(ggplot2) Many other approaches are available for creating plots in R. In fact, the plotting capabilities that come with a basic installation of R are already quite powerful. There are also other packages for creating graphics such as grid and lattice. We chose to use ggplot2 in this course because it breaks plots into components in a way that permits beginners to create relatively complex and aesthetically pleasing plots using syntax that is intuitive and comparatively easy to remember.\nOne reason ggplot2 is generally more intuitive for beginners is that it uses a so-called “grammar of graphics”2, the letters gg in ggplot2. This is analogous to the way learning grammar can help a beginner construct hundreds of different sentences by learning just a handful of verbs, nouns and adjectives without having to memorize each specific sentence. Similarly, by learning a handful of ggplot2 building blocks and its grammar, you will be able to create hundreds of different plots.\nAnother reason ggplot2 is easy for beginners is that its default behavior is carefully chosen to satisfy the great majority of cases and is visually pleasing. As a result, it is possible to create informative and elegant graphs with relatively simple and readable code.\nOne limitation is that ggplot2 is designed to work exclusively with data tables in tidy format (where rows are observations and columns are variables). However, a substantial percentage of datasets that beginners work with are in, or can be converted into, this format. An advantage of this approach is that, assuming that our data is tidy, ggplot2 simplifies plotting code and the learning of grammar for a variety of plots. You should review the previous content about tidy data if you are feeling lost.\nTo use ggplot2 you will have to learn several functions and arguments. These are hard to memorize, so we highly recommend you have the ggplot2 cheat sheet handy. You can get a copy here: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf or simply perform an internet search for “ggplot2 cheat sheet”.\nThe components of a graph We will construct a graph that summarizes the US murders dataset that looks like this:\nWe can clearly see how much states vary across population size and the total number of murders. Not surprisingly, we also see a clear relationship between murder totals and population size. A state falling on the dashed grey line has the same murder rate as the US average. The four geographic regions are denoted with color, which depicts how most southern states have murder rates above the average.\nThis data visualization shows us pretty much all the information in the data table. The code needed to make this plot is relatively simple. We will learn to create the plot part by part.\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main three components to note are:\n Data: The US murders data table is being summarized. We refer to this as the data component. Geometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, and boxplot. We will learn more about these in the Data Visualization part of the book. Aesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.  We also note that:\n The points are labeled with the state abbreviations. The range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales. There are labels, a title, a legend, and we use the style of The Economist magazine.  We will now construct the plot piece by piece.\nWe start by loading the dataset:\nlibrary(dslabs) data(murders)  ggplot objects The first step in creating a ggplot2 graph is to define a ggplot object. We do this with the function ggplot, which initializes the graph. If we read the help file for this function, we see that the first argument is used to specify what data is associated with this object:\nggplot(data = murders) We can also pipe the data in as the first argument. So this line of code is equivalent to the one above:\nmurders %\u0026gt;% ggplot() It renders a plot, in this case a blank slate since no geometry has been defined. The only style choice we see is a grey background.\nWhat has happened above is that the object was created and, because it was not assigned, it was automatically evaluated. But we can assign our plot to an object, for example like this:\np \u0026lt;- ggplot(data = murders) class(p) ## [1] \u0026quot;gg\u0026quot; \u0026quot;ggplot\u0026quot; To render the plot associated with this object, we simply print the object p. The following two lines of code each produce the same plot we see above:\nprint(p) p  Geometries In ggplot2 we create graphs by adding layers. Layers can define geometries, compute summary statistics, define what scales to use, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n DATA %\u0026gt;% ggplot() + LAYER 1 + LAYER 2 + … + LAYER N\n Usually, the first added layer defines the geometry. We want to make a scatterplot. What geometry do we use?\nTaking a quick look at the cheat sheet, we see that the function used to create plots with this geometry is geom_point.\n(Image courtesy of RStudio3. CC-BY-4.0 license4.)\nGeometry function names follow the pattern: geom_X where X is the name of some specific geometry. Some examples include geom_point, geom_bar, and geom_histogram. You’ve already seen a few of these.\nFor geom_point to run properly we need to provide data and a mapping. We have already connected the object p with the murders data table, and if we add the layer geom_point it defaults to using this data. To find out what mappings are expected, we read the Aesthetics section of the help file geom_point help file:\n\u0026gt; Aesthetics \u0026gt; \u0026gt; geom_point understands the following aesthetics (required aesthetics are in bold): \u0026gt; \u0026gt; x \u0026gt; \u0026gt; y \u0026gt; \u0026gt; alpha \u0026gt; \u0026gt; colour and—although it does not show in bold above—we see that at least two arguments are required: x and y.\n Aesthetic mappings Aesthetic mappings describe how properties of the data connect with features of the graph, such as distance along an axis, size, or color. The aes function connects data with what we see on the graph by defining aesthetic mappings and will be one of the functions you use most often when plotting. The outcome of the aes function is often used as the argument of a geometry function. This example produces a scatterplot of total murders versus population in millions:\nmurders %\u0026gt;% ggplot() + geom_point(aes(x = population/10^6, y = total)) We can drop the x = and y = if we wanted to since these are the first and second expected arguments, as seen in the help page.\nInstead of defining our plot from scratch, we can also add a layer to the p object that was defined above as p \u0026lt;- ggplot(data = murders):\np + geom_point(aes(population/10^6, total)) The scale and labels are defined by default when adding this layer. Like dplyr functions, aes also uses the variable names from the object component: we can use population and total without having to call them as murders$population and murders$total. The behavior of recognizing the variables from the data component is quite specific to aes. With most functions, if you try to access the values of population or total outside of aes you receive an error.\n Layers A second layer in the plot we wish to make involves adding a label to each point to identify the state. The geom_label and geom_text functions permit us to add text to the plot with and without a rectangle behind the text, respectively.\nBecause each point (each state in this case) has a label, we need an aesthetic mapping to make the connection between points and labels. By reading the help file, we learn that we supply the mapping between point and label through the label argument of aes. So the code looks like this:\np + geom_point(aes(population/10^6, total)) + geom_text(aes(population/10^6, total, label = abb)) We have successfully added a second layer to the plot.\nAs an example of the unique behavior of aes mentioned above, note that this call:\np_test \u0026lt;- p + geom_text(aes(population/10^6, total, label = abb)) is fine, whereas this call:\np_test \u0026lt;- p + geom_text(aes(population/10^6, total), label = abb) will give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable.\nTinkering with arguments Each geometry function has many arguments other than aes and data. They tend to be specific to the function. For example, in the plot we wish to make, the points are larger than the default size. In the help file we see that size is an aesthetic and we can change it like this:\np + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb)) size is not a mapping: whereas mappings use data from specific observations and need to be inside aes(), operations we want to affect all the points the same way do not need to be included inside aes.\nNow because the points are larger it is hard to see the labels. If we read the help file for geom_text, we see the nudge_x argument, which moves the text slightly to the right or to the left:\np + geom_point(aes(population/10^6, total), size = 3) + geom_text(aes(population/10^6, total, label = abb), nudge_x = 1.5) This is preferred as it makes it easier to read the text. There are alternatives, though, and we will pepper in examples with better labels as we move forward.\n  Global versus local aesthetic mappings In the previous line of code, we define the mapping aes(population/10^6, total) twice, once in each geometry. We can avoid this by using a global aesthetic mapping. We can do this when we define the blank slate ggplot object. Remember that the function ggplot contains an argument that permits us to define aesthetic mappings:\nargs(ggplot) ## function (data = NULL, mapping = aes(), ..., environment = parent.frame()) ## NULL If we define a mapping in ggplot, all the geometries that are added as layers will default to this mapping. We redefine p:\np \u0026lt;- murders %\u0026gt;% ggplot(aes(population/10^6, total, label = abb)) and then we can simply write the following code to produce the previous plot:\np + geom_point(size = 3) + geom_text(nudge_x = 1.5) We keep the size and nudge_x arguments in geom_point and geom_text, respectively, because we want to only increase the size of points and only nudge the labels. If we put those arguments in aes then they would apply to both plots. Also note that the geom_point function does not need a label argument and therefore ignores that aesthetic.\nIf necessary, we can override the global mapping by defining a new mapping within each layer. These local definitions override the global. Here is an example:\np + geom_point(size = 3) + geom_text(aes(x = 10, y = 800, label = \u0026quot;Hello there!\u0026quot;)) Clearly, the second call to geom_text does not use population and total.\n Scales First, our desired scales are in log-scale. This is not the default, so this change needs to be added through a scales layer. A quick look at the cheat sheet reveals the scale_x_continuous function lets us control the behavior of scales. We use them like this:\np + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_continuous(trans = \u0026quot;log10\u0026quot;) + scale_y_continuous(trans = \u0026quot;log10\u0026quot;) Because we are in the log-scale now, the nudge must be made smaller.\nThis particular transformation is so common that ggplot2 provides the specialized functions scale_x_log10 and scale_y_log10, which we can use to rewrite the code like this:\np + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10()  Labels and titles Similarly, the cheat sheet quickly reveals that to change labels and add a title, we use the following functions:\np + geom_point(size = 3) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(\u0026quot;Populations in millions (log scale)\u0026quot;) + ylab(\u0026quot;Total number of murders (log scale)\u0026quot;) + ggtitle(\u0026quot;US Gun Murders in 2010\u0026quot;) We are almost there! All we have left to do is add color, a legend, and optional changes to the style.\n Categories as colors We can change the color of the points using the col argument in the geom_point function. To facilitate demonstration of new features, we will redefine p to be everything except the points layer:\np \u0026lt;- murders %\u0026gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_text(nudge_x = 0.05) + scale_x_log10() + scale_y_log10() + xlab(\u0026quot;Populations in millions (log scale)\u0026quot;) + ylab(\u0026quot;Total number of murders (log scale)\u0026quot;) + ggtitle(\u0026quot;US Gun Murders in 2010\u0026quot;) and then test out what happens by adding different calls to geom_point. We can make all the points blue by adding the color argument:\np + geom_point(size = 3, color =\u0026quot;blue\u0026quot;) This, of course, is not what we want. We want to assign color depending on the geographical region. A nice default behavior of ggplot2 is that if we assign a categorical variable to color, it automatically assigns a different color to each category and also adds a legend.\nSince the choice of color is determined by a feature of each observation, this is an aesthetic mapping. To map each point to a color, we need to use aes. We use the following code:\np + geom_point(aes(col=region), size = 3) The x and y mappings are inherited from those already defined in p, so we do not redefine them. We also move aes to the first argument since that is where mappings are expected in this function call.\nHere we see yet another useful default behavior: ggplot2 automatically adds a legend that maps color to region. To avoid adding this legend we set the geom_point argument show.legend = FALSE.\n Annotation, shapes, and adjustments We often want to add shapes or annotation to figures that are not derived directly from the aesthetic mapping; examples include labels, boxes, shaded areas, and lines.\nHere we want to add a line that represents the average murder rate for the entire country. Once we determine the per million rate to be \\(r\\), this line is defined by the formula: \\(y = r x\\), with \\(y\\) and \\(x\\) our axes: total murders and population in millions, respectively. In the log-scale this line turns into: \\(\\log(y) = \\log(r) + \\log(x)\\). So in our plot it’s a line with slope 1 and intercept \\(\\log(r)\\). To compute this value, we use our dplyr skills:\nr \u0026lt;- murders %\u0026gt;% summarize(rate = sum(total) / sum(population) * 10^6) %\u0026gt;% pull(rate) To add a line we use the geom_abline function. ggplot2 uses ab in the name to remind us we are supplying the intercept (a) and slope (b). The default line has slope 1 and intercept 0 so we only have to define the intercept:\np + geom_point(aes(col=region), size = 3) + geom_abline(intercept = log10(r)) Here geom_abline does not use any information from the data object.\nWe can change the line type and color of the lines using arguments. Also, we draw it first so it doesn’t go over our points.\np \u0026lt;- p + geom_abline(intercept = log10(r), lty = 2, color = \u0026quot;darkgrey\u0026quot;) + geom_point(aes(col=region), size = 3) Note that we have redefined p and used this new p below and in the next section.\nThe default plots created by ggplot2 are already very useful. However, we frequently need to make minor tweaks to the default behavior. Although it is not always obvious how to make these even with the cheat sheet, ggplot2 is very flexible.\nFor example, we can make changes to the legend via the scale_color_discrete function. In our plot the word region is capitalized and we can change it like this:\np \u0026lt;- p + scale_color_discrete(name = \u0026quot;Region\u0026quot;)  Add-on packages The power of ggplot2 is augmented further due to the availability of add-on packages. The remaining changes needed to put the finishing touches on our plot require the ggthemes and ggrepel packages.\nThe style of a ggplot2 graph can be changed using the theme functions. Several themes are included as part of the ggplot2 package. In fact, for most of the plots in this book, we use a function in the dslabs package that automatically sets a default theme:\nds_theme_set() Many other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this:\nlibrary(ggthemes) p + theme_economist() You can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead.\nThe final difference has to do with the position of the labels. In our plot, some of the labels fall on top of each other. The add-on package ggrepel includes a geometry that adds labels while ensuring that they don’t fall on top of each other. We simply change geom_text with geom_text_repel.\n Putting it all together Now that we are done testing, we can write one piece of code that produces our desired plot from scratch.\nlibrary(ggthemes) library(ggrepel) r \u0026lt;- murders %\u0026gt;% summarize(rate = sum(total) / sum(population) * 10^6) %\u0026gt;% pull(rate) murders %\u0026gt;% ggplot(aes(population/10^6, total, label = abb)) + geom_abline(intercept = log10(r), lty = 2, color = \u0026quot;darkgrey\u0026quot;) + geom_point(aes(col=region), size = 3) + geom_text_repel() + scale_x_log10() + scale_y_log10() + xlab(\u0026quot;Populations in millions (log scale)\u0026quot;) + ylab(\u0026quot;Total number of murders (log scale)\u0026quot;) + ggtitle(\u0026quot;US Gun Murders in 2010\u0026quot;) + scale_color_discrete(name = \u0026quot;Region\u0026quot;) + theme_economist_white()  Quick plots with qplot We have learned the powerful approach to generating visualization with ggplot. However, there are instances in which all we want is to make a quick plot of, for example, a histogram of the values in a vector, a scatterplot of the values in two vectors, or a boxplot using categorical and numeric vectors. We demonstrated how to generate these plots with hist, plot, and boxplot. However, if we want to keep consistent with the ggplot style, we can use the function qplot.\nIf we have values in two vectors, say:\ndata(murders) x \u0026lt;- log10(murders$population) y \u0026lt;- murders$total and we want to make a scatterplot with ggplot, we would have to type something like:\ndata.frame(x = x, y = y) %\u0026gt;% ggplot(aes(x, y)) + geom_point() This seems like too much code for such a simple plot. The qplot function sacrifices the flexibility provided by the ggplot approach, but allows us to generate a plot quickly.\nqplot(x, y) Although we won’t discuss qtplot in much detail, you should feel free to use it in the early stages of your data exploration. Once you’re settled on a final design, then move to ggplot.\n Grids of plots There are often reasons to graph plots next to each other. The gridExtra package permits us to do that:\nlibrary(gridExtra) p1 \u0026lt;- qplot(x) p2 \u0026lt;- qplot(x,y) grid.arrange(p1, p2, ncol = 2) TRY IT\nStart by loading the dplyr and ggplot2 library as well as the murders and heights data.\nlibrary(dplyr) library(ggplot2) library(dslabs) data(heights) data(murders) With ggplot2 plots can be saved as objects. For example we can associate a dataset with a plot object like this  p \u0026lt;- ggplot(data = murders) Because data is the first argument we don’t need to spell it out\np \u0026lt;- ggplot(murders) and we can also use the pipe:\np \u0026lt;- murders %\u0026gt;% ggplot() What is class of the object p?\nRemember that to print an object you can use the command print or simply type the object. Print the object p defined in exercise one and describe what you see.  Nothing happens. A blank slate plot. A scatterplot. A histogram.  Using the pipe %\u0026gt;%, create an object p but this time associated with the heights dataset instead of the murders dataset.\n What is the class of the object p you have just created?\n Now we are going to add a layer and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the murders data frame to remind yourself what are the names for these two variables and select the correct answer. Hint: Look at ?murders.\n  state and abb. total_murders and population_size. total and population. murders and size.  To create the scatterplot we add a layer with geom_point. The aesthetic mappings require us to define the x-axis and y-axis variables, respectively. So the code looks like this:  murders %\u0026gt;% ggplot(aes(x = , y = )) + geom_point() except we have to define the two variables x and y. Fill this out with the correct variable names.\nNote that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this:  murders %\u0026gt;% ggplot(aes(population, total)) + geom_point() Remake the plot but now with total in the x-axis and population in the y-axis.\nIf instead of points we want to add text, we can use the geom_text() or geom_label() geometries. The following code  murders %\u0026gt;% ggplot(aes(population, total)) + geom_label() will give us the error message: Error: geom_label requires the following missing aesthetics: label\nWhy is this?\nWe need to map a character to each point through the label argument in aes. We need to let geom_label know what character to use in the plot. The geom_label geometry does not require x-axis and y-axis values. geom_label is not a ggplot2 command.  Rewrite the code above to use abbreviation as the label through aes\n Change the color of the labels to blue. How will we do this?\n  Adding a column called blue to murders. Because each label needs a different color we map the colors through aes. Use the color argument in ggplot. Because we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.  Rewrite the code above to make the labels blue.\n Now suppose we want to use color to represent the different regions. In this case which of the following is most appropriate:\n  Adding a column called color to murders with the color we want to use. Because each label needs a different color we map the colors through the color argument of aes . Use the color argument in ggplot. Because we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.  Rewrite the code above to make the labels’ color be determined by the state’s region.\n Now we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by defining an object p holding the plot we have made up to now\n  p \u0026lt;- murders %\u0026gt;% ggplot(aes(population, total, label = abb, color = region)) + geom_label() To change the y-axis to a log scale we learned about the scale_x_log10() function. Add this layer to the object p to change the scale and render the plot.\nRepeat the previous exercise but now change both axes to be in the log scale.\n Now edit the code above to add the title “Gun murder data” to the plot. Hint: use the ggtitle function.\n      https://ggplot2.tidyverse.org/↩︎\n http://www.springer.com/us/book/9780387245447↩︎\n https://github.com/rstudio/cheatsheets↩︎\n https://github.com/rstudio/cheatsheets/blob/master/LICENSE↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600351227,"objectID":"ce879375c9ab42490f4d0d112e48c07c","permalink":"/example/02-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/02-example/","section":"example","summary":"ggplot2 The components of a graph ggplot objects Geometries Aesthetic mappings Layers Tinkering with arguments  Global versus local aesthetic mappings Scales Labels and titles Categories as colors Annotation, shapes, and adjustments Add-on packages Putting it all together Quick plots with qplot Grids of plots    ggplot2 Exploratory data visualization is perhaps the greatest strength of R. One can quickly go from idea to data to plot with a unique balance of flexibility and ease.","tags":null,"title":"ggplot2: Everything you ever wanted to know","type":"docs"},{"authors":null,"categories":null,"content":"  Getting started Bonus Exercise Turning everything in Postscript: how we got this unemployment data    NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, September 28.\n Getting started For this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, we describe how we built this dataset down below).\n  unemployment.csv  To help you, I’ve created a skeleton R Markdown file with a template for this exercise, along with some code to help you clean and summarize the data. Download that here and include it in your project:\n  03-lab.Rmd  In the end, to help you master file organization, we suggest that the structure of your project directory should look something like this:\nyour-project-name\\ 03-lab.Rmd your-project-name.Rproj data\\ unemployment.csv The example for today’s session will be incredibly helpful for this exercise. Reference it.\nFor this week, you need to start making your plots look nice. Label axes. Label the plot. Experiment with themes. Experiment with adding a labs() layer or changing colors. Or, if you’re super brave, try modifying a theme and its elements.\nYou’ll need to insert your own code chunks where needed. Rather than typing them by hand (that’s tedious and you might miscount the number of backticks!), use the “Insert” button at the top of the editing window, or type ctrl + alt + i on Windows, or ⌘ + ⌥ + i on macOS.\nEXERCISE 1\nUse data from the US Bureau of Labor Statistics (BLS) to show the trends in employment rate for all 50 states between 2006 and 2016. What stories does this plot tell? Which states struggled to recover from the 2008–09 recession?\nSome hints/tips:\n You won’t need to filter out any missing rows because the data here is complete—there are no state-year combinations with missing unemployment data.\n You’ll be plotting 51 facets. You can filter out DC if you want to have a better grid (like 5 × 10), or you can try using facet_geo() from the geofacet package to lay out the plots like a map of the US (try this!).\n Plot the date column along the x-axis, not the year column. If you plot by year, you’ll get weird looking lines (try it for fun?), since these observations are monthly. If you really want to plot by year only, you’ll need to create a different data frame where you group by year and state and calculate the average unemployment rate for each year/state combination (i.e. group_by(year, state) %\u0026gt;% summarize(avg_unemployment = mean(unemployment)))\n Try mapping other aesthetics onto the graph too. You’ll notice there are columns for region and division—play with those as colors, for instance.\n This plot might be big, so make sure you adjust fig.width and fig.height in the chunk options so that it’s visible when you knit it. You might also want to used ggsave() to save it with extra large dimensions.\n  EXERCISE 2\nUse data from the BLS to create a slopegraph that compares the unemployment rate in January 2006 with the unemployment rate in January 2009, either for all 50 states at once (good luck with that!) or for a specific region or division. Make sure the plot doesn’t look too busy or crowded in the end.\nWhat story does this plot tell? Which states in the US (or in the specific region you selected) were the most/least affected the Great Recession?\nSome hints/tips:\n You should use filter() to only select rows where the year is 2006 or 2009 (i.e. filter(year %in% c(2006, 2009)) and to select rows where the month is January (filter(month == 1) or filter(month_name == \"January\"))\n In order for the year to be plotted as separate categories on the x-axis, it needs to be a factor, so use mutate(year = factor(year)) to convert it.\n To make ggplot draw lines between the 2006 and 2009 categories, you need to include group = state in the aesthetics.\n   Bonus Exercise This is entirely optional but might be fun. Then again, it might not be fun. I don’t know.\nFor extra fun times, if you feel like it, create a bump chart showing something from the unemployment data (perhaps the top 10 states or bottom 10 states in unemployment?) Adapt the code in the example for today’s session.\nIf you do this, plotting 51 lines is going to be a huge mess. But filtering the data is also a bad idea, because states could drop in and out of the top/bottom 10 over time, and we don’t want to get rid of them. Instead, you can zoom in on a specific range of data in your plot with coord_cartesian(ylim = c(1, 10)), for instance.\n Turning everything in When you’re all done, click on the “Knit” button at the top of the editing window and create a PDF. If you haven’t already install tinytex) to ensure that works. Upload the PDF file to D2L.\n Postscript: how we got this unemployment data For the curious, here’s the code we used to download the unemployment data from the BLS.\nAnd to pull the curtain back and show how much googling is involved in data visualization (and data analysis and programming in general), here was my process for getting this data:\nWe thought “We want to have students show variation in something domestic over time” and then we googled “us data by state”. Nothing really came up (since it was an exceedingly vague search in the first place), but some results mentioned unemployment rates, so we figured that could be cool. We googled “unemployment statistics by state over time” and found that the BLS keeps statistics on this. We clicked on the “Data Tools” link in their main navigation bar, clicked on “Unemployment”, and then clicked on the “Multi-screen data search” button for the Local Area Unemployment Statistics (LAUS). We walked through the multiple screens and got excited that we’d be able to download all unemployment stats for all states for a ton of years, but then the final page had links to 51 individual Excel files, which was dumb. So we went back to Google and searched for “download bls data r” and found a few different packages people have written to do this. The first one we clicked on was blscrapeR at GitHub, and it looked like it had been updated recently, so we went with it. We followed the examples in the blscrapeR package and downloaded data for every state.  Another day in the life of doing modern data science. This is an example of something you will be able to do by the end of this class. we had no idea people had written R packages to access BLS data, but there are (at least) 3 packages out there. After a few minutes of tinkering, we got it working and it is relatively straightforward.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600904980,"objectID":"af0b365ca488e9ad2d9f06d6c238b02e","permalink":"/assignment/03-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/03-assignment/","section":"assignment","summary":"Getting started Bonus Exercise Turning everything in Postscript: how we got this unemployment data    NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, September 28.\n Getting started For this exercise you’ll use state-level unemployment data from 2006 to 2016 that comes from the US Bureau of Labor Statistics (if you’re curious, we describe how we built this dataset down below).","tags":null,"title":"Visualizing Large(ish) Data","type":"docs"},{"authors":null,"categories":null,"content":"  Preliminaries Complete code Load and clean data Small multiples Sparklines Slopegraphs Bump charts    Today’s example will continue (and conclude) some of the discussion from Tuesday. The code may be useful for future work.\nPreliminaries For today’s example, we’re going to use cross-national data. But instead of using the typical gapminder dataset as with the Tuuesday lecture, we’re going to collect data directly from the World Bank’s Open Data portal\nIf you want to skip the data downloading, you can download the data below (you’ll likely need to right click and choose “Save Link As”). However, it may be instructive for your group projects to explore the collection process. It’s not particularly hard!\n  wdi_raw.csv   Complete code Load and clean data First, we load the libraries we’ll be using. Note: there are some new packages below. You will almost surely need to add these. Moreover, these will almost surely throw an error unless you use the dependencies = TRUE argument when installing.\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(WDI) # For getting data from the World Bank library(geofacet) # For map-shaped facets library(scales) # For helpful scale functions like dollar() library(ggrepel) # For non-overlapping labels The World Bank has a ton of country-level data at data.worldbank.org. We can use a package named WDI (world development indicators) to access their servers and download the data directly into R.\nTo do this, we need to find the special World Bank codes for specific variables we want to get. These codes come from the URLs of the World Bank’s website. For instance, if you search for “access to electricity” at the World Bank’s website, you’ll find this page. If you look at the end of the URL, you’ll see a cryptic code: EG.ELC.ACCS.ZS. That’s the World Bank’s ID code for the “Access to electricity (% of population)” indicator.\nWe can feed a list of ID codes to the WDI() function to download data for those specific indicators. We want data from 1995-2015, so we set the start and end years accordingly. The extra=TRUE argument means that it’ll also include other helpful details like region, aid status, etc. Without it, it would only download the indicators we listed.\nindicators \u0026lt;- c(\u0026quot;SP.DYN.LE00.IN\u0026quot;, # Life expectancy \u0026quot;EG.ELC.ACCS.ZS\u0026quot;, # Access to electricity \u0026quot;EN.ATM.CO2E.PC\u0026quot;, # CO2 emissions \u0026quot;NY.GDP.PCAP.KD\u0026quot;) # GDP per capita wdi_raw \u0026lt;- WDI(country = \u0026quot;all\u0026quot;, indicators, extra = TRUE, start = 1995, end = 2015) head(wdi_raw) Downloading data from the World Bank every time you knit will get tedious and take a long time (plus if their servers are temporarily down, you won’t be able to get the data). It’s good practice to save this raw data as a CSV file and then work with that.\nwrite_csv(wdi_raw, \u0026quot;data/wdi_raw.csv\u0026quot;) Since we care about reproducibility, we still want to include the code we used to get data from the World Bank, we just don’t want it to actually run. You can include chunks but not run them by setting eval=FALSE in the chunk options. In this little example, we show the code for downloading the data, but we don’t evaluate the chunk. We then include a chunk that loads the data from a CSV file with read_csv(), but we don’t include it (include=FALSE). That way, in the knitted file we see the WDI() code, but in reality it’s loading the data from CSV. Super tricky.\nI first download data from the World Bank: ```{r get-wdi-data, eval=FALSE} wdi_raw \u0026lt;- WDI(...) write_csv(wdi_raw, \u0026quot;data/wdi_raw.csv\u0026quot;) ``` ```{r load-wdi-data-real, include=FALSE} wdi_raw \u0026lt;- read_csv(\u0026quot;data/wdi_raw.csv\u0026quot;) ``` Then we clean up the data a little, filtering out rows that aren’t actually countries and renaming the ugly World Bank code columns to actual words:\nwdi_clean \u0026lt;- wdi_raw %\u0026gt;% filter(region != \u0026quot;Aggregates\u0026quot;) %\u0026gt;% select(iso2c, country, year, life_expectancy = SP.DYN.LE00.IN, access_to_electricity = EG.ELC.ACCS.ZS, co2_emissions = EN.ATM.CO2E.PC, gdp_per_cap = NY.GDP.PCAP.KD, region, income) head(wdi_clean) ## # A tibble: 6 x 9 ## iso2c country year life_expectancy access_to_electricity co2_emissions gdp_per_cap region income ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AD Andorra 2015 NA 100 NA 41768. Europe \u0026amp; Central Asia High income ## 2 AD Andorra 2004 NA 100 7.36 47033. Europe \u0026amp; Central Asia High income ## 3 AD Andorra 2001 NA 100 7.79 41421. Europe \u0026amp; Central Asia High income ## 4 AD Andorra 2002 NA 100 7.59 42396. Europe \u0026amp; Central Asia High income ## 5 AD Andorra 2014 NA 100 5.83 40790. Europe \u0026amp; Central Asia High income ## 6 AD Andorra 1995 NA 100 6.66 32918. Europe \u0026amp; Central Asia High income  Small multiples First we can make some small multiples plots and show life expectancy over time for a handful of countries. We’ll make a list of some countries chosen at random while I scrolled through the data, and then filter our data to include only those rows. We then plot life expectancy, faceting by country.\nlife_expectancy_small \u0026lt;- wdi_clean %\u0026gt;% filter(country %in% c(\u0026quot;Argentina\u0026quot;, \u0026quot;Bolivia\u0026quot;, \u0026quot;Brazil\u0026quot;, \u0026quot;Belize\u0026quot;, \u0026quot;Canada\u0026quot;, \u0026quot;Chile\u0026quot;)) ggplot(data = life_expectancy_small, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country)) Small multiples! That’s all we need to do.\nWe can do some fancier things, though. We can make this plot hyper minimalist:\nggplot(data = life_expectancy_small, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country), scales = \u0026quot;free_y\u0026quot;) + theme_void() + theme(strip.text = element_text(face = \u0026quot;bold\u0026quot;)) We can do a whole part of a continent (poor Iraq and Syria 😭)\nlife_expectancy_mena \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;Middle East \u0026amp; North Africa\u0026quot;) ggplot(data = life_expectancy_mena, mapping = aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_wrap(vars(country), scales = \u0026quot;free_y\u0026quot;, nrow = 3) + theme_void() + theme(strip.text = element_text(face = \u0026quot;bold\u0026quot;)) We can use the geofacet package to arrange these facets by geography:\nlife_expectancy_eu \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;Europe \u0026amp; Central Asia\u0026quot;) ggplot(life_expectancy_eu, aes(x = year, y = life_expectancy)) + geom_line(size = 1) + facet_geo(vars(country), grid = \u0026quot;eu_grid1\u0026quot;, scales = \u0026quot;free_y\u0026quot;) + labs(x = NULL, y = NULL, title = \u0026quot;Life expectancy from 1995–2015\u0026quot;, caption = \u0026quot;Source: The World Bank (SP.DYN.LE00.IN)\u0026quot;) + theme_minimal() + theme(strip.text = element_text(face = \u0026quot;bold\u0026quot;), plot.title = element_text(face = \u0026quot;bold\u0026quot;), axis.text.x = element_text(angle = 45, hjust = 1)) Neat!\n Sparklines Sparklines are just line charts (or bar charts) that are really really small.\nindia_co2 \u0026lt;- wdi_clean %\u0026gt;% filter(country == \u0026quot;India\u0026quot;) plot_india \u0026lt;- ggplot(india_co2, aes(x = year, y = co2_emissions)) + geom_line() + theme_void() plot_india ggsave(\u0026quot;india_co2.pdf\u0026quot;, plot_india, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;india_co2.png\u0026quot;, plot_india, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;) china_co2 \u0026lt;- wdi_clean %\u0026gt;% filter(country == \u0026quot;China\u0026quot;) plot_china \u0026lt;- ggplot(china_co2, aes(x = year, y = co2_emissions)) + geom_line() + theme_void() plot_china ggsave(\u0026quot;china_co2.pdf\u0026quot;, plot_china, width = 1, heighlt = 0.15, units = \u0026quot;in\u0026quot;) ggsave(\u0026quot;china_co2.png\u0026quot;, plot_china, width = 1, height = 0.15, units = \u0026quot;in\u0026quot;) You can then use those saved tiny plots in your text.\n Both India and China have seen increased CO2 emissions over the past 20 years.\n  Slopegraphs We can make a slopegraph to show changes in GDP per capita between two time periods. We need to first filter our WDI to include only the start and end years (here 1995 and 2015). Then, to make sure that we’re using complete data, we’ll get rid of any country that has missing data for either 1995 or 2015. The group_by(...) %\u0026gt;% filter(...) %\u0026gt;% ungroup() pipeline does this, with the !any(is.na(gdp_per_cap)) test keeping any rows where any of the gdp_per_cap values are not missing for the whole country.\nWe then add a couple special columns for labels. The paste0() function concatenates strings and variables together, so that paste0(\"2 + 2 = \", 2 + 2) would show “2 + 2 = 4”. Here we make labels that say either “Country name: $GDP” or “$GDP” depending on the year.\ngdp_south_asia \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;South Asia\u0026quot;) %\u0026gt;% filter(year %in% c(1995, 2015)) %\u0026gt;% # Look at each country individually group_by(country) %\u0026gt;% # Remove the country if any of its gdp_per_cap values are missing filter(!any(is.na(gdp_per_cap))) %\u0026gt;% ungroup() %\u0026gt;% # Make year a factor mutate(year = factor(year)) %\u0026gt;% # Make some nice label columns # If the year is 1995, format it like \u0026quot;Country name: $GDP\u0026quot;. If the year is # 2015, format it like \u0026quot;$GDP\u0026quot; mutate(label_first = ifelse(year == 1995, paste0(country, \u0026quot;: \u0026quot;, dollar(round(gdp_per_cap))), NA), label_last = ifelse(year == 2015, dollar(round(gdp_per_cap, 0)), NA)) With the data filtered like this, we can plot it by mapping year to the x-axis, GDP per capita to the y-axis, and coloring by country. To make the lines go across the two categorical labels in the x-axis (since we made year a factor/category), we need to also specify the group aesthetic.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) Cool! We’re getting closer. We can definitely see different slopes, but with 7 different colors, it’s hard to see exactly which country is which. Instead, we can directly label each of these lines with geom_text():\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text(aes(label = country)) + guides(color = FALSE) That gets us a little closer, but the country labels are hard to see, and we could include more information, like the actual values. Remember those label_first and label_last columns we made? Let’s use those instead:\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text(aes(label = label_first)) + geom_text(aes(label = label_last)) + guides(color = FALSE) Now we have dollar amounts and country names, but the labels are still overlapping and really hard to read. To fix this, we can make the labels repel away from each other and randomly position in a way that makes them not overlap. The ggrepel package lets us do this with geom_text_repel()\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first)) + geom_text_repel(aes(label = label_last)) + guides(color = FALSE) Now none of the labels are on top of each other, but the labels are still on top of the lines. Also, some of the labels moved inward and outward along the x-axis, but they don’t need to do that—they just need to shift up and down. We can force the labels to only move up and down by setting the direction = \"y\" argument, and we can move all the labels to the left or right with the nudge_x argument. The seed argument makes sure that the random label placement is the same every time we run this. It can be whatever number you want—it just has to be a number.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first), direction = \u0026quot;y\u0026quot;, nudge_x = -1, seed = 1234) + geom_text_repel(aes(label = label_last), direction = \u0026quot;y\u0026quot;, nudge_x = 1, seed = 1234) + guides(color = FALSE) That’s it! Let’s take the theme off completely, change the colors a little, and it should be perfect.\nggplot(gdp_south_asia, aes(x = year, y = gdp_per_cap, group = country, color = country)) + geom_line(size = 1.5) + geom_text_repel(aes(label = label_first), direction = \u0026quot;y\u0026quot;, nudge_x = -1, seed = 1234) + geom_text_repel(aes(label = label_last), direction = \u0026quot;y\u0026quot;, nudge_x = 1, seed = 1234) + guides(color = FALSE) + scale_color_viridis_d(option = \u0026quot;magma\u0026quot;, end = 0.9) + theme_void()  Bump charts Finally, we can make a bump chart that shows changes in rankings over time. We’ll look at CO2 emissions in South Asia. First we need to calculate a new variable that shows the rank of each country within each year. We can do this if we group by year and then use the rank() function to rank countries by the co2_emissions column.\nsa_co2 \u0026lt;- wdi_clean %\u0026gt;% filter(region == \u0026quot;South Asia\u0026quot;) %\u0026gt;% filter(year \u0026gt;= 2004, year \u0026lt; 2015) %\u0026gt;% group_by(year) %\u0026gt;% mutate(rank = rank(co2_emissions)) We then plot this with points and lines, reversing the y-axis so 1 is at the top:\nggplot(sa_co2, aes(x = year, y = rank, color = country)) + geom_line() + geom_point() + scale_y_reverse(breaks = 1:8) Afghanistan and Nepal switched around for the number 1 spot, while India dropped from 4 to 6, switching places with Pakistan.\nAs with the slopegraph, there are 8 different colors in the legend and it’s hard to line them all up with the different lines, so we can plot the text directly instead. We’ll use geom_text() again. We don’t need to repel anything, since the text should fit in each row just fine. We need to change the data argument in geom_text() though and filter the data to only include one year, otherwise we’ll get labels on every point, which is excessive. We can also adjust the theme and colors to make it cleaner.\nggplot(sa_co2, aes(x = year, y = rank, color = country)) + geom_line(size = 2) + geom_point(size = 4) + geom_text(data = filter(sa_co2, year == 2004), aes(label = iso2c, x = 2003.25), fontface = \u0026quot;bold\u0026quot;) + geom_text(data = filter(sa_co2, year == 2014), aes(label = iso2c, x = 2014.75), fontface = \u0026quot;bold\u0026quot;) + guides(color = FALSE) + scale_y_reverse(breaks = 1:8) + scale_x_continuous(breaks = 2004:2014) + scale_color_viridis_d(option = \u0026quot;magma\u0026quot;, begin = 0.2, end = 0.9) + labs(x = NULL, y = \u0026quot;Rank\u0026quot;) + theme_minimal() + theme(panel.grid.major.y = element_blank(), panel.grid.minor.y = element_blank(), panel.grid.minor.x = element_blank()) If you want to be super fancy, you can use flags instead of country codes, but that’s a little more complicated (you need to install the ggflags package. See here for an example.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601143971,"objectID":"73664027ae41c739f0c70a62c901a4c5","permalink":"/example/03-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/03-example/","section":"example","summary":"Preliminaries Complete code Load and clean data Small multiples Sparklines Slopegraphs Bump charts    Today’s example will continue (and conclude) some of the discussion from Tuesday. The code may be useful for future work.\nPreliminaries For today’s example, we’re going to use cross-national data. But instead of using the typical gapminder dataset as with the Tuuesday lecture, we’re going to collect data directly from the World Bank’s Open Data portal","tags":null,"title":"Visualizations ","type":"docs"},{"authors":null,"categories":null,"content":"   Statistical models Poll aggregators Poll data Pollster bias  Data-driven models    NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 5.\n For this exercise you will need to ensure that you’ve carefully read this week’s content and example. We will build on both. The exercises (which you will turn in as this week’s lab) are at the bottom. Note that this week’s lab is much more theoretical than any other week in this class. This is to ensure that you have the foundations necessary to build rich statistical models and apply them to real-world data.\nStatistical models  “All models are wrong, but some are useful.” –George E. P. Box\n The day before the 2008 presidential election, Nate Silver’s FiveThirtyEight stated that “Barack Obama appears poised for a decisive electoral victory”. They went further and predicted that Obama would win the election with 349 electoral votes to 189, and the popular vote by a margin of 6.1%. FiveThirtyEight also attached a probabilistic statement to their prediction claiming that Obama had a 91% chance of winning the election. The predictions were quite accurate since, in the final results, Obama won the electoral college 365 to 173 and the popular vote by a 7.2% difference. Their performance in the 2008 election brought FiveThirtyEight to the attention of political pundits and TV personalities. Four years later, the week before the 2012 presidential election, FiveThirtyEight’s Nate Silver was giving Obama a 90% chance of winning despite many of the experts thinking the final results would be closer. Political commentator Joe Scarborough said during his show1:\n Anybody that thinks that this race is anything but a toss-up right now is such an ideologue … they’re jokes.\n To which Nate Silver responded via Twitter:\n If you think it’s a toss-up, let’s bet. If Obama wins, you donate $1,000 to the American Red Cross. If Romney wins, I do. Deal?\n In 2016, Silver was not as certain and gave Hillary Clinton only a 71% of winning. In contrast, most other forecasters were almost certain she would win. She lost. But 71% is still more than 50%, so was Mr. Silver wrong? And what does probability mean in this context anyway? Are dice being tossed somewhere?\nIn this lab we will demonstrate how poll aggregators, such as FiveThirtyEight, collected and combined data reported by different experts to produce improved predictions. We will introduce ideas behind the statistical models, also known as probability models, that were used by poll aggregators to improve election forecasts beyond the power of individual polls. First, we’ll motivate the models, building on the statistical inference concepts we learned in this week’s content and example. We start with relatively simple models, realizing that the actual data science exercise of forecasting elections involves rather complex ones. We will introduce such modeks towards the end of this section of the course.\nPoll aggregators A few weeks before the 2012 election Nate Silver was giving Obama a 90% chance of winning. How was Mr. Silver so confident? We will use a Monte Carlo simulation to illustrate the insight Mr. Silver had and others missed. To do this, we generate results for 12 polls taken the week before the election. We mimic sample sizes from actual polls and construct and report 95% confidence intervals for each of the 12 polls. We save the results from this simulation in a data frame and add a poll ID column.\nlibrary(tidyverse) library(dslabs) d \u0026lt;- 0.039 Ns \u0026lt;- c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516) p \u0026lt;- (d + 1) / 2 polls \u0026lt;- map_df(Ns, function(N) { x \u0026lt;- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p)) x_hat \u0026lt;- mean(x) se_hat \u0026lt;- sqrt(x_hat * (1 - x_hat) / N) list(estimate = 2 * x_hat - 1, low = 2*(x_hat - 1.96*se_hat) - 1, high = 2*(x_hat + 1.96*se_hat) - 1, sample_size = N) }) %\u0026gt;% mutate(poll = seq_along(Ns)) Here is a visualization showing the intervals the pollsters would have reported for the difference between Obama and Romney:\nNot surprisingly, all 12 polls report confidence intervals that include the election night result (dashed line). However, all 12 polls also include 0 (solid black line) as well. Therefore, if asked individually for a prediction, the pollsters would have to say: it’s a toss-up. Below we describe a key insight they are missing.\nPoll aggregators, such as Nate Silver, realized that by combining the results of different polls you could greatly improve precision. By doing this, we are effectively conducting a poll with a huge sample size. We can therefore report a smaller 95% confidence interval and a more precise prediction.\nAlthough as aggregators we do not have access to the raw poll data, we can use mathematics to reconstruct what we would have obtained had we made one large poll with:\nsum(polls$sample_size) ## [1] 11269 participants. Basically, we construct an estimate of the spread, let’s call it \\(d\\), with a weighted average in the following way:\nd_hat \u0026lt;- polls %\u0026gt;% summarize(avg = sum(estimate*sample_size) / sum(sample_size)) %\u0026gt;% pull(avg) Once we have an estimate of \\(d\\), we can construct an estimate for the proportion voting for Obama, which we can then use to estimate the standard error. Once we do this, we see that our margin of error is 0.0184545.\nThus, we can predict that the spread will be 3.1 plus or minus 1.8, which not only includes the actual result we eventually observed on election night, but is quite far from including 0. Once we combine the 12 polls, we become quite certain that Obama will win the popular vote.\nOf course, this was just a simulation to illustrate the idea. The actual data science exercise of forecasting elections is much more complicated and it involves modeling. Below we explain how pollsters fit multilevel models to the data and use this to forecast election results. In the 2008 and 2012 US presidential elections, Nate Silver used this approach to make an almost perfect prediction and silence the pundits.\nSince the 2008 elections, other organizations have started their own election forecasting group that, like Nate Silver’s, aggregates polling data and uses statistical models to make predictions. In 2016, forecasters underestimated Trump’s chances of winning greatly. The day before the election the New York Times reported2 the following probabilities for Hillary Clinton winning the presidency:\n   NYT  538  HuffPost  PW  PEC  DK  Cook  Roth      Win Prob  85%  71%  98%  89%  \u0026gt;99%  92%  Lean Dem  Lean Dem     For example, the Princeton Election Consortium (PEC) gave Trump less than 1% chance of winning, while the Huffington Post gave him a 2% chance. In contrast, FiveThirtyEight had Trump’s probability of winning at 29%, higher than tossing two coins and getting two heads. In fact, four days before the election FiveThirtyEight published an article titled Trump Is Just A Normal Polling Error Behind Clinton3. By understanding statistical models and how these forecasters use them, we will start to understand how this happened.\nAlthough not nearly as interesting as predicting the electoral college, for illustrative purposes we will start by looking at predictions for the popular vote. FiveThirtyEight predicted a 3.6% advantage for Clinton4, included the actual result of 2.1% (48.2% to 46.1%) in their interval, and was much more confident about Clinton winning the election, giving her an 81.4% chance. Their prediction was summarized with a chart like this:\nThe colored areas represent values with an 80% chance of including the actual result, according to the FiveThirtyEight model. We introduce actual data from the 2016 US presidential election to show how models are motivated and built to produce these predictions. To understand the “81.4% chance” statement we need to describe Bayesian statistics, which we do in Sections ?? and ??.\nPoll data We use public polling data organized by FiveThirtyEight for the 2016 presidential election. The data is included as part of the dslabs package:\ndata(polls_us_election_2016) The table includes results for national polls, as well as state polls, taken during the year prior to the election. For this first example, we will filter the data to include national polls conducted during the week before the election. We also remove polls that FiveThirtyEight has determined not to be reliable and graded with a “B” or less. Some polls have not been graded and we include those:\npolls \u0026lt;- polls_us_election_2016 %\u0026gt;% filter(state == \u0026quot;U.S.\u0026quot; \u0026amp; enddate \u0026gt;= \u0026quot;2016-10-31\u0026quot; \u0026amp; (grade %in% c(\u0026quot;A+\u0026quot;,\u0026quot;A\u0026quot;,\u0026quot;A-\u0026quot;,\u0026quot;B+\u0026quot;) | is.na(grade))) We add a spread estimate:\npolls \u0026lt;- polls %\u0026gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) For this example, we will assume that there are only two parties and call \\(p\\) the proportion voting for Clinton and \\(1-p\\) the proportion voting for Trump. We are interested in the spread \\(2p-1\\). Let’s call the spread \\(d\\) (for difference).\nWe have 49 estimates of the spread. The theory we learned tells us that these estimates are a random variable with a probability distribution that is approximately normal. The expected value is the election night spread \\(d\\) and the standard error is \\(2\\sqrt{p (1 - p) / N}\\). Assuming the urn model we described earlier is a good one, we can use this information to construct a confidence interval based on the aggregated data. The estimated spread is:\nd_hat \u0026lt;- polls %\u0026gt;% summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %\u0026gt;% pull(d_hat) and the standard error is:\np_hat \u0026lt;- (d_hat+1)/2 moe \u0026lt;- 1.96 * 2 * sqrt(p_hat * (1 - p_hat) / sum(polls$samplesize)) moe ## [1] 0.006623178 So we report a spread of 1.43% with a margin of error of 0.66%. On election night, we discover that the actual percentage was 2.1%, which is outside a 95% confidence interval. What happened?\nA histogram of the reported spreads shows a problem:\npolls %\u0026gt;% ggplot(aes(spread)) + geom_histogram(color=\u0026quot;black\u0026quot;, binwidth = .01) The data does not appear to be normally distributed and the standard error appears to be larger than 0.0066232. The theory is not quite working here.\n Pollster bias Notice that various pollsters are involved and some are taking several polls a week:\npolls %\u0026gt;% group_by(pollster) %\u0026gt;% summarize(n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 15 x 2 ## pollster `n()` ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 ABC News/Washington Post 7 ## 2 Angus Reid Global 1 ## 3 CBS News/New York Times 2 ## 4 Fox News/Anderson Robbins Research/Shaw \u0026amp; Company Research 2 ## 5 IBD/TIPP 8 ## 6 Insights West 1 ## 7 Ipsos 6 ## 8 Marist College 1 ## 9 Monmouth University 1 ## 10 Morning Consult 1 ## 11 NBC News/Wall Street Journal 1 ## 12 RKM Research and Communications, Inc. 1 ## 13 Selzer \u0026amp; Company 1 ## 14 The Times-Picayune/Lucid 8 ## 15 USC Dornsife/LA Times 8 Let’s visualize the data for the pollsters that are regularly polling:\nThis plot reveals an unexpected result. First, consider that the standard error predicted by theory for each poll:\npolls %\u0026gt;% group_by(pollster) %\u0026gt;% filter(n() \u0026gt;= 6) %\u0026gt;% summarize(se = 2 * sqrt(p_hat * (1-p_hat) / median(samplesize))) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 2 ## pollster se ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ABC News/Washington Post 0.0265 ## 2 IBD/TIPP 0.0333 ## 3 Ipsos 0.0225 ## 4 The Times-Picayune/Lucid 0.0196 ## 5 USC Dornsife/LA Times 0.0183 is between 0.018 and 0.033, which agrees with the within poll variation we see. However, there appears to be differences across the polls. Note, for example, how the USC Dornsife/LA Times pollster is predicting a 4% win for Trump, while Ipsos is predicting a win larger than 5% for Clinton. The theory we learned says nothing about different pollsters producing polls with different expected values. All the polls should have the same expected value. FiveThirtyEight refers to these differences as “house effects”. We also call them pollster bias.\nIn the following section, rather than use the urn model theory, we are instead going to develop a data-driven model.\n  Data-driven models For each pollster, let’s collect their last reported result before the election:\none_poll_per_pollster \u0026lt;- polls %\u0026gt;% group_by(pollster) %\u0026gt;% filter(enddate == max(enddate)) %\u0026gt;% ungroup() Here is a histogram of the data for these 15 pollsters:\nqplot(spread, data = one_poll_per_pollster, binwidth = 0.01) In the previous section, we saw that using the urn model theory to combine these results might not be appropriate due to the pollster effect. Instead, we will model this spread data directly.\nThe new model can also be thought of as an urn model, although the connection is not as direct. Rather than 0s (Republicans) and 1s (Democrats), our urn now contains poll results from all possible pollsters. We assume that the expected value of our urn is the actual spread \\(d=2p-1\\).\nBecause instead of 0s and 1s, our urn contains continuous numbers between -1 and 1, the standard deviation of the urn is no longer \\(\\sqrt{p(1-p)}\\). Rather than voter sampling variability, the standard error now includes the pollster-to-pollster variability. Our new urn also includes the sampling variability from the polling. Regardless, this standard deviation is now an unknown parameter. In statistics textbooks, the Greek symbol \\(\\sigma\\) is used to represent this parameter.\nIn summary, we have two unknown parameters: the expected value \\(d\\) and the standard deviation \\(\\sigma\\).\nOur task is to estimate \\(d\\). Because we model the observed values \\(X_1,\\dots X_N\\) as a random sample from the urn, the CLT might still work in this situation because it is an average of independent random variables. For a large enough sample size \\(N\\), the probability distribution of the sample average \\(\\bar{X}\\) is approximately normal with expected value \\(\\mu\\) and standard error \\(\\sigma/\\sqrt{N}\\). If we are willing to consider \\(N=15\\) large enough, we can use this to construct confidence intervals.\nA problem is that we don’t know \\(\\sigma\\). But theory tells us that we can estimate the urn model \\(\\sigma\\) with the sample standard deviation defined as \\(s = \\sqrt{ \\sum_{i=1}^N (X_i - \\bar{X})^2 / (N-1)}\\).\nUnlike for the population standard deviation definition, we now divide by \\(N-1\\). This makes \\(s\\) a better estimate of \\(\\sigma\\). There is a mathematical explanation for this, which is explained in most statistics textbooks, but we don’t cover it here.\nThe sd function in R computes the sample standard deviation:\nsd(one_poll_per_pollster$spread) ## [1] 0.02419369 We are now ready to form a new confidence interval based on our new data-driven model:\nresults \u0026lt;- one_poll_per_pollster %\u0026gt;% summarize(avg = mean(spread), se = sd(spread) / sqrt(length(spread))) %\u0026gt;% mutate(start = avg - 1.96 * se, end = avg + 1.96 * se) round(results * 100, 1) ## avg se start end ## 1 2.9 0.6 1.7 4.1 Our confidence interval is wider now since it incorporates the pollster variability. It does include the election night result of 2.1%. Also, note that it was small enough not to include 0, which means we were confident Clinton would win the popular vote.\nEXERCISES\nNote that using dollar signs $ $ to enclose some text is how you make the fancy math you see below. If you installed tinytex or some other Latex distribution in order to render your PDFs, you should be equipped to insert mathematics directly into your .Rmd file.\nIn this section, we talked about pollster bias. We used visualization to motivate the presence of such bias. Here we will give it a more rigorous treatment. We will consider two pollsters that conducted daily polls. We will look at national polls for the month before the election.  data(polls_us_election_2016) polls \u0026lt;- polls_us_election_2016 %\u0026gt;% filter(pollster %in% c(\u0026quot;Rasmussen Reports/Pulse Opinion Research\u0026quot;, \u0026quot;The Times-Picayune/Lucid\u0026quot;) \u0026amp; enddate \u0026gt;= \u0026quot;2016-10-15\u0026quot; \u0026amp; state == \u0026quot;U.S.\u0026quot;) %\u0026gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) We want to answer the question: is there a poll bias? First, make a plot showing the spreads for each poll.\nThe data does seem to suggest there is a difference. However, these data are subject to variability. Perhaps the differences we observe are due to chance.  The urn model theory says nothing about pollster effect. Under the urn model, both pollsters have the same expected value: the election day difference, that we call \\(d\\).\nWe will model the observed data \\(Y_{i,j}\\) in the following way:\n\\[ Y_{i,j} = d + b_i + \\varepsilon_{i,j} \\]\nwith \\(i=1,2\\) indexing the two pollsters, \\(b_i\\) the bias for pollster \\(i\\) and \\(\\varepsilon_ij\\) poll to poll chance variability. We assume the \\(\\varepsilon\\) are independent from each other, have expected value \\(0\\) and standard deviation \\(\\sigma_i\\) regardless of \\(j\\).\nWhich of the following best represents our question?\nIs \\(\\varepsilon_{i,j}\\) = 0? How close are the \\(Y_{i,j}\\) to \\(d\\)? Is \\(b_1 \\neq b_2\\)? Are \\(b_1 = 0\\) and \\(b_2 = 0\\) ?  Suppose we define \\(\\bar{Y}_1\\) as the average of poll results from the first poll, \\(Y_{1,1},\\dots,Y_{1,N_1}\\) with \\(N_1\\) the number of polls conducted by the first pollster:  polls %\u0026gt;% filter(pollster==\u0026quot;Rasmussen Reports/Pulse Opinion Research\u0026quot;) %\u0026gt;% summarize(N_1 = n()) What is the expected value of \\(\\bar{Y}_1\\)?\nWhat is the standard error of \\(\\bar{Y}_1\\)? (It may be helpful to compute the expected value and standard error of \\(\\bar{Y}_2\\) as well.)\n Suppose we define \\(\\bar{Y}_2\\) as the average of poll results from the first poll, \\(Y_{2,1},\\dots,Y_{2,N_2}\\) with \\(N_2\\) the number of polls conducted by the first pollster. What is the expected value \\(\\bar{Y}_2\\)?\n What does the CLT tell us about the distribution of \\(\\bar{Y}_2 - \\bar{Y}_1\\)?\n  Nothing because this is not the average of a sample. Because the \\(Y_{ij}\\) are approximately normal, so are the averages. Note that \\(\\bar{Y}_2\\) and \\(\\bar{Y}_1\\) are sample averages, so if we assume \\(N_2\\) and \\(N_1\\) are large enough, each is approximately normal. The difference of normals is also normal. The data are not 0 or 1, so CLT does not apply.  Construct a random variable that has expected value \\(b_2 - b_1\\), the pollster bias difference. If our model holds, then this random variable has an approximately normal distribution and we know its standard error. The standard error depends on \\(\\sigma_1\\) and \\(\\sigma_2\\) (the variances of the \\(Y\\) above), but we can plug the sample standard deviations. Compute those now.  The statistic formed by dividing our estimate of \\(b_2-b_1\\) by its estimated standard error:\n\\[ \\frac{\\bar{Y}_2 - \\bar{Y}_1}{\\sqrt{s_2^2/N_2 + s_1^2/N_1}} \\]\nis called the t-statistic. Now you should be able to answer the question: is \\(b_2 - b_1\\) different from 0?\nNotice that we have more than two pollsters. We can also test for pollster effect using all pollsters, not just two. The idea is to compare the variability across polls to variability within polls. We can actually construct statistics to test for effects and approximate their distribution. The area of statistics that does this is called Analysis of Variance or ANOVA. We do not cover it here, but ANOVA provides a very useful set of tools to answer questions such as: is there a pollster effect?\nFor this exercise, create a new table:\npolls \u0026lt;- polls_us_election_2016 %\u0026gt;% filter(enddate \u0026gt;= \u0026quot;2016-10-15\u0026quot; \u0026amp; state == \u0026quot;U.S.\u0026quot;) %\u0026gt;% group_by(pollster) %\u0026gt;% filter(n() \u0026gt;= 5) %\u0026gt;% mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %\u0026gt;% ungroup() Compute the average and standard deviation for each pollster and examine the variability across the averages and how it compares to the variability within the pollsters, summarized by the standard deviation.\n    https://www.youtube.com/watch?v=TbKkjm-gheY↩︎\n https://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html↩︎\n https://fivethirtyeight.com/features/trump-is-just-a-normal-polling-error-behind-clinton/↩︎\n https://projects.fivethirtyeight.com/2016-election-forecast/↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601831382,"objectID":"e52acfab5d487b6c8267ead23da8e20e","permalink":"/assignment/04-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/04-assignment/","section":"assignment","summary":"Statistical models Poll aggregators Poll data Pollster bias  Data-driven models    NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 5.\n For this exercise you will need to ensure that you’ve carefully read this week’s content and example. We will build on both. The exercises (which you will turn in as this week’s lab) are at the bottom.","tags":null,"title":"Statistical Models","type":"docs"},{"authors":null,"categories":null,"content":"   Part 1: Statistical Inference and Polls Polls The sampling model for polls  Populations, samples, parameters, and estimates The sample average Parameters Polling versus forecasting Properties of our estimate: expected value and standard error  Central Limit Theorem A Monte Carlo simulation The spread Bias: why not run a very large poll?   Part 2: (Supplemental) Additional Visualization Techniques Code Load and clean data Histograms Density plots Box, violin, and rain cloud plots     Probabilistic thinking is central in the human experience. How we describe that thinking is mixed, but most of the time we use (rather imprecise) language. With only a few moments of searching, one can find thousands of articles that use probabilistic words to describe events. Here are some examples:\n “‘Highly unlikely’ State of the Union will happen amid shutdown” – The Hill\n  “Tiger Woods makes Masters 15th and most improbable major” – Fox\n  “Trump predicts ‘very good chance’ of China trade deal” – CNN\n Yet people don’t have a good sense of what these things mean. Uncertainty is key to data analytics: if we were certain of things, A study in the 1960s explored the perception of probabilistic words like these among NATO officers. A more modern replication of this found the following basic pattern:\nA deep, basic fact about humans is that we struggle to understand probabilities. But visualizing things can help. The graphic above shows the uncertainty about uncertainty (very meta). We can convey all manner of uncertainty with clever graphics. Today’s practical example works through some of the myriad of ways to visualize variable data. We’ll cover some territory that we’ve already hit, in hopes of locking in some key concepts.\nPart 1: Statistical Inference and Polls In this Example we will describe, in some detail, how poll aggregators such as FiveThirtyEight use data to predict election outcomes. To understand how they do this, we first need to learn the basics of Statistical Inference, the part of statistics that helps distinguish patterns arising from signal from those arising from chance. Statistical inference is a broad topic and here we go over the very basics using polls as a motivating example. To describe the concepts, we complement the mathematical formulas with Monte Carlo simulations and R code.\nPolls Opinion polling has been conducted since the 19th century. The general goal is to describe the opinions held by a specific population on a given set of topics. In recent times, these polls have been pervasive during presidential elections. Polls are useful when interviewing every member of a particular population is logistically impossible. The general strategy is to interview a smaller group, chosen at random, and then infer the opinions of the entire population from the opinions of the smaller group. Statistical theory is used to justify the process. This theory is referred to as inference and it is the main topic of this chapter.\nPerhaps the best known opinion polls are those conducted to determine which candidate is preferred by voters in a given election. Political strategists make extensive use of polls to decide, among other things, how to invest resources. For example, they may want to know in which geographical locations to focus their “get out the vote” efforts.\nElections are a particularly interesting case of opinion polls because the actual opinion of the entire population is revealed on election day. Of course, it costs millions of dollars to run an actual election which makes polling a cost effective strategy for those that want to forecast the results.\nAlthough typically the results of these polls are kept private, similar polls are conducted by news organizations because results tend to be of interest to the general public and made public. We will eventually be looking at such data.\nReal Clear Politics1 is an example of a news aggregator that organizes and publishes poll results. For example, they present the following poll results reporting estimates of the popular vote for the 2016 presidential election2:\n  Poll  Date  Sample  MoE  Clinton  Trump  Spread      Final Results  –  –  –  48.2  46.1  Clinton +2.1    RCP Average  11/1 - 11/7  –  –  46.8  43.6  Clinton +3.2    Bloomberg  11/4 - 11/6  799 LV  3.5  46.0  43.0  Clinton +3    IBD  11/4 - 11/7  1107 LV  3.1  43.0  42.0  Clinton +1    Economist  11/4 - 11/7  3669 LV  –  49.0  45.0  Clinton +4    LA Times  11/1 - 11/7  2935 LV  4.5  44.0  47.0  Trump +3    ABC  11/3 - 11/6  2220 LV  2.5  49.0  46.0  Clinton +3    FOX News  11/3 - 11/6  1295 LV  2.5  48.0  44.0  Clinton +4    Monmouth  11/3 - 11/6  748 LV  3.6  50.0  44.0  Clinton +6    NBC News  11/3 - 11/5  1282 LV  2.7  48.0  43.0  Clinton +5    CBS News  11/2 - 11/6  1426 LV  3.0  47.0  43.0  Clinton +4    Reuters  11/2 - 11/6  2196 LV  2.3  44.0  39.0  Clinton +5     Although in the United States the popular vote does not determine the result of the presidential election, we will use it as an illustrative and simple example of how well polls work. Forecasting the election is a more complex process since it involves combining results from 50 states and DC and we will go into some detail on this later.\nLet’s make some observations about the table above. First, note that different polls, all taken days before the election, report a different spread: the estimated difference between support for the two candidates. Notice also that the reported spreads hover around what ended up being the actual result: Clinton won the popular vote by 2.1%. We also see a column titled MoE which stands for margin of error.\nIn this example, we will show how the probability concepts we learned in the previous content can be applied to develop the statistical approaches that make polls an effective tool. We will learn the statistical concepts necessary to define estimates and margins of errors, and show how we can use these to forecast final results relatively well and also provide an estimate of the precision of our forecast. Once we learn this, we will be able to understand two concepts that are ubiquitous in data science: confidence intervals and p-values. Finally, to understand probabilistic statements about the probability of a candidate winning, we will have to learn about Bayesian modeling. In the final sections, we put it all together to recreate the simplified version of the FiveThirtyEight model and apply it to the 2016 election.\nWe start by connecting probability theory to the task of using polls to learn about a population.\nThe sampling model for polls To help us understand the connection between polls and what we have learned, let’s construct a similar situation to the one pollsters face. To mimic the challenge real pollsters face in terms of competing with other pollsters for media attention, we will use an urn full of beads to represent voters and pretend we are competing for a $25 dollar prize. The challenge is to guess the spread between the proportion of blue and red beads in this hypothetical urn.\nBefore making a prediction, you can take a sample (with replacement) from the urn. To mimic the fact that running polls is expensive, it costs you $0.10 per each bead you sample. Therefore, if your sample size is 250, and you win, you will break even since you will pay $25 to collect your $25 prize. Your entry into the competition can be an interval. If the interval you submit contains the true proportion, you get half what you paid and pass to the second phase of the competition. In the second phase, the entry with the smallest interval is selected as the winner.\nThe dslabs package includes a function that shows a random draw from this urn:\nlibrary(tidyverse) library(dslabs) take_poll(25) Think about how you would construct your interval based on the data shown above.\nWe have just described a simple sampling model for opinion polls. The beads inside the urn represent the individuals that will vote on election day. Those that will vote for the Republican candidate are represented with red beads and the Democrats with the blue beads. For simplicity, assume there are no other colors. That is, that there are just two parties: Republican and Democratic.\n  Populations, samples, parameters, and estimates We want to predict the proportion of blue beads in the urn. Let’s call this quantity \\(p\\), which then tells us the proportion of red beads \\(1-p\\), and the spread \\(p - (1-p)\\), which simplifies to \\(2p - 1\\).\nIn statistical textbooks, the beads in the urn are called the population. The proportion of blue beads in the population \\(p\\) is called a parameter. The 25 beads we see in the previous plot are called a sample. The task of statistical inference is to predict the parameter \\(p\\) using the observed data in the sample.\nCan we do this with the 25 observations above? It is certainly informative. For example, given that we see 13 red and 12 blue beads, it is unlikely that \\(p\\) \u0026gt; .9 or \\(p\\) \u0026lt; .1. But are we ready to predict with certainty that there are more red beads than blue in the jar?\nWe want to construct an estimate of \\(p\\) using only the information we observe. An estimate should be thought of as a summary of the observed data that we think is informative about the parameter of interest. It seems intuitive to think that the proportion of blue beads in the sample \\(0.48\\) must be at least related to the actual proportion \\(p\\). But do we simply predict \\(p\\) to be 0.48? First, remember that the sample proportion is a random variable. If we run the command take_poll(25) four times, we get a different answer each time, since the sample proportion is a random variable.\nNote that in the four random samples shown above, the sample proportions range from 0.44 to 0.60. By describing the distribution of this random variable, we will be able to gain insights into how good this estimate is and how we can make it better.\nThe sample average Conducting an opinion poll is being modeled as taking a random sample from an urn. We are proposing the use of the proportion of blue beads in our sample as an estimate of the parameter \\(p\\). Once we have this estimate, we can easily report an estimate for the spread \\(2p-1\\), but for simplicity we will illustrate the concepts for estimating \\(p\\). We will use our knowledge of probability to defend our use of the sample proportion and quantify how close we think it is from the population proportion \\(p\\).\nWe start by defining the random variable \\(X\\) as: \\(X=1\\) if we pick a blue bead at random and \\(X=0\\) if it is red. This implies that the population is a list of 0s and 1s. If we sample \\(N\\) beads, then the average of the draws \\(X_1, \\dots, X_N\\) is equivalent to the proportion of blue beads in our sample. This is because adding the \\(X\\)s is equivalent to counting the blue beads and dividing this count by the total \\(N\\) is equivalent to computing a proportion. We use the symbol \\(\\bar{X}\\) to represent this average. In general, in statistics textbooks a bar on top of a symbol means the average. The theory we just learned about the sum of draws becomes useful because the average is a sum of draws multiplied by the constant \\(1/N\\):\n\\[\\bar{X} = 1/N \\times \\sum_{i=1}^N X_i\\]\nFor simplicity, let’s assume that the draws are independent: after we see each sampled bead, we return it to the urn. In this case, what do we know about the distribution of the sum of draws? First, we know that the expected value of the sum of draws is \\(N\\) times the average of the values in the urn. We know that the average of the 0s and 1s in the urn must be \\(p\\), the proportion of blue beads.\nHere we encounter an important difference with what we did in the Probability chapter: we don’t know what is in the urn. We know there are blue and red beads, but we don’t know how many of each. This is what we want to find out: we are trying to estimate \\(p\\).\n Parameters Just like we use variables to define unknowns in systems of equations, in statistical inference we define parameters to define unknown parts of our models. In the urn model which we are using to mimic an opinion poll, we do not know the proportion of blue beads in the urn. We define the parameters \\(p\\) to represent this quantity. \\(p\\) is the average of the urn because if we take the average of the 1s (blue) and 0s (red), we get the proportion of blue beads. Since our main goal is figuring out what is \\(p\\), we are going to estimate this parameter.\nThe ideas presented here on how we estimate parameters, and provide insights into how good these estimates are, extrapolate to many data science tasks. For example, we may want to determine the difference in health improvement between patients receiving treatment and a control group. We may ask, what are the health effects of smoking on a population? What are the differences in racial groups of fatal shootings by police? What is the rate of change in life expectancy in the US during the last 10 years? All these questions can be framed as a task of estimating a parameter from a sample.\n Polling versus forecasting Before we continue, let’s make an important clarification related to the practical problem of forecasting the election. If a poll is conducted four months before the election, it is estimating the \\(p\\) for that moment and not for election day. The \\(p\\) for election night might be different since people’s opinions fluctuate through time. The polls provided the night before the election tend to be the most accurate since opinions don’t change that much in a day. However, forecasters try to build tools that model how opinions vary across time and try to predict the election night results taking into consideration the fact that opinions fluctuate. We will describe some approaches for doing this in a later section.\n Properties of our estimate: expected value and standard error To understand how good our estimate is, we will describe the statistical properties of the random variable defined above: the sample proportion \\(\\bar{X}\\). Remember that \\(\\bar{X}\\) is the sum of independent draws so the rules we covered in the probability chapter apply.\nUsing what we have learned, the expected value of the sum \\(N\\bar{X}\\) is \\(N \\times\\) the average of the urn, \\(p\\). So dividing by the non-random constant \\(N\\) gives us that the expected value of the average \\(\\bar{X}\\) is \\(p\\). We can write it using our mathematical notation:\n\\[ \\mbox{E}(\\bar{X}) = p \\]\nWe can also use what we learned to figure out the standard error: the standard error of the sum is \\(\\sqrt{N} \\times\\) the standard deviation of the urn. Can we compute the standard error of the urn? We learned a formula that tells us that it is \\((1-0) \\sqrt{p (1-p)}\\) = \\(\\sqrt{p (1-p)}\\). Because we are dividing the sum by \\(N\\), we arrive at the following formula for the standard error of the average:\n\\[ \\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N} \\]\nThis result reveals the power of polls. The expected value of the sample proportion \\(\\bar{X}\\) is the parameter of interest \\(p\\) and we can make the standard error as small as we want by increasing \\(N\\). The law of large numbers tells us that with a large enough poll, our estimate converges to \\(p\\).\nIf we take a large enough poll to make our standard error about 1%, we will be quite certain about who will win. But how large does the poll have to be for the standard error to be this small?\nOne problem is that we do not know \\(p\\), so we can’t compute the standard error. However, for illustrative purposes, let’s assume that \\(p=0.51\\) and make a plot of the standard error versus the sample size \\(N\\):\nFrom the plot we see that we would need a poll of over 10,000 people to get the standard error that low. We rarely see polls of this size due in part to costs. From the Real Clear Politics table, we learn that the sample sizes in opinion polls range from 500-3,500 people. For a sample size of 1,000 and \\(p=0.51\\), the standard error is:\nsqrt(p*(1-p))/sqrt(1000) ## [1] 0.01580823 or 1.5 percentage points. So even with large polls, for close elections, \\(\\bar{X}\\) can lead us astray if we don’t realize it is a random variable. Nonetheless, we can actually say more about how close we get the \\(p\\).\n  Central Limit Theorem The Central Limit Theorem (CLT) tells us that the distribution function for a sum of draws is approximately normal. You also may recall that dividing a normally distributed random variable by a constant is also a normally distributed variable. This implies that the distribution of \\(\\bar{X}\\) is approximately normal.\nIn summary, we have that \\(\\bar{X}\\) has an approximately normal distribution with expected value \\(p\\) and standard error \\(\\sqrt{p(1-p)/N}\\).\nNow how does this help us? Suppose we want to know what is the probability that we are within 1% from \\(p\\). We are basically asking what is\n\\[ \\mbox{Pr}(| \\bar{X} - p| \\leq .01) \\] which is the same as:\n\\[ \\mbox{Pr}(\\bar{X}\\leq p + .01) - \\mbox{Pr}(\\bar{X} \\leq p - .01) \\]\nCan we answer this question? We can use the mathematical trick we learned in the previous chapter. Subtract the expected value and divide by the standard error to get a standard normal random variable, call it \\(Z\\), on the left. Since \\(p\\) is the expected value and \\(\\mbox{SE}(\\bar{X}) = \\sqrt{p(1-p)/N}\\) is the standard error we get:\n\\[ \\mbox{Pr}\\left(Z \\leq \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right) - \\mbox{Pr}\\left(Z \\leq - \\frac{ \\,.01} {\\mbox{SE}(\\bar{X})} \\right) \\]\nOne problem we have is that since we don’t know \\(p\\), we don’t know \\(\\mbox{SE}(\\bar{X})\\). But it turns out that the CLT still works if we estimate the standard error by using \\(\\bar{X}\\) in place of \\(p\\). We say that we plug-in the estimate. Our estimate of the standard error is therefore:\n\\[ \\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N} \\] In statistics textbooks, we use a little hat to denote estimates. The estimate can be constructed using the observed data and \\(N\\).\nNow we continue with our calculation, but dividing by \\(\\hat{\\mbox{SE}}(\\bar{X})=\\sqrt{\\bar{X}(1-\\bar{X})/N})\\) instead. In our first sample we had 12 blue and 13 red so \\(\\bar{X} = 0.48\\) and our estimate of standard error is:\nx_hat \u0026lt;- 0.48 se \u0026lt;- sqrt(x_hat*(1-x_hat)/25) se ## [1] 0.09991997 And now we can answer the question of the probability of being close to \\(p\\). The answer is:\npnorm(0.01/se) - pnorm(-0.01/se) ## [1] 0.07971926 Therefore, there is a small chance that we will be close. A poll of only \\(N=25\\) people is not really very useful, at least not for a close election.\nEarlier we mentioned the margin of error. Now we can define it because it is simply two times the standard error, which we can now estimate. In our case it is:\n1.96*se ## [1] 0.1958431 Why do we multiply by 1.96? Because if you ask what is the probability that we are within 1.96 standard errors from \\(p\\), we get:\n\\[ \\mbox{Pr}\\left(Z \\leq \\, 1.96\\,\\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right) - \\mbox{Pr}\\left(Z \\leq - 1.96\\, \\mbox{SE}(\\bar{X}) / \\mbox{SE}(\\bar{X}) \\right) \\] which is:\n\\[ \\mbox{Pr}\\left(Z \\leq 1.96 \\right) - \\mbox{Pr}\\left(Z \\leq - 1.96\\right) \\]\nwhich we know is about 95%:\npnorm(1.96)-pnorm(-1.96) ## [1] 0.9500042 Hence, there is a 95% probability that \\(\\bar{X}\\) will be within \\(1.96\\times \\hat{SE}(\\bar{X})\\), in our case within about 0.2, of \\(p\\). Note that 95% is somewhat of an arbitrary choice and sometimes other percentages are used, but it is the most commonly used value to define margin of error. We often round 1.96 up to 2 for simplicity of presentation.\nIn summary, the CLT tells us that our poll based on a sample size of \\(25\\) is not very useful. We don’t really learn much when the margin of error is this large. All we can really say is that the popular vote will not be won by a large margin. This is why pollsters tend to use larger sample sizes.\nFrom the table above, we see that typical sample sizes range from 700 to 3500. To see how this gives us a much more practical result, notice that if we had obtained a \\(\\bar{X}\\)=0.48 with a sample size of 2,000, our standard error \\(\\hat{\\mbox{SE}}(\\bar{X})\\) would have been 0.0111714. So our result is an estimate of 48% with a margin of error of 2%. In this case, the result is much more informative and would make us think that there are more red balls than blue. Keep in mind, however, that this is hypothetical. We did not take a poll of 2,000 since we don’t want to ruin the competition.\nA Monte Carlo simulation Suppose we want to use a Monte Carlo simulation to corroborate the tools we have built using probability theory. To create the simulation, we would write code like this:\nB \u0026lt;- 10000 N \u0026lt;- 1000 x_hat \u0026lt;- replicate(B, { x \u0026lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p)) mean(x) }) The problem is, of course, we don’t know p. We could construct an urn like the one pictured above and run an analog (without a computer) simulation. It would take a long time, but you could take 10,000 samples, count the beads and keep track of the proportions of blue. We can use the function take_poll(n=1000) instead of drawing from an actual urn, but it would still take time to count the beads and enter the results.\nOne thing we therefore do to corroborate theoretical results is to pick one or several values of p and run the simulations. Let’s set p=0.45. We can then simulate a poll:\np \u0026lt;- 0.45 N \u0026lt;- 1000 x \u0026lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p)) x_hat \u0026lt;- mean(x) In this particular sample, our estimate is x_hat. We can use that code to do a Monte Carlo simulation:\nB \u0026lt;- 10000 x_hat \u0026lt;- replicate(B, { x \u0026lt;- sample(c(0,1), size = N, replace = TRUE, prob = c(1-p, p)) mean(x) }) To review, the theory tells us that \\(\\bar{X}\\) is approximately normally distributed, has expected value \\(p=\\) 0.45 and standard error \\(\\sqrt{p(1-p)/N}\\) = 0.0157321. The simulation confirms this:\nmean(x_hat) ## [1] 0.4500761 sd(x_hat) ## [1] 0.01579523 A histogram and qq-plot confirm that the normal approximation is accurate as well:\nOf course, in real life we would never be able to run such an experiment because we don’t know \\(p\\). But we could run it for various values of \\(p\\) and \\(N\\) and see that the theory does indeed work well for most values. You can easily do this by re-running the code above after changing p and N.\n The spread The competition is to predict the spread, not the proportion \\(p\\). However, because we are assuming there are only two parties, we know that the spread is \\(p - (1-p) = 2p - 1\\). As a result, everything we have done can easily be adapted to an estimate of \\(2p - 1\\). Once we have our estimate \\(\\bar{X}\\) and \\(\\hat{\\mbox{SE}}(\\bar{X})\\), we estimate the spread with \\(2\\bar{X} - 1\\) and, since we are multiplying by 2, the standard error is \\(2\\hat{\\mbox{SE}}(\\bar{X})\\). Note that subtracting 1 does not add any variability so it does not affect the standard error.\nFor our 25 item sample above, our estimate \\(p\\) is .48 with margin of error .20 and our estimate of the spread is 0.04 with margin of error .40. Again, not a very useful sample size. However, the point is that once we have an estimate and standard error for \\(p\\), we have it for the spread \\(2p-1\\).\n Bias: why not run a very large poll? For realistic values of \\(p\\), say from 0.35 to 0.65, if we run a very large poll with 100,000 people, theory tells us that we would predict the election perfectly since the largest possible margin of error is around 0.3%:\nOne reason is that running such a poll is very expensive. Another possibly more important reason is that theory has its limitations. Polling is much more complicated than picking beads from an urn. Some people might lie to pollsters and others might not have phones. But perhaps the most important way an actual poll differs from an urn model is that we actually don’t know for sure who is in our population and who is not. How do we know who is going to vote? Are we reaching all possible voters? Hence, even if our margin of error is very small, it might not be exactly right that our expected value is \\(p\\). We call this bias. Historically, we observe that polls are indeed biased, although not by that much. The typical bias appears to be about 1-2%. This makes election forecasting a bit more interesting and we will talk about how to model this shortly.\n   Part 2: (Supplemental) Additional Visualization Techniques For this second part of the example, we’re going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  atl-weather-2019.csv  Code Load and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) library(lubridate) library(ggridges) library(gghalves) Then we load the data with read_csv(). Here we assume that the CSV file lives in a subfolder in my project named data. Naturally, you’ll need to point this to wherever you stashed the data.\nweather_atl_raw \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;) We’ll add a couple columns that we can use for faceting and filling using the month() and wday() functions from lubridate for extracting parts of the date:\nweather_atl \u0026lt;- weather_atl_raw %\u0026gt;% mutate(Month = month(time, label = TRUE, abbr = FALSE), Day = wday(time, label = TRUE, abbr = FALSE)) Now we’re ready to go!\n Histograms We can first make a histogram of wind speed. We’ll use a bin width of 1 and color the edges of the bars white:\nggplot(weather_atl, aes(x = windSpeed)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;) This is fine enough, but we can improve it by forcing the buckets/bins to start at whole numbers instead of containing ranges like 2.5–3.5. We’ll use the boundary argument for that. We also add scale_x_continuous() to add our own x-axis breaks instead of having things like 2.5, 5, and 7.5:\nggplot(weather_atl, aes(x = windSpeed)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) We can show the distribution of wind speed by month if we map the Month column we made onto the fill aesthetic:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) This is colorful, but it’s impossible to actually interpret. Instead of only filling, we’ll also facet by month to see separate graphs for each month. We can turn off the fill legend because it’s now redundant.\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_histogram(binwidth = 1, color = \u0026quot;white\u0026quot;, boundary = 1) + scale_x_continuous(breaks = seq(0, 12, by = 1)) + guides(fill = FALSE) + facet_wrap(vars(Month)) Neat! January, March, and April appear to have the most variation in windy days, with a few wind-less days and a few very-windy days, while August was very wind-less.\n Density plots The code to create a density plot is nearly identical to what we used for the histogram—the only thing we change is the geom layer:\nggplot(weather_atl, aes(x = windSpeed)) + geom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;) If we want, we can mess with some of the calculus options like the kernel and bandwidth:\nggplot(weather_atl, aes(x = windSpeed)) + geom_density(color = \u0026quot;grey20\u0026quot;, fill = \u0026quot;grey50\u0026quot;, bw = 0.1, kernel = \u0026quot;epanechnikov\u0026quot;) We can also fill by month. We’ll make the different layers 50% transparent so we can kind of see through the whole stack:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_density(alpha = 0.5) Even with the transparency, this is really hard to interpret. We can fix this by faceting, like we did with the histograms:\nggplot(weather_atl, aes(x = windSpeed, fill = Month)) + geom_density(alpha = 0.5) + guides(fill = FALSE) + facet_wrap(vars(Month)) Or we can stack the density plots behind each other with ggridges. For that to work, we also need to map Month to the y-axis. We can reverse the y-axis so that January is at the top if we use the fct_rev() function:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) + geom_density_ridges() + guides(fill = FALSE) We can add some extra information to geom_density_ridges() with some other arguments like quantile_lines. We can use the quantiles argument to tell the plow how many parts to be cut into. Since we just want to show the median, we’ll set that to 2 so each density plot is divided in half:\nggplot(weather_atl, aes(x = windSpeed, y = fct_rev(Month), fill = Month)) + geom_density_ridges(quantile_lines = TRUE, quantiles = 2) + guides(fill = FALSE) Now that we have good working code, we can easily substitute in other variables by changing the x mapping:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = Month)) + geom_density_ridges(quantile_lines = TRUE, quantiles = 2) + guides(fill = FALSE) We can get extra fancy if we fill by temperature instead of filling by month. To get this to work, we need to use geom_density_ridges_gradient(), and we need to change the fill mapping to the strange looking ..x.., which is a weird ggplot trick that tells it to use the variable we mapped to the x-axis. For whatever reason, fill = temperatureHigh doesn’t work 🤷:\nggplot(weather_atl, aes(x = temperatureHigh, y = fct_rev(Month), fill = ..x..)) + geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) + scale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) + labs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;) And finally, we can get extra fancy and show the distributions for both the high and low temperatures each month. To make this work, we need to manipulate the data a little. Right now there are two columns for high and low temperature: temperatureLow and temperatureHigh. To be able to map temperature to the x-axis and high vs. low to another aesthetic (like linetype), we need a column with the temperature and a column with an indicator variable for whether it is high or low. This data needs to be tidied (since right now we have a variable (high/low) encoded in the column name). We can tidy this data using pivot_longer() from tidyr, which was already loaded with library(tidyverse). In the RStudio primers, you did this same thing with gather()—pivot_longer() is the newer version of gather():\nweather_atl_long \u0026lt;- weather_atl %\u0026gt;% pivot_longer(cols = c(temperatureLow, temperatureHigh), names_to = \u0026quot;temp_type\u0026quot;, values_to = \u0026quot;temp\u0026quot;) %\u0026gt;% # Clean up the new temp_type column so that \u0026quot;temperatureHigh\u0026quot; becomes \u0026quot;High\u0026quot;, etc. mutate(temp_type = recode(temp_type, temperatureHigh = \u0026quot;High\u0026quot;, temperatureLow = \u0026quot;Low\u0026quot;)) %\u0026gt;% # This is optional—just select a handful of columns select(time, temp_type, temp, Month) # Show the first few rows head(weather_atl_long) ## # A tibble: 6 x 4 ## time temp_type temp Month ## \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;ord\u0026gt; ## 1 2019-01-01 05:00:00 Low 50.6 January ## 2 2019-01-01 05:00:00 High 63.9 January ## 3 2019-01-02 05:00:00 Low 49.0 January ## 4 2019-01-02 05:00:00 High 57.4 January ## 5 2019-01-03 05:00:00 Low 53.1 January ## 6 2019-01-03 05:00:00 High 55.3 January Now we have a column for the temperature (temp) and a column indicating if it is high or low (temp_type). The dataset is also twice as long (730 rows) because each day has two rows (high and low). Let’s plot it and map high/low to the linetype aesthetic to show high/low in the border of the plots:\nggplot(weather_atl_long, aes(x = temp, y = fct_rev(Month), fill = ..x.., linetype = temp_type)) + geom_density_ridges_gradient(quantile_lines = TRUE, quantiles = 2) + scale_fill_viridis_c(option = \u0026quot;plasma\u0026quot;) + labs(x = \u0026quot;High temperature\u0026quot;, y = NULL, color = \u0026quot;Temp\u0026quot;) We can see much wider temperature disparities during the summer, with large gaps between high and low, and relatively equal high/low temperatures during the winter.\n Box, violin, and rain cloud plots Finally, we can look at the distribution of variables with box plots, violin plots, and other similar graphs. First, we’ll make a box plot of windspeed, filled by the Day variable we made indicating weekday:\nggplot(weather_atl, aes(y = windSpeed, fill = Day)) + geom_boxplot() We can switch this to a violin plot by just changing the geom layer and mapping Day to the x-axis:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() With violin plots it’s typically good to overlay other geoms. We can add some jittered points for a strip plot:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) We can also add larger points for the daily averages. We’ll use a special layer for this: stat_summary(). It has a slightly different syntax, since we’re not actually mapping a column from the dataset. Instead, we’re feeding a column from a dataset into a function (here \"mean\") and then plotting that result:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + stat_summary(geom = \u0026quot;point\u0026quot;, fun = \u0026quot;mean\u0026quot;, size = 5, color = \u0026quot;white\u0026quot;) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) We can also show the mean and confidence interval at the same time by changing the summary function:\nggplot(weather_atl, aes(y = windSpeed, x = Day, fill = Day)) + geom_violin() + stat_summary(geom = \u0026quot;pointrange\u0026quot;, fun.data = \u0026quot;mean_se\u0026quot;, size = 1, color = \u0026quot;white\u0026quot;) + geom_point(size = 0.5, position = position_jitter(width = 0.1)) + guides(fill = FALSE) Overlaying the points directly on top of the violins shows extra information, but it’s also really crowded and hard to read. If we use the gghalves package, we can use special halved versions of some of these geoms like so:\nggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) + geom_half_boxplot(aes(fill = Day), side = \u0026quot;r\u0026quot;) + guides(color = FALSE, fill = FALSE) Note the side argument for specifying which half of the column the geom goes. We can also use geom_half_violin():\nggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) + geom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) + guides(color = FALSE, fill = FALSE) If we flip the plot, we can make a rain cloud plot:\nggplot(weather_atl, aes(x = fct_rev(Day), y = temperatureHigh)) + geom_half_boxplot(aes(fill = Day), side = \u0026quot;l\u0026quot;, width = 0.5, nudge = 0.1) + geom_half_point(aes(color = Day), side = \u0026quot;l\u0026quot;, size = 0.5) + geom_half_violin(aes(fill = Day), side = \u0026quot;r\u0026quot;) + guides(color = FALSE, fill = FALSE) + coord_flip()     http://www.realclearpolitics.com↩︎\n http://www.realclearpolitics.com/epolls/2016/president/us/general_election_trump_vs_clinton-5491.html↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601477330,"objectID":"c977a82d5f9c9c6bf14f43f56de0b41e","permalink":"/example/04-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/04-example/","section":"example","summary":"Part 1: Statistical Inference and Polls Polls The sampling model for polls  Populations, samples, parameters, and estimates The sample average Parameters Polling versus forecasting Properties of our estimate: expected value and standard error  Central Limit Theorem A Monte Carlo simulation The spread Bias: why not run a very large poll?   Part 2: (Supplemental) Additional Visualization Techniques Code Load and clean data Histograms Density plots Box, violin, and rain cloud plots     Probabilistic thinking is central in the human experience.","tags":null,"title":"Visualizing Uncertainty","type":"docs"},{"authors":null,"categories":null,"content":"  Backstory and Set Up Data Exploration and Processing    NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 12.\n This week’s lab will (hopefully) not repeat the disaster that was last week’s lab.\nBackstory and Set Up You have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nFirst, let’s load the data.\nameslist \u0026lt;- read.table(\u0026quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;) Before we proceed, let’s note a few things about the (simple) code above. First, we have specified header = TRUE because—you guessed it—the original dataset has headers. Although simple, this is an incredibly important step because it allows R to do some smart R things. Specifically, once the headers are in, the variables are formatted as int and factor where appropriate. It is absolutely vital that we format the data correctly; otherwise, many R commands will whine at us.\nTry it: Run the above, but instead specifying header = FALSE. What data type are the various columns? Now try ommitting the line altogether. What is the default behavior of the read.table function?1\nData Exploration and Processing We are not going to tell you anything about this data. This is intended to replicate a real-world experience that you will all encounter in the (possibly near) future: someone hands you data and you’re expected to make sense of it. Fortunately for us, this data is (somewhat) self-contained. We’ll first check the variable names to try to divine some information. Recall, we have a handy little function for that:\nnames(ameslist) Note that, when doing data exploration, we will sometimes choose to not save our output. This is a judgement call; here we’ve chosen to merely inspect the variables rather than diving in.\nInspection yields some obvious truths. For example:\n  Variable Explanation Type    ID Unique identifier for each row int  LotArea Size of lot (units unknown) int  SalePrice Sale price of house ($) int    …but we face some not-so-obvious things as well. For example:\n  Variable Explanation Type    LotShape ? Something about the lot factor  MSSubClass ? No clue at all int  Condition1 ? Seems like street info factor    It will be difficult to learn anything about the data that is of type int without outside documentation. However, we can learn something more about the factor-type variables. In order to understand these a little better, we need to review some of the values that each take on.\nTry it: Go through the variables in the dataset and make a note about your interpretation for each. Many will be obvious, but some require additional thought.\nWe now turn to another central issue—and one that explains our nomenclature choice thus far: the data object is of type list. To verify this for yourself, check:\ntypeof(ameslist) This isn’t ideal—for some visualization packages, for instance, we need data frames and not lists. We’ll make a mental note of this as something to potentially clean up if we desire.\nAlthough there are some variables that would be difficult to clean, there are a few that we can address with relative ease. Consider, for instance, the variable GarageType. This might not be that important, but, remember, the weather in Ames, IA is pretty crummy—a detached garage might be a dealbreaker for some would-be homebuyers. Let’s inspect the values:\n\u0026gt; unique(ameslist$GarageType) [1] Attchd Detchd BuiltIn CarPort \u0026lt;NA\u0026gt; Basment 2Types With this, we could make an informed decision and create a new variable. Let’s create OutdoorGarage to indicate, say, homes that have any type of garage that requires the homeowner to walk outdoors after parking their car. (For those who aren’t familiar with different garage types, a car port is not insulated and is therefore considered outdoors. A detached garage presumably requires that the person walks outside after parking. The three other types are inside the main structure, and 2Types we can assume includes at least one attached garage of some sort). This is going to require a bit more coding and we will have to think through each step carefully.\nFirst, let’s create a new object that has indicator variables (that is, a variable whose values are either zero or one) for each of the GarageType values. As with everything in R, there’s a handy function to do this for us:\nGarageTemp = model.matrix( ~ GarageType - 1, data=ameslist ) We now have two separate objects living in our computer’s memory: ameslist and GarageTemp—so named to indicate that it is a temporary object.2 We now need to stitch it back onto our original data; we’ll use a simple concatenation and write over our old list with the new one:\nameslist \u0026lt;- cbind(ameslist, GarageTemp) \u0026gt; Error in data.frame(..., check.names = FALSE) : arguments imply differing number of rows: 1460, 1379 Huh. What’s going on?\nTry it: Figure out what’s going on above. Fix this code so that you have a working version.\nNow that we’ve got that working (ha!) we can generate a new variable for our outdoor garage. We’ll use a somewhat gross version below because it is verbose; that said, this can be easily accomplished using logical indexing for those who like that approach.\nameslist$GarageOutside \u0026lt;- ifelse(ameslist$GarageTypeDetchd == 1 | ameslist$GarageTypeCarPort == 1, 1, 0) unique(ameslist$GarageOutside) [1] 0 1 NA This seems to have worked. The command above ifelse() does what it says: if some condition is met (here, either of two variables equals one) then it returns a one; else it returns a zero. Such functions are very handy, though as mentioned above, there are other ways of doing this. Also note, that while fixed the issue with NA above, we’ve got new issues: we definitely don’t want NA outputted from this operation. Accordingly, we’re going to need to deal with it somehow.\nTry it: Utilizing a similar approach to what you did above, fix this so that the only outputs are zero and one.\nGenerally speaking, this is a persistent issue, and you will spend an extraordinary amount of time dealing with missing data or data that does not encode a variable exactly as you want it. This is expecially true if you deal with real-world data: you will need to learn how to handle NAs. There are a number of fixes (as always, Google is your friend) and anything that works is good. But you should spend some time thinking about this and learning at least one approach.\nEXERCISES\nPrune the data to all of the variables that are type = int about which you have some reasonable intuition for what they mean. This must include the variable SalePrice. Save this new dataset as Ames. Produce documentation for this object in the form of a .txt file. This must describe each of the preserved variables, the values it can take (e.g., can it be negative?) and your interpretation of the variable.\n Produce a scatterplot matrix which includes 12 of the variables that are type = int in the data set. Choose those that you believe are likely to be correlated with SalePrice.3\n Compute a matrix of correlations between these variables using the function cor(). Does this match your prior beliefs? Briefly discuss the correlation between the miscellaneous variables and SalePrice.\n Produce a scatterplot between SalePrice and GrLivArea. Run a linear model using lm() to explore the relationship. Finally, use the abline() function to plot the relationship that you’ve found in the simple linear regression.\n What is the largest outlier that is above the regression line? Produce the other information about this house.   (Bonus) Create a visualization that shows the rise of air conditioning over time in homes in Ames.\n    Of course, you could find out the defaults of the function by simply using the handy ? command. Don’t forget about this tool!↩︎\n It’s not exactly true that these objects are in memory. They are… sort of. But how R handles memory is complicated and silly and blah blah who cares. It’s basically in memory.↩︎\n If you are not familiar with this type of visualization, consult the book (Introduction to Statistical Learning), Chapters 2 and 3. Google it; it’s free.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604325415,"objectID":"7337e9261c3514039025dedd39df2948","permalink":"/assignment/05-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/05-assignment/","section":"assignment","summary":"Backstory and Set Up Data Exploration and Processing    NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 12.\n This week’s lab will (hopefully) not repeat the disaster that was last week’s lab.\nBackstory and Set Up You have been recently hired to Zillow’s Zestimate product team as a junior analyst.","tags":null,"title":"Correlations and Simple Models","type":"docs"},{"authors":null,"categories":null,"content":"   Preliminaries Code  Load and clean data Legal dual y-axes Combining plots Scatterplot matrices Correlograms Simple regression Coefficient plots Marginal effects plots    Today’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again (in addition to the Atlanta weather data). The Ames data is linked below:\n  Ames.csv   Preliminaries For this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.754557, -84.390009) in 2019. We downloaded this data using Dark Sky’s (about-to-be-retired-because-they-were-bought-by-Apple) API using the darksky package.\nIf you want to follow along with this example, you can download the data below (you’ll likely need to right click and choose “Save Link As…”):\n  atl-weather-2019.csv   Code Load and clean data First, we load the libraries we’ll be using:\nlibrary(tidyverse) # For ggplot, dplyr, and friends library(patchwork) # For combining ggplot plots library(GGally) # For scatterplot matrices library(broom) # For converting model objects to data frames Then we load the data with read_csv(). Here we assume that the CSV file lives in a subfolder named data:\nweather_atl \u0026lt;- read_csv(\u0026quot;data/atl-weather-2019.csv\u0026quot;)  Legal dual y-axes It is fine (and often helpful) to use two y-axes if the two different scales measure the same thing, like counts and percentages, Fahrenheit and Celsius, pounds and kilograms, inches and centimeters, etc.\nTo do this, you need to add an argument (sec.axis) to scale_y_continuous() to tell it to use a second axis. This sec.axis argument takes a sec_axis() function that tells ggplot how to transform the scale. You need to specify a formula or function that defines how the original axis gets transformed. This formula uses a special syntax. It needs to start with a ~, which indicates that it’s a function, and it needs to use . to stand in for the original value in the original axis.\nSince the equation for converting Fahrenheit to Celsius is this…\n\\[ \\text{C} = (32 - \\text{F}) \\times -\\frac{5}{9} \\]\n…we can specify this with code like so (where . stands for the Fahrenheit value):\n~ (32 - .) * -5 / 9 Here’s a plot of daily high temperatures in Atlanta throughout 2019, with a second axis:\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) + geom_line() + scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9, name = \u0026quot;Celsius\u0026quot;)) + labs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) + theme_minimal() For fun, we could also convert it to Kelvin, which uses this formula:\n\\[ \\text{K} = (\\text{F} - 32) \\times \\frac{5}{9} + 273.15 \\]\nggplot(weather_atl, aes(x = time, y = temperatureHigh)) + geom_line() + scale_y_continuous(sec.axis = sec_axis(trans = ~ (. - 32) * 5/9 + 273.15, name = \u0026quot;Kelvin\u0026quot;)) + labs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) + theme_minimal()  Combining plots A good alternative to using two y-axes is to use two plots instead. The patchwork package makes this really easy to do with R. There are other similar packages that do this, like cowplot and gridExtra, but I’ve found that patchwork is the easiest to use and it actually aligns the different plot elements like axis lines and legends (yay alignment in CRAP!). The documentation for patchwork is really great and full of examples—you should check it out to see all the things you can do with it!\nTo use patchwork, we need to (1) save our plots as objects and (2) add them together with +.\nFor instance, is there a relationship between temperature and humidity in Atlanta? We can plot both:\n# Temperature in Atlanta temp_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = temperatureHigh)) + geom_line() + geom_smooth() + scale_y_continuous(sec.axis = sec_axis(trans = ~ (32 - .) * -5/9, name = \u0026quot;Celsius\u0026quot;)) + labs(x = NULL, y = \u0026quot;Fahrenheit\u0026quot;) + theme_minimal() temp_plot # Humidity in Atlanta humidity_plot \u0026lt;- ggplot(weather_atl, aes(x = time, y = humidity)) + geom_line() + geom_smooth() + labs(x = NULL, y = \u0026quot;Humidity\u0026quot;) + theme_minimal() humidity_plot Right now, these are two separate plots, but we can combine them with + if we load patchwork:\nlibrary(patchwork) temp_plot + humidity_plot By default, patchwork will put these side-by-side, but we can change that with the plot_layout() function:\ntemp_plot + humidity_plot + plot_layout(ncol = 1) We can also play with other arguments in plot_layout(). If we want to make the temperature plot taller and shrink the humidity section, we can specify the proportions for the plot heights. Here, the temperature plot is 70% of the height and the humidity plot is 30%:\ntemp_plot + humidity_plot + plot_layout(ncol = 1, heights = c(0.7, 0.3))  Scatterplot matrices We can visualize the correlations between pairs of variables with the ggpairs() function in the GGally package. For instance, how correlated are high and low temperatures, humidity, wind speed, and the chance of precipitation? We first make a smaller dataset with just those columns, and then we feed that dataset into ggpairs() to see all the correlation information:\nlibrary(GGally) weather_correlations \u0026lt;- weather_atl %\u0026gt;% select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) ggpairs(weather_correlations) It looks like high and low temperatures are extremely highly positively correlated (r = 0.92). Wind spped and temperature are moderately negatively correlated, with low temperatures having a stronger negative correlation (r = -0.45). There’s no correlation whatsoever between low temperatures and the precipitation probability (r = -0.03) or humidity and high temperatures (r = -0.03).\nEven though ggpairs() doesn’t use the standard ggplot(...) + geom_whatever() syntax we’re familiar with, it does behind the scenes, so you can add regular ggplot layers to it:\nggpairs(weather_correlations) + labs(title = \u0026quot;Correlations!\u0026quot;) + theme_dark() TRY IT\nMake a ggpairs plot for some of the Ames data.\n  Correlograms Scatterplot matrices typically include way too much information to be used in actual publications. I use them when doing my own analysis just to see how different variables are related, but I rarely polish them up for public consumption. In the readings for today, Claus Wilke showed a type of plot called a correlogram which is more appropriate for publication.\nThese are essentially heatmaps of the different correlation coefficients. To make these with ggplot, we need to do a little bit of extra data processing, mostly to reshape data into a long, tidy format that we can plot. Here’s how.\nFirst we need to build a correlation matrix of the main variables we care about. Ordinarily the cor() function in R takes two arguments—x and y—and it will return a single correlation value. If you feed a data frame into cor() though, it’ll calculate the correlation between each pair of columns\n# Create a correlation matrix things_to_correlate \u0026lt;- weather_atl %\u0026gt;% select(temperatureHigh, temperatureLow, humidity, windSpeed, precipProbability) %\u0026gt;% cor() things_to_correlate ## temperatureHigh temperatureLow humidity windSpeed precipProbability ## temperatureHigh 1.00 0.920 -0.030 -0.377 -0.124 ## temperatureLow 0.92 1.000 0.112 -0.450 -0.026 ## humidity -0.03 0.112 1.000 0.011 0.722 ## windSpeed -0.38 -0.450 0.011 1.000 0.196 ## precipProbability -0.12 -0.026 0.722 0.196 1.000 The two halves of this matrix (split along the diagonal line) are identical, so we can remove the lower triangle with this code (which will set all the cells in the lower triangle to NA):\n# Get rid of the lower triangle things_to_correlate[lower.tri(things_to_correlate)] \u0026lt;- NA things_to_correlate ## temperatureHigh temperatureLow humidity windSpeed precipProbability ## temperatureHigh 1 0.92 -0.03 -0.377 -0.124 ## temperatureLow NA 1.00 0.11 -0.450 -0.026 ## humidity NA NA 1.00 0.011 0.722 ## windSpeed NA NA NA 1.000 0.196 ## precipProbability NA NA NA NA 1.000 Finally, in order to plot this, the data needs to be in tidy (or long) format. Here we convert the things_to_correlate matrix into a data frame, add a column for the row names, take all the columns and put them into a single column named measure1, and take all the correlation numbers and put them in a column named cor In the end, we make sure the measure variables are ordered by their order of appearance (otherwise they plot alphabetically and don’t make a triangle)\nthings_to_correlate_long \u0026lt;- things_to_correlate %\u0026gt;% # Convert from a matrix to a data frame as.data.frame() %\u0026gt;% # Matrixes have column names that don\u0026#39;t get converted to columns when using # as.data.frame(), so this adds those names as a column rownames_to_column(\u0026quot;measure2\u0026quot;) %\u0026gt;% # Make this long. Take all the columns except measure2 and put their names in # a column named measure1 and their values in a column named cor pivot_longer(cols = -measure2, names_to = \u0026quot;measure1\u0026quot;, values_to = \u0026quot;cor\u0026quot;) %\u0026gt;% # Make a new column with the rounded version of the correlation value mutate(nice_cor = round(cor, 2)) %\u0026gt;% # Remove rows where the two measures are the same (like the correlation # between humidity and humidity) filter(measure2 != measure1) %\u0026gt;% # Get rid of the empty triangle filter(!is.na(cor)) %\u0026gt;% # Put these categories in order mutate(measure1 = fct_inorder(measure1), measure2 = fct_inorder(measure2)) things_to_correlate_long ## # A tibble: 10 x 4 ## measure2 measure1 cor nice_cor ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 temperatureHigh temperatureLow 0.920 0.92 ## 2 temperatureHigh humidity -0.0301 -0.03 ## 3 temperatureHigh windSpeed -0.377 -0.38 ## 4 temperatureHigh precipProbability -0.124 -0.12 ## 5 temperatureLow humidity 0.112 0.11 ## 6 temperatureLow windSpeed -0.450 -0.45 ## 7 temperatureLow precipProbability -0.0255 -0.03 ## 8 humidity windSpeed 0.0108 0.01 ## 9 humidity precipProbability 0.722 0.72 ## 10 windSpeed precipProbability 0.196 0.2 Phew. With the data all tidied like that, we can make a correlogram with a heatmap. We have manipulated the fill scale a little so that it’s diverging with three colors: a high value, a midpoint value, and a low value.\nggplot(things_to_correlate_long, aes(x = measure2, y = measure1, fill = cor)) + geom_tile() + geom_text(aes(label = nice_cor)) + scale_fill_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;, limits = c(-1, 1)) + labs(x = NULL, y = NULL) + coord_equal() + theme_minimal() + theme(panel.grid = element_blank()) Instead of using a heatmap, we can also use points, which encode the correlation information both as color and as size. To do that, we just need to switch geom_tile() to geom_point() and set the size = cor mapping:\nggplot(things_to_correlate_long, aes(x = measure2, y = measure1, color = cor)) + # Size by the absolute value so that -0.7 and 0.7 are the same size geom_point(aes(size = abs(cor))) + scale_color_gradient2(low = \u0026quot;#E16462\u0026quot;, mid = \u0026quot;white\u0026quot;, high = \u0026quot;#0D0887\u0026quot;, limits = c(-1, 1)) + scale_size_area(max_size = 15, limits = c(-1, 1), guide = FALSE) + labs(x = NULL, y = NULL) + coord_equal() + theme_minimal() + theme(panel.grid = element_blank())  Simple regression We finally get to this week’s content. Although correlation is nice, we eventually may want to visualize regression. The lecture has shown us some very intuitive, straightforward ways to think about regression (aka, a line). Simple regression is easy to visualize, since you’re only working with an \\(X\\) and a \\(Y\\). For instance, what’s the relationship between humidity and high temperatures during the summer?\nFirst, let’s filter the data to only look at the summer. We also add a column to scale up the humidity value—right now it’s on a scale of 0-1 (for percentages), but when interpreting regression we talk about increases in whole units, so we’d talk about moving from 0% humidity to 100% humidity, which isn’t helpful, so instead we multiply everything by 100 so we can talk about moving from 50% humidity to 51% humidity. We also scale up a couple other variables that we’ll use in the larger model later.\nweather_atl_summer \u0026lt;- weather_atl %\u0026gt;% filter(time \u0026gt;= \u0026quot;2019-05-01\u0026quot;, time \u0026lt;= \u0026quot;2019-09-30\u0026quot;) %\u0026gt;% mutate(humidity_scaled = humidity * 100, moonPhase_scaled = moonPhase * 100, precipProbability_scaled = precipProbability * 100, cloudCover_scaled = cloudCover * 100) Then we can build a simple regression model:\nmodel_simple \u0026lt;- lm(temperatureHigh ~ humidity_scaled, data = weather_atl_summer) tidy(model_simple, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 104. 2.35 44.3 1.88e-88 99.5 109. ## 2 humidity_scaled -0.241 0.0358 -6.74 3.21e-10 -0.312 -0.170 We can interpret these coefficients like so:\n The intercept shows that the average temperature when there’s 0% humidity is 104°. There are no days with 0% humidity though, so we can ignore the intercept—it’s really just here so that we can draw the line. The coefficient for humidity_scaled shows that a one percent increase in humidity is associated with a 0.241° decrease in temperature, on average.  Visualizing this model is simple, since there are only two variables:\nggplot(weather_atl_summer, aes(x = humidity_scaled, y = temperatureHigh)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; And indeed, as humidity increases, temperatures decrease.\n Coefficient plots But if we use multiple variables in the model (and we will do this a lot going forward), it gets really hard to visualize the results since we’re working with multiple dimensions. Instead, we can use coefficient plots to see the individual coefficients in the model.\nFirst, let’s build a more complex model:\nmodel_complex \u0026lt;- lm(temperatureHigh ~ humidity_scaled + moonPhase_scaled + precipProbability_scaled + windSpeed + pressure + cloudCover_scaled, data = weather_atl_summer) tidy(model_complex, conf.int = TRUE) ## # A tibble: 7 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 262. 125. 2.09 0.0380 14.8 510. ## 2 humidity_scaled -0.111 0.0757 -1.47 0.143 -0.261 0.0381 ## 3 moonPhase_scaled 0.0116 0.0126 0.917 0.360 -0.0134 0.0366 ## 4 precipProbability_scaled 0.0356 0.0203 1.75 0.0820 -0.00458 0.0758 ## 5 windSpeed -1.78 0.414 -4.29 0.0000326 -2.59 -0.958 ## 6 pressure -0.157 0.122 -1.28 0.203 -0.398 0.0854 ## 7 cloudCover_scaled -0.0952 0.0304 -3.14 0.00207 -0.155 -0.0352 We can interpret these coefficients like so:\n Holding everything else constant, a 1% increase in humidity is associated with a 0.11° decrease in the high temperature, on average, but the effect is not statistically significant Holding everything else constant, a 1% increase in moon visibility is associated with a 0.01° increase in the high temperature, on average, and the effect is not statistically significant Holding everything else constant, a 1% increase in the probability of precipitation is associated with a 0.04° increase in the high temperature, on average, and the effect is not statistically significant Holding everything else constant, a 1 mph increase in the wind speed is associated with a 1.8° decrease in the high temperature, on average, and the effect is statistically significant Holding everything else constant, a 1 unit increase in barometric pressure is associated with a 0.15° decrease in the high temperature, on average, and the effect is not statistically significant Holding everything else constant, a 1% increase in cloud cover is associated with a 0.01° decrease in the high temperature, on average, and the effect is statistically significant The intercept is pretty useless. It shows that the predicted temperature will be 262° when humidity is 0%, the moon is invisible, there’s no chance of precipitation, no wind, no barometric pressure, and no cloud cover. Yikes.  To plot all these things at once, we’ll store the results of tidy(model_complex) as a data frame, remove the useless intercept, and plot it using geom_pointrange():\nmodel_tidied \u0026lt;- tidy(model_complex, conf.int = TRUE) %\u0026gt;% filter(term != \u0026quot;(Intercept)\u0026quot;) ggplot(model_tidied, aes(x = estimate, y = term)) + geom_vline(xintercept = 0, color = \u0026quot;red\u0026quot;, linetype = \u0026quot;dotted\u0026quot;) + geom_pointrange(aes(xmin = conf.low, xmax = conf.high)) + labs(x = \u0026quot;Coefficient estimate\u0026quot;, y = NULL) + theme_minimal() Neat! Now we can see how big these different coefficients are and how close they are to zero. Wind speed has a big significant effect on temperature. The others are all very close to zero.\n Marginal effects plots Instead of just looking at the coefficients, we can also see the effect of moving different variables up and down like sliders and switches. Remember that regression coefficients allow us to build actual mathematical formulas that predict the value of Y. The coefficients from model_compex yield the following big hairy ugly equation:\n\\[ \\begin{aligned} \\hat{\\text{High temperature}} =\u0026amp; 262 - 0.11 \\times \\text{humidity_scaled } \\\\ \u0026amp; + 0.01 \\times \\text{moonPhase_scaled } + 0.04 \\times \\text{precipProbability_scaled } \\\\ \u0026amp; - 1.78 \\times \\text{windSpeed} - 0.16 \\times \\text{pressure} - 0.095 \\times \\text{cloudCover_scaled} \\end{aligned} \\]\nIf we plug in values for each of the explanatory variables, we can get the predicted value of high temperature, or \\(\\hat{y}\\).\nThe augment() function in the broom library allows us to take a data frame of explanatory variable values, plug them into the model equation, and get predictions out. For example, let’s set each of the variables to some arbitrary values (50% for humidity, moon phase, chance of rain, and cloud cover; 1000 for pressure, and 1 MPH for wind speed).\nnewdata_example \u0026lt;- tibble(humidity_scaled = 50, moonPhase_scaled = 50, precipProbability_scaled = 50, windSpeed = 1, pressure = 1000, cloudCover_scaled = 50) newdata_example ## # A tibble: 1 x 6 ## humidity_scaled moonPhase_scaled precipProbability_scaled windSpeed pressure cloudCover_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 50 50 50 1 1000 50 We can plug these values into the model with augment():\n# I use select() here because augment() returns columns for all the explanatory # variables, and the .fitted column with the predicted value is on the far right # and gets cut off augment(model_complex, newdata = newdata_example, se_fit=TRUE) %\u0026gt;% select(.fitted, .se.fit) Given these weather conditions, the predicted high temperature is 96.2°. Now you’re an armchair meteorologist!\nWe can follow the same pattern to show how the predicted temperature changes as specific variables change across a whole range. Here, we create a data frame of possible wind speeds and keep all the other explanatory variables at their means:\nnewdata \u0026lt;- tibble(windSpeed = seq(0, 8, 0.5), pressure = mean(weather_atl_summer$pressure), precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled), moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled), humidity_scaled = mean(weather_atl_summer$humidity_scaled), cloudCover_scaled = mean(weather_atl_summer$cloudCover_scaled)) newdata ## # A tibble: 17 x 6 ## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 1016. 40.2 50.7 64.8 29.5 ## 2 0.5 1016. 40.2 50.7 64.8 29.5 ## 3 1 1016. 40.2 50.7 64.8 29.5 ## 4 1.5 1016. 40.2 50.7 64.8 29.5 ## 5 2 1016. 40.2 50.7 64.8 29.5 ## 6 2.5 1016. 40.2 50.7 64.8 29.5 ## 7 3 1016. 40.2 50.7 64.8 29.5 ## 8 3.5 1016. 40.2 50.7 64.8 29.5 ## 9 4 1016. 40.2 50.7 64.8 29.5 ## 10 4.5 1016. 40.2 50.7 64.8 29.5 ## 11 5 1016. 40.2 50.7 64.8 29.5 ## 12 5.5 1016. 40.2 50.7 64.8 29.5 ## 13 6 1016. 40.2 50.7 64.8 29.5 ## 14 6.5 1016. 40.2 50.7 64.8 29.5 ## 15 7 1016. 40.2 50.7 64.8 29.5 ## 16 7.5 1016. 40.2 50.7 64.8 29.5 ## 17 8 1016. 40.2 50.7 64.8 29.5 If we feed this big data frame into augment(), we can get the predicted high temperature for each row. We can also use the .se.fit column to calculate the 95% confidence interval for each predicted value. We take the standard error, multiply it by -1.96 and 1.96 (or the quantile of the normal distribution at 2.5% and 97.5%), and add that value to the estimate.\npredicted_values \u0026lt;- augment(model_complex, newdata = newdata, se_fit=TRUE) %\u0026gt;% mutate(conf.low = .fitted + (-1.96 * .se.fit), conf.high = .fitted + (1.96 * .se.fit)) predicted_values %\u0026gt;% select(windSpeed, .fitted, .se.fit, conf.low, conf.high) %\u0026gt;% head() ## # A tibble: 6 x 5 ## windSpeed .fitted .se.fit conf.low conf.high ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 95.3 1.63 92.2 98.5 ## 2 0.5 94.5 1.42 91.7 97.2 ## 3 1 93.6 1.22 91.2 96.0 ## 4 1.5 92.7 1.03 90.7 94.7 ## 5 2 91.8 0.836 90.1 93.4 ## 6 2.5 90.9 0.653 89.6 92.2 Cool! Just looking at the data in the table, we can see that predicted temperature drops as windspeed increases. We can plot this to visualize the effect:\nggplot(predicted_values, aes(x = windSpeed, y = .fitted)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \u0026quot;#BF3984\u0026quot;, alpha = 0.5) + geom_line(size = 1, color = \u0026quot;#BF3984\u0026quot;) + labs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) + theme_minimal() We just manipulated one of the model coefficients and held everything else at its mean. We can manipulate multiple variables too and encode them all on the graph. For instance, what is the effect of windspeed and cloud cover on the temperature?\nWe’ll follow the same process, but vary both windSpeed and cloudCover_scaled. Instead of using tibble(), we use exapnd_grid(), which creates every combination of the variables we specify. Instead of varying cloud cover by every possible value (like from 0 to 100), we’ll choose four possible cloud cover types: 0%, 33%, 66%, and 100%. Everything else will be at its mean.\nnewdata_fancy \u0026lt;- expand_grid(windSpeed = seq(0, 8, 0.5), pressure = mean(weather_atl_summer$pressure), precipProbability_scaled = mean(weather_atl_summer$precipProbability_scaled), moonPhase_scaled = mean(weather_atl_summer$moonPhase_scaled), humidity_scaled = mean(weather_atl_summer$humidity_scaled), cloudCover_scaled = c(0, 33, 66, 100)) newdata_fancy ## # A tibble: 68 x 6 ## windSpeed pressure precipProbability_scaled moonPhase_scaled humidity_scaled cloudCover_scaled ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 1016. 40.2 50.7 64.8 0 ## 2 0 1016. 40.2 50.7 64.8 33 ## 3 0 1016. 40.2 50.7 64.8 66 ## 4 0 1016. 40.2 50.7 64.8 100 ## 5 0.5 1016. 40.2 50.7 64.8 0 ## 6 0.5 1016. 40.2 50.7 64.8 33 ## 7 0.5 1016. 40.2 50.7 64.8 66 ## 8 0.5 1016. 40.2 50.7 64.8 100 ## 9 1 1016. 40.2 50.7 64.8 0 ## 10 1 1016. 40.2 50.7 64.8 33 ## # … with 58 more rows Notice now that windSpeed repeats four times (0, 0, 0, 0, 0.5, 0.5, etc.), since there are four possible cloudCover_scaled values (0, 33, 66, 100).\nWe can plot this now, just like before, with wind speed on the x-axis, the predicted temperature on the y-axis, and colored and faceted by cloud cover:\npredicted_values_fancy \u0026lt;- augment(model_complex, newdata = newdata_fancy, se_fit=TRUE) %\u0026gt;% mutate(conf.low = .fitted + (-1.96 * .se.fit), conf.high = .fitted + (1.96 * .se.fit)) %\u0026gt;% # Make cloud cover a categorical variable mutate(cloudCover_scaled = factor(cloudCover_scaled)) ggplot(predicted_values_fancy, aes(x = windSpeed, y = .fitted)) + geom_ribbon(aes(ymin = conf.low, ymax = conf.high, fill = cloudCover_scaled), alpha = 0.5) + geom_line(aes(color = cloudCover_scaled), size = 1) + labs(x = \u0026quot;Wind speed (MPH)\u0026quot;, y = \u0026quot;Predicted high temperature (F)\u0026quot;) + theme_minimal() + guides(fill = FALSE, color = FALSE) + facet_wrap(vars(cloudCover_scaled), nrow = 1) Nice. Temperatures go down slightly as cloud cover increases. If we wanted to improve the model, we’d add an interaction term between cloud cover and windspeed so that each line would have a different slope in addition to a different intercept, but that’s beyond the scope of this class.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601930279,"objectID":"ac11eb4b300887ac6e4fc5c1906670ba","permalink":"/example/05-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/05-example/","section":"example","summary":"Preliminaries Code  Load and clean data Legal dual y-axes Combining plots Scatterplot matrices Correlograms Simple regression Coefficient plots Marginal effects plots    Today’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again (in addition to the Atlanta weather data). The Ames data is linked below:\n  Ames.csv   Preliminaries For this example, we’re again going to use historical weather data from Dark Sky about wind speed and temperature trends for downtown Atlanta (specifically 33.","tags":null,"title":"Introduction to Regression","type":"docs"},{"authors":null,"categories":null,"content":"  Backstory and Set Up Building a Model   NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 19.\n This week’s lab will extend last week’s lab. The introduction is a direct repeat.\nBackstory and Set Up You have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data. Your job is to present some basic predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nFirst, let’s load the data.\nameslist \u0026lt;- read.table(\u0026quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;)  Building a Model We’re now ready to start playing with a model. We will start by using the lm() function to fit a simple linear regression model, with SalePrice as the response and lstat as the predictor.\nRecall that the basic lm() syntax is lm(y∼x,data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept. Let’s quickly run this with two variables:\nlm.fit = lm(SalePrice ~ GrLivArea) This yields: Error in eval(expr, envir, enclos) : Object \"SalePrice\" not found\nThis command causes an error because R does not know where to find the variables. We can fix this by attaching the data:\nattach(Ames) lm.fit = lm(SalePrice ~ GrLivArea) # Alternatively... lm.fit = lm(SalePrice ~ GrLivArea, data=Ames) The next line tells R that the variables are in the object known as Ames. If you haven’t created this object yet (as in the previous lab) you’ll get an error at this stage. But once we attach Ames, the first line works fine because R now recognizes the variables. Alternatively, we could specify this within the lm() call using data = Ames. We’ve presented this way because it may be new to you; choose whichever you find most reasonable.\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us p-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the entire model.1\nUtilizing these functions hels us see some interesting results. Note that we built (nearly) the simplest possible model:\n\\[\\text{SalePrice} = \\beta_0 + \\beta_1*(\\text{GrLivArea}) + \\epsilon.\\]\nBut even on its own, this model is instructive. It suggest that an increase in overall living area of 1 ft \\(^2\\) is correlated with an expected increase in sales price of $107. (Note that we cannot make causal claims!)\nSaving the model as we did above is useful because we can explore other pieces of information it stores. Specifically, we can use the names() function in order to find out what else is stored in lm.fit. Although we can extract these quan- tities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them. We can also use a handy tool like plot() applied directly to lm.fit to see some interesting data that is automatically stored by the model.\nTry it: Use plot() to explore the model above. Do you suspect that some outliers have a large influence on the data? We will explore this point specifically in the future.\nWe can now go crazy adding variables to our model. It’s as simple as appending them to the previous code—though you should be careful executing this, as it will overwrite your previous output:\nlm.fit = lm(SalePrice ~ GrLivArea + LotArea) Try it: Does controlling for LotArea change the qualitative conclusions from the previous regression? What about the quantitative results? Does the direction of the change in the quantitative results make sense to you?\nEXERCISES\nUse the lm() function in a simple linear regression (e.g., with only one predictor) with SalePrice as the response to determine the value of a garage.\n Use the lm() function to perform a multiple linear regression with SalePrice as the response and all other variables from your Ames data as the predictors. Use the summary() function to print the results. Comment on the output. For instance:\n Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? (Hint: look for stars) What does the coefficient for the year variable suggest?  Use the : symbols to fit a linear regression model with one well-chosen interaction effects. Why did you do this?\n Try a few (e.g., two) different transformations of the variables, such as \\(ln(x)\\), \\(x^2\\), \\(\\sqrt x\\). Do any of these make sense to include in a model of SalePrice? Comment on your findings.\n  (Bonus; very very challenging) How might we build a model to estimate the elasticity of demand from this dataset?\n   When we use the simple regression model with a single input, the \\(F\\)-stat includes the intercept term. Otherwise, it does not. See Lecture 5 for more detail.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604325415,"objectID":"e1ed0bbd55230066a650c8b68228a273","permalink":"/assignment/06-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/06-assignment/","section":"assignment","summary":"Backstory and Set Up Building a Model   NOTE\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 19.\n This week’s lab will extend last week’s lab. The introduction is a direct repeat.\nBackstory and Set Up You have been recently hired to Zillow’s Zestimate product team as a junior analyst. As a part of their regular hazing, they have given you access to a small subset of their historic sales data.","tags":null,"title":"Model Building","type":"docs"},{"authors":null,"categories":null,"content":"  Preliminaries Dummy Variables Interactions Factor Variables Factors with More Than Two Levels  Parameterization Building Larger Models   Today’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n  Ames.csv   Preliminaries So far in each of our analyses, we have only used numeric variables as predictors. We have also only used additive models, meaning the effect any predictor had on the response was not dependent on the other predictors. In this chapter, we will remove both of these restrictions. We will fit models with categorical predictors, and use models that allow predictors to interact. The mathematics of multiple regression will remain largely unchanging, however, we will pay close attention to interpretation, as well as some difference in R usage.\n Dummy Variables For this example and discussion, we will briefly use the built in dataset mtcars before returning to our favorite autompg dataset. During the in-class lecture / example, I will also use much more interesting datasets. The reason to use these easy, straightforward datasets is that they make visualization of the entire dataset trivially easy. Accordingly, the mtcars dataset is small, so we’ll quickly take a look at the entire dataset.\nmtcars ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 We will be interested in three of the variables: mpg, hp, and am.\n mpg: fuel efficiency, in miles per gallon. hp: horsepower, in foot-pounds per second. am: transmission. Automatic or manual.  As we often do, we will start by plotting the data. We are interested in mpg as the response variable, and hp as a predictor.\nplot(mpg ~ hp, data = mtcars, cex = 2) Since we are also interested in the transmission type, we could also label the points accordingly.\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2) legend(\u0026quot;topright\u0026quot;, c(\u0026quot;Automatic\u0026quot;, \u0026quot;Manual\u0026quot;), col = c(1, 2), pch = c(1, 2)) We now fit the SLR model\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\epsilon, \\]\nwhere \\(Y\\) is mpg and \\(x_1\\) is hp. For notational brevity, we drop the index \\(i\\) for observations.\nmpg_hp_slr = lm(mpg ~ hp, data = mtcars) We then re-plot the data and add the fitted line to the plot.\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2) abline(mpg_hp_slr, lwd = 3, col = \u0026quot;grey\u0026quot;) legend(\u0026quot;topright\u0026quot;, c(\u0026quot;Automatic\u0026quot;, \u0026quot;Manual\u0026quot;), col = c(1, 2), pch = c(1, 2)) We should notice a pattern here. The red, manual observations largely fall above the line, while the black, automatic observations are mostly below the line. This means our model underestimates the fuel efficiency of manual transmissions, and overestimates the fuel efficiency of automatic transmissions. To correct for this, we will add a predictor to our model, namely, am as \\(x_2\\).\nOur new model is\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon, \\]\nwhere \\(x_1\\) and \\(Y\\) remain the same, but now\n\\[ x_2 = \\begin{cases} 1 \u0026amp; \\text{manual transmission} \\\\ 0 \u0026amp; \\text{automatic transmission} \\end{cases}. \\]\nIn this case, we call \\(x_2\\) a dummy variable. A dummy variable (also called an “indicator” variable) is somewhat unfortunately named, as it is in no way “dumb”. In fact, it is actually somewhat clever. A dummy variable is a numerical variable that is used in a regression analysis to “code” for a binary categorical variable. Let’s see how this works.\nFirst, note that am is already a dummy variable, since it uses the values 0 and 1 to represent automatic and manual transmissions. Often, a variable like am would store the character values auto and man and we would either have to convert these to 0 and 1, or, as we will see later, R will take care of creating dummy variables for us.\nSo, to fit the above model, we do so like any other multiple regression model we have seen before.\nmpg_hp_add = lm(mpg ~ hp + am, data = mtcars) Briefly checking the output, we see that R has estimated the three \\(\\beta\\) parameters.\nmpg_hp_add ## ## Call: ## lm(formula = mpg ~ hp + am, data = mtcars) ## ## Coefficients: ## (Intercept) hp am ## 26.58491 -0.05889 5.27709 Since \\(x_2\\) can only take values 0 and 1, we can effectively write two different models, one for manual and one for automatic transmissions.\nFor automatic transmissions, that is \\(x_2 = 0\\), we have,\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\epsilon. \\]\nThen for manual transmissions, that is \\(x_2 = 1\\), we have,\n\\[ Y = (\\beta_0 + \\beta_2) + \\beta_1 x_1 + \\epsilon. \\]\nNotice that these models share the same slope, \\(\\beta_1\\), but have different intercepts, differing by \\(\\beta_2\\). So the change in mpg is the same for both models, but on average mpg differs by \\(\\beta_2\\) between the two transmission types.\nWe’ll now calculate the estimated slope and intercept of these two models so that we can add them to a plot. Note that:\n \\(\\hat{\\beta}_0\\) = coef(mpg_hp_add)[1] = 26.5849137 \\(\\hat{\\beta}_1\\) = coef(mpg_hp_add)[2] = -0.0588878 \\(\\hat{\\beta}_2\\) = coef(mpg_hp_add)[3] = 5.2770853  We can then combine these to calculate the estimated slope and intercepts.\nint_auto = coef(mpg_hp_add)[1] int_manu = coef(mpg_hp_add)[1] + coef(mpg_hp_add)[3] slope_auto = coef(mpg_hp_add)[2] slope_manu = coef(mpg_hp_add)[2] Re-plotting the data, we use these slopes and intercepts to add the “two” fitted models to the plot.\nplot(mpg ~ hp, data = mtcars, col = am + 1, pch = am + 1, cex = 2) abline(int_auto, slope_auto, col = 1, lty = 1, lwd = 2) # add line for auto abline(int_manu, slope_manu, col = 2, lty = 2, lwd = 2) # add line for manual legend(\u0026quot;topright\u0026quot;, c(\u0026quot;Automatic\u0026quot;, \u0026quot;Manual\u0026quot;), col = c(1, 2), pch = c(1, 2)) We notice right away that the points are no longer systematically incorrect. The red, manual observations vary about the red line in no particular pattern without underestimating the observations as before. The black, automatic points vary about the black line, also without an obvious pattern.\nThey say a picture is worth a thousand words, but as a statistician, sometimes a picture is worth an entire analysis. The above picture makes it plainly obvious that \\(\\beta_2\\) is significant, but let’s verify mathematically. Essentially we would like to test:\n\\[ H_0: \\beta_2 = 0 \\quad \\text{vs} \\quad H_1: \\beta_2 \\neq 0. \\]\nThis is nothing new. Again, the math is the same as the multiple regression analyses we have seen before. We could perform either a \\(t\\) or \\(F\\) test here. The only difference is a slight change in interpretation. We could think of this as testing a model with a single line (\\(H_0\\)) against a model that allows two lines (\\(H_1\\)).\nTo obtain the test statistic and p-value for the \\(t\\)-test, we would use\nsummary(mpg_hp_add)$coefficients[\u0026quot;am\u0026quot;,] ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## 5.277085e+00 1.079541e+00 4.888270e+00 3.460318e-05 To do the same for the \\(F\\) test, we would use\nanova(mpg_hp_slr, mpg_hp_add) ## Analysis of Variance Table ## ## Model 1: mpg ~ hp ## Model 2: mpg ~ hp + am ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 30 447.67 ## 2 29 245.44 1 202.24 23.895 3.46e-05 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Notice that these are indeed testing the same thing, as the p-values are exactly equal. (And the \\(F\\) test statistic is the \\(t\\) test statistic squared.)\nRecapping some interpretations:\n \\(\\hat{\\beta}_0 = 26.5849137\\) is the estimated average mpg for a car with an automatic transmission and 0 hp.\n \\(\\hat{\\beta}_0 + \\hat{\\beta}_2 = 31.8619991\\) is the estimated average mpg for a car with a manual transmission and 0 hp.\n \\(\\hat{\\beta}_2 = 5.2770853\\) is the estimated difference in average mpg for cars with manual transmissions as compared to those with automatic transmission, for any hp.\n \\(\\hat{\\beta}_1 = -0.0588878\\) is the estimated change in average mpg for an increase in one hp, for either transmission types.\n  We should take special notice of those last two. In the model,\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon, \\]\nwe see \\(\\beta_1\\) is the average change in \\(Y\\) for an increase in \\(x_1\\), no matter the value of \\(x_2\\). Also, \\(\\beta_2\\) is always the difference in the average of \\(Y\\) for any value of \\(x_1\\). These are two restrictions we won’t always want, so we need a way to specify a more flexible model.\nHere we restricted ourselves to a single numerical predictor \\(x_1\\) and one dummy variable \\(x_2\\). However, the concept of a dummy variable can be used with larger multiple regression models. We only use a single numerical predictor here for ease of visualization since we can think of the “two lines” interpretation. But in general, we can think of a dummy variable as creating “two models,” one for each category of a binary categorical variable.\n Interactions To remove the “same slope” restriction, we will now discuss interaction. To illustrate this concept, we will return to the autompg dataset we created in the last chapter, with a few more modifications.\n# read data frame from the web autompg = read.table( \u0026quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\u0026quot;, quote = \u0026quot;\\\u0026quot;\u0026quot;, comment.char = \u0026quot;\u0026quot;, stringsAsFactors = FALSE) # give the dataframe headers colnames(autompg) = c(\u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;acc\u0026quot;, \u0026quot;year\u0026quot;, \u0026quot;origin\u0026quot;, \u0026quot;name\u0026quot;) # remove missing data, which is stored as \u0026quot;?\u0026quot; autompg = subset(autompg, autompg$hp != \u0026quot;?\u0026quot;) # remove the plymouth reliant, as it causes some issues autompg = subset(autompg, autompg$name != \u0026quot;plymouth reliant\u0026quot;) # give the dataset row names, based on the engine, year and name rownames(autompg) = paste(autompg$cyl, \u0026quot;cylinder\u0026quot;, autompg$year, autompg$name) # remove the variable for name autompg = subset(autompg, select = c(\u0026quot;mpg\u0026quot;, \u0026quot;cyl\u0026quot;, \u0026quot;disp\u0026quot;, \u0026quot;hp\u0026quot;, \u0026quot;wt\u0026quot;, \u0026quot;acc\u0026quot;, \u0026quot;year\u0026quot;, \u0026quot;origin\u0026quot;)) # change horsepower from character to numeric autompg$hp = as.numeric(autompg$hp) # create a dummary variable for foreign vs domestic cars. domestic = 1. autompg$domestic = as.numeric(autompg$origin == 1) # remove 3 and 5 cylinder cars (which are very rare.) autompg = autompg[autompg$cyl != 5,] autompg = autompg[autompg$cyl != 3,] # the following line would verify the remaining cylinder possibilities are 4, 6, 8 #unique(autompg$cyl) # change cyl to a factor variable autompg$cyl = as.factor(autompg$cyl) str(autompg) ## \u0026#39;data.frame\u0026#39;: 383 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : Factor w/ 3 levels \u0026quot;4\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;8\u0026quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ disp : num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num 3504 3693 3436 3433 3449 ... ## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : int 1 1 1 1 1 1 1 1 1 1 ... ## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ... We’ve removed cars with 3 and 5 cylinders , as well as created a new variable domestic which indicates whether or not a car was built in the United States. Removing the 3 and 5 cylinders is simply for ease of demonstration later in the chapter and would not be done in practice. The new variable domestic takes the value 1 if the car was built in the United States, and 0 otherwise, which we will refer to as “foreign.” (We are arbitrarily using the United States as the reference point here.) We have also made cyl and origin into factor variables, which we will discuss later.\nWe’ll now be concerned with three variables: mpg, disp, and domestic. We will use mpg as the response. We can fit a model,\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon, \\]\nwhere\n \\(Y\\) is mpg, the fuel efficiency in miles per gallon, \\(x_1\\) is disp, the displacement in cubic inches, \\(x_2\\) is domestic as described above, which is a dummy variable.  \\[ x_2 = \\begin{cases} 1 \u0026amp; \\text{Domestic} \\\\ 0 \u0026amp; \\text{Foreign} \\end{cases} \\]\nWe will fit this model, extract the slope and intercept for the “two lines,” plot the data and add the lines.\nmpg_disp_add = lm(mpg ~ disp + domestic, data = autompg) int_for = coef(mpg_disp_add)[1] int_dom = coef(mpg_disp_add)[1] + coef(mpg_disp_add)[3] slope_for = coef(mpg_disp_add)[2] slope_dom = coef(mpg_disp_add)[2] plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1) abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # add line for foreign cars abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # add line for domestic cars legend(\u0026quot;topright\u0026quot;, c(\u0026quot;Foreign\u0026quot;, \u0026quot;Domestic\u0026quot;), pch = c(1, 2), col = c(1, 2)) This is a model that allows for two parallel lines, meaning the mpg can be different on average between foreign and domestic cars of the same engine displacement, but the change in average mpg for an increase in displacement is the same for both. We can see this model isn’t doing very well here. The red line fits the red points fairly well, but the black line isn’t doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes.\nConsider the following model,\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon, \\]\nwhere \\(x_1\\), \\(x_2\\), and \\(Y\\) are the same as before, but we have added a new interaction term \\(x_1 x_2\\) which multiplies \\(x_1\\) and \\(x_2\\), so we also have an additional \\(\\beta\\) parameter \\(\\beta_3\\).\nThis model essentially creates two slopes and two intercepts, \\(\\beta_2\\) being the difference in intercepts and \\(\\beta_3\\) being the difference in slopes. To see this, we will break down the model into the two “sub-models” for foreign and domestic cars.\nFor foreign cars, that is \\(x_2 = 0\\), we have\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\epsilon. \\]\nFor domestic cars, that is \\(x_2 = 1\\), we have\n\\[ Y = (\\beta_0 + \\beta_2) + (\\beta_1 + \\beta_3) x_1 + \\epsilon. \\]\nThese two models have both different slopes and intercepts.\n \\(\\beta_0\\) is the average mpg for a foreign car with 0 disp. \\(\\beta_1\\) is the change in average mpg for an increase of one disp, for foreign cars. \\(\\beta_0 + \\beta_2\\) is the average mpg for a domestic car with 0 disp. \\(\\beta_1 + \\beta_3\\) is the change in average mpg for an increase of one disp, for domestic cars.  How do we fit this model in R? There are a number of ways.\nOne method would be to simply create a new variable, then fit a model like any other.\nautompg$x3 = autompg$disp * autompg$domestic # THIS CODE NOT RUN! do_not_do_this = lm(mpg ~ disp + domestic + x3, data = autompg) # THIS CODE NOT RUN! You should only do this as a last resort. We greatly prefer not to have to modify our data simply to fit a model. Instead, we can tell R we would like to use the existing data with an interaction term, which it will create automatically when we use the : operator.\nmpg_disp_int = lm(mpg ~ disp + domestic + disp:domestic, data = autompg) An alternative method, which will fit the exact same model as above would be to use the * operator. This method automatically creates the interaction term, as well as any “lower order terms,” which in this case are the first order terms for disp and domestic\nmpg_disp_int2 = lm(mpg ~ disp * domestic, data = autompg) We can quickly verify that these are doing the same thing.\ncoef(mpg_disp_int) ## (Intercept) disp domestic disp:domestic ## 46.0548423 -0.1569239 -12.5754714 0.1025184 coef(mpg_disp_int2) ## (Intercept) disp domestic disp:domestic ## 46.0548423 -0.1569239 -12.5754714 0.1025184 We see that both the variables, and their coefficient estimates are indeed the same for both models.\nsummary(mpg_disp_int) ## ## Call: ## lm(formula = mpg ~ disp + domestic + disp:domestic, data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8332 -2.8956 -0.8332 2.2828 18.7749 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 46.05484 1.80582 25.504 \u0026lt; 2e-16 *** ## disp -0.15692 0.01668 -9.407 \u0026lt; 2e-16 *** ## domestic -12.57547 1.95644 -6.428 3.90e-10 *** ## disp:domestic 0.10252 0.01692 6.060 3.29e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 4.308 on 379 degrees of freedom ## Multiple R-squared: 0.7011, Adjusted R-squared: 0.6987 ## F-statistic: 296.3 on 3 and 379 DF, p-value: \u0026lt; 2.2e-16 We see that using summary() gives the usual output for a multiple regression model. We pay close attention to the row for disp:domestic which tests,\n\\[ H_0: \\beta_3 = 0. \\]\nIn this case, testing for \\(\\beta_3 = 0\\) is testing for two lines with parallel slopes versus two lines with possibly different slopes. The disp:domestic line in the summary() output uses a \\(t\\)-test to perform the test.\nWe could also use an ANOVA \\(F\\)-test. The additive model, without interaction is our null model, and the interaction model is the alternative.\nanova(mpg_disp_add, mpg_disp_int) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp + domestic ## Model 2: mpg ~ disp + domestic + disp:domestic ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 380 7714.0 ## 2 379 7032.6 1 681.36 36.719 3.294e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Again we see this test has the same p-value as the \\(t\\)-test. Also the p-value is extremely low, so between the two, we choose the interaction model.\nint_for = coef(mpg_disp_int)[1] int_dom = coef(mpg_disp_int)[1] + coef(mpg_disp_int)[3] slope_for = coef(mpg_disp_int)[2] slope_dom = coef(mpg_disp_int)[2] + coef(mpg_disp_int)[4] Here we again calculate the slope and intercepts for the two lines for use in plotting.\nplot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1) abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # line for foreign cars abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # line for domestic cars legend(\u0026quot;topright\u0026quot;, c(\u0026quot;Foreign\u0026quot;, \u0026quot;Domestic\u0026quot;), pch = c(1, 2), col = c(1, 2)) We see that these lines fit the data much better, which matches the result of our tests.\nSo far we have only seen interaction between a categorical variable (domestic) and a numerical variable (disp). While this is easy to visualize, since it allows for different slopes for two lines, it is not the only type of interaction we can use in a model. We can also consider interactions between two numerical variables.\nConsider the model,\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon, \\]\nwhere\n \\(Y\\) is mpg, the fuel efficiency in miles per gallon, \\(x_1\\) is disp, the displacement in cubic inches, \\(x_2\\) is hp, the horsepower, in foot-pounds per second.  How does mpg change based on disp in this model? We can rearrange some terms to see how.\n\\[ Y = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon \\]\nSo, for a one unit increase in \\(x_1\\) (disp), the mean of \\(Y\\) (mpg) increases \\(\\beta_1 + \\beta_3 x_2\\), which is a different value depending on the value of \\(x_2\\) (hp)!\nSince we’re now working in three dimensions, this model can’t be easily justified via visualizations like the previous example. Instead, we will have to rely on a test.\nmpg_disp_add_hp = lm(mpg ~ disp + hp, data = autompg) mpg_disp_int_hp = lm(mpg ~ disp * hp, data = autompg) summary(mpg_disp_int_hp) ## ## Call: ## lm(formula = mpg ~ disp * hp, data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.7849 -2.3104 -0.5699 2.1453 17.9211 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.241e+01 1.523e+00 34.42 \u0026lt;2e-16 *** ## disp -1.002e-01 6.638e-03 -15.09 \u0026lt;2e-16 *** ## hp -2.198e-01 1.987e-02 -11.06 \u0026lt;2e-16 *** ## disp:hp 5.658e-04 5.165e-05 10.96 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 3.896 on 379 degrees of freedom ## Multiple R-squared: 0.7554, Adjusted R-squared: 0.7535 ## F-statistic: 390.2 on 3 and 379 DF, p-value: \u0026lt; 2.2e-16 Using summary() we focus on the row for disp:hp which tests,\n\\[ H_0: \\beta_3 = 0. \\]\nAgain, we see a very low p-value so we reject the null (additive model) in favor of the interaction model. Again, there is an equivalent \\(F\\)-test.\nanova(mpg_disp_add_hp, mpg_disp_int_hp) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp + hp ## Model 2: mpg ~ disp * hp ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 380 7576.6 ## 2 379 5754.2 1 1822.3 120.03 \u0026lt; 2.2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 We can take a closer look at the coefficients of our fitted interaction model.\ncoef(mpg_disp_int_hp) ## (Intercept) disp hp disp:hp ## 52.4081997848 -0.1001737655 -0.2198199720 0.0005658269  \\(\\hat{\\beta}_0 = 52.4081998\\) is the estimated average mpg for a car with 0 disp and 0 hp. \\(\\hat{\\beta}_1 = -0.1001738\\) is the estimated change in average mpg for an increase in 1 disp, for a car with 0 hp. \\(\\hat{\\beta}_2 = -0.21982\\) is the estimated change in average mpg for an increase in 1 hp, for a car with 0 disp. \\(\\hat{\\beta}_3 = 5.658269\\times 10^{-4}\\) is an estimate of the modification to the change in average mpg for an increase in disp, for a car of a certain hp (or vice versa).  That last coefficient needs further explanation. Recall the rearrangement we made earlier\n\\[ Y = \\beta_0 + (\\beta_1 + \\beta_3 x_2) x_1 + \\beta_2 x_2 + \\epsilon. \\]\nSo, our estimate for \\(\\beta_1 + \\beta_3 x_2\\), is \\(\\hat{\\beta}_1 + \\hat{\\beta}_3 x_2\\), which in this case is\n\\[ -0.1001738 + 5.658269\\times 10^{-4} x_2. \\]\nThis says that, for an increase of one disp we see an estimated change in average mpg of \\(-0.1001738 + 5.658269\\times 10^{-4} x_2\\). So how disp and mpg are related, depends on the hp of the car.\nSo for a car with 50 hp, the estimated change in average mpg for an increase of one disp is\n\\[ -0.1001738 + 5.658269\\times 10^{-4} \\cdot 50 = -0.0718824 \\]\nAnd for a car with 350 hp, the estimated change in average mpg for an increase of one disp is\n\\[ -0.1001738 + 5.658269\\times 10^{-4} \\cdot 350 = 0.0978657 \\]\nNotice the sign changed!\n Factor Variables So far in this chapter, we have limited our use of categorical variables to binary categorical variables. Specifically, we have limited ourselves to dummy variables which take a value of 0 or 1 and represent a categorical variable numerically.\nWe will now discuss factor variables, which is a special way that R deals with categorical variables. With factor variables, a human user can simply think about the categories of a variable, and R will take care of the necessary dummy variables without any 0/1 assignment being done by the user.\nis.factor(autompg$domestic) ## [1] FALSE Earlier when we used the domestic variable, it was not a factor variable. It was simply a numerical variable that only took two possible values, 1 for domestic, and 0 for foreign. Let’s create a new variable origin that stores the same information, but in a different way.\nautompg$origin[autompg$domestic == 1] = \u0026quot;domestic\u0026quot; autompg$origin[autompg$domestic == 0] = \u0026quot;foreign\u0026quot; head(autompg$origin) ## [1] \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; \u0026quot;domestic\u0026quot; Now the origin variable stores \"domestic\" for domestic cars and \"foreign\" for foreign cars.\nis.factor(autompg$origin) ## [1] FALSE However, this is simply a vector of character values. A vector of car models is a character variable in R. A vector of Vehicle Identification Numbers (VINs) is a character variable as well. But those don’t represent a short list of levels that might influence a response variable. We will want to coerce this origin variable to be something more: a factor variable.\nautompg$origin = as.factor(autompg$origin) Now when we check the structure of the autompg dataset, we see that origin is a factor variable.\nstr(autompg) ## \u0026#39;data.frame\u0026#39;: 383 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : Factor w/ 3 levels \u0026quot;4\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;8\u0026quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ disp : num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num 3504 3693 3436 3433 3449 ... ## $ acc : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : Factor w/ 2 levels \u0026quot;domestic\u0026quot;,\u0026quot;foreign\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ domestic: num 1 1 1 1 1 1 1 1 1 1 ... Factor variables have levels which are the possible values (categories) that the variable may take, in this case foreign or domestic.\nlevels(autompg$origin) ## [1] \u0026quot;domestic\u0026quot; \u0026quot;foreign\u0026quot; Recall that previously we have fit the model\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon, \\]\nwhere\n \\(Y\\) is mpg, the fuel efficiency in miles per gallon, \\(x_1\\) is disp, the displacement in cubic inches, \\(x_2\\) is domestic a dummy variable where 1 indicates a domestic car.  (mod_dummy = lm(mpg ~ disp * domestic, data = autompg)) ## ## Call: ## lm(formula = mpg ~ disp * domestic, data = autompg) ## ## Coefficients: ## (Intercept) disp domestic disp:domestic ## 46.0548 -0.1569 -12.5755 0.1025 So here we see that\n\\[ \\hat{\\beta}_0 + \\hat{\\beta}_2 = 46.0548423 + -12.5754714 = 33.4793709 \\]\nis the estimated average mpg for a domestic car with 0 disp.\nNow let’s try to do the same, but using our new factor variable.\n(mod_factor = lm(mpg ~ disp * origin, data = autompg)) ## ## Call: ## lm(formula = mpg ~ disp * origin, data = autompg) ## ## Coefficients: ## (Intercept) disp originforeign disp:originforeign ## 33.47937 -0.05441 12.57547 -0.10252 It seems that it doesn’t produce the same results. Right away we notice that the intercept is different, as is the the coefficient in front of disp. We also notice that the remaining two coefficients are of the same magnitude as their respective counterparts using the domestic variable, but with a different sign. Why is this happening?\nIt turns out, that by using a factor variable, R is automatically creating a dummy variable for us. However, it is not the dummy variable that we had originally used ourselves.\nR is fitting the model\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon, \\]\nwhere\n \\(Y\\) is mpg, the fuel efficiency in miles per gallon, \\(x_1\\) is disp, the displacement in cubic inches, \\(x_2\\) is a dummy variable created by R. It uses 1 to represent a foreign car.  So now,\n\\[ \\hat{\\beta}_0 = 33.4793709 \\]\nis the estimated average mpg for a domestic car with 0 disp, which is indeed the same as before.\nWhen R created \\(x_2\\), the dummy variable, it used domestic cars as the reference level, that is the default value of the factor variable. So when the dummy variable is 0, the model represents this reference level, which is domestic. (R makes this choice because domestic comes before foreign alphabetically.)\nSo the two models have different estimated coefficients, but due to the different model representations, they are actually the same model.\nFactors with More Than Two Levels Let’s now consider a factor variable with more than two levels. In this dataset, cyl is an example.\nis.factor(autompg$cyl) ## [1] TRUE levels(autompg$cyl) ## [1] \u0026quot;4\u0026quot; \u0026quot;6\u0026quot; \u0026quot;8\u0026quot; Here the cyl variable has three possible levels: 4, 6, and 8. You may wonder, why not simply use cyl as a numerical variable? You certainly could.\nHowever, that would force the difference in average mpg between 4 and 6 cylinders to be the same as the difference in average mpg between 6 and 8 cylinders. That usually make senses for a continuous variable, but not for a discrete variable with so few possible values. In the case of this variable, there is no such thing as a 7-cylinder engine or a 6.23-cylinder engine in personal vehicles. For these reasons, we will simply consider cyl to be categorical. This is a decision that will commonly need to be made with ordinal variables. Often, with a large number of categories, the decision to treat them as numerical variables is appropriate because, otherwise, a large number of dummy variables are then needed to represent these variables.\nLet’s define three dummy variables related to the cyl factor variable.\n\\[ v_1 = \\begin{cases} 1 \u0026amp; \\text{4 cylinder} \\\\ 0 \u0026amp; \\text{not 4 cylinder} \\end{cases} \\]\n\\[ v_2 = \\begin{cases} 1 \u0026amp; \\text{6 cylinder} \\\\ 0 \u0026amp; \\text{not 6 cylinder} \\end{cases} \\]\n\\[ v_3 = \\begin{cases} 1 \u0026amp; \\text{8 cylinder} \\\\ 0 \u0026amp; \\text{not 8 cylinder} \\end{cases} \\]\nNow, let’s fit an additive model in R, using mpg as the response, and disp and cyl as predictors. This should be a model that uses “three regression lines” to model mpg, one for each of the possible cyl levels. They will all have the same slope (since it is an additive model), but each will have its own intercept.\n(mpg_disp_add_cyl = lm(mpg ~ disp + cyl, data = autompg)) ## ## Call: ## lm(formula = mpg ~ disp + cyl, data = autompg) ## ## Coefficients: ## (Intercept) disp cyl6 cyl8 ## 34.99929 -0.05217 -3.63325 -2.03603 The question is, what is the model that R has fit here? It has chosen to use the model\n\\[ Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon, \\]\nwhere\n \\(Y\\) is mpg, the fuel efficiency in miles per gallon, \\(x\\) is disp, the displacement in cubic inches, \\(v_2\\) and \\(v_3\\) are the dummy variables define above.  Why doesn’t R use \\(v_1\\)? Essentially because it doesn’t need to. To create three lines, it only needs two dummy variables since it is using a reference level, which in this case is a 4 cylinder car. The three “sub models” are then:\n 4 Cylinder: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\) 6 Cylinder: \\(Y = (\\beta_0 + \\beta_2) + \\beta_1 x + \\epsilon\\) 8 Cylinder: \\(Y = (\\beta_0 + \\beta_3) + \\beta_1 x + \\epsilon\\)  Notice that they all have the same slope. However, using the two dummy variables, we achieve the three intercepts.\n \\(\\beta_0\\) is the average mpg for a 4 cylinder car with 0 disp. \\(\\beta_0 + \\beta_2\\) is the average mpg for a 6 cylinder car with 0 disp. \\(\\beta_0 + \\beta_3\\) is the average mpg for a 8 cylinder car with 0 disp.  So because 4 cylinder is the reference level, \\(\\beta_0\\) is specific to 4 cylinders, but \\(\\beta_2\\) and \\(\\beta_3\\) are used to represent quantities relative to 4 cylinders.\nAs we have done before, we can extract these intercepts and slopes for the three lines, and plot them accordingly.\nint_4cyl = coef(mpg_disp_add_cyl)[1] int_6cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[3] int_8cyl = coef(mpg_disp_add_cyl)[1] + coef(mpg_disp_add_cyl)[4] slope_all_cyl = coef(mpg_disp_add_cyl)[2] plot_colors = c(\u0026quot;Darkorange\u0026quot;, \u0026quot;Darkgrey\u0026quot;, \u0026quot;Dodgerblue\u0026quot;) plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl)) abline(int_4cyl, slope_all_cyl, col = plot_colors[1], lty = 1, lwd = 2) abline(int_6cyl, slope_all_cyl, col = plot_colors[2], lty = 2, lwd = 2) abline(int_8cyl, slope_all_cyl, col = plot_colors[3], lty = 3, lwd = 2) legend(\u0026quot;topright\u0026quot;, c(\u0026quot;4 Cylinder\u0026quot;, \u0026quot;6 Cylinder\u0026quot;, \u0026quot;8 Cylinder\u0026quot;), col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3)) On this plot, we have\n 4 Cylinder: orange dots, solid orange line. 6 Cylinder: grey dots, dashed grey line. 8 Cylinder: blue dots, dotted blue line.  The odd result here is that we’re estimating that 8 cylinder cars have better fuel efficiency than 6 cylinder cars at any displacement! The dotted blue line is always above the dashed grey line. That doesn’t seem right. Maybe for very large displacement engines that could be true, but that seems wrong for medium to low displacement.\nTo attempt to fix this, we will try using an interaction model, that is, instead of simply three intercepts and one slope, we will allow for three slopes. Again, we’ll let R take the wheel, (no pun intended) then figure out what model it has applied.\n(mpg_disp_int_cyl = lm(mpg ~ disp * cyl, data = autompg)) ## ## Call: ## lm(formula = mpg ~ disp * cyl, data = autompg) ## ## Coefficients: ## (Intercept) disp cyl6 cyl8 disp:cyl6 disp:cyl8 ## 43.59052 -0.13069 -13.20026 -20.85706 0.08299 0.10817 # could also use mpg ~ disp + cyl + disp:cyl R has again chosen to use 4 cylinder cars as the reference level, but this also now has an effect on the interaction terms. R has fit the model.\n\\[ Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon \\]\nWe’re using \\(\\gamma\\) like a \\(\\beta\\) parameter for simplicity, so that, for example \\(\\beta_2\\) and \\(\\gamma_2\\) are both associated with \\(v_2\\).\nNow, the three “sub models” are:\n 4 Cylinder: \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\). 6 Cylinder: \\(Y = (\\beta_0 + \\beta_2) + (\\beta_1 + \\gamma_2) x + \\epsilon\\). 8 Cylinder: \\(Y = (\\beta_0 + \\beta_3) + (\\beta_1 + \\gamma_3) x + \\epsilon\\).  Interpreting some parameters and coefficients then:\n \\((\\beta_0 + \\beta_2)\\) is the average mpg of a 6 cylinder car with 0 disp \\((\\hat{\\beta}_1 + \\hat{\\gamma}_3) = -0.1306935 + 0.1081714 = -0.0225221\\) is the estimated change in average mpg for an increase of one disp, for an 8 cylinder car.  So, as we have seen before \\(\\beta_2\\) and \\(\\beta_3\\) change the intercepts for 6 and 8 cylinder cars relative to the reference level of \\(\\beta_0\\) for 4 cylinder cars.\nNow, similarly \\(\\gamma_2\\) and \\(\\gamma_3\\) change the slopes for 6 and 8 cylinder cars relative to the reference level of \\(\\beta_1\\) for 4 cylinder cars.\nOnce again, we extract the coefficients and plot the results.\nint_4cyl = coef(mpg_disp_int_cyl)[1] int_6cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[3] int_8cyl = coef(mpg_disp_int_cyl)[1] + coef(mpg_disp_int_cyl)[4] slope_4cyl = coef(mpg_disp_int_cyl)[2] slope_6cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[5] slope_8cyl = coef(mpg_disp_int_cyl)[2] + coef(mpg_disp_int_cyl)[6] plot_colors = c(\u0026quot;Darkorange\u0026quot;, \u0026quot;Darkgrey\u0026quot;, \u0026quot;Dodgerblue\u0026quot;) plot(mpg ~ disp, data = autompg, col = plot_colors[cyl], pch = as.numeric(cyl)) abline(int_4cyl, slope_4cyl, col = plot_colors[1], lty = 1, lwd = 2) abline(int_6cyl, slope_6cyl, col = plot_colors[2], lty = 2, lwd = 2) abline(int_8cyl, slope_8cyl, col = plot_colors[3], lty = 3, lwd = 2) legend(\u0026quot;topright\u0026quot;, c(\u0026quot;4 Cylinder\u0026quot;, \u0026quot;6 Cylinder\u0026quot;, \u0026quot;8 Cylinder\u0026quot;), col = plot_colors, lty = c(1, 2, 3), pch = c(1, 2, 3)) This looks much better! We can see that for medium displacement cars, 6 cylinder cars now perform better than 8 cylinder cars, which seems much more reasonable than before.\nTo completely justify the interaction model (i.e., a unique slope for each cyl level) compared to the additive model (single slope), we can perform an \\(F\\)-test. Notice first, that there is no \\(t\\)-test that will be able to do this since the difference between the two models is not a single parameter.\nWe will test,\n\\[ H_0: \\gamma_2 = \\gamma_3 = 0 \\]\nwhich represents the parallel regression lines we saw before,\n\\[ Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon. \\]\nAgain, this is a difference of two parameters, thus no \\(t\\)-test will be useful.\nanova(mpg_disp_add_cyl, mpg_disp_int_cyl) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp + cyl ## Model 2: mpg ~ disp * cyl ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 379 7299.5 ## 2 377 6551.7 2 747.79 21.515 1.419e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 As expected, we see a very low p-value, and thus reject the null. We prefer the interaction model over the additive model.\nRecapping a bit:\n Null Model: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\epsilon\\)  Number of parameters: \\(q = 4\\)  Full Model: \\(Y = \\beta_0 + \\beta_1 x + \\beta_2 v_2 + \\beta_3 v_3 + \\gamma_2 x v_2 + \\gamma_3 x v_3 + \\epsilon\\)  Number of parameters: \\(p = 6\\)   length(coef(mpg_disp_int_cyl)) - length(coef(mpg_disp_add_cyl)) ## [1] 2 We see there is a difference of two parameters, which is also displayed in the resulting ANOVA table from R. Notice that the following two values also appear on the ANOVA table.\nnrow(autompg) - length(coef(mpg_disp_int_cyl)) ## [1] 377 nrow(autompg) - length(coef(mpg_disp_add_cyl)) ## [1] 379   Parameterization So far we have been simply letting R decide how to create the dummy variables, and thus R has been deciding the parameterization of the models. To illustrate the ability to use alternative parameterizations, we will recreate the data, but directly creating the dummy variables ourselves.\nnew_param_data = data.frame( y = autompg$mpg, x = autompg$disp, v1 = 1 * as.numeric(autompg$cyl == 4), v2 = 1 * as.numeric(autompg$cyl == 6), v3 = 1 * as.numeric(autompg$cyl == 8)) head(new_param_data, 20) ## y x v1 v2 v3 ## 1 18 307 0 0 1 ## 2 15 350 0 0 1 ## 3 18 318 0 0 1 ## 4 16 304 0 0 1 ## 5 17 302 0 0 1 ## 6 15 429 0 0 1 ## 7 14 454 0 0 1 ## 8 14 440 0 0 1 ## 9 14 455 0 0 1 ## 10 15 390 0 0 1 ## 11 15 383 0 0 1 ## 12 14 340 0 0 1 ## 13 15 400 0 0 1 ## 14 14 455 0 0 1 ## 15 24 113 1 0 0 ## 16 22 198 0 1 0 ## 17 18 199 0 1 0 ## 18 21 200 0 1 0 ## 19 27 97 1 0 0 ## 20 26 97 1 0 0 Now,\n y is mpg x is disp, the displacement in cubic inches, v1, v2, and v3 are dummy variables as defined above.  First let’s try to fit an additive model using x as well as the three dummy variables.\nlm(y ~ x + v1 + v2 + v3, data = new_param_data) ## ## Call: ## lm(formula = y ~ x + v1 + v2 + v3, data = new_param_data) ## ## Coefficients: ## (Intercept) x v1 v2 v3 ## 32.96326 -0.05217 2.03603 -1.59722 NA What is happening here? Notice that R is essentially ignoring v3, but why? Well, because R uses an intercept, it cannot also use v3. This is because\n\\[ \\boldsymbol{1} = v_1 + v_2 + v_3 \\]\nwhich means that \\(\\boldsymbol{1}\\), \\(v_1\\), \\(v_2\\), and \\(v_3\\) are linearly dependent. This would make the \\(X^\\top X\\) matrix singular, but we need to be able to invert it to solve the normal equations and obtain \\(\\hat{\\beta}.\\) With the intercept, v1, and v2, R can make the necessary “three intercepts”. So, in this case v3 is the reference level.\nIf we remove the intercept, then we can directly obtain all “three intercepts” without a reference level.\nlm(y ~ 0 + x + v1 + v2 + v3, data = new_param_data) ## ## Call: ## lm(formula = y ~ 0 + x + v1 + v2 + v3, data = new_param_data) ## ## Coefficients: ## x v1 v2 v3 ## -0.05217 34.99929 31.36604 32.96326 Here, we are fitting the model\n\\[ Y = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta x +\\epsilon. \\]\nThus we have:\n 4 Cylinder: \\(Y = \\mu_1 + \\beta x + \\epsilon\\) 6 Cylinder: \\(Y = \\mu_2 + \\beta x + \\epsilon\\) 8 Cylinder: \\(Y = \\mu_3 + \\beta x + \\epsilon\\)  We could also do something similar with the interaction model, and give each line an intercept and slope, without the need for a reference level.\nlm(y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data) ## ## Call: ## lm(formula = y ~ 0 + v1 + v2 + v3 + x:v1 + x:v2 + x:v3, data = new_param_data) ## ## Coefficients: ## v1 v2 v3 v1:x v2:x v3:x ## 43.59052 30.39026 22.73346 -0.13069 -0.04770 -0.02252 \\[ Y = \\mu_1 v_1 + \\mu_2 v_2 + \\mu_3 v_3 + \\beta_1 x v_1 + \\beta_2 x v_2 + \\beta_3 x v_3 +\\epsilon \\]\n 4 Cylinder: \\(Y = \\mu_1 + \\beta_1 x + \\epsilon\\) 6 Cylinder: \\(Y = \\mu_2 + \\beta_2 x + \\epsilon\\) 8 Cylinder: \\(Y = \\mu_3 + \\beta_3 x + \\epsilon\\)  Using the original data, we have (at least) three equivalent ways to specify the interaction model with R.\nlm(mpg ~ disp * cyl, data = autompg) ## ## Call: ## lm(formula = mpg ~ disp * cyl, data = autompg) ## ## Coefficients: ## (Intercept) disp cyl6 cyl8 disp:cyl6 disp:cyl8 ## 43.59052 -0.13069 -13.20026 -20.85706 0.08299 0.10817 lm(mpg ~ 0 + cyl + disp : cyl, data = autompg) ## ## Call: ## lm(formula = mpg ~ 0 + cyl + disp:cyl, data = autompg) ## ## Coefficients: ## cyl4 cyl6 cyl8 cyl4:disp cyl6:disp cyl8:disp ## 43.59052 30.39026 22.73346 -0.13069 -0.04770 -0.02252 lm(mpg ~ 0 + disp + cyl + disp : cyl, data = autompg) ## ## Call: ## lm(formula = mpg ~ 0 + disp + cyl + disp:cyl, data = autompg) ## ## Coefficients: ## disp cyl4 cyl6 cyl8 disp:cyl6 disp:cyl8 ## -0.13069 43.59052 30.39026 22.73346 0.08299 0.10817 They all fit the same model, importantly each using six parameters, but the coefficients mean slightly different things in each. However, once they are interpreted as slopes and intercepts for the “three lines” they will have the same result.\nUse ?all.equal to learn about the all.equal() function, and think about how the following code verifies that the residuals of the two models are the same.\nall.equal(fitted(lm(mpg ~ disp * cyl, data = autompg)), fitted(lm(mpg ~ 0 + cyl + disp : cyl, data = autompg))) ## [1] TRUE  Building Larger Models Now that we have seen how to incorporate categorical predictors as well as interaction terms, we can start to build much larger, much more flexible models which can potentially fit data better.\nLet’s define a “big” model,\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon. \\]\nHere,\n \\(Y\\) is mpg. \\(x_1\\) is disp. \\(x_2\\) is hp. \\(x_3\\) is domestic, which is a dummy variable we defined, where 1 is a domestic vehicle.  First thing to note here, we have included a new term \\(x_1 x_2 x_3\\) which is a three-way interaction. Interaction terms can be larger and larger, up to the number of predictors in the model.\nSince we are using the three-way interaction term, we also use all possible two-way interactions, as well as each of the first order (main effect) terms. This is the concept of a hierarchy. Any time a “higher-order” term is in a model, the related “lower-order” terms should also be included. Mathematically their inclusion or exclusion is sometimes irrelevant, but from an interpretation standpoint, it is best to follow the hierarchy rules.\nLet’s do some rearrangement to obtain a “coefficient” in front of \\(x_1\\).\n\\[ Y = \\beta_0 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_6 x_2 x_3 + (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3)x_1 + \\epsilon. \\]\nSpecifically, the “coefficient” in front of \\(x_1\\) is\n\\[ (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3). \\]\nLet’s discuss this “coefficient” to help us understand the idea of the flexibility of a model. Recall that,\n \\(\\beta_1\\) is the coefficient for a first order term, \\(\\beta_4\\) and \\(\\beta_5\\) are coefficients for two-way interactions, \\(\\beta_7\\) is the coefficient for the three-way interaction.  If the two and three way interactions were not in the model, the whole “coefficient” would simply be\n\\[ \\beta_1. \\]\nThus, no matter the values of \\(x_2\\) and \\(x_3\\), \\(\\beta_1\\) would determine the relationship between \\(x_1\\) (disp) and \\(Y\\) (mpg).\nWith the addition of the two-way interactions, now the “coefficient” would be\n\\[ (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3). \\]\nNow, changing \\(x_1\\) (disp) has a different effect on \\(Y\\) (mpg), depending on the values of \\(x_2\\) and \\(x_3\\).\nLastly, adding the three-way interaction gives the whole “coefficient”\n\\[ (\\beta_1 + \\beta_4 x_2 + \\beta_5 x_3 + \\beta_7 x_2 x_3) \\]\nwhich is even more flexible. Now changing \\(x_1\\) (disp) has a different effect on \\(Y\\) (mpg), depending on the values of \\(x_2\\) and \\(x_3\\), but in a more flexible way which we can see with some more rearrangement. Now the “coefficient” in front of \\(x_3\\) in this “coefficient” is dependent on \\(x_2\\).\n\\[ (\\beta_1 + \\beta_4 x_2 + (\\beta_5 + \\beta_7 x_2) x_3) \\]\nIt is so flexible, it is becoming hard to interpret!\nLet’s fit this three-way interaction model in R.\nbig_model = lm(mpg ~ disp * hp * domestic, data = autompg) summary(big_model) ## ## Call: ## lm(formula = mpg ~ disp * hp * domestic, data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.9410 -2.2147 -0.4008 1.9430 18.4094 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 6.065e+01 6.600e+00 9.189 \u0026lt; 2e-16 *** ## disp -1.416e-01 6.344e-02 -2.232 0.0262 * ## hp -3.545e-01 8.123e-02 -4.364 1.65e-05 *** ## domestic -1.257e+01 7.064e+00 -1.780 0.0759 . ## disp:hp 1.369e-03 6.727e-04 2.035 0.0426 * ## disp:domestic 4.933e-02 6.400e-02 0.771 0.4414 ## hp:domestic 1.852e-01 8.709e-02 2.126 0.0342 * ## disp:hp:domestic -9.163e-04 6.768e-04 -1.354 0.1766 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 3.88 on 375 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7556 ## F-statistic: 169.7 on 7 and 375 DF, p-value: \u0026lt; 2.2e-16 Do we actually need this large of a model? Let’s first test for the necessity of the three-way interaction term. That is,\n\\[ H_0: \\beta_7 = 0. \\]\nSo,\n Full Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\beta_7 x_1 x_2 x_3 + \\epsilon\\) Null Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\)  We fit the null model in R as two_way_int_mod, then use anova() to perform an \\(F\\)-test as usual.\ntwo_way_int_mod = lm(mpg ~ disp * hp + disp * domestic + hp * domestic, data = autompg) #two_way_int_mod = lm(mpg ~ (disp + hp + domestic) ^ 2, data = autompg) anova(two_way_int_mod, big_model) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp * hp + disp * domestic + hp * domestic ## Model 2: mpg ~ disp * hp * domestic ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 376 5673.2 ## 2 375 5645.6 1 27.599 1.8332 0.1766 We see the p-value is somewhat large, so we would fail to reject. We prefer the smaller, less flexible, null model, without the three-way interaction.\nA quick note here: the full model does still “fit better.” Notice that it has a smaller RMSE than the null model, which means the full model makes smaller (squared) errors on average.\nmean(resid(big_model) ^ 2) ## [1] 14.74053 mean(resid(two_way_int_mod) ^ 2) ## [1] 14.81259 However, it is not much smaller. We could even say that, the difference is insignificant. This is an idea we will return to later in greater detail.\nNow that we have chosen the model without the three-way interaction, can we go further? Do we need the two-way interactions? Let’s test\n\\[ H_0: \\beta_4 = \\beta_5 = \\beta_6 = 0. \\]\nRemember we already chose \\(\\beta_7 = 0\\), so,\n Full Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon\\) Null Model: \\(Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\\)  We fit the null model in R as additive_mod, then use anova() to perform an \\(F\\)-test as usual.\nadditive_mod = lm(mpg ~ disp + hp + domestic, data = autompg) anova(additive_mod, two_way_int_mod) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp + hp + domestic ## Model 2: mpg ~ disp * hp + disp * domestic + hp * domestic ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 379 7369.7 ## 2 376 5673.2 3 1696.5 37.478 \u0026lt; 2.2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 Here the p-value is small, so we reject the null, and we prefer the full (alternative) model. Of the models we have considered, our final preference is for\n\\[ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + \\beta_6 x_2 x_3 + \\epsilon. \\]\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602332948,"objectID":"a66b8951081722f5755747f3314e83f0","permalink":"/example/06-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/06-example/","section":"example","summary":"Preliminaries Dummy Variables Interactions Factor Variables Factors with More Than Two Levels  Parameterization Building Larger Models   Today’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n  Ames.csv   Preliminaries So far in each of our analyses, we have only used numeric variables as predictors.","tags":null,"title":"Linear Regression: Interpreting Coefficients","type":"docs"},{"authors":null,"categories":null,"content":"  Backstory and Set Up Linear Models Assesing Model Accuracy Model Complexity Test-Train Split Adding Flexibility to Linear Models    READ THIS CAREFULLY\nThe content below describes both Lab 7 and Lab 8. Lab 7 is Exercise 1; Lab 8 is Exercise 2. Also, you may find some other tasks in the text…\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 26 for Lab 7; turn in Lab 8 by 11:59 PM Eastern Time on Monday, November 2.\n Backstory and Set Up You still work for Zillow as a junior analyst (sorry). But you’re hunting a promotion. Your job is to present some more advanced predictions for housing values in a small geographic area (Ames, IA) using this historical pricing.\nAs always, let’s load the data.\nAmes \u0026lt;- read.table(\u0026quot;https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;)  Linear Models When exploring linear models in other classes, we often emphasize asymptotic results under distributional assumptions. That is, we make assumptions about the model in order to derive properties of large samples. This general approach is useful for creating and performing hypothesis tests. Frequently, when developing a linear regression model, part of the goal is to explain a relationship. However, this isn’t always the case. And it’s often not a valid approach, as we discussed in this week’s content.\nSo, we will ignore much of what we have learned in other classes (sorry, EC420) and instead simply use regression as a tool to predict. Instead of a model which supposedly explains relationships, we seek a model which minimizes errors.\nTo discuss linear models in the context of prediction, we return to the Ames data. Accordingly, you should utilize some of the early code from Lab 2 to hasten your progress in this lab.\nAssesing Model Accuracy There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.\n\\[ \\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\]\nWhile for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be \\(n\\).\nFor a linear model , the estimate of \\(f\\), \\(\\hat{f}\\), is given by the fitted regression line.\n\\[ \\hat{y}({\\bf{x}_i}) = \\hat{f}({\\bf{x}_i}) \\]\nWe can write an R function that will be useful for performing this calculation.\nrmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) }  Model Complexity Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, \\(p\\).\nWe write a simple R function to extract this information from a model.\nget_complexity = function(model) { length(coef(model)) - 1 } When deciding how complex of a model to use, we can utilize two techniques: forward selection or backward selection. Forward selection means that we start with the simplest model (with a single predictor) and then add one at a time until we decide to stop. Backward selection means that we start with the most complex model (with every available predictor) and then remove one at a time until we decide to stop. There are many criteria for “when to stop”. Below, we’ll try to give you some intuition on the model-building process.\nEXERCISE 1\nLoad the Ames data. Drop the variables OverallCond and OverallQual.\n Using forward selection (that is, select one variable, then select another) create a series of models up to complexity length 15. You may use any variable within the dataset, including categorical variables.\n Create a chart plotting the model complexity as the \\(x\\)-axis variable and RMSE as the \\(y\\)-axis variable. Describe any patterns you see. Do you think you should use the full-size model? Why or why not? What criterion are you using to make this statement?\n   Weekly writing: After completing the exercise above, reflect on the process. Was this efficient? Was it enjoyable? Do you think you created an highly predictive model? Write a short paragraph. As always, submit this separately into the “weekly writing” assigment on D2L.\n Test-Train Split There is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts: it is essentially cheating. As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down—or, in very specific cases where a new predictor is completely uncorrelated with the target, stay the same. This might seem to suggest that in order to predict well, we should use the largest possible model. However, in reality we have fit to a specific dataset, but as soon as we see new data, a large model may (in fact) predict poorly. This is called overfitting.\nThe most common approach to overfitting is to take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data. Test data should never be used to train a model—its pupose is to evaluate the fitted model once you’ve settled on something.1\nHere we use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to allow use to reproduce the same random split each time we perform this analysis. Sometimes we don’t want to do this; if we want to run lots of independent splits, then we do not need to set the initial seed.\nset.seed(9) num_obs = nrow(Ames) train_index = sample(num_obs, size = trunc(0.50 * num_obs)) train_data = Ames[train_index, ] test_data = Ames[-train_index, ] We will look at two measures that assess how well a model is predicting: train RMSE and test RMSE.\n\\[ \\text{RMSE}_\\text{Train} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\sum_{i \\in \\text{Train}}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\]\nHere \\(n_{Tr}\\) is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.\n\\[ \\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\sum_{i \\in \\text{Test}} \\left ( y_i - \\hat{f}(\\bf{x}_i) \\right ) ^2} \\]\nHere \\(n_{Te}\\) is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.\nWe will start with the simplest possible linear model, that is, a model with no predictors.\nfit_0 = lm(SalePrice ~ 1, data = train_data) get_complexity(fit_0) ## [1] 0 # train RMSE sqrt(mean((train_data$SalePrice - predict(fit_0, train_data)) ^ 2)) ## [1] 80875.98 # test RMSE sqrt(mean((test_data$SalePrice - predict(fit_0, test_data)) ^ 2)) ## [1] 77928.62 The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.\n# train RMSE rmse(actual = train_data$SalePrice, predicted = predict(fit_0, train_data)) ## [1] 80875.98 # test RMSE rmse(actual = test_data$SalePrice, predicted = predict(fit_0, test_data)) ## [1] 77928.62 This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.\nget_rmse = function(model, data, response) { rmse(actual = subset(data, select = response, drop = TRUE), predicted = predict(model, data)) } By using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.\nget_rmse(model = fit_0, data = train_data, response = \u0026quot;SalePrice\u0026quot;) # train RMSE ## [1] 80875.98 get_rmse(model = fit_0, data = test_data, response = \u0026quot;SalePrice\u0026quot;) # test RMSE ## [1] 77928.62 Try it: Apply this basic function with different arguments. Do you understand how we’ve nested functions within functions?\nTry it: Define a total of five models using the first five models you fit in Exercise 1. Define these as fit_1 through fit_5\n Adding Flexibility to Linear Models Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting. To better understand the relationship between train RMSE, test RMSE, and model complexity, we’ll explore the results from Exercise 1.\nHopefully, you tried the in-line excercise above. If so, we can create a list of the models fit.\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5) We then obtain train RMSE, test RMSE, and model complexity for each. In doing so, we’ll introduce a handy function from R called sapply(). You can likely intuit what it does by looking at the code below.\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \u0026quot;SalePrice\u0026quot;) test_rmse = sapply(model_list, get_rmse, data = test_data, response = \u0026quot;SalePrice\u0026quot;) model_complexity = sapply(model_list, get_complexity) Try it: Run ?sapply() to understand what are valid arguments to the function.\nOnce you’ve done this, you’ll notice the following:\n# This is the same as the apply command above test_rmse = c(get_rmse(fit_1, test_data, \u0026quot;SalePrice\u0026quot;), get_rmse(fit_2, test_data, \u0026quot;SalePrice\u0026quot;), get_rmse(fit_3, test_data, \u0026quot;SalePrice\u0026quot;), get_rmse(fit_4, test_data, \u0026quot;SalePrice\u0026quot;), get_rmse(fit_5, test_data, \u0026quot;SalePrice\u0026quot;)) We can plot the results. If you execute the code below, you’ll see the train RMSE in blue, while the test RMSE is given in orange.2\nplot(model_complexity, train_rmse, type = \u0026quot;b\u0026quot;, ylim = c(min(c(train_rmse, test_rmse)) - 0.02, max(c(train_rmse, test_rmse)) + 0.02), col = \u0026quot;dodgerblue\u0026quot;, xlab = \u0026quot;Model Size\u0026quot;, ylab = \u0026quot;RMSE\u0026quot;) lines(model_complexity, test_rmse, type = \u0026quot;b\u0026quot;, col = \u0026quot;darkorange\u0026quot;) We could also summarize the results as a table. fit_1 is the least flexible, and fit_5 is the most flexible. If we were to do this (see the exercise below) we would see that Train RMSE decreases as flexibility increases forever. However, this may not be the case for the Test RMSE.\n    Model Train RMSE Test RMSE Predictors    fit_1 RMSE\\(_{\\text{train}}\\) for model 1 RMSE\\(_{\\text{test}}\\) for model 1 put predictors here  … … …. …  fit_5 RMSE\\(_{\\text{train}}\\) for model 5 RMSE\\(_{\\text{train}}\\) for model 5 \\(p\\) predictors    To summarize:\n Underfitting models: In general High Train RMSE, High Test RMSE. Overfitting models: In general Low Train RMSE, High Test RMSE.  Specifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE.3 Then a model is underfitting if there exists a more complex model with lower Test RMSE.\nEXERCISE 2\n(AKA Lab 8)\nMake a table exactly like the table above for the 15 models you fit in Exercise 1.\n This question should be the most time-consuming question. Using any method you choose and any number of regressors, predict SalePrice. Calculate the Train and Test RMSE. Your goal is to have a lower Test RMSE than others in the class.\n In a short paragraph, describe the resulting model. Discuss how you arrived at this model, what interactions you’re using (if any) and how confident you are that your prediction will perform well, relative to other people in the class.\n Difficult; extra credit: Visualize your final model in a sensible way and provide a two-paragraph interpretation.\n   A final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that predicted well, and paid no attention to a model for explaination. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)\n   Note that sometimes the terms evaluation set and test set are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.↩︎\n The train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. We often see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later. Also, we might intuitively expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get (un)lucky and this will not be true.↩︎\n The labels of under and overfitting are relative to the best model we see. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604325415,"objectID":"734e226209bb9037b527a2c610184860","permalink":"/assignment/07-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/07-assignment/","section":"assignment","summary":"Backstory and Set Up Linear Models Assesing Model Accuracy Model Complexity Test-Train Split Adding Flexibility to Linear Models    READ THIS CAREFULLY\nThe content below describes both Lab 7 and Lab 8. Lab 7 is Exercise 1; Lab 8 is Exercise 2. Also, you may find some other tasks in the text…\nYou must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, October 26 for Lab 7; turn in Lab 8 by 11:59 PM Eastern Time on Monday, November 2.","tags":null,"title":"Advanced Model Building","type":"docs"},{"authors":null,"categories":null,"content":"   Model Selection Assesing Model Accuracy Model Complexity Test-Train Split Adding Flexibility to Linear Models Choosing a Model   Today’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n  Ames.csv   Model Selection Often when we are developing a linear regression model, part of our goal is to explain a relationship. Now, we will ignore much of what we have learned and instead simply use regression as a tool to predict. Instead of a model which explains relationships, we seek a model which minimizes errors.\nFirst, note that a linear model is one of many methods used in regression.\nTo discuss linear models in the context of prediction, we introduce the (very boring) Advertising data that is discussed in the ISL text (see supplemental readings).\nAdvertising ## # A tibble: 200 x 4 ## TV Radio Newspaper Sales ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 230. 37.8 69.2 22.1 ## 2 44.5 39.3 45.1 10.4 ## 3 17.2 45.9 69.3 9.3 ## 4 152. 41.3 58.5 18.5 ## 5 181. 10.8 58.4 12.9 ## 6 8.7 48.9 75 7.2 ## 7 57.5 32.8 23.5 11.8 ## 8 120. 19.6 11.6 13.2 ## 9 8.6 2.1 1 4.8 ## 10 200. 2.6 21.2 10.6 ## # … with 190 more rows library(caret) featurePlot(x = Advertising[ , c(\u0026quot;TV\u0026quot;, \u0026quot;Radio\u0026quot;, \u0026quot;Newspaper\u0026quot;)], y = Advertising$Sales)  Assesing Model Accuracy There are many metrics to assess the accuracy of a regression model. Most of these measure in some way the average error that the model makes. The metric that we will be most interested in is the root-mean-square error.\n\\[ \\text{RMSE}(\\hat{f}, \\text{Data}) = \\sqrt{\\frac{1}{n}\\displaystyle\\sum_{i = 1}^{n}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\]\nWhile for the sake of comparing models, the choice between RMSE and MSE is arbitrary, we have a preference for RMSE, as it has the same units as the response variable. Also, notice that in the prediction context MSE refers to an average, whereas in an ANOVA context, the denominator for MSE may not be \\(n\\).\nFor a linear model , the estimate of \\(f\\), \\(\\hat{f}\\), is given by the fitted regression line.\n\\[ \\hat{y}({\\bf{x}_i}) = \\hat{f}({\\bf{x}_i}) \\]\nWe can write an R function that will be useful for performing this calculation.\nrmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) }  Model Complexity Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors, \\(p\\).\nWe write a simple R function to extract this information from a model.\nget_complexity = function(model) { length(coef(model)) - 1 }  Test-Train Split There is an issue with fitting a model to all available data then using RMSE to determine how well the model predicts. It is essentially cheating! As a linear model becomes more complex, the RSS, thus RMSE, can never go up. It will only go down, or in very specific cases, stay the same.\nThis would suggest that to predict well, we should use the largest possible model! However, in reality we have hard fit to a specific dataset, but as soon as we see new data, a large model may in fact predict poorly. This is called overfitting.\nFrequently we will take a dataset of interest and split it in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data. Test data should never be used to train a model.\nNote that sometimes the terms evaluation set and test set are used interchangeably. We will give somewhat specific definitions to these later. For now we will simply use a single test set for a training set.\nHere we use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to allow use to reproduce the same random split each time we perform this analysis.\nset.seed(9) num_obs = nrow(Advertising) train_index = sample(num_obs, size = trunc(0.50 * num_obs)) train_data = Advertising[train_index, ] test_data = Advertising[-train_index, ] We will look at two measures that assess how well a model is predicting, the train RMSE and the test RMSE.\n\\[ \\text{RMSE}_{\\text{Train}} = \\text{RMSE}(\\hat{f}, \\text{Train Data}) = \\sqrt{\\frac{1}{n_{\\text{Tr}}}\\displaystyle\\sum_{i \\in \\text{Train}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\]\nHere \\(n_{Tr}\\) is the number of observations in the train set. Train RMSE will still always go down (or stay the same) as the complexity of a linear model increases. That means train RMSE will not be useful for comparing models, but checking that it decreases is a useful sanity check.\n\\[ \\text{RMSE}_{\\text{Test}} = \\text{RMSE}(\\hat{f}, \\text{Test Data}) = \\sqrt{\\frac{1}{n_{\\text{Te}}}\\displaystyle\\sum_{i \\in \\text{Test}}^{}\\left(y_i - \\hat{f}(\\bf{x}_i)\\right)^2} \\]\nHere \\(n_{Te}\\) is the number of observations in the test set. Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.\nWe will start with the simplest possible linear model, that is, a model with no predictors.\nfit_0 = lm(Sales ~ 1, data = train_data) get_complexity(fit_0) ## [1] 0 # train RMSE sqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2)) ## [1] 5.529258 # test RMSE sqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) ## [1] 4.914163 The previous two operations obtain the train and test RMSE. Since these are operations we are about to use repeatedly, we should use the function that we happen to have already written.\n# train RMSE rmse(actual = train_data$Sales, predicted = predict(fit_0, train_data)) ## [1] 5.529258 # test RMSE rmse(actual = test_data$Sales, predicted = predict(fit_0, test_data)) ## [1] 4.914163 This function can actually be improved for the inputs that we are using. We would like to obtain train and test RMSE for a fitted model, given a train or test dataset, and the appropriate response variable.\nget_rmse = function(model, data, response) { rmse(actual = subset(data, select = response, drop = TRUE), predicted = predict(model, data)) } By using this function, our code becomes easier to read, and it is more obvious what task we are accomplishing.\nget_rmse(model = fit_0, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE ## [1] 5.529258 get_rmse(model = fit_0, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE ## [1] 4.914163  Adding Flexibility to Linear Models Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.\nfit_1 = lm(Sales ~ ., data = train_data) get_complexity(fit_1) ## [1] 3 get_rmse(model = fit_1, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE ## [1] 1.888488 get_rmse(model = fit_1, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE ## [1] 1.461661 fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data) get_complexity(fit_2) ## [1] 7 get_rmse(model = fit_2, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE ## [1] 1.016822 get_rmse(model = fit_2, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE ## [1] 0.9117228 fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data) get_complexity(fit_3) ## [1] 8 get_rmse(model = fit_3, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE ## [1] 0.6553091 get_rmse(model = fit_3, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE ## [1] 0.6633375 fit_4 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data) get_complexity(fit_4) ## [1] 10 get_rmse(model = fit_4, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE ## [1] 0.6421909 get_rmse(model = fit_4, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE ## [1] 0.7465957 fit_5 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data) get_complexity(fit_5) ## [1] 14 get_rmse(model = fit_5, data = train_data, response = \u0026quot;Sales\u0026quot;) # train RMSE ## [1] 0.6120887 get_rmse(model = fit_5, data = test_data, response = \u0026quot;Sales\u0026quot;) # test RMSE ## [1] 0.7864181  Choosing a Model To better understand the relationship between train RMSE, test RMSE, and model complexity, we summarize our results, as the above is somewhat cluttered.\nFirst, we recap the models that we have fit.\nfit_1 = lm(Sales ~ ., data = train_data) fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data) fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data) fit_4 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data) fit_5 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data) Next, we create a list of the models fit.\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5) We then obtain train RMSE, test RMSE, and model complexity for each.\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \u0026quot;Sales\u0026quot;) test_rmse = sapply(model_list, get_rmse, data = test_data, response = \u0026quot;Sales\u0026quot;) model_complexity = sapply(model_list, get_complexity) We then plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\nplot(model_complexity, train_rmse, type = \u0026quot;b\u0026quot;, ylim = c(min(c(train_rmse, test_rmse)) - 0.02, max(c(train_rmse, test_rmse)) + 0.02), col = \u0026quot;dodgerblue\u0026quot;, xlab = \u0026quot;Model Size\u0026quot;, ylab = \u0026quot;RMSE\u0026quot;) lines(model_complexity, test_rmse, type = \u0026quot;b\u0026quot;, col = \u0026quot;darkorange\u0026quot;) We also summarize the results as a table. fit_1 is the least flexible, and fit_5 is the most flexible. We see the Train RMSE decrease as flexibility increases. We see that the Test RMSE is smallest for fit_3, thus is the model we believe will perform the best on future data not used to train the model. Note this may not be the best model, but it is the best model of the models we have seen in this example.\n  Model Train RMSE Test RMSE Predictors    fit_1 1.8884884 1.4616608 3  fit_2 1.0168223 0.9117228 7  fit_3 0.6553091 0.6633375 8  fit_4 0.6421909 0.7465957 10  fit_5 0.6120887 0.7864181 14    To summarize:\n Underfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2. Overfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.  Specifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE. Then a model is underfitting if there exists a more complex model with lower Test RMSE.\nA number of notes on these results:\n The labels of under and overfitting are relative to the best model we see, fit_3. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting. The train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. Here we see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later. Often we expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get lucky and this will not be true.  A final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that predicted well, and paid no attention to a model for explaination. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603200224,"objectID":"40eeabbfe9d21a7292000fc4fe538d43","permalink":"/example/07-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/07-example/","section":"example","summary":"Model Selection Assesing Model Accuracy Model Complexity Test-Train Split Adding Flexibility to Linear Models Choosing a Model   Today’s example will pivot between the content from this week and the example below.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n  Ames.csv   Model Selection Often when we are developing a linear regression model, part of our goal is to explain a relationship.","tags":null,"title":"Linear Regression: Model Selection","type":"docs"},{"authors":null,"categories":null,"content":"  Today’s example will come from the “Content” tab.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n  Ames.csv   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604585704,"objectID":"66dae6a89dc933d1691fce47e0612205","permalink":"/example/08-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/08-example/","section":"example","summary":"  Today’s example will come from the “Content” tab.\nWe will also use the Ames data again. Maybe some new data. Who knows. The Ames data is linked below:\n  Ames.csv   ","tags":null,"title":"Nonparametric Regression","type":"docs"},{"authors":null,"categories":null,"content":"  Today’s example will come from the “Content” tab.\nWe may use a new dataset (it is covered in the Lab). You can find it below.\n  bank.csv   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594131235,"objectID":"4ca8d8f57d9585dcab132a17e9a5a6e7","permalink":"/example/09-example/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/example/09-example/","section":"example","summary":"  Today’s example will come from the “Content” tab.\nWe may use a new dataset (it is covered in the Lab). You can find it below.\n  bank.csv   ","tags":null,"title":"Illustrating Bias vs. Variance","type":"docs"},{"authors":null,"categories":null,"content":"   Backstory and Set Up   You must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, November 9.\n Backstory and Set Up You work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.) They’ve given you a dataset on defaults (encoded as the variable y). You’re going to try to predict this.\nThis is some new data. The snippet below loads it.\nbank \u0026lt;- read.table(\u0026quot;https://msudataanalytics.github.io/SSC442/Labs/data/bank.csv\u0026quot;, header = TRUE, sep = \u0026quot;,\u0026quot;) There’s not going to be a whole lot of wind-up here. You should be well-versed in doing these sorts of things by now (if not, look back at the previous lab for sample code).\nEXERCISE 1\nSplit the data into an 80/20 train vs. test split. Make sure you explicitly set the seed for replicability, but do not share your seed with others in the class. (We may compare some results across people.)\n Run a series of KNN models with \\(k\\) ranging from 2 to 100. (You need not do every \\(k\\) between 2 and 100, but you can easily write a short function to do this; see the Content tab).\n Create a chart plotting the model complexity as the \\(x\\)-axis variable and RMSE as the \\(y\\)-axis variable for both the training and test data. What do you think is the optimal \\(k\\)?\n    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604586673,"objectID":"31ea2e7e9d2a752ab8d50f8bdb0af971","permalink":"/assignment/09-assignment/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/assignment/09-assignment/","section":"assignment","summary":"Backstory and Set Up   You must turn in a PDF document of your R Markdown code. Submit this to D2L by 11:59 PM Eastern Time on Monday, November 9.\n Backstory and Set Up You work for a bank. This bank is trying to predict defaults on loans (a relatively uncommon occurence, but one that costs the bank a great deal of money when it does happen.","tags":null,"title":"Model Evaluation","type":"docs"},{"authors":null,"categories":null,"content":"   Accessibility Colors Fonts Graphic assets  Images Vectors Vectors, photos, videos, and other assets    Accessibility  Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based) Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes. ColourLovers: Like Facebook for color palettes. viridis: Percetually uniform color scales. Scientific Colour-Maps: Perceptually uniform color scales like viridis. Use them in R with scico. ColorBrewer: Sequential, diverging, and qualitative color palettes that take accessibility into account. Colorgorical: Create color palettes based on fancy mathematical rules for perceptual distance. Colorpicker for data: More fancy mathematical rules for color palettes (explanation). iWantHue: Yet another perceptual distance-based color palette builder. Photochrome: Word-based color pallettes. PolicyViz Design Color Tools: Large collection of useful color resources   Fonts  Google Fonts: Huge collection of free, well-made fonts. The Ultimate Collection of Google Font Pairings: A list of great, well-designed font pairings from all those fonts hosted by Google (for when you’re looking for good contrasting or complementary fonts).   Graphic assets Images  Use the Creative Commons filters on Google Images or Flickr Unsplash Pexels Pixabay StockSnap.io Burst freephotos.cc   Vectors  Noun Project: Thousands of free simple vector images aiconica: 1,000+ vector icons Vecteezy: Thousands of free vector images   Vectors, photos, videos, and other assets  Stockio    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"16fd04c4714e3d096bffcf19e6c524ca","permalink":"/resource/design/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/design/","section":"resource","summary":"Accessibility Colors Fonts Graphic assets  Images Vectors Vectors, photos, videos, and other assets    Accessibility  Vischeck: Simulate how your images look for people with different forms of colorblindness (web-based) Color Oracle: Simulate how your images look for people with different forms of colorblindness (desktop-based, more types of colorblindness)   Colors  Adobe Color: Create, share, and explore rule-based and custom color palettes. ColourLovers: Like Facebook for color palettes.","tags":null,"title":"Design","type":"docs"},{"authors":null,"categories":null,"content":"  Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph. More text in the next paragraph. Always use empty lines between paragraphs.  Some text in a paragraph.\nMore text in the next paragraph. Always use empty lines between paragraphs.\n  *Italic* _Italic_ Italic  **Bold** __Bold__ Bold  # Heading 1  Heading 1   ## Heading 2  Heading 2   ### Heading 3  Heading 3   (Go up to heading level 6 with ######)    [Link text](http://www.example.com)  Link text  ![Image caption](/path/to/image.png)    `Inline code` with backticks  Inline code with backticks  \u0026gt; Blockquote   Blockquote\n  - Things in - an unordered - list * Things in * an unordered * list  Things in an unordered list   1. Things in 2. an ordered 3. list 1) Things in 2) an ordered 3) list Things in an ordered list   Horizontal line --- Horizontal line *** Horizontal line\n     Math Markdown uses LaTeX to create fancy mathematical equations. There are like a billion little options and features available for math equations—you can find helpful examples of the the most common basic commands here.\nYou can use math in two different ways: inline or in a display block. To use math inline, wrap it in single dollar signs, like $y = mx + b$:\n    Type… …to get    Based on the DAG, the regression model for estimating the effect of education on wages is $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon$, or $\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon$. Based on the DAG, the regression model for estimating the effect of education on wages is \\(\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\epsilon\\), or \\(\\text{Wages} = \\beta_0 + \\beta_1 \\text{Education} + \\epsilon\\).    To put an equation on its own line in a display block, wrap it in double dollar signs, like this:\nType…\nThe quadratic equation was an important part of high school math: $$ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$ But now we just use computers to solve for $x$. …to get…\n The quadratic equation was an important part of high school math:\n\\[ x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} \\]\nBut now we just use computers to solve for \\(x\\).\n Because dollar signs are used to indicate math equations, you can’t just use dollar signs like normal if you’re writing about actual dollars. For instance, if you write This book costs $5.75 and this other costs $40, Markdown will treat everything that comes between the dollar signs as math, like so: “This book costs $5.75 and this other costs $40”.\nTo get around that, put a backslash (\\) in front of the dollar signs, so that This book costs \\$5.75 and this other costs \\$40 becomes “This book costs $5.75 and this other costs $40”.\n Tables There are 4 different ways to hand-create tables in Markdown—I say “hand-create” because it’s normally way easier to use R to generate these things with packages like pander (use pandoc.table()) or knitr (use kable()). The two most common are simple tables and pipe tables. You should look at the full documentation here.\nFor simple tables, type…\n Right Left Center Default ------- ------ ---------- ------- 12 12 12 12 123 123 123 123 1 1 1 1 Table: Caption goes here …to get…\n Caption goes here  Right Left Center Default    12 12 12 12  123 123 123 123  1 1 1 1    For pipe tables, type…\n| Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | Table: Caption goes here …to get…\n Caption goes here  Right Left Default Center    12 12 12 12  123 123 123 123  1 1 1 1     Footnotes There are two different ways to add footnotes (see here for complete documentation): regular and inline.\nRegular notes need (1) an identifier and (2) the actual note. The identifier can be whatever you want. Some people like to use numbers like [^1], but if you ever rearrange paragraphs or add notes before #1, the numbering will be wrong (in your Markdown file, not in the output; everything will be correct in the output). Because of that, I prefer to use some sort of text label:\nType…\nHere is a footnote reference[^1] and here is another [^note-on-dags]. [^1]: This is a note. [^note-on-dags]: DAGs are neat. And here\u0026#39;s more of the document. …to get…\n Here is a footnote reference1 and here is another.2\nAnd here’s more of the document.\n  This is a note.↩︎   DAGs are neat.↩︎     You can also use inline footnotes with ^[Text of the note goes here], which are often easier because you don’t need to worry about identifiers:\nType…\nCausal inference is neat.^[But it can be hard too!] …to get…\n Causal inference is neat.1\n  But it can be hard too!↩︎      Front matter You can include a special section at the top of a Markdown document that contains metadata (or data about your document) like the title, date, author, etc. This section uses a special simple syntax named YAML (or “YAML Ain’t Markup Language”) that follows this basic outline: setting: value for setting. Here’s an example YAML metadata section. Note that it must start and end with three dashes (---).\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; --- You can put the values inside quotes (like the date and name in the example above), or you can leave them outside of quotes (like the title in the example above). I typically use quotes just to be safe—if the value you’re using has a colon (:) in it, it’ll confuse Markdown since it’ll be something like title: My cool title: a subtitle, which has two colons. It’s better to do this:\n--- title: \u0026quot;My cool title: a subtitle\u0026quot; --- If you want to use quotes inside one of the values (e.g. your document is An evaluation of \"scare quotes\"), you can use single quotes instead:\n--- title: \u0026#39;An evaluation of \u0026quot;scare quotes\u0026quot;\u0026#39; ---  Citations One of the most powerful features of Markdown + pandoc is the ability to automatically cite things and generate bibliographies. to use citations, you need to create a BibTeX file (ends in .bib) that contains a database of the things you want to cite. You can do this with bibliography managers designed to work with BibTeX directly (like BibDesk on macOS), or you can use Zotero (macOS and Windows) to export a .bib file. You can download an example .bib file of all the readings from this class here.\nComplete details for using citations can be found here. In brief, you need to do three things:\nAdd a bibliography: entry to the YAML metadata:\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib --- Choose a citation style based on a CSL file. The default is Chicago author-date, but you can choose from 2,000+ at this repository. Download the CSL file, put it in your project folder, and add an entry to the YAML metadata (or provide a URL to the online version):\n--- title: Title of your document date: \u0026quot;January 13, 2020\u0026quot; author: \u0026quot;Your name\u0026quot; bibliography: name_of_file.bib csl: \u0026quot;https://raw.githubusercontent.com/citation-style-language/styles/master/apa.csl\u0026quot; --- Some of the most common CSLs are:\n Chicago author-date Chicago note-bibliography Chicago full note-bibliography (no shortened notes or ibids) APA 7th edition MLA 8th edition  Cite things in your document. Check the documentation for full details of how to do this. Essentially, you use @citationkey inside square brackets ([]):\n    Type… …to get…    Causal inference is neat [@Rohrer:2018; @AngristPischke:2015]. Causal inference is neat (Rohrer 2018; Angrist and Pischke 2015).  Causal inference is neat [see @Rohrer:2018, p. 34; also @AngristPischke:2015, chapter 1]. Causal inference is neat (see Rohrer 2018, 34; also Angrist and Pischke 2015, chap. 1).  Angrist and Pischke say causal inference is neat [-@AngristPischke:2015; see also @Rohrer:2018]. Angrist and Pischke say causal inference is neat (2015; see also Rohrer 2018).  @AngristPischke:2015 [chapter 1] say causal inference is neat, and @Rohrer:2018 agrees. Angrist and Pischke (2015, chap. 1) say causal inference is neat, and Rohrer (2018) agrees.    After compiling, you should have a perfectly formatted bibliography added to the end of your document too:\n Angrist, Joshua D., and Jörn-Steffen Pischke. 2015. Mastering ’Metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\nRohrer, Julia M. 2018. “Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data.” Advances in Methods and Practices in Psychological Science 1 (1): 27–42. https://doi.org/10.1177/2515245917745629.\n   Other references These websites have additional details and examples and practice tools:\n CommonMark’s Markdown tutorial: A quick interactive Markdown tutorial. Markdown tutorial: Another interactive tutorial to practice using Markdown. Markdown cheatsheet: Useful one-page reminder of Markdown syntax. The Plain Person’s Guide to Plain Text Social Science: A comprehensive explanation and tutorial about why you should write data-based reports in Markdown.   ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594409288,"objectID":"dcf6a5ae191a1cca4f4c8ff8ac114538","permalink":"/resource/markdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/markdown/","section":"resource","summary":"Basic Markdown formatting Math Tables Footnotes Front matter Citations Other references   Markdown is a special kind of markup language that lets you format text with simple syntax. You can then use a converter program like pandoc to convert Markdown into whatever format you want: HTML, PDF, Word, PowerPoint, etc. (see the full list of output types here)\nBasic Markdown formatting     Type… …or… …to get    Some text in a paragraph.","tags":null,"title":"Using Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  The Stories Behind a Line Australia as 100 people: You can make something like this with d3 and the potato project. Marrying Later, Staying Single Longer   How to select the appropriate chart type Many people have created many useful tools for selecting the correct chart type for a given dataset or question. Here are some of the best:\n The Data Visualisation Catalogue: Descriptions, explanations, examples, and tools for creating 60 different types of visualizations. The Data Viz Project: Descriptions and examples for 150 different types of visualizations. Also allows you to search by data shape and chart function (comparison, correlation, distribution, geographical, part to whole, trend over time, etc.). From Data to Viz: A decision tree for dozens of chart types with links to R and Python code. The Chartmaker Directory: Examples of how to create 51 different types of visualizations in 31 different software packages, including Excel, Tableau, and R. R Graph Catalog: R code for 124 ggplot graphs. Emery’s Essentials: Descriptions and examples of 26 different chart types.   General resources  Storytelling with Data: Blog and site full of resources by Cole Nussbaumer Knaflic. Ann K. Emery’s blog: Blog and tutorials by Ann Emery. Evergreen Data: Helful resources by Stephanie Evergreen. PolicyViz: Regular podcast and site full of helpful resources by Jon Schwabisch. Visualising Data: Fantastic collection of visualization resources, articles, and tutorials by Andy Kirk. Info We Trust: Detailed explorations of visualizations by RJ Andrews, including a beautiful visual history of the field. FlowingData: Blog by Nathan Yau. Information is Beautiful: Blog by David McCandless. Junk Charts: Blog by Kaiser Fung. WTF Visualizations: Visualizations that make you ask “wtf?” The Data Visualization Checklist: A helpful set of criteria for grading the effectiveness of a graphic. Data Literacy Starter Kit: Compilation of resources to become data literate by Laura Calloway. Seeing Data: A series of research projects about perceptions and visualizations.   Visualization in Excel  How to Build Data Visualizations in Excel: Detailed tutorials for creating 14 different visualizations in Excel. Ann Emery’s tutorials: Fantastic series of tutorials for creating charts in Excel.   Visualization in Tableau Because it is focused entirely on visualization (and because it’s a well-supported commercial product), Tableau has a phenomenal library of tutorials and training videos. There’s a helpful collections of videos here, as well.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"ca403ba352e0871f06b445d2470037b3","permalink":"/resource/visualization/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/visualization/","section":"resource","summary":"Interesting and excellent real world examples How to select the appropriate chart type General resources Visualization in Excel Visualization in Tableau   Interesting and excellent real world examples  The Stories Behind a Line Australia as 100 people: You can make something like this with d3 and the potato project. Marrying Later, Staying Single Longer   How to select the appropriate chart type Many people have created many useful tools for selecting the correct chart type for a given dataset or question.","tags":null,"title":"Visualization","type":"docs"},{"authors":null,"categories":null,"content":"   Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown. This whole course website is created with R Markdown (and a package named blogdown).\nThe documentation for R Markdown is extremely comprehensive, and their tutorials and cheatsheets are excellent—rely on those.\nHere are the most important things you’ll need to know about R Markdown in this class:\nKey terms  Document: A Markdown file where you type stuff\n Chunk: A piece of R code that is included in your document. It looks like this:\n```{r} # Code goes here ``` There must be an empty line before and after the chunk. The final three backticks must be the only thing on the line—if you add more text, or if you forget to add the backticks, or accidentally delete the backticks, your document will not knit correctly.\n Knit: When you “knit” a document, R runs each of the chunks sequentially and converts the output of each chunk into Markdown. R then runs the knitted document through pandoc to convert it to HTML or PDF or Word (or whatever output you’ve selected).\nYou can knit by clicking on the “Knit” button at the top of the editor window, or by pressing ⌘⇧K on macOS or control + shift + K on Windows.\n   Add chunks There are three ways to insert chunks:\n Press ⌘⌥I on macOS or control + alt + I on Windows\n Click on the “Insert” button at the top of the editor window\n Manually type all the backticks and curly braces (don’t do this)\n   Chunk names You can add names to chunks to make it easier to navigate your document. If you click on the little dropdown menu at the bottom of your editor in RStudio, you can see a table of contents that shows all the headings and chunks. If you name chunks, they’ll appear in the list. If you don’t include a name, the chunk will still show up, but you won’t know what it does.\nTo add a name, include it immediately after the {r in the first line of the chunk. Names cannot contain spaces, but they can contain underscores and dashes. All chunk names in your document must be unique.\n```{r name-of-this-chunk} # Code goes here ```  Chunk options There are a bunch of different options you can set for each chunk. You can see a complete list in the RMarkdown Reference Guide or at knitr’s website.\nOptions go inside the {r} section of the chunk:\n```{r name-of-this-chunk, warning=FALSE, message=FALSE} # Code goes here ``` The most common chunk options are these:\n fig.width=5 and fig.height=3 (or whatever number you want): Set the dimensions for figures echo=FALSE: The code is not shown in the final document, but the results are message=FALSE: Any messages that R generates (like all the notes that appear after you load a package) are omitted warning=FALSE: Any warnings that R generates are omitted include=FALSE: The chunk still runs, but the code and results are not included in the final document  You can also set chunk options by clicking on the little gear icon in the top right corner of any chunk:\n Inline chunks You can also include R output directly in your text, which is really helpful if you want to report numbers from your analysis. To do this, use `r r_code_here`.\nIt’s generally easiest to calculate numbers in a regular chunk beforehand and then use an inline chunk to display the value in your text. For instance, this document…\n```{r find-avg-mpg, echo=FALSE} avg_mpg \u0026lt;- mean(mtcars$mpg) ``` The average fuel efficiency for cars from 1974 was `r round(avg_mpg, 1)` miles per gallon. … would knit into this:\n The average fuel efficiency for cars from 1974 was 20.1 miles per gallon.\n  Output formats You can specify what kind of document you create when you knit in the YAML front matter.\ntitle: \u0026quot;My document\u0026quot; output: html_document: default pdf_document: default word_document: default You can also click on the down arrow on the “Knit” button to choose the output and generate the appropriate YAML. If you click on the gear icon next to the “Knit” button and choose “Output options”, you change settings for each specific output type, like default figure dimensions or whether or not a table of contents is included.\nThe first output type listed under output: will be what is generated when you click on the “Knit” button or press the keyboard shortcut (⌘⇧K on macOS; control + shift + K on Windows). If you choose a different output with the “Knit” button menu, that output will be moved to the top of the output section.\nThe indentation of the YAML section matters, especially when you have settings nested under each output type. Here’s what a typical output section might look like:\n--- title: \u0026quot;My document\u0026quot; author: \u0026quot;My name\u0026quot; date: \u0026quot;January 13, 2020\u0026quot; output: html_document: toc: yes fig_caption: yes fig_height: 8 fig_width: 10 pdf_document: latex_engine: xelatex # More modern PDF typesetting engine toc: yes word_document: toc: yes fig_caption: yes fig_height: 4 fig_width: 5 ---  ","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"00c0b36df90b91640842af65d1311657","permalink":"/resource/rmarkdown/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/resource/rmarkdown/","section":"resource","summary":"Key terms Add chunks Chunk names Chunk options Inline chunks Output formats   R Markdown is regular Markdown with R code and output sprinkled in. You can do everything you can with regular Markdown, but you can incorporate graphs, tables, and other R output directly in your document. You can create HTML, PDF, and Word documents, PowerPoint and HTML presentations, websites, books, and even interactive dashboards with R Markdown.","tags":null,"title":"Using R Markdown","type":"docs"},{"authors":null,"categories":null,"content":"   R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.\nTo help improve readability and make it easier to share code with others, there’s an unofficial style guide for writing R code. It’s fairly short and just has lots of examples of good and bad ways of writing code (naming variables, dealing with long lines, using proper indentation levels, etc.)—you should glance through it some time.\nRStudio has a built-in way of cleaning up your code. Select some code, press ctrl + i (on Windows) or ⌘ + i (on macOS), and R will reformat the code for you. It’s not always perfect, but it’s really helpful for getting indentation right without having to manually hit space a billion times.\n Main style things to pay attention to for this class  Important note: I won’t ever grade you on any of this! If you submit something like filter(mpg,cty\u0026gt;10,class==\"compact\"), I might recommend adding spaces, but it won’t affect your grade or points or anything.\n Spacing  See the “Spacing” section in the tidyverse style guide.\n Put spaces after commas (like in regular English):\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg , cty \u0026gt; 10) filter(mpg ,cty \u0026gt; 10) filter(mpg,cty \u0026gt; 10) Put spaces around operators like +, -, \u0026gt;, =, etc.:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter(mpg, cty\u0026gt;10) filter(mpg, cty\u0026gt; 10) filter(mpg, cty \u0026gt;10) Don’t put spaces around parentheses that are parts of functions:\n# Good filter(mpg, cty \u0026gt; 10) # Bad filter (mpg, cty \u0026gt; 10) filter ( mpg, cty \u0026gt; 10) filter( mpg, cty \u0026gt; 10 )  Long lines  See the “Long lines” section in the tidyverse style guide.\n It’s generally good practice to not have really long lines of code. A good suggestion is to keep lines at a maximum of 80 characters. Instead of counting characters by hand (ew), in RStudio go to “Tools” \u0026gt; “Global Options” \u0026gt; “Code” \u0026gt; “Display” and check the box for “Show margin”. You should now see a really thin line indicating 80 characters. Again, you can go beyond this—that’s fine. It’s just good practice to avoid going too far past it.\nYou can add line breaks inside longer lines of code. Line breaks should come after commas, and things like function arguments should align within the function:\n# Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Good filter(mpg, cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) # Bad filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;)) # Good filter(mpg, cty \u0026gt; 10, class %in% c(\u0026quot;compact\u0026quot;, \u0026quot;pickup\u0026quot;, \u0026quot;midsize\u0026quot;, \u0026quot;subcompact\u0026quot;, \u0026quot;suv\u0026quot;, \u0026quot;2seater\u0026quot;, \u0026quot;minivan\u0026quot;))  Pipes (%\u0026gt;%) and ggplot layers (+) Put each layer of a ggplot plot on separate lines, with the + at the end of the line, indented with two spaces:\n# Good ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() # Super bad and won\u0026#39;t even work ggplot(mpg, aes(x = cty, y = hwy, color = class)) + geom_point() + geom_smooth() + theme_bw() Put each step in a dplyr pipeline on separate lines, with the %\u0026gt;% at the end of the line, indented with two spaces:\n# Good mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy)) # Super bad and won\u0026#39;t even work mpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% group_by(class) %\u0026gt;% summarize(avg_hwy = mean(hwy))  Comments  See the “Comments” section in the tidyverse style guide.\n Comments should start with a comment symbol and a single space: #\n# Good #Bad #Bad If the comment is really short (and won’t cause you to go over 80 characters in the line), you can include it in the same line as the code, separated by at least two spaces (it works with one space, but using a couple can enhance readability):\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group You can add extra spaces to get inline comments to align, if you want:\nmpg %\u0026gt;% filter(cty \u0026gt; 10) %\u0026gt;% # Only rows where cty is 10 + group_by(class) %\u0026gt;% # Divide into class groups summarize(avg_hwy = mean(hwy)) # Find the average hwy in each group If the comment is really long, you can break it into multiple lines. RStudio can do this for you if you go to “Code” \u0026gt; “Reflow comment”\n# Good # Happy families are all alike; every unhappy family is unhappy in its own way. # Everything was in confusion in the Oblonskys’ house. The wife had discovered # that the husband was carrying on an intrigue with a French girl, who had been # a governess in their family, and she had announced to her husband that she # could not go on living in the same house with him. This position of affairs # had now lasted three days, and not only the husband and wife themselves, but # all the members of their family and household, were painfully conscious of it. # Bad # Happy families are all alike; every unhappy family is unhappy in its own way. Everything was in confusion in the Oblonskys’ house. The wife had discovered that the husband was carrying on an intrigue with a French girl, who had been a governess in their family, and she had announced to her husband that she could not go on living in the same house with him. This position of affairs had now lasted three days, and not only the husband and wife themselves, but all the members of their family and household, were painfully conscious of it. Though, if you’re dealing with comments that are that long, consider putting the text in R Markdown instead and having it be actual prose.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"f4734e734c67442efdc8d228e91ad766","permalink":"/resource/style/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/style/","section":"resource","summary":"R style conventions Main style things to pay attention to for this class  Spacing Long lines Pipes (%\u0026gt;%) and ggplot layers (+) Comments    R style conventions R is fairly forgiving about how you type code (unlike other languages like Python, where miscounting spaces can ruin your code!). All of these things will do exactly the same thing:\nmpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty\u0026gt;10, class==\u0026quot;compact\u0026quot;) filter(mpg,cty\u0026gt;10,class==\u0026quot;compact\u0026quot;) mpg %\u0026gt;% filter(cty \u0026gt; 10, class == \u0026quot;compact\u0026quot;) filter ( mpg,cty\u0026gt;10, class==\u0026quot;compact\u0026quot; ) But you’ll notice that only a few of those iterations (the first three) are easily readable.","tags":null,"title":"R style suggestions","type":"docs"},{"authors":null,"categories":null,"content":"  Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.\nUnzipping files on macOS is trivial, but unzipping files on Windows can mess you up if you don’t pay careful attention. Here’s a helpful guide to unzipping files on both macOS and Windows.\nUnzipping files on macOS Double click on the downloaded .zip file. macOS will automatically create a new folder with the same name as the .zip file, and all the file’s contents will be inside. Double click on the RStudio Project file (.Rproj) to get started.\n Unzipping files on Windows tl;dr: Right click on the .zip file, select “Extract All…”, and work with the resulting unzipped folder.\nUnlike macOS, Windows does not automatically unzip things for you. If you double click on the .zip file, Windows will show you what’s inside, but it will do so without actually extracting anything. This can be is incredibly confusing! Here’s what it looks like—the only clues that this folder is really a .zip file are that there’s a “Compressed Folder Tools” tab at the top, and there’s a “Ratio” column that shows how much each file is compressed.\nIt is very tempting to try to open files from this view. However, if you do, things will break and you won’t be able to correctly work with any of the files in the zipped folder. If you open the R Project file, for instance, RStudio will point to a bizarre working directory buried deep in some temporary folder:\nYou most likely won’t be able to open any data files or save anything, which will be frustrating.\nInstead, you need to right click on the .zip file and select “Extract All…”:\nThen choose where you want to unzip all the files and click on “Extract”\nYou should then finally have a real folder with all the contents of the zipped file. Open the R Project file and RStudio will point to the correct working directory and everything will work.\n ","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"c14c352fd4c4ab8c12a3cd60b30b9d8c","permalink":"/resource/unzipping/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/resource/unzipping/","section":"resource","summary":"Because RStudio projects typically consist of multiple files (R scripts, datasets, graphical output, etc.) the easiest way to distribute them to you for examples, assignments, and projects is to combine all the different files in to a single compressed collection called a zip file. When you unzip a zipped file, your operating system extracts all the files that are contained inside into a new folder on your computer.","tags":null,"title":"Unzipping files","type":"docs"},{"authors":null,"categories":null,"content":"  There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.\n Kaggle: Kaggle hosts machine learning competitions where people compete to create the fastest, most efficient, most predictive algorithms. A byproduct of these competitions is a host of fascinating datasets that are generally free and open to the public. See, for example, the European Soccer Database, the Salem Witchcraft Dataset or results from an Oreo flavors taste test.\n 360Giving: Dozens of British foundations follow a standard file format for sharing grant data and have made that data available online.\n US City Open Data Census: More than 100 US cities have committed to sharing dozens of types of data, including data about crime, budgets, campaign finance, lobbying, transit, and zoning. This site from the Sunlight Foundation and Code for America collects this data and rates cities by how well they’re doing.\n Political science and economics datasets: There’s a wealth of data available for political science- and economics-related topics:\n François Briatte’s extensive curated lists: Includes data from/about intergovernmental organizations (IGOs), nongovernmental organizations (NGOs), public opinion surveys, parliaments and legislatures, wars, human rights, elections, and municipalities. Thomas Leeper’s list of political science datasets: Good short list of useful datasets, divided by type of data (country-level data, survey data, social media data, event data, text data, etc.). Erik Gahner’s list of political science datasets: Huge list of useful datasets, divided by topic (governance, elections, policy, political elites, etc.)   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592849563,"objectID":"2210aa8aeb5724b04bdf63d813d61030","permalink":"/resource/data/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resource/data/","section":"resource","summary":"There are a ton of places to find data related to public policy and administration (as well as data on pretty much any topic you want) online:\n Data is Plural newsletter: Jeremy Singer-Vine sends a weekly newsletter of the most interesting public datasets he’s found. You should subscribe to it. He also has an archive of all the datasets he’s highlighted.\n Google Dataset Search: Google indexes thousands of public datasets; search for them here.","tags":null,"title":"Data","type":"docs"},{"authors":null,"categories":null,"content":"  Requirements Teams Suggested outline Introduction Theory and Background Data and Analyses Conclusion    Requirements Data analytics is inherently a hands-on endeavor. Accordingly, the final project for this class is hands-on. As per the overview page, the final project has the following elements:\nFor your final project in this class, you will analyze existing data in some area of interest to you.1 Aggregating data from multiple sources is encouraged, but is not required.  You must visualize (at least) three interesting features of that data. Visualizations should aid the reader in understanding something about the data that might not be readily aparent.[^4]\n You must come up with some analysis—using tools from the course—which relates your data to either a prediction or a policy conclusion. For example, if you collected data from Major League Baseball games, you could try to “predict” whether a left-hander was pitching based solely on the outcomes of the batsmen.\n You will submit three things via D2L:\n   A PDF of your report (see the outline below for details of what this needs to contain). You should compile this with R Markdown. You might want to write the prose-heavy sections in a word processor like Word or Google Docs and copy/paste the text into your R Markdown document, since RStudio doesn’t have a nice spell checker or grammar checker. This should have no visible R code, warnings, or messages in it. To do this, you must set echo = FALSE at the beginning of your document before you knit. The same PDF as above, but with all the R code in it (set echo = TRUE at the beginning of your document and reknit the file). Please label files in an obvious way. A CSV file of your data; or a link to the data online if your code pulls from the internet. This must be a separate file titled “data.csv” or “data.txt” as applicable.  This project is due by 7:00 PM on Monday, December 14, 2020. No late work will be accepted.\nYou can either run the analysis in RStudio locally on your computer (highly recommended!), since you won’t have to worry about keeping all your work on RStudio’s servers), or use an RStudio.cloud project.\nThere is no final exam. This project is your final exam.\nThe project will not be graded using a check system, and will be graded by me (the main instructor, not a TA). I will evaluate the following four elements of your project:\nTechnical skills: Was the project easy? Does it showcase mastery of data analysis? Visual design: Was the information smartly conveyed and usable? Was it beautiful? Analytic design: Was the analysis appropriate? Was it sensible, given the dataset? Story: Did we learn something? Following instructions: Did you surpress R code as asked? Did you submit a separate datafile and label it correctly?  If you’ve engaged with the course content and completed the exercises and mini projects throughout the course, you should do just fine with the final project.\n Teams Most importantly, you must work with classmates. You will work in groups of four people on your project. (There may be some groups of three). Your team must come up with a name and a Github site for your project and labs. Your team will earn the same scores on all projects. To combat additional freeloading, we will use a reporting system. Any team member can email me to report another team member’s lack of participation secretly. See below for details. Two strikes will result in a 25% grade deduction on the mini projects and final project; three strikes will result in a 50% deduction.\nHere’s how we will select teams:\nIf you choose to work in teams of your choosing, your group will receive 0 bonus points.\nIf you choose to work in a team with a partner, you will be randomly matched with another pair of students and your group will receive 20 bonus points.\nIf you choose to work in a randomly assigned team, your group will receive 40 bonus points.\nYou must make this selection by the end of the second full week of class.\n My team sucks; how can I switch teams?\n Life is full of small disappointments. While we would love to spend 12 weeks carefully optimizing groups, that would require a collosal amount of effort that would ultimately not yield anything fruitful. You’re stuck.\n My team sucks; how can I punish them for their lack of effort?\n On this front, we will be more supportive. While you have to put up with your team regardless of their quality, you can indicate that your team members are not carrying their fair share by issuing a strike. This processs works as follows: 1. A team member systematically fails to exert effort on collaborative projects (for example, by not showing up for meetings or not communicating, or by simply leeching off others without contributing.) 2. Your frustration reaches a boiling point. You decide this has to stop. You decide to issue a strike 3. You send an email with the following information: - Subject line: [SSC442] Strike against [Last name of Recipient] - Body: You do not need to provide detailed reasoning. However, you must discuss the actions (plural) you took to remedy the situation before sending the strike email.\nA strike is a serious matter, and will reduce that team member’s grade on joint work by 10%. If any team-member gets strikes from all other members of his or her team, their grade will be reduced by 50%.\nStrikes are anonymous so that you do not need to fear social retaliation. However, they are not anonymous to allow you to issue them without thoughtful consideration. Perhaps the other person has a serious issue that is preventing them from completing work (e.g., a relative passing away). Please be thoughtful in using this remedy and consider it a last resort.\n Do I really need to create a team GitHub repository? I don’t like GitHub / programming/ work.\n Yes, you need to become familiar with GitHub and you and your team will work in a central repository for mini-projects and your final project.\nThis is for two reasons. First, computer scientists spent a huge amount of time coming up with the solutions that are implemented in GitHub (and other flavors of git). Their efforts are largely dedicated toward solving a very concrete goal: how can two people edit the same thing at the same time without creating a ton of new issues. While you could use a paid variant of GitHub (e.g., you could all collaborate over the Microsoft Office suite as implemented by the 360 software that MSU provides), you’d ultimately have the following issues: 1. The software doesn’t support some file types. 2. The software doesn’t autosave versions.2 If someone accidentally deletes something, you’re in trouble. 3. You have to learn an entirely new system every time you change classes / universities / jobs, because said institute doesn’t buy the product you love.3\n I’m on a smaller-than-normal team. Does this mean that I have to do more work?\n Your instructors are able to count and are aware the teams are imbalanced. Evaluations of final projects will take this into account. While your final product should reflect the best ability of your team, we do not anticipate that the uneven teams will lead to substantively different outputs.\n Suggested outline You must write and present your analysis as if presenting to a C-suite executive. If you are not familiar with this terminology, the C-suite includes, e.g., the CEO, CFO, and COO of a given company. Generally speaking, such executives are not particularly analytically oriented, and therefore your explanations need to be clear, consise (their time is valuable) and contain actionable (or valuable) information.4 - Concretely, this requires a written memo, which describes the data, analyses, and results. This must be clear and easy to understand for a non-expert in your field. Figures and tables do not apply to the page limit.\nBelow is a very loose guide to the sort of content that we expect for the final project. Word limits are suggestions only. Note your final report will be approximately\nIntroduction Describe the motivation for this analysis. Briefly describe the dataset, and explain why the analysis you’re undertaking matters for society. (Or matters for some decision-making. You should not feel constrained to asking only “big questions.” The best projects will be narrow-scope but well-defined.) (≈300 words)\n Theory and Background Provide in-depth background about the data of interest and about your analytics question. (≈300 words)\n“Theory” Provide some theoretical guidance to the functional relationship you hope to explore. If you’re interested on how, say, height affects scoring in the NBA, write down a proposed function that might map height to scoring. Describe how you might look for this unknown relationship in the data.(≈300 words)\n Hypotheses Make predictions. Declare what you think will happen. (Note, this may carry over from second project.) (≈250 words)\n  Data and Analyses Data Given your motivations, limits on feasibility, and hypotheses, describe the data you use. (≈100 words)\n Analyses Generate the analyses relevant to your hypotheses and interests. Here you must include three figures and must describe what they contain in simple, easy to digest language. Why did you visualize these elements? Your analyses also must include brief discussion.\n(As many words as you need to fully describe your analysis and results)\n  Conclusion What caveats should we consider? Do you believe this is a truly causal relationship? Why does any of this matter to the decision-maker? (≈75 words)\n   Note that existing is taken to mean that you are not permitted to collect data by interacting with other people. That is not to say that you cannot gather data that previously has not been gathered into a single place—this sort of exercise is encouraged. But you cannot stand with a clipboard outside a store and count visitors (for instance).↩︎\n Some products, of course, solve this problem a little bit. For example, Dropbox allows users to share files with ease (of any file type) and saves a (coarse) version history. However, Dropbox does not allow multiple users to work on the same file, and has no way of merging edits together.↩︎\n This logic is also why we utilize only free software in this course. It sucks to get really good at, say, SAS (as I did many years ago) only to realize that the software costs about $10000 and many firms are unwilling to spent that. We will try our best to avoid giving you dead-end skills.↩︎\n This exercise provides you with an opportunity to identify your marketable skills and to practice them. I encourage those who will be looking for jobs soon to take this exercise seriously.↩︎\n   ","date":1607904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604325415,"objectID":"8d16837a0c729f9c31150a71deaf1f1e","permalink":"/assignment/final-project/","publishdate":"2020-12-14T00:00:00Z","relpermalink":"/assignment/final-project/","section":"assignment","summary":"Requirements Teams Suggested outline Introduction Theory and Background Data and Analyses Conclusion    Requirements Data analytics is inherently a hands-on endeavor. Accordingly, the final project for this class is hands-on. As per the overview page, the final project has the following elements:\nFor your final project in this class, you will analyze existing data in some area of interest to you.1 Aggregating data from multiple sources is encouraged, but is not required.","tags":null,"title":"Final project","type":"docs"},{"authors":null,"categories":null,"content":"  Part 1: Hypotheses Past 2: Instructions Evaluation  Data cleaning code Data to possibly use in your plot Country totals over time Cumulative country totals over time Continent totals over time Cumulative continent totals over time    Each member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.\n The United States has resettled more than 600,000 refugees from 60 different countries since 2006.1\nIn this project, you will use R, ggplot and some form of graphics editor to explore where these refugees have come from.\nPart 1: Hypotheses For this part of the assignment, you need to provide five hypotheses about the relationship between variables in a dataset. You can (and should) consider making hypotheses about the dataset that you plan to use for your final project. However, this is not a requirement. All that is required is that you provide five hypotheses about some data. Your write-up should have an enumerated list of questions (e.g., “1. Are there more murders in states that have high unemployment.”). You will receive 2 points for each hypothesis.\n Past 2: Instructions Here’s what you need to do:\n Download the Department of Homeland Security’s annual count of people granted refugee status between 2006-2015:\n DHS refugees, 2006-2015\nSave this somewhere on your computer (you might need to right click on this link and choose “Save link as…”, since your browser may try to display it as text). This data was originally uploaded by the Department of Homeland Security to Kaggle, and is provided with a public domain license.\n Clean the data using the code we’ve given you below. As always, this code is presented without guarantee. You may need to deal with a few issues, depending on your computer’s setup.\n Summarize the data somehow. There is data for 60 countries over 10 years, so you’ll probably need to aggregate or reshape the data somehow (unless you do a 60-country sparkline). I’ve included some examples down below.\n Create an appropriate time-based visualization based on the data. I’ve shown a few different ways to summarize the data so that it’s plottable down below. Don’t just calculate overall averages or totals per country—the visualization needs to deal with change over time. Do as much polishing and refining in R—make adjustments to the colors, scales, labels, grid lines, and even fonts, etc.\n Refine and polish the saved image, adding annotations, changing colors, and otherwise enhancing it.\n Design and write a poster (no word limit). Your poster should look like a polished image that you might see in a newspaper. You can (and should consider) integrating other images like national flags or arrows to convey some semantic meaning.\n Upload the following outputs to D2L:\n Your code (.Rmd) that generates the unpolished graphic. Your final poster, saved as a PDF.   For this assignment, we are less concerned with the code (that’s why we gave most of it to you), and more concerned with the design. Choose good colors based on palettes. Choose good, clean fonts. Use the heck out of theme(). Add informative design elements. Make it look beautiful. Refer to the design resources here.\nPlease seek out help when you need it! You know enough R (and have enough examples of code from class and your readings) to be able to do this. You can do this, and you’ll feel like a budding dataviz witch/wizard when you’re done.\nEvaluation I will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:\nPart 1\nEach hypothesis is worth 2 points. (This is intended to be some free points for all; 10 points)\nPart 2\nTechnical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (15 points)\n Professionalism of visuals: Does the visualizations look like something you might see on TV or in the newspaper? (15 points)\n Poster clarity: Does your poster clearly convey some point? (10 points)\n    Data cleaning code The data isn’t perfectly clean and tidy, but it’s real world data, so this is normal. Because the emphasis for this assignment is on design, not code, we’ve provided code to help you clean up the data.\nThese are the main issues with the data:\n There are non-numeric values in the data, like -, X, and D. The data isn’t very well documented; we’re assuming - indicates a missing value, but we’re not sure what X and D mean, so for this assignment, we’ll just assume they’re also missing.\n The data generally includes rows for dozens of countries, but there are also rows for some continents, “unknown,” “other,” and a total row. Because Africa is not a country, and neither are the other continents, we want to exclude all non-countries.\n Maintaining consistent country names across different datasets is literally the woooooooorst. Countries have different formal official names and datasets are never consistent in how they use those names.2 It’s such a tricky problem that social scientists have spent their careers just figuring out how to properly name and code countries. Really.3 There are international standards for country codes, though, like ISO 3166-1 alpha 3 (my favorite), also known as ISO3. It’s not perfect—it omits microstates (some Polynesian countries) and gray area states (Palestine, Kosovo)—but it’s an international standard, so it has that going for it.\n To ensure that country names are consistent in this data, we use the countrycode package (install it if you don’t have it), which is amazing. The countrycode() function will take a country name in a given coding scheme and convert it to a different coding scheme using this syntax:\n countrycode(variable, \u0026quot;current-coding-scheme\u0026quot;, \u0026quot;new-coding-scheme\u0026quot;) It also does a farily good job at guessing and parsing inconsistent country names (e.g. it will recognize “Congo, Democratic Republic”, even though it should technically be “Democratic Republic of the Congo”). Here, we use countrycode() to convert the inconsistent country names into ISO3 codes. We then create a cleaner version of the origin_country column by converting the ISO3 codes back into country names. Note that the function chokes on North Korea initially, since it’s included as “Korea, North”—we use the custom_match argument to help the function out.\n The data isn’t tidy—there are individual columns for each year. gather() takes every column and changes it to a row. We exclude the country, region, continent, and ISO3 code from the change-into-rows transformation with -origin_country, -iso3, -origin_region, -origin_continent.\n Currently, the year is being treated as a number, but it’s helpful to also treat it as an actual date. We create a new variable named year_date that converts the raw number (e.g. 2009) into a date. The date needs to have at least a month, day, and year, so we actually convert it to January 1, 2009 with ymd(paste0(year, \"-01-01\")).\n  library(tidyverse) # For ggplot, dplyr, and friends library(countrycode) # For dealing with country names, abbreviations, and codes library(lubridate) # For dealing with dates refugees_raw \u0026lt;- read_csv(\u0026quot;data/refugee_status.csv\u0026quot;, na = c(\u0026quot;-\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;D\u0026quot;)) non_countries \u0026lt;- c(\u0026quot;Africa\u0026quot;, \u0026quot;Asia\u0026quot;, \u0026quot;Europe\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;Oceania\u0026quot;, \u0026quot;South America\u0026quot;, \u0026quot;Unknown\u0026quot;, \u0026quot;Other\u0026quot;, \u0026quot;Total\u0026quot;) refugees_clean \u0026lt;- refugees_raw %\u0026gt;% # Make this column name easier to work with rename(origin_country = `Continent/Country of Nationality`) %\u0026gt;% # Get rid of non-countries filter(!(origin_country %in% non_countries)) %\u0026gt;% # Convert country names to ISO3 codes mutate(iso3 = countrycode(origin_country, \u0026quot;country.name\u0026quot;, \u0026quot;iso3c\u0026quot;, custom_match = c(\u0026quot;Korea, North\u0026quot; = \u0026quot;PRK\u0026quot;))) %\u0026gt;% # Convert ISO3 codes to country names, regions, and continents mutate(origin_country = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;country.name\u0026quot;), origin_region = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;region\u0026quot;), origin_continent = countrycode(iso3, \u0026quot;iso3c\u0026quot;, \u0026quot;continent\u0026quot;)) %\u0026gt;% # Make this data tidy gather(year, number, -origin_country, -iso3, -origin_region, -origin_continent) %\u0026gt;% # Make sure the year column is numeric + make an actual date column for years mutate(year = as.numeric(year), year_date = ymd(paste0(year, \u0026quot;-01-01\u0026quot;)))  Data to possibly use in your plot Here are some possible summaries of the data you might use…\nCountry totals over time This is just the refugees_clean data frame I gave you. You’ll want to filter it and select specific countries, though—you won’t really be able to plot 60 countries all at once unless you use sparklines.\n## # A tibble: 6 x 7 ## origin_country iso3 origin_region origin_continent year number year_date ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; ## 1 Afghanistan AFG South Asia Asia 2006 651 2006-01-01 ## 2 Angola AGO Sub-Saharan Afr… Africa 2006 13 2006-01-01 ## 3 Armenia ARM Europe \u0026amp; Centra… Asia 2006 87 2006-01-01 ## 4 Azerbaijan AZE Europe \u0026amp; Centra… Asia 2006 77 2006-01-01 ## 5 Belarus BLR Europe \u0026amp; Centra… Europe 2006 350 2006-01-01 ## 6 Bhutan BTN South Asia Asia 2006 3 2006-01-01  Cumulative country totals over time Note the cumsum() function—it calculates the cumulative sum of a column.\nrefugees_countries_cumulative \u0026lt;- refugees_clean %\u0026gt;% arrange(year_date) %\u0026gt;% group_by(origin_country) %\u0026gt;% mutate(cumulative_total = cumsum(number)) ## # A tibble: 6 x 7 ## # Groups: origin_country [1] ## origin_country iso3 origin_continent year number year_date cumulative_total ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Afghanistan AFG Asia 2006 651 2006-01-01 651 ## 2 Afghanistan AFG Asia 2007 441 2007-01-01 1092 ## 3 Afghanistan AFG Asia 2008 576 2008-01-01 1668 ## 4 Afghanistan AFG Asia 2009 349 2009-01-01 2017 ## 5 Afghanistan AFG Asia 2010 515 2010-01-01 2532 ## 6 Afghanistan AFG Asia 2011 428 2011-01-01 2960  Continent totals over time Note the na.rm = TRUE argument in sum(). This makes R ignore any missing data when calculating the total. Without it, if R finds a missing value in the column, it will mark the final sum as NA too, which we don’t want.\nrefugees_continents \u0026lt;- refugees_clean %\u0026gt;% group_by(origin_continent, year_date) %\u0026gt;% summarize(total = sum(number, na.rm = TRUE)) ## `summarise()` regrouping output by \u0026#39;origin_continent\u0026#39; (override with `.groups` argument) ## # A tibble: 6 x 3 ## # Groups: origin_continent [1] ## origin_continent year_date total ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 2006-01-01 18116 ## 2 Africa 2007-01-01 17473 ## 3 Africa 2008-01-01 8931 ## 4 Africa 2009-01-01 9664 ## 5 Africa 2010-01-01 13303 ## 6 Africa 2011-01-01 7677  Cumulative continent totals over time Note that there are two group_by() functions here. First we get the total number of refugees per continent per year, then we group by continent only to get the cumulative sum of refugees across continents.\nrefugees_continents_cumulative \u0026lt;- refugees_clean %\u0026gt;% group_by(origin_continent, year_date) %\u0026gt;% summarize(total = sum(number, na.rm = TRUE)) %\u0026gt;% arrange(year_date) %\u0026gt;% group_by(origin_continent) %\u0026gt;% mutate(cumulative_total = cumsum(total)) ## `summarise()` regrouping output by \u0026#39;origin_continent\u0026#39; (override with `.groups` argument) ## # A tibble: 6 x 4 ## # Groups: origin_continent [1] ## origin_continent year_date total cumulative_total ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Africa 2006-01-01 18116 18116 ## 2 Africa 2007-01-01 17473 35589 ## 3 Africa 2008-01-01 8931 44520 ## 4 Africa 2009-01-01 9664 54184 ## 5 Africa 2010-01-01 13303 67487 ## 6 Africa 2011-01-01 7677 75164    You found the footnote! Week 8’s writing assignment has three parts. First, write down your guess (a probability; stated as a percentage) for the odds that Joe Biden is elected President of the United States. Second, write a short paragraph about why you made that guess. Are you integrating data into your guess? If so, what data? Do you believe that personal experience is useful for making such predictions, or that personal experience can be misleading. As always, the total writing should be a couple hundred words.↩︎\n For instance, “North Korea”, “Korea, North”, “DPRK”, “Korea, Democratic People’s Republic of”, and “Democratic People’s Republic of Korea”, and “Korea (DPRK)” are all perfectly normal versions of the country’s name and you’ll find them all in the wild.↩︎\n See Gleditsch, Kristian S. \u0026amp; Michael D. Ward. 1999. “Interstate System Membership: A Revised List of the Independent States since 1816.” International Interactions 25: 393-413; or the “ICOW Historical State Names Data Set”.↩︎\n   ","date":1605225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604325415,"objectID":"5b66ec5f9db457ac4a522cdf36fd69ce","permalink":"/assignment/project2/","publishdate":"2020-11-13T00:00:00Z","relpermalink":"/assignment/project2/","section":"assignment","summary":"Part 1: Hypotheses Past 2: Instructions Evaluation  Data cleaning code Data to possibly use in your plot Country totals over time Cumulative country totals over time Continent totals over time Cumulative continent totals over time    Each member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.","tags":null,"title":"Project 2","type":"docs"},{"authors":null,"categories":null,"content":"   Required Reading  Guiding Questions  The Bias–Variance Tradeoff  R Setup and Source The Regression Setup Reducible and Irreducible Error Bias-Variance Decomposition Using Simulation to Estimate Bias and Variance Estimating Expected Prediction Error Model Flexibility  Linear Models k-Nearest Neighbors Decision Trees     Required Reading  This page.  Chapter 2 in Introduction to Statistical Learning with Applications in R.  Guiding Questions  What is the relationship between bias, variance, and mean squared error? What is the relationship between model flexibility and training error? What is the relationship between model flexibility and validation (or test) error?    The Bias–Variance Tradeoff This lecture will begin to dig into some theoretical details of estimating regression functions, in particular how the bias-variance tradeoff helps explain the relationship between model flexibility and the errors a model makes.\nThis content is currently under construction. You can expect it to be a lot less polished than other sections.\nDon’t freak out if this seems mathematically overwhelming. We’ll walk through relatively slowly. It’s not super important to follow the nitty-gritty details; but the broad takeaways are quite important.\nR Setup and Source library(tibble) # data frame printing library(dplyr) # data manipulation library(caret) # fitting knn library(rpart) # fitting trees library(rpart.plot) # plotting trees  The Regression Setup Consider the general regression setup where we are given a random pair \\((X, Y) \\in \\mathbb{R}^p \\times \\mathbb{R}\\). We would like to “predict” \\(Y\\) with some function of \\(X\\), say, \\(f(X)\\).\nTo clarify what we mean by “predict,” we specify that we would like \\(f(X)\\) to be “close” to \\(Y\\). To further clarify what we mean by “close,” we define the squared error loss of estimating \\(Y\\) using \\(f(X)\\).\n\\[ L(Y, f(X)) \\triangleq (Y - f(X)) ^ 2 \\]\nNow we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the risk of estimating \\(Y\\) using \\(f(X)\\).\n\\[ R(Y, f(X)) \\triangleq \\mathbb{E}[L(Y, f(X))] = \\mathbb{E}_{X, Y}[(Y - f(X)) ^ 2] \\]\nBefore attempting to minimize the risk, we first re-write the risk after conditioning on \\(X\\).\n\\[ \\mathbb{E}_{X, Y} \\left[ (Y - f(X)) ^ 2 \\right] = \\mathbb{E}_{X} \\mathbb{E}_{Y \\mid X} \\left[ ( Y - f(X) ) ^ 2 \\mid X = x \\right] \\]\nMinimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to \\(Y \\mid X\\), essentially minimizing the risk pointwise, for each \\(x\\).\nIt turns out, that the risk is minimized by setting \\(f(x)\\) to be equal the conditional mean of \\(Y\\) given \\(X\\),\n\\[ f(x) = \\mathbb{E}(Y \\mid X = x) \\]\nwhich we call the regression function.1\nNote that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss.\n\\[ L(Y, f(X)) \\triangleq | Y - f(X) | \\]\nThe risk would then be minimized setting \\(f(x)\\) equal to the conditional median.\n\\[ f(x) = \\text{median}(Y \\mid X = x) \\]\nDespite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations.\nNow, given data \\(\\mathcal{D} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}\\), our goal becomes finding some \\(\\hat{f}\\) that is a good estimate of the regression function \\(f\\). We’ll see that this amounts to minimizing what we call the reducible error.\n Reducible and Irreducible Error Suppose that we obtain some \\(\\hat{f}\\), how well does it estimate \\(f\\)? We define the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\). A good \\(\\hat{f}\\) will have a low expected prediction error.\n\\[ \\text{EPE}\\left(Y, \\hat{f}(X)\\right) \\triangleq \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ \\left( Y - \\hat{f}(X) \\right)^2 \\right] \\]\nThis expectation is over \\(X\\), \\(Y\\), and also \\(\\mathcal{D}\\). The estimate \\(\\hat{f}\\) is actually random depending on the data, \\(\\mathcal{D}\\), used to estimate \\(\\hat{f}\\). We could actually write \\(\\hat{f}(X, \\mathcal{D})\\) to make this dependence explicit, but our notation will become cumbersome enough as it is.\nLike before, we’ll condition on \\(X\\). This results in the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\).\n\\[ \\text{EPE}\\left(Y, \\hat{f}(x)\\right) = \\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[ \\left(Y - \\hat{f}(X) \\right)^2 \\mid X = x \\right] = \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]}_\\textrm{reducible error} + \\underbrace{\\mathbb{V}_{Y \\mid X} \\left[ Y \\mid X = x \\right]}_\\textrm{irreducible error} \\]\nA number of things to note here:\n The expected prediction error is for a random \\(Y\\) given a fixed \\(x\\) and a random \\(\\hat{f}\\). As such, the expectation is over \\(Y \\mid X\\) and \\(\\mathcal{D}\\). Our estimated function \\(\\hat{f}\\) is random depending on the data, \\(\\mathcal{D}\\), which is used to perform the estimation. The expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\) has been decomposed into two errors:  The reducible error, which is the expected squared error loss of estimation \\(f(x)\\) using \\(\\hat{f}(x)\\) at a fixed point \\(x\\). The only thing that is random here is \\(\\mathcal{D}\\), the data used to obtain \\(\\hat{f}\\). (Both \\(f\\) and \\(x\\) are fixed.) We’ll often call this reducible error the mean squared error of estimating \\(f(x)\\) using \\(\\hat{f}\\) at a fixed point \\(x\\). \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) \\triangleq \\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]\\] The irreducible error. This is simply the variance of \\(Y\\) given that \\(X = x\\), essentially noise that we do not want to learn. This is also called the Bayes error.   As the name suggests, the reducible error is the error that we have some control over. But how do we control this error?\n Bias-Variance Decomposition After decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error.\nRecall the definition of the bias of an estimator.\n\\[ \\text{bias}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta \\]\nAlso recall the definition of the variance of an estimator.\n\\[ \\mathbb{V}(\\hat{\\theta}) = \\text{var}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left [ ( \\hat{\\theta} -\\mathbb{E}\\left[\\hat{\\theta}\\right] )^2 \\right] \\]\nUsing this, we further decompose the reducible error (mean squared error) into bias squared and variance.\n\\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right] = \\underbrace{\\left(f(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}(x) \\right)} + \\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}(x) \\right)} \\]\nThis is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function \\(f\\) using \\(\\hat{f}\\) at some point \\(x\\).\n\\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\text{bias}^2 \\left(\\hat{f}(x) \\right) + \\text{var} \\left(\\hat{f}(x) \\right) \\]\nIn a perfect world, we would be able to find some \\(\\hat{f}\\) which is unbiased, that is \\(\\text{bias}\\left(\\hat{f}(x) \\right) = 0\\), which also has low variance. In practice, this isn’t always possible.\nIt turns out, there is a bias-variance tradeoff. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance.\nIn the context of regression, models are biased when:\n Parametric: The form of the model does not incorporate all the necessary variables, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic. Non-parametric: The model provides too much smoothing.  In the context of regression, models are variable when:\n Parametric: The form of the model incorporates too many variables, or the form of the relationship is too flexible. For example, a parametric model assumes a cubic relationship, but the true relationship is linear. Non-parametric: The model does not provide enough smoothing. It is very, “wiggly.”  So for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data.\nRecall that when fitting models, we’ve seen that train RMSE decreases as model flexibility is increasing. (Technically it is non-increasing.) For validation RMSE, we expect to see a U-shaped curve. Importantly, validation RMSE decreases, until a certain flexibility, then begins to increase.\nNow we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this.\nThe three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the flexibility that minimizes the prediction error.\nTo summarize, if we assume that irreducible error can be written as\n\\[ \\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2 \\]\nthen we can write the full decomposition of the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}\\) when \\(X = x\\) as\n\\[ \\text{EPE}\\left(Y, \\hat{f}(x)\\right) = \\underbrace{\\text{bias}^2\\left(\\hat{f}(x)\\right) + \\text{var}\\left(\\hat{f}(x)\\right)}_\\textrm{reducible error} + \\sigma^2. \\]\nAs model flexibility increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model flexibility to find a model that will predict well on unseen observations.\nTying this all together, the above image shows how we “expect” training and validation error to behavior in relation to model flexibility.2 In practice, we won’t always see such a nice “curve” in the validation error, but we expect to see the general trends.\n Using Simulation to Estimate Bias and Variance We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function \\(f(x) = x^2\\).\nf = function(x) { x ^ 2 } More specifically, we’d like to predict an observation, \\(Y\\), given that \\(X = x\\) by using \\(\\hat{f}(x)\\) where\n\\[ \\mathbb{E}[Y \\mid X = x] = f(x) = x^2 \\]\nand\n\\[ \\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2. \\]\nAlternatively, we could write this as\n\\[ Y = f(X) + \\epsilon \\]\nwhere \\(\\mathbb{E}[\\epsilon] = 0\\) and \\(\\mathbb{V}[\\epsilon] = \\sigma ^ 2\\). In this formulation, we call \\(f(X)\\) the signal and \\(\\epsilon\\) the noise.\nTo carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following R code.\ngen_sim_data = function(f, sample_size = 100) { x = runif(n = sample_size, min = 0, max = 1) y = rnorm(n = sample_size, mean = f(x), sd = 0.3) tibble(x, y) } Also note that if you prefer to think of this situation using the \\(Y = f(X) + \\epsilon\\) formulation, the following code represents the same data generating process.\ngen_sim_data = function(f, sample_size = 100) { x = runif(n = sample_size, min = 0, max = 1) eps = rnorm(n = sample_size, mean = 0, sd = 0.75) y = f(x) + eps tibble(x, y) } To completely specify the data generating process, we have made more model assumptions than simply \\(\\mathbb{E}[Y \\mid X = x] = x^2\\) and \\(\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\\). In particular,\n The \\(x_i\\) in \\(\\mathcal{D}\\) are sampled from a uniform distribution over \\([0, 1]\\). The \\(x_i\\) and \\(\\epsilon\\) are independent. The \\(y_i\\) in \\(\\mathcal{D}\\) are sampled from the conditional normal distribution.  \\[ Y \\mid X \\sim N(f(x), \\sigma^2) \\]\nUsing this setup, we will generate datasets, \\(\\mathcal{D}\\), with a sample size \\(n = 100\\) and fit four models.\n\\[ \\begin{aligned} \\texttt{predict(fit0, x)} \u0026amp;= \\hat{f}_0(x) = \\hat{\\beta}_0\\\\ \\texttt{predict(fit1, x)} \u0026amp;= \\hat{f}_1(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\\\ \\texttt{predict(fit2, x)} \u0026amp;= \\hat{f}_2(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 \\\\ \\texttt{predict(fit9, x)} \u0026amp;= \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9 \\end{aligned} \\]\nTo get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.\nset.seed(1) sim_data = gen_sim_data(f) fit_0 = lm(y ~ 1, data = sim_data) fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data) fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data) fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data) Note that technically we’re being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes.\nPlotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.\nThe following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each.\nThis plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.\nWhile the ninth degree model doesn’t appear to be correct for any of these three simulations, we’ll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn’t the only goal we’re after, since in practice, we’ll only have a single dataset. This is why we’d also like our models to exhibit low variance.\nWe could have also fit \\(k\\)-nearest neighbors models to these three datasets.\nHere we see that when \\(k = 100\\) we have a biased model with very low variance.3 When \\(k = 5\\), we again have a highly variable model.\nThese two sets of plots reinforce our intuition about the bias-variance tradeoff. Flexible models (ninth degree polynomial and \\(k\\) = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and \\(k = 100\\)) are very biased, but have extremely low variance.\nWe will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates of \\(f(x)\\) given by these four models at the point \\(x = 0.90\\). We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.\nset.seed(1) n_sims = 250 n_models = 4 x = data.frame(x = 0.90) # fixed point at which we make predictions predictions = matrix(0, nrow = n_sims, ncol = n_models) for (sim in 1:n_sims) { # simulate new, random, training data # this is the only random portion of the bias, var, and mse calculations # this allows us to calculate the expectation over D sim_data = gen_sim_data(f) # fit models fit_0 = lm(y ~ 1, data = sim_data) fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data) fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data) fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data) # get predictions predictions[sim, 1] = predict(fit_0, x) predictions[sim, 2] = predict(fit_1, x) predictions[sim, 3] = predict(fit_2, x) predictions[sim, 4] = predict(fit_9, x) } Note that this is one of many ways we could have accomplished this task using R. For example we could have used a combination of replicate() and *apply() functions. Alternatively, we could have used a tidyverse approach, which likely would have used some combination of dplyr, tidyr, and purrr.\nOur approach, which would be considered a base R approach, was chosen to make it as clear as possible what is being done. The tidyverse approach is rapidly gaining popularity in the R community, but might make it more difficult to see what is happening here, unless you are already familiar with that approach.\nAlso of note, while it may seem like the output stored in predictions would meet the definition of tidy data given by Hadley Wickham since each row represents a simulation, it actually falls slightly short. For our data to be tidy, a row should store the simulation number, the model, and the resulting prediction. We’ve actually already aggregated one level above this. Our observational unit is a simulation (with four predictions), but for tidy data, it should be a single prediction.\nThe above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, \\(f(x = 0.90) = (0.9)^2 = 0.81\\), is given by the solid black horizontal line.\nTwo things are immediately clear:\n As flexibility increases, bias decreases. The mean of a model’s predictions is closer to the truth. As flexibility increases, variance increases. The variance about the mean of a model’s predictions increases.  The goal of this simulation study is to show that the following holds true for each of the four models.\n\\[ \\text{MSE}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\underbrace{\\left(\\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] - f(0.90) \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}_k(0.90) \\right)} + \\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}_k(0.90) - \\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}_k(0.90) \\right)} \\]\nWe’ll use the empirical results of our simulations to estimate these quantities. (Yes, we’re using estimation to justify facts about estimation.) Note that we’ve actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we’ve performed just enough simulations to obtain the desired results. (Since we’re estimating estimation, the bigger the sample size, the better.)\nTo estimate the mean squared error of our predictions, we’ll use\n\\[ \\widehat{\\text{MSE}}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(f(0.90) - \\hat{f}_k^{[i]}(0.90) \\right)^2 \\]\nwhere \\(\\hat{f}_k^{[i]}(0.90)\\) is the estimate of \\(f(0.90)\\) using the \\(i\\)-th from the polynomial degree \\(k\\) model.\nWe also write an accompanying R function.\nget_mse = function(truth, estimate) { mean((estimate - truth) ^ 2) } Similarly, for the bias of our predictions we use,\n\\[ \\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90) \\]\nAnd again, we write an accompanying R function.\nget_bias = function(estimate, truth) { mean(estimate) - truth } Lastly, for the variance of our predictions we have\n\\[ \\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2 \\]\nWhile there is already R function for variance, the following is more appropriate in this situation.\nget_var = function(estimate) { mean((estimate - mean(estimate)) ^ 2) } To quickly obtain these results for each of the four models, we utilize the apply() function.\nbias = apply(predictions, 2, get_bias, truth = f(x = 0.90)) variance = apply(predictions, 2, get_var) mse = apply(predictions, 2, get_mse, truth = f(x = 0.90)) We summarize these results in the following table.\n  Degree Mean Squared Error Bias Squared Variance    0 0.22643 0.22476 0.00167  1 0.00829 0.00508 0.00322  2 0.00387 0.00005 0.00381  9 0.01019 0.00002 0.01017    A number of things to notice here:\n We use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as flexibility increases. The squared bias trend which we see here is decreasing as flexibility increases, which we expect to see in general. The exact opposite is true of variance. As model flexibility increases, variance increases. The mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error!  We can check for these trends with the diff() function in R.\nall(diff(bias ^ 2) \u0026lt; 0) ## [1] TRUE all(diff(variance) \u0026gt; 0) ## [1] TRUE diff(mse) \u0026lt; 0 ## 1 2 9 ## TRUE TRUE FALSE The models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance.\nModels with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees.\n\\[ \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9 \\]\nThen, since least squares estimation is unbiased, importantly,\n\\[ \\mathbb{E}\\left[\\hat{\\beta}_d\\right] = \\beta_d = 0 \\]\nfor \\(d = 3, 4, \\ldots 9\\), we have\n\\[ \\mathbb{E}\\left[\\hat{f}_9(x)\\right] = \\beta_0 + \\beta_1 x + \\beta_2 x^2 \\]\nNow we can finally verify the bias-variance decomposition.\nbias ^ 2 + variance == mse ## 0 1 2 9 ## FALSE FALSE FALSE TRUE But wait, this says it isn’t true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models.\nall.equal(bias ^ 2 + variance, mse) ## [1] TRUE See ?all.equal() for details.\nSo far, we’ve focused our efforts on looking at the mean squared error of estimating \\(f(0.90)\\) using \\(\\hat{f}(0.90)\\). We could also look at the expected prediction error of using \\(\\hat{f}(X)\\) when \\(X = 0.90\\) to estimate \\(Y\\).\n\\[ \\text{EPE}\\left(Y, \\hat{f}_k(0.90)\\right) = \\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[ \\left(Y - \\hat{f}_k(X) \\right)^2 \\mid X = 0.90 \\right] \\]\nWe can estimate this quantity for each of the four models using the simulation study we already performed.\nget_epe = function(realized, estimate) { mean((realized - estimate) ^ 2) } y = rnorm(n = nrow(predictions), mean = f(x = 0.9), sd = 0.3) epe = apply(predictions, 2, get_epe, realized = y) epe ## 0 1 2 9 ## 0.3180470 0.1104055 0.1095955 0.1205570 What about the unconditional expected prediction error. That is, for any \\(X\\), not just \\(0.90\\). Specifically, the expected prediction error of estimating \\(Y\\) using \\(\\hat{f}(X)\\). The following (new) simulation study provides an estimate of\n\\[ \\text{EPE}\\left(Y, \\hat{f}_k(X)\\right) = \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ \\left( Y - \\hat{f}_k(X) \\right)^2 \\right] \\]\nfor the quadratic model, that is \\(k = 2\\) as we have defined \\(k\\).\nset.seed(42) n_sims = 2500 X = runif(n = n_sims, min = 0, max = 1) Y = rnorm(n = n_sims, mean = f(X), sd = 0.3) f_hat_X = rep(0, length(X)) for (i in seq_along(X)) { sim_data = gen_sim_data(f) fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data) f_hat_X[i] = predict(fit_2, newdata = data.frame(x = X[i])) } # truth 0.3 ^ 2 ## [1] 0.09 # via simulation mean((Y - f_hat_X) ^ 2) ## [1] 0.09566445 Note that in practice, we should use many more simulations in this study.\n Estimating Expected Prediction Error While previously, we only decomposed the expected prediction error conditionally, a similar argument holds unconditionally.\nAssuming\n\\[ \\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2. \\]\nwe have\n\\[ \\text{EPE}\\left(Y, \\hat{f}(X)\\right) = \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ (Y - \\hat{f}(X))^2 \\right] = \\underbrace{\\mathbb{E}_{X} \\left[\\text{bias}^2\\left(\\hat{f}(X)\\right)\\right] + \\mathbb{E}_{X} \\left[\\text{var}\\left(\\hat{f}(X)\\right)\\right]}_\\textrm{reducible error} + \\sigma^2 \\]\nLastly, we note that if\n\\[ \\mathcal{D} = \\mathcal{D}_{\\texttt{trn}} \\cup \\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i = 1, 2, \\ldots n \\]\nwhere\n\\[ \\mathcal{D}_{\\texttt{trn}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{trn} \\]\nand\n\\[ \\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{tst} \\]\nThen, if we have a model fit to the training data \\(\\mathcal{D}_{\\texttt{trn}}\\), we can use the test mean squared error\n\\[ \\sum_{i \\in \\texttt{tst}}\\left(y_i - \\hat{f}(x_i)\\right) ^ 2 \\]\nas an estimate of\n\\[ \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ (Y - \\hat{f}(X))^2 \\right] \\]\nthe expected prediction error.4\nHow good is this estimate? Well, if \\(\\mathcal{D}\\) is a random sample from \\((X, Y)\\), and the \\(\\texttt{tst}\\) data are randomly sampled observations randomly sampled from \\(i = 1, 2, \\ldots, n\\), then it is a reasonable estimate. However, it is rather variable due to the randomness of selecting the observations for the test set, if the test set is small.\n Model Flexibility Let’s return to the simiulated dataset we used occaisionally in the linear regression content. Recall there was a single feature \\(x\\) with the following properties:\n# define regression function cubic_mean = function(x) { 1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3 } We then generated some data around this function with some added noise:\n# define full data generating process gen_slr_data = function(sample_size = 100, mu) { x = runif(n = sample_size, min = -1, max = 1) y = mu(x) + rnorm(n = sample_size) tibble(x, y) } After defining the data generating process, we generate and split the data.\n# simulate entire dataset set.seed(3) sim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean) # test-train split slr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data)) slr_trn = sim_slr_data[slr_trn_idx, ] slr_tst = sim_slr_data[-slr_trn_idx, ] # estimation-validation split slr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn)) slr_est = slr_trn[slr_est_idx, ] slr_val = slr_trn[-slr_est_idx, ] # check data head(slr_trn, n = 10) ## # A tibble: 10 x 2 ## x y ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.573 -1.18 ## 2 0.807 0.576 ## 3 0.272 -0.973 ## 4 -0.813 -1.78 ## 5 -0.161 0.833 ## 6 0.736 1.07 ## 7 -0.242 2.97 ## 8 0.520 -1.64 ## 9 -0.664 0.269 ## 10 -0.777 -2.02 For validating models, we will use RMSE.\n# helper function for calculating RMSE calc_rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } Let’s check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility.\nThis picture is an idealized version of what we expect to see, but we’ll illustrate the sorts of validate “curves” that we might see in practice.\nNote that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details.\nLinear Models First up, linear models. We will fit polynomial models with degree from one to nine, and then validate.\n# fit polynomial models poly_mod_est_list = list( poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est), poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est), poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est), poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est), poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est), poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est), poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est), poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est), poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est) ) The plot below visualizes the results.\nWhat do we see here? As the polynomial degree increases:\n The training error decreases. The validation error decreases, then increases.  This more of less matches the idealized version above, but the validation “curve” is much more jagged. This is something that we can expect in practice.\nWe have previously noted that training error isn’t particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.5\n k-Nearest Neighbors Next up, k-nearest neighbors. We will consider values for \\(k\\) that are odd and between \\(1\\) and \\(45\\) inclusive.\n# helper function for fitting knn models fit_knn_mod = function(neighbors) { knnreg(y ~ x, data = slr_est, k = neighbors) } # define values of tuning parameter k to evaluate k_to_try = seq(from = 1, to = 45, by = 2) # fit knn models knn_mod_est_list = lapply(k_to_try, fit_knn_mod) The plot below visualizes the results.\nHere we see the “opposite” of the usual plot. Why? Because with k-nearest neighbors, a small value of \\(k\\) generates a flexible model compared to larger values of \\(k\\). So visually, this plot is flipped. That is we see that as \\(k\\) increases:\n The training error increases. The validation error decreases, then increases.  Important to note here: the pattern above only holds “in general,” that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set.\n Decision Trees Lastly, we evaluate some decision tree models. We choose some arbitrary values of cp to evaluate, while holding minsplit constant at 5. There are arbitrary choices that produce a plot that is useful for discussion.\n# helper function for fitting decision tree models tree_knn_mod = function(flex) { rpart(y ~ x, data = slr_est, cp = flex, minsplit = 5) } # define values of tuning parameter cp to evaluate cp_to_try = c(0.5, 0.3, 0.1, 0.05, 0.01, 0.001, 0.0001) # fit decision tree models tree_mod_est_list = lapply(cp_to_try, tree_knn_mod) The plot below visualizes the results.\nBased on this plot, how is cp related to model flexibility?6\n    Note that in this section, we will refer to \\(f(x)\\) as the regression function instead of \\(\\mu(x)\\) for unimportant and arbitrary reasons.↩︎\n Someday, someone will tell you this is a lie. They aren’t wrong. In modern deep learning, there is a concept called Deep Double Descent. See also @belkin2018reconciling.↩︎\n It’s actually the same as the 0 predictor linear model. Can you see why?↩︎\n In practice we prefer RMSE to MSE for comparing models and reporting because of the units.↩︎\n In practice, if you already know how your model’s flexibility works, by checking that the training error goes down as you increase flexibility, you can check that you have done your coding and model training correctly.↩︎\n As cp increases, model flexibility decreases.↩︎\n   ","date":1604534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604584875,"objectID":"35bbbb8e03581e65c2b2e10271df2fce","permalink":"/content/09-content/","publishdate":"2020-11-05T00:00:00Z","relpermalink":"/content/09-content/","section":"content","summary":"Required Reading  Guiding Questions  The Bias–Variance Tradeoff  R Setup and Source The Regression Setup Reducible and Irreducible Error Bias-Variance Decomposition Using Simulation to Estimate Bias and Variance Estimating Expected Prediction Error Model Flexibility  Linear Models k-Nearest Neighbors Decision Trees     Required Reading  This page.  Chapter 2 in Introduction to Statistical Learning with Applications in R.  Guiding Questions  What is the relationship between bias, variance, and mean squared error?","tags":null,"title":"The Bias-Variance Tradeoff","type":"docs"},{"authors":null,"categories":null,"content":"    Required Reading  Guiding Ideas  Nonparametric Regression  R Setup Mathematical Setup k-Nearest Neighbors Decision Trees Example: Credit Card Data    Required Reading  This page.  Guiding Ideas  How to use k-nearest neighbors for regression through the use of the knnreg() function from the caret package How to use decision trees for regression through the use of the rpart() function from the rpart package. How “making predictions” can be thought of as estimating the regression function, that is, the conditional mean of the response given values of the features. The difference between parametric and nonparametric methods. The difference between model parameters and tuning parameters methods. How these nonparametric methods deal with categorical variables and interactions. What is model flexibility? What is overfitting and how do we avoid it?    Nonparametric Regression In the next weeks, we will continue to explore models for making predictions, but now we will introduce nonparametric models that will contrast the parametric models that we have used previously.\nThis content is currently under construction. You can expect it to be a lot less polished than other sections.\nR Setup library(tibble) # data frame printing library(dplyr) # data manipulation library(caret) # fitting knn library(rpart) # fitting trees library(rpart.plot) # plotting trees library(knitr) # creating tables library(kableExtra) # styling tables  Mathematical Setup Let’s return to the setup we defined in the previous lectures. Consider a random variable \\(Y\\) which represents a response variable, and \\(p\\) feature variables \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\). We assume that the response variable \\(Y\\) is some function of the features, plus some random noise.\n\\[ Y = f(\\boldsymbol{X}) + \\epsilon \\]\nOur goal is to find some \\(f\\) such that \\(f(\\boldsymbol{X})\\) is close to \\(Y\\). More specifically we want to minimize the risk under squared error loss.\n\\[ \\mathbb{E}_{\\boldsymbol{X}, Y} \\left[ (Y - f(\\boldsymbol{X})) ^ 2 \\right] = \\mathbb{E}_{\\boldsymbol{X}} \\mathbb{E}_{Y \\mid \\boldsymbol{X}} \\left[ ( Y - f(\\boldsymbol{X}) ) ^ 2 \\mid \\boldsymbol{X} = \\boldsymbol{x} \\right] \\]\nWe saw previously (see the slides from last two content days) that this risk is minimized by the conditional mean of \\(Y\\) given \\(\\boldsymbol{X}\\),\n\\[ \\mu(\\boldsymbol{x}) \\triangleq \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] \\]\nwhich we call the regression function.\nOur goal then is to estimate this regression function. Let’s use an example where we know the true probability model:\n\\[ Y = 1 - 2x - 3x ^ 2 + 5x ^ 3 + \\epsilon \\]\nwhere \\(\\epsilon \\sim \\text{N}(0, \\sigma^2)\\).\nRecall that this implies that the regression function is\n\\[ \\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] = 1 - 2x - 3x ^ 2 + 5x ^ 3 \\]\nLet’s also pretend that we do not actually know this information, but instead have some data, \\((x_i, y_i)\\) for \\(i = 1, 2, \\ldots, n\\).\nWe simulate enough data to make the “pattern” clear-ish to recognize.\nWhen we use a linear model, we first need to make an assumption about the form of the regression function.\nFor example, we could assume that\n\\[ \\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\]\nwhich is fit in R using the lm() function\nlm(y ~ x + I(x ^ 2) + I(x ^ 3), data = sim_slr_data) ## ## Call: ## lm(formula = y ~ x + I(x^2) + I(x^3), data = sim_slr_data) ## ## Coefficients: ## (Intercept) x I(x^2) I(x^3) ## 0.8397 -2.7257 -2.3752 6.0906 Notice that what is returned are (maximum likelihood or least squares) estimates of the unknown \\(\\beta\\) coefficients. That is, the “learning” that takes place with a linear models is “learning” the values of the coefficients.\nFor this reason, we call linear regression models parametric models. They have unknown model parameters, in this case the \\(\\beta\\) coefficients that must be learned from the data. The form of the regression function is assumed.\nWhat if we don’t want to make an assumption about the form of the regression function? While in this case, you might look at the plot and arrive at a reasonable guess of assuming a third order polynomial, what if it isn’t so clear? What if you have 100 features? Making strong assumptions might not work well.\nEnter nonparametric models. We will consider two examples: k-nearest neighbors and decision trees.\n k-Nearest Neighbors We’ll start with k-nearest neighbors which is possibly a more intuitive procedure than linear models.1\nIf our goal is to estimate the mean function,\n\\[ \\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] \\]\nthe most natural approach would be to use\n\\[ \\text{average}(\\{ y_i : x_i = x \\}). \\]\nThat is, to estimate the conditional mean at \\(x\\), average the \\(y_i\\) values for each data point where \\(x_i = x\\).\nWhile this sounds nice, it has an obvious flaw. For most values of \\(x\\) there will not be any \\(x_i\\) in the data where \\(x_i = x\\)!\nSo what’s the next best thing? Pick values of \\(x_i\\) that are “close” to \\(x\\).\n\\[ \\text{average}( \\{ y_i : x_i \\text{ equal to (or very close to) x} \\} ). \\]\nThis is the main idea behind many nonparametric approaches. The details often just amount to very specifically defining what “close” means.\nIn the case of k-nearest neighbors we use\n\\[ \\hat{\\mu}_k(x) = \\frac{1}{k} \\sum_{ \\{i \\ : \\ x_i \\in \\mathcal{N}_k(x, \\mathcal{D}) \\} } y_i \\]\nas our estimate of the regression function at \\(x\\). While this looks complicated, it is actually very simple. Here, we are using an average of the \\(y_i\\) values of for the \\(k\\) nearest neighbors to \\(x\\).\nThe \\(k\\) “nearest” neighbors are the \\(k\\) data points \\((x_i, y_i)\\) that have \\(x_i\\) values that are nearest to \\(x\\). We can define “nearest” using any distance we like, but unless otherwise noted, we are referring to euclidean distance.2 We are using the notation \\(\\{i \\ : \\ x_i \\in \\mathcal{N}_k(x, \\mathcal{D}) \\}\\) to define the \\(k\\) observations that have \\(x_i\\) values that are nearest to the value \\(x\\) in a dataset \\(\\mathcal{D}\\), in other words, the \\(k\\) nearest neighbors.\nThe plots below begin to illustrate this idea.\n In the left plot, to estimate the mean of \\(Y\\) at \\(x = -0.5\\) we use the three nearest neighbors, which are highlighted with green. Our estimate is the average of the \\(y_i\\) values of these three points indicated by the black x. In the middle plot, to estimate the mean of \\(Y\\) at \\(x = 0\\) we use the five nearest neighbors, which are highlighted with green. Our estimate is the average of the \\(y_i\\) values of these five points indicated by the black x. In the right plot, to estimate the mean of \\(Y\\) at \\(x = 0.75\\) we use the nine nearest neighbors, which are highlighted with green. Our estimate is the average of the \\(y_i\\) values of these nine points indicated by the black x.  You might begin to notice a bit of an issue here. We have to do a new calculation each time we want to estimate the regression function at a different value of \\(x\\)! For this reason, k-nearest neighbors is often said to be “fast to train” and “slow to predict.” Training, is instant. You just memorize the data! Prediction involves finding the distance between the \\(x\\) considered and all \\(x_i\\) in the data!3\nSo, how then, do we choose the value of the tuning parameter \\(k\\)? We validate!\nFirst, let’s take a look at what happens with this data if we consider three different values of \\(k\\).\nFor each plot, the black dashed curve is the true mean function.\n In the left plot we use \\(k = 25\\). The red “curve” is the estimate of the mean function for each \\(x\\) shown in the plot. In the left plot we use \\(k = 5\\). The blue “curve” is the estimate of the mean function for each \\(x\\) shown in the plot. In the left plot we use \\(k = 1\\). The green “curve” is the estimate of the mean function for each \\(x\\) shown in the plot.  Some things to notice here:\n The left plot with \\(k = 25\\) is performing poorly. The estimated “curve” does not “move” enough. This is an example of an inflexible model. The right plot with \\(k = 1\\) might not perform too well. The estimated “curve” seems to “move” too much. (Notice, that it goes through each point. We’ve fit to the noise.) This is an example of a flexible model.  While the middle plot with \\(k = 5\\) is not “perfect” it seems to roughly capture the “motion” of the true regression function. We can begin to see that if we generated new data, this estimated regression function would perform better than the other two.\nBut remember, in practice, we won’t know the true regression function, so we will need to determine how our model performs using only the available data!\nThis \\(k\\), the number of neighbors, is an example of a tuning parameter. Instead of being learned from the data, like model parameters such as the \\(\\beta\\) coefficients in linear regression, a tuning parameter tells us how to learn from data. It is user-specified. To determine the value of \\(k\\) that should be used, many models are fit to the estimation data, then evaluated on the validation. Using the information from the validation data, a value of \\(k\\) is chosen. (More on this in a bit.)\n Model parameters are “learned” using the same data that was used to fit the model. Tuning parameters are “chosen” using data not used to fit the model.  This tuning parameter \\(k\\) also defines the flexibility of the model. In KNN, a small value of \\(k\\) is a flexible model, while a large value of \\(k\\) is inflexible.4\nBefore moving to an example of tuning a KNN model, we will first introduce decision trees.\n Decision Trees Decision trees are similar to k-nearest neighbors but instead of looking for neighbors, decision trees create neighborhoods. We won’t explore the full details of trees, but just start to understand the basic concepts, as well as learn to fit them in R.\nNeighborhoods are created via recursive binary partitions. In simpler terms, pick a feature and a possible cutoff value. Data that have a value less than the cutoff for the selected feature are in one neighborhood (the left) and data that have a value greater than the cutoff are in another (the right). Within these two neighborhoods, repeat this procedure until a stopping rule is satisfied. To make a prediction, check which neighborhood a new piece of data would belong to and predict the average of the \\(y_i\\) values of data in that neighborhood.\nWith the data above, which has a single feature \\(x\\), consider three possible cutoffs: -0.5, 0.0, and 0.75.\nFor each plot, the black vertical line defines the neighborhoods. The green horizontal lines are the average of the \\(y_i\\) values for the points in the left neighborhood. The red horizontal lines are the average of the \\(y_i\\) values for the points in the right neighborhood.\nWhat makes a cutoff good? Large differences in the average \\(y_i\\) between the two neighborhoods. More formally we want to find a cutoff value that minimizes\n\\[ \\sum_{i \\in N_L} \\left( y_i - \\hat{\\mu}_{N_L} \\right) ^ 2 + \\sum_{i \\in N_R} \\left(y_i - \\hat{\\mu}_{N_R} \\right) ^ 2 \\]\nwhere\n \\(N_L\\) are the data in the left neighborhood, that is \\(x \u0026lt; c\\) \\(N_R\\) are the data in the right neighborhood, that is \\(x \u0026gt; c\\) \\(\\hat{\\mu}_{N_L}\\) is the mean of the \\(y_i\\) for data in the left neighborhood \\(\\hat{\\mu}_{N_R}\\) is the mean of the \\(y_i\\) for data in the right neighborhood  This quantity is the sum of two sum of squared errors, one for the left neighborhood, and one for the right neighborhood.\n  Cutoff  Total SSE  Left SSE  Right SSE      -0.50  45.02  21.28  23.74    0.00  58.94  44.68  14.26    0.75  56.71  55.46  1.25     The table above summarizes the results of the three potential splits. We see that (of the splits considered, which are not exhaustive5) the split based on a cutoff of \\(x = -0.50\\) creates the best partitioning of the space.\nNow let’s consider building a full tree.\nIn the plot above, the true regression function is the dashed black curve, and the solid orange curve is the estimated regression function using a decision tree. We see that there are two splits, which we can visualize as a tree.\nThe above “tree”6 shows the splits that were made. It informs us of the variable used, the cutoff value, and some summary of the resulting neighborhood. In “tree” terminology the resulting neighborhoods are “terminal nodes” of the tree. In contrast, “internal nodes” are neighborhoods that are created, but then further split.\nThe “root node” is the neighborhood contains all observations, before any splitting, and can be seen at the top of the image above. We see that this node represents 100% of the data. The other number, 0.21, is the mean of the response variable, in this case, \\(y_i\\).\nLooking at a terminal node, for example the bottom left node, we see that 23% of the data is in this node. The average value of the \\(y_i\\) in this node is -1, which can be seen in the plot above.\nWe also see that the first split is based on the \\(x\\) variable, and a cutoff of \\(x = -0.52\\). Note that because there is only one variable here, all splits are based on \\(x\\), but in the future, we will have multiple features that can be split and neighborhoods will no longer be one-dimensional. However, this is hard to plot.\nLet’s build a bigger, more flexible tree.\nThere are two tuning parameters at play here which we will call by their names in R which we will see soon:\n cp or the “complexity parameter” as it is called.7 This parameter determines which splits are accepted. A split must improve the performance of the tree by more than cp in order to be used. When we get to R, we will see that the default value is 0.1. minsplit, the minimum number of observations in a node (neighborhood) in order to consider splitting within a neighborhood.  There are actually many more possible tuning parameters for trees, possibly differing depending on who wrote the code you’re using. We will limit discussion to these two.8 Note that they effect each other, and they effect other parameters which we are not discussing. The main takeaway should be how they effect model flexibility.\nFirst let’s look at what happens for a fixed minsplit by variable cp.\nWe see that as cp decreases, model flexibility increases. We see more splits, because the increase in performance needed to accept a split is smaller as cp is reduced.\nNow the reverse, fix cp and vary minsplit.\nWe see that as minsplit decreases, model flexibility increases. By allowing splits of neighborhoods with fewer observations, we obtain more splits, which results in a more flexible model.\n Example: Credit Card Data # load data, coerce to tibble crdt = as_tibble(ISLR::Credit) Again, we are using the Credit data form the ISLR package. Note: this is not real data. It has been simulated.\n# data prep crdt = crdt %\u0026gt;% select(-ID) %\u0026gt;% select(-Rating, everything()) We remove the ID variable as it should have no predictive power. We also move the Rating variable to the last column with a clever dplyr trick. This is in no way necessary, but is useful in creating some plots.\n# test-train split set.seed(1) crdt_trn_idx = sample(nrow(crdt), size = 0.8 * nrow(crdt)) crdt_trn = crdt[crdt_trn_idx, ] crdt_tst = crdt[-crdt_trn_idx, ] # estimation-validation split crdt_est_idx = sample(nrow(crdt_trn), size = 0.8 * nrow(crdt_trn)) crdt_est = crdt_trn[crdt_est_idx, ] crdt_val = crdt_trn[-crdt_est_idx, ] After train-test (with 80% of the data) and estimation-validation splitting the data, we look at the train data.\n# check data head(crdt_trn, n = 10) ## # A tibble: 10 x 11 ## Income Limit Cards Age Education Gender Student Married Ethnicity Balance ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 183. 13913 4 98 17 \u0026quot; Mal… No Yes Caucasian 1999 ## 2 35.7 2880 2 35 15 \u0026quot; Mal… No No African … 0 ## 3 123. 8376 2 89 17 \u0026quot; Mal… Yes No African … 1259 ## 4 20.8 2672 1 70 18 \u0026quot;Fema… No No African … 0 ## 5 39.1 5565 4 48 18 \u0026quot;Fema… No Yes Caucasian 772 ## 6 36.5 3806 2 52 13 \u0026quot; Mal… No No African … 188 ## 7 45.1 3762 3 80 8 \u0026quot; Mal… No Yes Caucasian 70 ## 8 43.5 2906 4 69 11 \u0026quot; Mal… No No Caucasian 0 ## 9 23.1 3476 2 50 15 \u0026quot;Fema… No No Caucasian 209 ## 10 53.2 4943 2 46 16 \u0026quot;Fema… No Yes Asian 382 ## # … with 1 more variable: Rating \u0026lt;int\u0026gt; In this simulated data, we would like to predict the Rating variable. For now, let’s try to use only demographic information as predictors.9 In particular, let’s focus on Age (numeric), Gender (categorical), and Student (categorical).\nLet’s fit KNN models with these features, and various values of \\(k\\). To do so, we use the knnreg() function from the caret package.10 Use ?knnreg for documentation and details.\ncrdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 1) crdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 10) crdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 25) Here, we fit three models to the estimation data. We supply the variables that will be used as features as we would with lm(). We also specify how many neighbors to consider via the k argument.\nBut wait a second, what is the distance from non-student to student? From male to female? In other words, how does KNN handle categorical variables? It doesn’t! Like lm() it creates dummy variables under the hood.\nNote: To this point, and until we specify otherwise, we will always coerce categorical variables to be factor variables in R. We will then let modeling functions such as lm() or knnreg() deal with the creation of dummy variables internally.\nhead(crdt_knn_10$learn$X) ## Age GenderFemale StudentYes ## 1 30 0 0 ## 2 25 0 0 ## 3 44 0 0 ## 4 73 1 0 ## 5 44 0 1 ## 6 71 0 0 Once these dummy variables have been created, we have a numeric \\(X\\) matrix, which makes distance calculations easy.11 For example, the distance between the 3rd and 4th observation here is 29.017.\ndist(head(crdt_knn_10$learn$X)) ## 1 2 3 4 5 ## 2 5.000000 ## 3 14.000000 19.000000 ## 4 43.011626 48.010416 29.017236 ## 5 14.035669 19.026298 1.000000 29.034462 ## 6 41.000000 46.000000 27.000000 2.236068 27.018512 sqrt(sum((crdt_knn_10$learn$X[3, ] - crdt_knn_10$learn$X[4, ]) ^ 2)) ## [1] 29.01724 What about interactions? Basically, you’d have to create them the same way as you do for linear models. We only mention this to contrast with trees in a bit.\nOK, so of these three models, which one performs best? (Where for now, “best” is obtaining the lowest validation RMSE.)\nFirst, note that we return to the predict() function as we did with lm().\npredict(crdt_knn_10, crdt_val[1:5, ]) ## [1] 337.7857 356.0000 295.7692 360.8182 306.8000 This uses the 10-NN (10 nearest neighbors) model to make predictions (estimate the regression function) given the first five observations of the validation data. Note: We did not name the second argument to predict(). Again, you’ve been warned.\nNow that we know how to use the predict() function, let’s calculate the validation RMSE for each of these models.\nknn_mod_list = list( crdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 1), crdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 10), crdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 25) ) knn_val_pred = lapply(knn_mod_list, predict, crdt_val) calc_rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } sapply(knn_val_pred, calc_rmse, crdt_val$Rating) ## crdt_knn_01 crdt_knn_10 crdt_knn_25 ## 182.3469 149.2172 138.6527 So, of these three values of \\(k\\), the model with \\(k = 25\\) achieves the lowest validation RMSE.\nThis process, fitting a number of models with different values of the tuning parameter, in this case \\(k\\), and then finding the “best” tuning parameter value based on performance on the validation data is called tuning. In practice, we would likely consider more values of \\(k\\), but this should illustrate the point.\nIn the next lectures, we will discuss the details of model flexibility and model tuning, and how these concepts are tied together. However, even though we will present some theory behind this relationship, in practice, you must tune and validate your models. There is no theory that will inform you ahead of tuning and validation which model will be the best. By teaching you how to fit KNN models in R and how to calculate validation RMSE, you already have all a set of tools you can use to find a good model.\nLet’s turn to decision trees which we will fit with the rpart() function from the rpart package. Use ?rpart and ?rpart.control for documentation and details. In particular, ?rpart.control will detail the many tuning parameters of this implementation of decision tree models in R.\nWe’ll start by using default tuning parameters.\ncrdt_tree = rpart(Rating ~ Age + Gender + Student, data = crdt_est) crdt_tree ## n= 256 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 256 6667400.0 357.0781 ## 2) Age\u0026lt; 82.5 242 5865419.0 349.3719 ## 4) Age\u0026gt;=69.5 52 1040678.0 313.0385 * ## 5) Age\u0026lt; 69.5 190 4737307.0 359.3158 ## 10) Age\u0026lt; 38.5 55 700013.2 326.6000 * ## 11) Age\u0026gt;=38.5 135 3954443.0 372.6444 ## 22) Student=Yes 14 180764.4 297.7857 * ## 23) Student=No 121 3686148.0 381.3058 ## 46) Age\u0026gt;=50.5 64 1881299.0 359.2344 ## 92) Age\u0026lt; 53.5 9 48528.0 278.3333 * ## 93) Age\u0026gt;=53.5 55 1764228.0 372.4727 * ## 47) Age\u0026lt; 50.5 57 1738665.0 406.0877 * ## 3) Age\u0026gt;=82.5 14 539190.9 490.2857 * Above we see the resulting tree printed, however, this is difficult to read. Instead, we use the rpart.plot() function from the rpart.plot package to better visualize the tree.\nrpart.plot(crdt_tree) At each split, the variable used to split is listed together with a condition. If the condition is true for a data point, send it to the left neighborhood. Although the Gender available for creating splits, we only see splits based on Age and Student. This hints at the relative importance of these variables for prediction. More on this much later.\nCategorical variables are split based on potential categories! This is excellent. This means that trees naturally handle categorical features without needing to convert to numeric under the hood. We see a split that puts students into one neighborhood, and non-students into another.\nNotice that the splits happen in order. So for example, the third terminal node (with an average rating of 298) is based on splits of:\n Age \u0026lt; 83 Age \u0026lt; 70 Age \u0026gt; 39 Student = Yes  In other words, individuals in this terminal node are students who are between the ages of 39 and 70. (Only 5% of the data is represented here.) This is basically an interaction between Age and Student without any need to directly specify it! What a great feature of trees.\nTo recap:\n Trees do not make assumptions about the form of the regression function. Trees automatically handle categorical features. Trees naturally incorporate interaction.  Now let’s fit another tree that is more flexible by relaxing some tuning parameters. Recall that by default, cp = 0.1 and minsplit = 20.\ncrdt_tree_big = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.0, minsplit = 20) rpart.plot(crdt_tree_big) To make the tree even bigger, we could reduce minsplit, but in practice we mostly consider the cp parameter. Since minsplit has been kept the same, but cp was reduced, we see the same splits as the smaller tree, but many additional splits.\nNow let’s fit a bunch of trees, with different values of cp, for tuning.\ntree_mod_list = list( crdt_tree_0000 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.000), crdt_tree_0001 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.001), crdt_tree_0010 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.010), crdt_tree_0100 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.100) ) tree_val_pred = lapply(tree_mod_list, predict, crdt_val) sapply(tree_val_pred, calc_rmse, crdt_val$Rating) ## crdt_tree_0000 crdt_tree_0001 crdt_tree_0010 crdt_tree_0100 ## 156.3527 155.4262 151.9081 140.0806 Here we see the least flexible model, with cp = 0.100, performs best.\nNote that by only using these three features, we are severely limiting our models performance. Let’s quickly assess using all available predictors.\ncrdt_tree_all = rpart(Rating ~ ., data = crdt_est) rpart.plot(crdt_tree_all) Notice that this model only splits based on Limit despite using all features. This should be a big hint about which variables are useful for prediction.\ncalc_rmse( actual = crdt_val$Rating, predicted = predict(crdt_tree_all, crdt_val) ) ## [1] 28.8498 This model performs much better. You should try something similar with the KNN models above. Also, consider comparing this result to results from last lectures using linear models.\nNotice that we’ve been using that trusty predict() function here again.\npredict(crdt_tree_all, crdt_val[1:5, ]) ## 1 2 3 4 5 ## 292.8182 467.5152 467.5152 467.5152 772.4000 What does this code do? It estimates the mean Rating given the feature information (the “x” values) from the first five observations from the validation data using a decision tree model with default tuning parameters. Hopefully a theme is emerging.\n   We chose to start with linear regression because most students the social sciences should already be familiar with the basic notion.↩︎\n The usual distance when you hear distance. That is, unless you drive a taxicab.↩︎\n For this reason, KNN is often not used in practice, but it is very useful learning tool.↩︎\n Many texts use the term complex instead of flexible. We feel this is confusing as complex is often associated with difficult. KNN with \\(k = 1\\) is actually a very simple model to understand, but it is very flexible as defined here.↩︎\n To exhaust all possible splits of a variable, we would need to consider the midpoint between each of the order statistics of the variable. To exhaust all possible splits, we would need to do this for each of the feature variables.↩︎\n It’s really an upside tree isn’t it?↩︎\n Flexibility parameter would be a better name.↩︎\n The rpart function in R would allow us to use others, but we will always just leave their values as the default values.↩︎\n There is a question of whether or not we should use these variables. For example, should men and women be given different ratings when all other variables are the same? Using the Gender variable allows for this to happen. Also, you might think, just don’t use the Gender variable. Unfortunately, it’s not that easy. There is an increasingly popular field of study centered around these ideas called machine learning fairness.↩︎\n There are many other KNN functions in R. However, the operation and syntax of knnreg() better matches other functions we use in this course.↩︎\n Wait. Doesn’t this sort of create an arbitrary distance between the categories? Why \\(0\\) and \\(1\\) and not \\(-42\\) and \\(51\\)? Good question. This hints at the notion of pre-processing. We’re going to hold off on this for now, but, often when performing k-nearest neighbors, you should try scaling all of the features to have mean \\(0\\) and variance \\(1\\).↩︎\n   ","date":1603756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603718387,"objectID":"d3f7fc26f58a06eab9a68e8656b03674","permalink":"/content/08-content/","publishdate":"2020-10-27T00:00:00Z","relpermalink":"/content/08-content/","section":"content","summary":"Required Reading  Guiding Ideas  Nonparametric Regression  R Setup Mathematical Setup k-Nearest Neighbors Decision Trees Example: Credit Card Data    Required Reading  This page.  Guiding Ideas  How to use k-nearest neighbors for regression through the use of the knnreg() function from the caret package How to use decision trees for regression through the use of the rpart() function from the rpart package.","tags":null,"title":"Nonparametric Regression","type":"docs"},{"authors":null,"categories":null,"content":"   Required Reading  Guiding Question Slides  Association is not causation  Spurious correlation Outliers Reversing cause and effect Confounders  Example: UC Berkeley admissions Confounding explained graphically Average after stratifying  Simpson’s paradox    Required Reading  This page.  Guiding Question  When can we make causal claims about the relationship between variables?   Slides Note that the slides below are from last lecture; we will reference these and they contain a lot of useful information.\nIntroduction       As with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into R.\n  Ames.csv    Association is not causation Association is not causation is perhaps the most important lesson one learns in a statistics class. Correlation is not causation is another way to say this. Throughout the previous parts of this class, we have described tools useful for quantifying associations between variables. However, we must be careful not to over-interpret these associations.\nThere are many reasons that a variable \\(X\\) can be correlated with a variable \\(Y\\) without having any direct effect on \\(Y\\). Here we examine four common ways that can lead to misinterpreting data.\nSpurious correlation The following comical example underscores that correlation is not causation. It shows a very strong correlation between divorce rates and margarine consumption.\nDoes this mean that margarine causes divorces? Or do divorces cause people to eat more margarine? Of course the answer to both these questions is no. This is just an example of what we call a spurious correlation.\nYou can see many more absurd examples on the Spurious Correlations website1.\nThe cases presented in the spurious correlation site are all instances of what is generally called data dredging, data fishing, or data snooping. It’s basically a form of what in the US they call cherry picking. An example of data dredging would be if you look through many results produced by a random process and pick the one that shows a relationship that supports a theory you want to defend.\nA Monte Carlo simulation can be used to show how data dredging can result in finding high correlations among uncorrelated variables. We will save the results of our simulation into a tibble:\nN \u0026lt;- 25 g \u0026lt;- 1000000 sim_data \u0026lt;- tibble(group = rep(1:g, each=N), x = rnorm(N * g), y = rnorm(N * g)) The first column denotes group. We created groups and for each one we generated a pair of independent vectors, \\(X\\) and \\(Y\\), with 25 observations each, stored in the second and third columns. Because we constructed the simulation, we know that \\(X\\) and \\(Y\\) are not correlated.\nNext, we compute the correlation between X and Y for each group and look at the max:\nres \u0026lt;- sim_data %\u0026gt;% group_by(group) %\u0026gt;% summarize(r = cor(x, y)) %\u0026gt;% arrange(desc(r)) ## `summarise()` ungrouping output (override with `.groups` argument) res ## # A tibble: 1,000,000 x 2 ## group r ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 861679 0.815 ## 2 387275 0.786 ## 3 455283 0.786 ## 4 442746 0.783 ## 5 737678 0.777 ## 6 113036 0.775 ## 7 454360 0.773 ## 8 553579 0.766 ## 9 656133 0.758 ## 10 660249 0.749 ## # … with 999,990 more rows We see a maximum correlation of 0.8146411 and if you just plot the data from the group achieving this correlation, it shows a convincing plot that \\(X\\) and \\(Y\\) are in fact correlated:\nsim_data %\u0026gt;% filter(group == res$group[which.max(res$r)]) %\u0026gt;% ggplot(aes(x, y)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; Remember that the correlation summary is a random variable. Here is the distribution generated by the Monte Carlo simulation:\nres %\u0026gt;% ggplot(aes(x=r)) + geom_histogram(binwidth = 0.1, color = \u0026quot;black\u0026quot;) It’s just a mathematical fact that if we observe random correlations that are expected to be 0, but have a standard error of 0.2041007, the largest one will be close to 1.\nIf we performed regression on this group and interpreted the p-value, we would incorrectly claim this was a statistically significant relation:\nlibrary(broom) sim_data %\u0026gt;% filter(group == res$group[which.max(res$r)]) %\u0026gt;% do(tidy(lm(y ~ x, data = .))) %\u0026gt;% filter(term == \u0026quot;x\u0026quot;) ## # A tibble: 1 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 x 1.04 0.154 6.74 0.000000716 This particular form of data dredging is referred to as p-hacking. P-hacking is a topic of much discussion because it is a problem in scientific publications. Because publishers tend to reward statistically significant results over negative results, there is an incentive to report significant results. In epidemiology and the social sciences, for example, researchers may look for associations between an adverse outcome and several exposures and report only the one exposure that resulted in a small p-value. Furthermore, they might try fitting several different models to account for confounding and pick the one that yields the smallest p-value. In experimental disciplines, an experiment might be repeated more than once, yet only the results of the one experiment with a small p-value reported. This does not necessarily happen due to unethical behavior, but rather as a result of statistical ignorance or wishful thinking. In advanced statistics courses, you can learn methods to adjust for these multiple comparisons.\n Outliers Suppose we take measurements from two independent outcomes, \\(X\\) and \\(Y\\), and we standardize the measurements. However, imagine we make a mistake and forget to standardize entry 23. We can simulate such data using:\nset.seed(1985) x \u0026lt;- rnorm(100,100,1) y \u0026lt;- rnorm(100,84,1) x[-23] \u0026lt;- scale(x[-23]) y[-23] \u0026lt;- scale(y[-23]) The data look like this:\nqplot(x, y) Not surprisingly, the correlation is very high:\ncor(x,y) ## [1] 0.9878382 But this is driven by the one outlier. If we remove this outlier, the correlation is greatly reduced to almost 0, which is what it should be:\ncor(x[-23], y[-23]) ## [1] -0.04419032 Previously, we (briefly) described alternatives to the average and standard deviation that are robust to outliers. There is also an alternative to the sample correlation for estimating the population correlation that is robust to outliers. It is called Spearman correlation. The idea is simple: compute the correlation on the ranks of the values. Here is a plot of the ranks plotted against each other:\nqplot(rank(x), rank(y)) The outlier is no longer associated with a very large value and the correlation comes way down:\ncor(rank(x), rank(y)) ## [1] 0.002508251 Spearman correlation can also be calculated like this:\ncor(x, y, method = \u0026quot;spearman\u0026quot;) ## [1] 0.002508251 There are also methods for robust fitting of linear models which you can learn about in, for instance, this book: Robust Statistics: Edition 2 by Peter J. Huber \u0026amp; Elvezio M. Ronchetti.\n Reversing cause and effect Another way association is confused with causation is when the cause and effect are reversed. An example of this is claiming that tutoring makes students perform worse because they test lower than peers that are not tutored. In this case, the tutoring is not causing the low test scores, but the other way around.\nA form of this claim actually made it into an op-ed in the New York Times titled Parental Involvement Is Overrated2. Consider this quote from the article:\n When we examined whether regular help with homework had a positive impact on children’s academic performance, we were quite startled by what we found. Regardless of a family’s social class, racial or ethnic background, or a child’s grade level, consistent homework help almost never improved test scores or grades… Even more surprising to us was that when parents regularly helped with homework, kids usually performed worse.\n A very likely possibility is that the children needing regular parental help, receive this help because they don’t perform well in school.\nWe can easily construct an example of cause and effect reversal using the father and son height data. If we fit the model:\n\\[X_i = \\beta_0 + \\beta_1 y_i + \\varepsilon_i, i=1, \\dots, N\\]\nto the father and son height data, with \\(X_i\\) the father height and \\(y_i\\) the son height, we do get a statistically significant result:\nlibrary(HistData) data(\u0026quot;GaltonFamilies\u0026quot;) GaltonFamilies %\u0026gt;% filter(childNum == 1 \u0026amp; gender == \u0026quot;male\u0026quot;) %\u0026gt;% select(father, childHeight) %\u0026gt;% rename(son = childHeight) %\u0026gt;% do(tidy(lm(father ~ son, data = .))) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 34.0 4.57 7.44 4.31e-12 ## 2 son 0.499 0.0648 7.70 9.47e-13 The model fits the data very well. If we look at the mathematical formulation of the model above, it could easily be incorrectly interpreted so as to suggest that the son being tall caused the father to be tall. But given what we know about genetics and biology, we know it’s the other way around. The model is technically correct. The estimates and p-values were obtained correctly as well. What is wrong here is the interpretation.\n Confounders Confounders are perhaps the most common reason that leads to associations begin misinterpreted.\nIf \\(X\\) and \\(Y\\) are correlated, we call \\(Z\\) a confounder if changes in \\(Z\\) causes changes in both \\(X\\) and \\(Y\\). Earlier, when studying baseball data, we saw how Home Runs was a confounder that resulted in a higher correlation than expected when studying the relationship between Bases on Balls and Runs. In some cases, we can use linear models to account for confounders. However, this is not always the case.\nIncorrect interpretation due to confounders is ubiquitous in the lay press and they are often hard to detect. Here, we present a widely used example related to college admissions.\nExample: UC Berkeley admissions Admission data from six U.C. Berkeley majors, from 1973, showed that more men were being admitted than women: 44% men were admitted compared to 30% women. PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). We can load the data and % group_by(gender) %% summarize(percentage = round(sum(admitted*applicants)/sum(applicants),1)) ``` ``` ## `summarise()` ungrouping output (override with `.groups` argument) ``` ``` ## # A tibble: 2 x 2 ## gender percentage ##   ## 1 men 44.5 ## 2 women 30.3 ``` -- a statistical test, which clearly rejects the hypothesis that gender and admission are independent:\ndata(admissions) admissions %\u0026gt;% group_by(gender) %\u0026gt;% summarize(total_admitted = round(sum(admitted / 100 * applicants)), not_admitted = sum(applicants) - sum(total_admitted)) %\u0026gt;% select(-gender) %\u0026gt;% do(tidy(chisq.test(.))) %\u0026gt;% .$p.value ## `summarise()` ungrouping output (override with `.groups` argument) ## [1] 1.055797e-21 But closer inspection shows a paradoxical result. Here are the percent admissions by major:\nadmissions %\u0026gt;% select(major, gender, admitted) %\u0026gt;% spread(gender, admitted) %\u0026gt;% mutate(women_minus_men = women - men) ## major men women women_minus_men ## 1 A 62 82 20 ## 2 B 63 68 5 ## 3 C 37 34 -3 ## 4 D 33 35 2 ## 5 E 28 24 -4 ## 6 F 6 7 1 Four out of the six majors favor women. More importantly, all the differences are much smaller than the 14.2 difference that we see when examining the totals.\nThe paradox is that analyzing the totals suggests a dependence between admission and gender, but when the data is grouped by major, this dependence seems to disappear. What’s going on? This actually can happen if an uncounted confounder is driving most of the variability.\nSo let’s define three variables: \\(X\\) is 1 for men and 0 for women, \\(Y\\) is 1 for those admitted and 0 otherwise, and \\(Z\\) quantifies the selectivity of the major. A gender bias claim would be based on the fact that \\(\\mbox{Pr}(Y=1 | X = x)\\) is higher for \\(x=1\\) than \\(x=0\\). However, \\(Z\\) is an important confounder to consider. Clearly \\(Z\\) is associated with \\(Y\\), as the more selective a major, the lower \\(\\mbox{Pr}(Y=1 | Z = z)\\). But is major selectivity \\(Z\\) associated with gender \\(X\\)?\nOne way to see this is to plot the total percent admitted to a major versus the percent of women that made up the applicants:\nadmissions %\u0026gt;% group_by(major) %\u0026gt;% summarize(major_selectivity = sum(admitted * applicants)/sum(applicants), percent_women_applicants = sum(applicants * (gender==\u0026quot;women\u0026quot;)) / sum(applicants) * 100) %\u0026gt;% ggplot(aes(major_selectivity, percent_women_applicants, label = major)) + geom_text() ## `summarise()` ungrouping output (override with `.groups` argument) There seems to be association. The plot suggests that women were much more likely to apply to the two “hard” majors: gender and major’s selectivity are confounded. Compare, for example, major B and major E. Major E is much harder to enter than major B and over 60% of applicants to major E were women, while less than 30% of the applicants of major B were women.\n Confounding explained graphically The following plot shows the number of applicants that were admitted and those that were not by:\n% mutate(percent_admitted = admitted * applicants/sum(applicants)) %% ggplot(aes(gender, y = percent_admitted, fill = major)) + geom_bar(stat = \"identity\", position = \"stack\") ``` -- It also breaks down the acceptances by major. This breakdown allows us to see that the majority of accepted men came from two majors: A and B. It also lets us see that few women applied to these majors.\n Average after stratifying In this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away:\nadmissions %\u0026gt;% ggplot(aes(major, admitted, col = gender, size = applicants)) + geom_point() Now we see that major by major, there is not much difference. The size of the dot represents the number of applicants, and explains the paradox: we see large red dots and small blue dots for the easiest majors, A and B.\nIf we average the difference by major, we find that the percent is actually 3.5% higher for women.\nadmissions %\u0026gt;% group_by(gender) %\u0026gt;% summarize(average = mean(admitted)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## gender average ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 men 38.2 ## 2 women 41.7   Simpson’s paradox The case we have just covered is an example of Simpson’s paradox. It is called a paradox because we see the sign of the correlation flip when comparing the entire publication and specific strata. As an illustrative example, suppose you have three random variables \\(X\\), \\(Y\\), and \\(Z\\) and that we observe realizations of these. Here is a plot of simulated observations for \\(X\\) and \\(Y\\) along with the sample correlation:\nYou can see that \\(X\\) and \\(Y\\) are negatively correlated. However, once we stratify by \\(Z\\) (shown in different colors below) another pattern emerges:\n## `summarise()` ungrouping output (override with `.groups` argument) It is really \\(Z\\) that is negatively correlated with \\(X\\). If we stratify by \\(Z\\), the \\(X\\) and \\(Y\\) are actually positively correlated as seen in the plot above.\n   http://tylervigen.com/spurious-correlations↩︎\n https://opinionator.blogs.nytimes.com/2014/04/12/parental-involvement-is-overrated↩︎\n   ","date":1603324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603718301,"objectID":"79f283e309555def72f172af8fa16106","permalink":"/content/07-content/","publishdate":"2020-10-22T00:00:00Z","relpermalink":"/content/07-content/","section":"content","summary":"Required Reading  Guiding Question Slides  Association is not causation  Spurious correlation Outliers Reversing cause and effect Confounders  Example: UC Berkeley admissions Confounding explained graphically Average after stratifying  Simpson’s paradox    Required Reading  This page.  Guiding Question  When can we make causal claims about the relationship between variables?   Slides Note that the slides below are from last lecture; we will reference these and they contain a lot of useful information.","tags":null,"title":"Linear Regression III","type":"docs"},{"authors":null,"categories":null,"content":"  Part 1: Rats, rats, rats. Instructions Starter code  Part 2: Data Hunting Evaluations   Each member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.\n Part 1: Rats, rats, rats. New York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots. Rats in NYC are plentiful, but they also deliver food, so they’re useful too.\n  NYC keeps incredibly detailed data regarding animal sightings, including rats, and it makes this data publicly available.\nFor this first project, you will use R and ggplot2 to tell an interesting story hidden in the data. You must create a story by looking carefully at the data.\nInstructions Here’s what you need to do:\nDownload New York City’s database of rat sightings since 2010:\n  Rat_sightings.csv  Summarize the data somehow. The raw data has more than 100,000 rows, which means you’ll need to aggregate the data (filter(), group_by(), and summarize() will be your friends). Consider looking at the number of sightings per borough, per year, per dwelling type, etc., or a combination of these, like the change in the number sightings across the 5 boroughs between 2010 and 2016.\n Create an appropriate visualization based on the data you summarized.\n Write a memo explaining your process. We are specifically looking for a discussion of the following:\n What story are you telling with your new graphic? How have you applied reasonable standards in visual storytelling? What policy implication is there (if any)?  Upload the following outputs to D2L:\n A PDF file of your memo with your final code and graphic embedded in it.1 This means you’ll need to do all your coding in an R Markdown file and embed your code in chunks. Note that Part 2 of this project should be included in this PDF (see below). A standalone PDF version of your graphic. Use ggsave(plot_name, filename = \"output/blah.pdf\", width = XX, height = XX)    Starter code I’ve provided some starter code below. A couple comments about it:\n By default, read_csv() treats cells that are empty or “NA” as missing values. This rat dataset uses “N/A” to mark missing values, so we need to add that as a possible marker of missingness (hence na = c(\"\", \"NA\", \"N/A\")) To make life easier, I’ve renamed some of the key variables you might work with. You can rename others if you want. I’ve also created a few date-related variables (sighting_year, sighting_month, sighting_day, and sighting_weekday). You don’t have to use them, but they’re there if you need them. The functions that create these, like year() and wday() are part of the lubridate library. The date/time variables are formatted like 04/03/2017 12:00:00 AM, which R is not able to automatically parse as a date when reading the CSV file. You can use the mdy_hms() function in the lubridate library to parse dates that are structured as “month-day-year-hour-minute”. There are also a bunch of other iterations of this function, like ymd(), dmy(), etc., for other date formats. There’s one row with an unspecified borough, so I filter that out.  library(tidyverse) library(lubridate) rats_raw \u0026lt;- read_csv(\u0026quot;data/Rat_Sightings.csv\u0026quot;, na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;N/A\u0026quot;)) # If you get an error that says \u0026quot;All formats failed to parse. No formats # found\u0026quot;, it\u0026#39;s because the mdy_hms function couldn\u0026#39;t parse the date. The date # variable *should* be in this format: \u0026quot;04/03/2017 12:00:00 AM\u0026quot;, but in some # rare instances, it might load without the seconds as \u0026quot;04/03/2017 12:00 AM\u0026quot;. # If there are no seconds, use mdy_hm() instead of mdy_hms(). rats_clean \u0026lt;- rats_raw %\u0026gt;% rename(created_date = `Created Date`, location_type = `Location Type`, borough = Borough) %\u0026gt;% mutate(created_date = mdy_hms(created_date)) %\u0026gt;% mutate(sighting_year = year(created_date), sighting_month = month(created_date), sighting_day = day(created_date), sighting_weekday = wday(created_date, label = TRUE, abbr = FALSE)) %\u0026gt;% filter(borough != \u0026quot;Unspecified\u0026quot;) You’ll summarize the data with functions from dplyr, including stuff like count(), arrange(), filter(), group_by(), summarize(), and mutate(). Here are some examples of ways to summarize the data:\n# See the count of rat sightings by weekday rats_clean %\u0026gt;% count(sighting_weekday) # Assign a summarized data frame to an object to use it in a plot rats_by_weekday \u0026lt;- rats_clean %\u0026gt;% count(sighting_weekday, sighting_year) ggplot(rats_by_weekday, aes(x = fct_rev(sighting_weekday), y = n)) + geom_col() + coord_flip() + facet_wrap(~ sighting_year) # See the count of rat sightings by weekday and borough rats_clean %\u0026gt;% count(sighting_weekday, borough, sighting_year) # An alternative to count() is to specify the groups with group_by() and then # be explicit about how you\u0026#39;re summarizing the groups, such as calculating the # mean, standard deviation, or number of observations (we do that here with # `n()`). rats_clean %\u0026gt;% group_by(sighting_weekday, borough) %\u0026gt;% summarize(n = n())   Part 2: Data Hunting For the second part of the project, your task is simple. Your group must identify three different data sources2 for potential use in your final project. You are not bound to this decision.\nFor each, you must write a single paragraph about what about this data interests you. Add this to the memo from Part 1.\n Evaluations I will evaluate these projects (not the TA). I will only give top marks to those groups showing initiative and cleverness. I will use the following weights for final scores:\nPart 1\nTechnical difficulty: Does the final project show mastery of the skills we’ve discussed thus far? (10 points)\n Appropriateness of visuals: Do the visualizations tell a clear story? Have we learned something? (10 points)\n Storytelling: Does your memo clearly convey what you’re doing and why? (9 points)\n  Part 2\nEach piece of data (and description) is worth 7 points. (21 points total)\n  You can approach this in a couple different ways—you can write the memo and then include the full figure and code at the end, similar to this blog post, or you can write the memo in an incremental way, describing the different steps of creating the figure, ultimately arriving at a clean final figure, like this blog post.↩︎\n The three different sources need not be different websites or from different organizations. For example, three different tables from the US Census would be sufficient↩︎\n   ","date":1602806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604325415,"objectID":"1ee2ebbd4938399d5199cb912763ad52","permalink":"/assignment/project1/","publishdate":"2020-10-16T00:00:00Z","relpermalink":"/assignment/project1/","section":"assignment","summary":"Part 1: Rats, rats, rats. Instructions Starter code  Part 2: Data Hunting Evaluations   Each member of the group should submit a copy of the project to D2L (for ease of evaluation and to ensure communication across the group). Please write your group number and the group members’ names across the top.\n Part 1: Rats, rats, rats. New York City is full of urban wildlife, and rats are one of the city’s most infamous animal mascots.","tags":null,"title":"Project 1","type":"docs"},{"authors":null,"categories":null,"content":"    Required Reading  Supplemental Readings Guiding Questions Slides  Linear Models II  Case study: Moneyball  Sabermetics Baseball basics No awards for BB Base on balls or stolen bases? Regression applied to baseball statistics  Confounding  Understanding confounding through stratification Multivariate regression  Least squares estimates  Interpreting linear models Least Squares Estimates (LSE) The lm function LSE are random variables Predicted values are random variables  Exercises Linear regression in the tidyverse  The broom package  Case study: Moneyball (continued)  Adding salary and position information Picking nine players  The regression fallacy Measurement error models    Required Reading  This page.  Supplemental Readings   Chapter 3 in Introduction to Statistical Learning   Guiding Questions  How do we interpret linear regression outputs? How are the standard errors derived? When should we turn to linear regression versus alternative approaches? Why do we use linear regression so often in data analytics?   Slides Introduction       As with recent weeks, we will work with real data during the lecture. Please download the following dataset and load it into R.\n  Ames.csv    Linear Models II Since Galton’s original development, regression has become one of the most widely used tools in data science. One reason has to do with the fact that regression permits us to find relationships between two variables taking into account the effects of other variables that affect both. This has been particularly popular in fields where randomized experiments are hard to run, such as economics and epidemiology.\nWhen we are not able to randomly assign each individual to a treatment or control group, confounding is particularly prevalent. For example, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in a jurisdiction. Fast food consumers are more likely to be smokers, drinkers, and have lower incomes. Therefore, a naive regression model may lead to an overestimate of the negative health effect of fast food. So how do we account for confounding in practice? In this lecture we learn how linear models can help with such situations and can be used to describe how one or more variables affect an outcome variable.\nCase study: Moneyball Moneyball: The Art of Winning an Unfair Game is a book by Michael Lewis about the Oakland Athletics (A’s) baseball team and its general manager, the person tasked with building the team, Billy Beane.\nTraditionally, baseball teams use scouts to help them decide what players to hire. These scouts evaluate players by observing them perform. Scouts tend to favor athletic players with observable physical abilities. For this reason, scouts tend to agree on who the best players are and, as a result, these players tend to be in high demand. This in turn drives up their salaries.\nFrom 1989 to 1991, the A’s had one of the highest payrolls in baseball. They were able to buy the best players and, during that time, they were one of the best teams. However, in 1995 the A’s team owner changed and the new management cut the budget drastically, leaving then general manager, Sandy Alderson, with one of the lowest payrolls in baseball. He could no longer afford the most sought-after players. Alderson began using a statistical approach to find inefficiencies in the market. Alderson was a mentor to Billy Beane, who succeeded him in 1998 and fully embraced data science, as opposed to scouts, as a method for finding low-cost players that data predicted would help the team win. Today, this strategy has been adapted by most baseball teams. As we will see, regression plays a large role in this approach.\nAs motivation for this lecture, we will pretend it is 2002 (holy shit I’m old) and try to build a baseball team with a limited budget, just like the A’s had to do. To appreciate what you are up against, note that in 2002 the Yankees’ payroll of $125,928,583 more than tripled the Oakland A’s $39,679,746:\nSabermetics Statistics have been used in baseball since its beginnings. The dataset we will be using, included in the Lahman library, goes back to the 19th century. For example, a summary statistics we will describe soon, the batting average, has been used for decades to summarize a batter’s success. Other statistics1 such as home runs (HR), runs batted in (RBI), and stolen bases (SB) are reported for each player in the game summaries included in the sports section of newspapers, with players rewarded for high numbers. Although summary statistics such as these were widely used in baseball, data analysis per se was not. These statistics were arbitrarily decided on without much thought as to whether they actually predicted anything or were related to helping a team win.\nThis changed with Bill James2. In the late 1970s, this aspiring writer and baseball fan started publishing articles describing more in-depth analysis of baseball data. He named the approach of using data to predict what outcomes best predicted if a team would win sabermetrics3. Until Billy Beane made sabermetrics the center of his baseball operation, Bill James’ work was mostly ignored by the baseball world. Currently, sabermetrics popularity is no longer limited to just baseball; other sports have started to use this approach as well.\nIn this lecture, to simplify the exercise, we will focus on scoring runs and ignore the two other important aspects of the game: pitching and fielding. We will see how regression analysis can help develop strategies to build a competitive baseball team with a constrained budget. The approach can be divided into two separate data analyses. In the first, we determine which recorded player-specific statistics predict runs. In the second, we examine if players were undervalued based on what our first analysis predicts.\n Baseball basics To see how regression will help us find undervalued players, we actually don’t need to understand all the details about the game of baseball, which has over 100 rules. Here, we distill the sport to the basic knowledge one needs to know how to effectively attack the data science problem.\nThe goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter has had their turn, the first batter bats again, then the second, and so on. Each time a batter has an opportunity to bat, we call it a plate appearance (PA). At each PA, the other team’s pitcher throws the ball and the batter tries to hit it. The PA ends with an binary outcome: the batter either makes an out (failure) and returns to the bench or the batter doesn’t (success) and can run around the bases, and potentially score a run (reach all 4 bases). Each team gets nine tries, referred to as innings, to score runs and each inning ends after three outs (three failures).\nHere is a video showing a success: https://www.youtube.com/watch?v=HL-XjMCPfio. And here is one showing a failure: https://www.youtube.com/watch?v=NeloljCx-1g. In these videos, we see how luck is involved in the process. When at bat, the batter wants to hit the ball hard. If the batter hits it hard enough, it is a HR, the best possible outcome as the batter gets at least one automatic run. But sometimes, due to chance, the batter hits the ball very hard and a defender catches it, resulting in an out. In contrast, sometimes the batter hits the ball softly, but it lands just in the right place. The fact that there is chance involved hints at why probability models will be involved.\nNow there are several ways to succeed. Understanding this distinction will be important for our analysis. When the batter hits the ball, the batter wants to pass as many bases as possible. There are four bases with the fourth one called home plate. Home plate is where batters start by trying to hit, so the bases form a cycle.\n(Courtesy of Cburnett4. CC BY-SA 3.0 license5.) A batter who goes around the bases and arrives home, scores a run.\nWe are simplifying a bit, but there are five ways a batter can succeed, that is, not make an out:\n Bases on balls (BB) - the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), so the batter is permitted to go to first base. Single - Batter hits the ball and gets to first base. Double (2B) - Batter hits the ball and gets to second base. Triple (3B) - Batter hits the ball and gets to third base. Home Run (HR) - Batter hits the ball and goes all the way home and scores a run.  Here is an example of a HR: https://www.youtube.com/watch?v=xYxSZJ9GZ-w. If a batter gets to a base, the batter still has a chance of getting home and scoring a run if the next batter hits successfully. While the batter is on base, the batter can also try to steal a base (SB). If a batter runs fast enough, the batter can try to go from one base to the next without the other team tagging the runner. [Here] is an example of a stolen base: https://www.youtube.com/watch?v=JSE5kfxkzfk.\nAll these events are kept track of during the season and are available to us through the Lahman package. Now we will start discussing how data analysis can help us decide how to use these statistics to evaluate players.\n No awards for BB Historically, the batting average has been considered the most important offensive statistic. To define this average, we define a hit (H) and an at bat (AB). Singles, doubles, triples, and home runs are hits. The fifth way to be successful, BB, is not a hit. An AB is the number of times you either get a hit or make an out; BBs are excluded. The batting average is simply H/AB and is considered the main measure of a success rate. Today this success rate ranges from 20% to 38%. We refer to the batting average in thousands so, for example, if your success rate is 28%, we call it batting 280.\n(Picture courtesy of Keith Allison6. CC BY-SA 2.0 license7.)\nOne of Bill James’ first important insights is that the batting average ignores BB, but a BB is a success. He proposed we use the on base percentage (OBP) instead of batting average. He defined OBP as (H+BB)/(AB+BB) which is simply the proportion of plate appearances that don’t result in an out, a very intuitive measure. He noted that a player that gets many more BB than the average player might not be recognized if the batter does not excel in batting average. But is this player not helping produce runs? No award is given to the player with the most BB. However, bad habits are hard to break and baseball did not immediately adopt OBP as an important statistic. In contrast, total stolen bases were considered important and an award8 given to the player with the most. But players with high totals of SB also made more outs as they did not always succeed. Does a player with high SB total help produce runs? Can we use data science to determine if it’s better to pay for players with high BB or SB?\n Base on balls or stolen bases? One of the challenges in this analysis is that it is not obvious how to determine if a player produces runs because so much depends on his teammates. We do keep track of the number of runs scored by a player. However, remember that if a player X bats right before someone who hits many HRs, batter X will score many runs. But these runs don’t necessarily happen if we hire player X but not his HR hitting teammate. However, we can examine team-level statistics. How do teams with many SB compare to teams with few? How about BB? We have data! Let’s examine some.\nLet’s start with an obvious one: HRs. Do teams that hit more home runs score more runs? We examine data from 1961 to 2001. The visualization of choice when exploring the relationship between two variables, such as HRs and wins, is a scatterplot:\nlibrary(Lahman) Teams %\u0026gt;% filter(yearID %in% 1961:2001) %\u0026gt;% mutate(HR_per_game = HR / G, R_per_game = R / G) %\u0026gt;% ggplot(aes(HR_per_game, R_per_game)) + geom_point(alpha = 0.5) The plot shows a strong association: teams with more HRs tend to score more runs. Now let’s examine the relationship between stolen bases and runs:\nTeams %\u0026gt;% filter(yearID %in% 1961:2001) %\u0026gt;% mutate(SB_per_game = SB / G, R_per_game = R / G) %\u0026gt;% ggplot(aes(SB_per_game, R_per_game)) + geom_point(alpha = 0.5) Here the relationship is not as clear. Finally, let’s examine the relationship between BB and runs:\nTeams %\u0026gt;% filter(yearID %in% 1961:2001) %\u0026gt;% mutate(BB_per_game = BB/G, R_per_game = R/G) %\u0026gt;% ggplot(aes(BB_per_game, R_per_game)) + geom_point(alpha = 0.5) Here again we see a clear association. But does this mean that increasing a team’s BBs causes an increase in runs? One of the most important lessons you learn in this book is that association is not causation.\nIn fact, it looks like BBs and HRs are also associated:\nTeams %\u0026gt;% filter(yearID %in% 1961:2001 ) %\u0026gt;% mutate(HR_per_game = HR/G, BB_per_game = BB/G) %\u0026gt;% ggplot(aes(HR_per_game, BB_per_game)) + geom_point(alpha = 0.5) We know that HRs cause runs because, as the name “home run” implies, when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs? When this happens we say there is confounding, an important concept we will learn more about throughout this lecture.\nLinear regression will help us parse all this out and quantify the associations. This will then help us determine what players to recruit. Specifically, we will try to predict things like how many more runs will a team score if we increase the number of BBs, but keep the HRs fixed? Regression will help us answer questions like this one.\n Regression applied to baseball statistics Can we use regression with these data? First, notice that the HR and Run data appear to be bivariate normal. We save the plot into the object p as we will use it again later.\nlibrary(Lahman) p \u0026lt;- Teams %\u0026gt;% filter(yearID %in% 1961:2001 ) %\u0026gt;% mutate(HR_per_game = HR/G, R_per_game = R/G) %\u0026gt;% ggplot(aes(HR_per_game, R_per_game)) + geom_point(alpha = 0.5) p The qq-plots confirm that the normal approximation is useful here:\nTeams %\u0026gt;% filter(yearID %in% 1961:2001 ) %\u0026gt;% mutate(z_HR = round((HR - mean(HR))/sd(HR)), R_per_game = R/G) %\u0026gt;% filter(z_HR %in% -2:3) %\u0026gt;% ggplot() + stat_qq(aes(sample=R_per_game)) + facet_wrap(~z_HR) Now we are ready to use linear regression to predict the number of runs a team will score if we know how many home runs the team hits. All we need to do is compute the five summary statistics:\nsummary_stats \u0026lt;- Teams %\u0026gt;% filter(yearID %in% 1961:2001 ) %\u0026gt;% mutate(HR_per_game = HR/G, R_per_game = R/G) %\u0026gt;% summarize(avg_HR = mean(HR_per_game), s_HR = sd(HR_per_game), avg_R = mean(R_per_game), s_R = sd(R_per_game), r = cor(HR_per_game, R_per_game)) summary_stats ## avg_HR s_HR avg_R s_R r ## 1 0.8547104 0.2429707 4.355262 0.5885791 0.7615597 and use the formulas given above to create the regression lines:\nreg_line \u0026lt;- summary_stats %\u0026gt;% summarize(slope = r*s_R/s_HR, intercept = avg_R - slope*avg_HR) p + geom_abline(intercept = reg_line$intercept, slope = reg_line$slope) Soon we will learn R functions, such as lm, that make fitting regression lines much easier. Another example is the ggplot2 function geom_smooth which computes and adds a regression line to plot along with confidence intervals, which we also learn about later. We use the argument method = \"lm\" which stands for linear model, the title of an upcoming section. So we can simplify the code above like this:\np + geom_smooth(method = \u0026quot;lm\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; In the example above, the slope is 1.8448241. So this tells us that teams that hit 1 more HR per game than the average team, score 1.8448241 more runs per game than the average team. Given that the most common final score is a difference of a run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because we are working on a budget, we will need to find some other way to increase wins. So in the next section we move our attention to BB.\n  Confounding Previously, we noted a strong relationship between Runs and BB. If we find the regression line for predicting runs from bases on balls, we a get slope of:\nlibrary(tidyverse) library(Lahman) get_slope \u0026lt;- function(x, y) cor(x, y) * sd(y) / sd(x) bb_slope \u0026lt;- Teams %\u0026gt;% filter(yearID %in% 1961:2001 ) %\u0026gt;% mutate(BB_per_game = BB/G, R_per_game = R/G) %\u0026gt;% summarize(slope = get_slope(BB_per_game, R_per_game)) bb_slope ## slope ## 1 0.7353288 So does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score 1.5 more runs per game?\nWe are again reminded that association is not causation. The data does provide strong evidence that a team with two more BB per game than the average team, scores 1.5 runs per game. But this does not mean that BB are the cause.\nNote that if we compute the regression line slope for singles we get:\nsingles_slope \u0026lt;- Teams %\u0026gt;% filter(yearID %in% 1961:2001 ) %\u0026gt;% mutate(Singles_per_game = (H-HR-X2B-X3B)/G, R_per_game = R/G) %\u0026gt;% summarize(slope = get_slope(Singles_per_game, R_per_game)) singles_slope ## slope ## 1 0.4494253 which is a lower value than what we obtain for BB.\nAlso, notice that a single gets you to first base just like a BB. Those that know about baseball will tell you that with a single, runners on base have a better chance of scoring than with a BB. So how can BB be more predictive of runs? The reason this happen is because of confounding. Here we show the correlation between HR, BB, and singles:\nTeams %\u0026gt;% filter(yearID %in% 1961:2001 ) %\u0026gt;% mutate(Singles = (H-HR-X2B-X3B)/G, BB = BB/G, HR = HR/G) %\u0026gt;% summarize(cor(BB, HR), cor(Singles, HR), cor(BB, Singles)) ## cor(BB, HR) cor(Singles, HR) cor(BB, Singles) ## 1 0.4039313 -0.1737435 -0.05603822 It turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BBs and a team with many HRs will also have more BBs. Although it may appear that BBs cause runs, it is actually the HRs that cause most of these runs. We say that BBs are confounded with HRs. Nonetheless, could it be that BBs still help? To find out, we somehow have to adjust for the HR effect. Regression can help with this as well.\nUnderstanding confounding through stratification A first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest ten. We filter out the strata with few points to avoid highly variable estimates:\ndat \u0026lt;- Teams %\u0026gt;% filter(yearID %in% 1961:2001) %\u0026gt;% mutate(HR_strata = round(HR/G, 1), BB_per_game = BB / G, R_per_game = R / G) %\u0026gt;% filter(HR_strata \u0026gt;= 0.4 \u0026amp; HR_strata \u0026lt;=1.2) and then make a scatterplot for each strata:\ndat %\u0026gt;% ggplot(aes(BB_per_game, R_per_game)) + geom_point(alpha = 0.5) + geom_smooth(method = \u0026quot;lm\u0026quot;) + facet_wrap( ~ HR_strata) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; Remember that the regression slope for predicting runs with BB was 0.7. Once we stratify by HR, these slopes are substantially reduced:\ndat %\u0026gt;% group_by(HR_strata) %\u0026gt;% summarize(slope = get_slope(BB_per_game, R_per_game)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 9 x 2 ## HR_strata slope ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.4 0.734 ## 2 0.5 0.566 ## 3 0.6 0.412 ## 4 0.7 0.285 ## 5 0.8 0.365 ## 6 0.9 0.261 ## 7 1 0.512 ## 8 1.1 0.454 ## 9 1.2 0.440 The slopes are reduced, but they are not 0, which indicates that BBs are helpful for producing runs, just not as much as previously thought. In fact, the values above are closer to the slope we obtained from singles, 0.45, which is more consistent with our intuition. Since both singles and BB get us to first base, they should have about the same predictive power.\nAlthough our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BBs to get this plot:\n## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; In this case, the slopes do not change much from the original:\ndat %\u0026gt;% group_by(BB_strata) %\u0026gt;% summarize(slope = get_slope(HR_per_game, R_per_game)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 12 x 2 ## BB_strata slope ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2.8 1.52 ## 2 2.9 1.57 ## 3 3 1.52 ## 4 3.1 1.49 ## 5 3.2 1.58 ## 6 3.3 1.56 ## 7 3.4 1.48 ## 8 3.5 1.63 ## 9 3.6 1.83 ## 10 3.7 1.45 ## 11 3.8 1.70 ## 12 3.9 1.30 They are reduced a bit, which is consistent with the fact that BB do in fact cause some runs.\nhr_slope \u0026lt;- Teams %\u0026gt;% filter(yearID %in% 1961:2001 ) %\u0026gt;% mutate(HR_per_game = HR/G, R_per_game = R/G) %\u0026gt;% summarize(slope = get_slope(HR_per_game, R_per_game)) hr_slope ## slope ## 1 1.844824 Regardless, it seems that if we stratify by HR, we have bivariate distributions for runs versus BB. Similarly, if we stratify by BB, we have approximate bivariate normal distributions for HR versus runs.\n Multivariate regression It is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:\n\\[ \\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1(x_2) x_1 + \\beta_2(x_1) x_2 \\]\nwith the slopes for \\(x_1\\) changing for different values of \\(x_2\\) and vice versa. But is there an easier approach?\nIf we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that \\(\\beta_1(x_2)\\) and \\(\\beta_2(x_1)\\) are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this:\n\\[ \\mbox{E}[R \\mid BB = x_1, \\, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\]\nThis model suggests that if the number of HR is fixed at \\(x_2\\), we observe a linear relationship between runs and BB with an intercept of \\(\\beta_0 + \\beta_2 x_2\\). Our exploratory data analysis suggested this. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by \\(\\beta_1 x_1\\).\nIn this analysis, referred to as multivariate regression, you will often hear people say that the BB slope \\(\\beta_1\\) is adjusted for the HR effect. If the model is correct then confounding has been accounted for. But how do we estimate \\(\\beta_1\\) and \\(\\beta_2\\) from the data? For this, we learn about linear models and least squares estimates.\n  Least squares estimates We have described how if data is bivariate normal then the conditional expectations follow the regression line. The fact that the conditional expectation is a line is not an extra assumption but rather a derived result. However, in practice it is common to explicitly write down a model that describes the relationship between two or more variables using a linear model.\nWe note that “linear” here does not refer to lines exclusively, but rather to the fact that the conditional expectation is a linear combination of known quantities. In mathematics, when we multiply each variable by a constant and then add them together, we say we formed a linear combination of the variables. For example, \\(3x - 4y + 5z\\) is a linear combination of \\(x\\), \\(y\\), and \\(z\\). We can also add a constant so \\(2 + 3x - 4y + 5z\\) is also linear combination of \\(x\\), \\(y\\), and \\(z\\).\nSo \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\), is a linear combination of \\(x_1\\) and \\(x_2\\). The simplest linear model is a constant \\(\\beta_0\\); the second simplest is a line \\(\\beta_0 + \\beta_1 x\\). If we were to specify a linear model for Galton’s data, we would denote the \\(N\\) observed father heights with \\(x_1, \\dots, x_n\\), then we model the \\(N\\) son heights we are trying to predict with:\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\, i=1,\\dots,N. \\]\nHere \\(x_i\\) is the father’s height, which is fixed (not random) due to the conditioning, and \\(Y_i\\) is the random son’s height that we want to predict. We further assume that \\(\\varepsilon_i\\) are independent from each other, have expected value 0 and the standard deviation, call it \\(\\sigma\\), does not depend on \\(i\\).\nIn the above model, we know the \\(x_i\\), but to have a useful model for prediction, we need \\(\\beta_0\\) and \\(\\beta_1\\). We estimate these from the data. Once we do this, we can predict son’s heights for any father’s height \\(x\\). We show how to do this in the next section.\nNote that if we further assume that the \\(\\varepsilon\\) is normally distributed, then this model is exactly the same one we derived earlier by assuming bivariate normal data. A somewhat nuanced difference is that in the first approach we assumed the data was bivariate normal and that the linear model was derived, not assumed. In practice, linear models are just assumed without necessarily assuming normality: the distribution of the \\(\\varepsilon\\)s is not specified. Nevertheless, if your data is bivariate normal, the above linear model holds. If your data is not bivariate normal, then you will need to have other ways of justifying the model.\nInterpreting linear models One reason linear models are popular is that they are interpretable. In the case of Galton’s data, we can interpret the data like this: due to inherited genes, the son’s height prediction grows by \\(\\beta_1\\) for each inch we increase the father’s height \\(x\\). Because not all sons with fathers of height \\(x\\) are of equal height, we need the term \\(\\varepsilon\\), which explains the remaining variability. This remaining variability includes the mother’s genetic effect, environmental factors, and other biological randomness.\nGiven how we wrote the model above, the intercept \\(\\beta_0\\) is not very interpretable as it is the predicted height of a son with a father with no height. Due to regression to the mean, the prediction will usually be a bit larger than 0. To make the slope parameter more interpretable, we can rewrite the model slightly as:\n\\[ Y_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\varepsilon_i, \\, i=1,\\dots,N \\]\nwith \\(\\bar{x} = 1/N \\sum_{i=1}^N x_i\\) the average of the \\(x\\). In this case \\(\\beta_0\\) represents the height when \\(x_i = \\bar{x}\\), which is the height of the son of an average father.\n Least Squares Estimates (LSE) For linear models to be useful, we have to estimate the unknown \\(\\beta\\)s. The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this lecture. For Galton’s data, we would write:\n\\[ RSS = \\sum_{i=1}^n \\left\\{ y_i - \\left(\\beta_0 + \\beta_1 x_i \\right)\\right\\}^2 \\]\nThis quantity is called the residual sum of squares (RSS). Once we find the values that minimize the RSS, we will call the values the least squares estimates (LSE) and denote them with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Let’s demonstrate this with the previously defined dataset:\nlibrary(HistData) data(\u0026quot;GaltonFamilies\u0026quot;) set.seed(1983) galton_heights \u0026lt;- GaltonFamilies %\u0026gt;% filter(gender == \u0026quot;male\u0026quot;) %\u0026gt;% group_by(family) %\u0026gt;% sample_n(1) %\u0026gt;% ungroup() %\u0026gt;% select(father, childHeight) %\u0026gt;% rename(son = childHeight) Let’s write a function that computes the RSS for any pair of values \\(\\beta_0\\) and \\(\\beta_1\\).\nrss \u0026lt;- function(beta0, beta1, data){ resid \u0026lt;- galton_heights$son - (beta0+beta1*galton_heights$father) return(sum(resid^2)) } So for any pair of values, we get an RSS. Here is a plot of the RSS as a function of \\(\\beta_1\\) when we keep the \\(\\beta_0\\) fixed at 25.\nbeta1 = seq(0, 1, len=nrow(galton_heights)) results \u0026lt;- data.frame(beta1 = beta1, rss = sapply(beta1, rss, beta0 = 25)) results %\u0026gt;% ggplot(aes(beta1, rss)) + geom_line() + geom_line(aes(beta1, rss)) We can see a clear minimum for \\(\\beta_1\\) at around 0.65. However, this minimum for \\(\\beta_1\\) is for when \\(\\beta_0 = 25\\), a value we arbitrarily picked. We don’t know if (25, 0.65) is the pair that minimizes the equation across all possible pairs.\nTrial and error is not going to work in this case. We could search for a minimum within a fine grid of \\(\\beta_0\\) and \\(\\beta_1\\) values, but this is unnecessarily time-consuming since we can use calculus: take the partial derivatives, set them to 0 and solve for \\(\\beta_1\\) and \\(\\beta_2\\). Of course, if we have many parameters, these equations can get rather complex. But there are functions in R that do these calculations for us. We will learn these next. To learn the mathematics behind this, you can consult a book on linear models.\n The lm function In R, we can obtain the least squares estimates using the lm function. To fit the model:\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\]\nwith \\(Y_i\\) the son’s height and \\(x_i\\) the father’s height, we can use this code to obtain the least squares estimates.\nfit \u0026lt;- lm(son ~ father, data = galton_heights) fit$coef ## (Intercept) father ## 37.287605 0.461392 The most common way we use lm is by using the character ~ to let lm know which is the variable we are predicting (left of ~) and which we are using to predict (right of ~). The intercept is added automatically to the model that will be fit.\nThe object fit includes more information about the fit. We can use the function summary to extract more of this information (not shown):\nsummary(fit) ## ## Call: ## lm(formula = son ~ father, data = galton_heights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3543 -1.5657 -0.0078 1.7263 9.4150 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 37.28761 4.98618 7.478 3.37e-12 *** ## father 0.46139 0.07211 6.398 1.36e-09 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.45 on 177 degrees of freedom ## Multiple R-squared: 0.1878, Adjusted R-squared: 0.1833 ## F-statistic: 40.94 on 1 and 177 DF, p-value: 1.36e-09 To understand some of the information included in this summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables\n LSE are random variables The LSE is derived from the data \\(y_1,\\dots,y_N\\), which are a realization of random variables \\(Y_1, \\dots, Y_N\\). This implies that our estimates are random variables. To see this, we can run a Monte Carlo simulation in which we assume the son and father height data defines a population, take a random sample of size \\(N=50\\), and compute the regression slope coefficient for each one:\nB \u0026lt;- 1000 N \u0026lt;- 50 lse \u0026lt;- replicate(B, { sample_n(galton_heights, N, replace = TRUE) %\u0026gt;% lm(son ~ father, data = .) %\u0026gt;% .$coef }) lse \u0026lt;- data.frame(beta_0 = lse[1,], beta_1 = lse[2,]) We can see the variability of the estimates by plotting their distributions:\n## ## Attaching package: \u0026#39;gridExtra\u0026#39; ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## combine The reason these look normal is because the central limit theorem applies here as well: for large enough \\(N\\), the least squares estimates will be approximately normal with expected value \\(\\beta_0\\) and \\(\\beta_1\\), respectively. The standard errors are a bit complicated to compute, but mathematical theory does allow us to compute them and they are included in the summary provided by the lm function. Here it is for one of our simulated data sets:\n sample_n(galton_heights, N, replace = TRUE) %\u0026gt;% lm(son ~ father, data = .) %\u0026gt;% summary %\u0026gt;% .$coef ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 19.2791952 11.6564590 1.653950 0.1046637693 ## father 0.7198756 0.1693834 4.249977 0.0000979167 You can see that the standard errors estimates reported by the summary are close to the standard errors from the simulation:\nlse %\u0026gt;% summarize(se_0 = sd(beta_0), se_1 = sd(beta_1)) ## se_0 se_1 ## 1 8.83591 0.1278812 The summary function also reports t-statistics (t value) and p-values (Pr(\u0026gt;|t|)). The t-statistic is not actually based on the central limit theorem but rather on the assumption that the \\(\\varepsilon\\)s follow a normal distribution. Under this assumption, mathematical theory tells us that the LSE divided by their standard error, \\(\\hat{\\beta}_0 / \\hat{\\mbox{SE}}(\\hat{\\beta}_0 )\\) and \\(\\hat{\\beta}_1 / \\hat{\\mbox{SE}}(\\hat{\\beta}_1 )\\), follow a t-distribution with \\(N-p\\) degrees of freedom, with \\(p\\) the number of parameters in our model. In the case of height \\(p=2\\), the two p-values are testing the null hypothesis that \\(\\beta_0 = 0\\) and \\(\\beta_1=0\\), respectively.\nRemember that, as we described in Section ?? for large enough \\(N\\), the CLT works and the t-distribution becomes almost the same as the normal distribution. Also, notice that we can construct confidence intervals, but we will soon learn about broom, an add-on package that makes this easy.\nAlthough we do not show examples in this book, hypothesis testing with regression models is commonly used in epidemiology and economics to make statements such as “the effect of A on B was statistically significant after adjusting for X, Y, and Z”. However, several assumptions have to hold for these statements to be true.\n Predicted values are random variables Once we fit our model, we can obtain prediction of \\(Y\\) by plugging in the estimates into the regression model. For example, if the father’s height is \\(x\\), then our prediction \\(\\hat{Y}\\) for the son’s height will be:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\]\nWhen we plot \\(\\hat{Y}\\) versus \\(x\\), we see the regression line.\nKeep in mind that the prediction \\(\\hat{Y}\\) is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the ggplot2 layer geom_smooth(method = \"lm\") that we previously used plots \\(\\hat{Y}\\) and surrounds it by confidence intervals:\ngalton_heights %\u0026gt;% ggplot(aes(son, father)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) ## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; The R function predict takes an lm object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals is provided:\nfit \u0026lt;- galton_heights %\u0026gt;% lm(son ~ father, data = .) y_hat \u0026lt;- predict(fit, se.fit = TRUE) names(y_hat) ## [1] \u0026quot;fit\u0026quot; \u0026quot;se.fit\u0026quot; \u0026quot;df\u0026quot; \u0026quot;residual.scale\u0026quot;   Exercises We have shown how BB and singles have similar predictive power for scoring runs. Another way to compare the usefulness of these baseball metrics is by assessing how stable they are across the years. Since we have to pick players based on their previous performances, we will prefer metrics that are more stable. In these exercises, we will compare the stability of singles and BBs.\n1. Before we get started, we want to generate two tables. One for 2002 and another for the average of 1999-2001 seasons. We want to define per plate appearance statistics. Here is how we create the 2017 table. Keeping only players with more than 100 plate appearances.\nlibrary(Lahman) dat \u0026lt;- Batting %\u0026gt;% filter(yearID == 2002) %\u0026gt;% mutate(pa = AB + BB, singles = (H - X2B - X3B - HR) / pa, bb = BB / pa) %\u0026gt;% filter(pa \u0026gt;= 100) %\u0026gt;% select(playerID, singles, bb) Now compute a similar table but with rates computed over 1999-2001.\n2. In Section ?? we learn about the inner_join, which you can use to have the 2001 data and averages in the same table:\ndat \u0026lt;- inner_join(dat, avg, by = \u0026quot;playerID\u0026quot;) Compute the correlation between 2002 and the previous seasons for singles and BB.\n3. Note that the correlation is higher for BB. To quickly get an idea of the uncertainty associated with this correlation estimate, we will fit a linear model and compute confidence intervals for the slope coefficient. However, first make scatterplots to confirm that fitting a linear model is appropriate.\n4. Now fit a linear model for each metric and use the confint function to compare the estimates.\n Linear regression in the tidyverse To see how we use the lm function in a more complex analysis, let’s go back to the baseball example. In a previous example, we estimated regression lines to predict runs for BB in different HR strata. We first constructed a data frame similar to this:\ndat \u0026lt;- Teams %\u0026gt;% filter(yearID %in% 1961:2001) %\u0026gt;% mutate(HR = round(HR/G, 1), BB = BB/G, R = R/G) %\u0026gt;% select(HR, BB, R) %\u0026gt;% filter(HR \u0026gt;= 0.4 \u0026amp; HR\u0026lt;=1.2) Since we didn’t know the lm function, to compute the regression line in each strata, we used the formula directly like this:\nget_slope \u0026lt;- function(x, y) cor(x, y) * sd(y) / sd(x) dat %\u0026gt;% group_by(HR) %\u0026gt;% summarize(slope = get_slope(BB, R)) We argued that the slopes are similar and that the differences were perhaps due to random variation. To provide a more rigorous defense of the slopes being the same, which led to our multivariate model, we could compute confidence intervals for each slope. We have not learned the formula for this, but the lm function provides enough information to construct them.\nFirst, note that if we try to use the lm function to get the estimated slope like this:\ndat %\u0026gt;% group_by(HR) %\u0026gt;% lm(R ~ BB, data = .) %\u0026gt;% .$coef ## (Intercept) BB ## 2.1983658 0.6378804 we don’t get the result we want. The lm function ignores the group_by. This is expected because lm is not part of the tidyverse and does not know how to handle the outcome of a grouped tibble.\nThe tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %\u0026gt;%, tidyverse functions consistently return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The lm function is an example. The do functions serves as a bridge between R functions, such as lm, and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nSo, let’s try to use the do function to fit a regression line to each HR strata:\ndat %\u0026gt;% group_by(HR) %\u0026gt;% do(fit = lm(R ~ BB, data = .)) ## # A tibble: 9 x 2 ## # Rowwise: ## HR fit ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 0.4 \u0026lt;lm\u0026gt; ## 2 0.5 \u0026lt;lm\u0026gt; ## 3 0.6 \u0026lt;lm\u0026gt; ## 4 0.7 \u0026lt;lm\u0026gt; ## 5 0.8 \u0026lt;lm\u0026gt; ## 6 0.9 \u0026lt;lm\u0026gt; ## 7 1 \u0026lt;lm\u0026gt; ## 8 1.1 \u0026lt;lm\u0026gt; ## 9 1.2 \u0026lt;lm\u0026gt; Notice that we did in fact fit a regression line to each strata. The do function will create a data frame with the first column being the strata value and a column named fit (we chose the name, but it can be anything). The column will contain the result of the lm call. Therefore, the returned tibble has a column with lm objects, which is not very useful.\nAlso, if we do not name a column (note above we named it fit), then do will return the actual output of lm, not a data frame, and this will result in an error since do is expecting a data frame as output.\ndat %\u0026gt;% group_by(HR) %\u0026gt;% do(lm(R ~ BB, data = .)) Error: Results 1, 2, 3, 4, 5, ... must be data frames, not lm\nFor a useful data frame to be constructed, the output of the function must be a data frame too. We could build a function that returns only what we want in the form of a data frame:\nget_slope \u0026lt;- function(data){ fit \u0026lt;- lm(R ~ BB, data = data) data.frame(slope = fit$coefficients[2], se = summary(fit)$coefficient[2,2]) } And then use do without naming the output, since we are already getting a data frame:\ndat %\u0026gt;% group_by(HR) %\u0026gt;% do(get_slope(.)) ## # A tibble: 9 x 3 ## # Groups: HR [9] ## HR slope se ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.4 0.734 0.208 ## 2 0.5 0.566 0.110 ## 3 0.6 0.412 0.0974 ## 4 0.7 0.285 0.0705 ## 5 0.8 0.365 0.0653 ## 6 0.9 0.261 0.0751 ## 7 1 0.512 0.0751 ## 8 1.1 0.454 0.0855 ## 9 1.2 0.440 0.0801 If we name the output, then we get something we do not want, a column containing data frames:\ndat %\u0026gt;% group_by(HR) %\u0026gt;% do(slope = get_slope(.)) ## # A tibble: 9 x 2 ## # Rowwise: ## HR slope ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 0.4 \u0026lt;df[,2] [1 × 2]\u0026gt; ## 2 0.5 \u0026lt;df[,2] [1 × 2]\u0026gt; ## 3 0.6 \u0026lt;df[,2] [1 × 2]\u0026gt; ## 4 0.7 \u0026lt;df[,2] [1 × 2]\u0026gt; ## 5 0.8 \u0026lt;df[,2] [1 × 2]\u0026gt; ## 6 0.9 \u0026lt;df[,2] [1 × 2]\u0026gt; ## 7 1 \u0026lt;df[,2] [1 × 2]\u0026gt; ## 8 1.1 \u0026lt;df[,2] [1 × 2]\u0026gt; ## 9 1.2 \u0026lt;df[,2] [1 × 2]\u0026gt; This is not very useful, so let’s cover one last feature of do. If the data frame being returned has more than one row, these will be concatenated appropriately. Here is an example in which we return both estimated parameters:\nget_lse \u0026lt;- function(data){ fit \u0026lt;- lm(R ~ BB, data = data) data.frame(term = names(fit$coefficients), slope = fit$coefficients, se = summary(fit)$coefficient[,2]) } dat %\u0026gt;% group_by(HR) %\u0026gt;% do(get_lse(.)) ## # A tibble: 18 x 4 ## # Groups: HR [9] ## HR term slope se ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.4 (Intercept) 1.36 0.631 ## 2 0.4 BB 0.734 0.208 ## 3 0.5 (Intercept) 2.01 0.344 ## 4 0.5 BB 0.566 0.110 ## 5 0.6 (Intercept) 2.53 0.305 ## 6 0.6 BB 0.412 0.0974 ## 7 0.7 (Intercept) 3.21 0.225 ## 8 0.7 BB 0.285 0.0705 ## 9 0.8 (Intercept) 3.07 0.213 ## 10 0.8 BB 0.365 0.0653 ## 11 0.9 (Intercept) 3.54 0.251 ## 12 0.9 BB 0.261 0.0751 ## 13 1 (Intercept) 2.88 0.256 ## 14 1 BB 0.512 0.0751 ## 15 1.1 (Intercept) 3.21 0.300 ## 16 1.1 BB 0.454 0.0855 ## 17 1.2 (Intercept) 3.40 0.291 ## 18 1.2 BB 0.440 0.0801 If you think this is all a bit too complicated, you are not alone. To simplify things, we introduce the broom package which was designed to facilitate the use of model fitting functions, such as lm, with the tidyverse.\nThe broom package Our original task was to provide an estimate and confidence interval for the slope estimates of each strata. The broom package will make this quite easy.\nThe broom package has three main functions, all of which extract information from the object returned by lm and return it in a tidyverse friendly data frame. These functions are tidy, glance, and augment. The tidy function returns estimates and related information as a data frame:\nlibrary(broom) fit \u0026lt;- lm(R ~ BB, data = dat) tidy(fit) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 2.20 0.113 19.4 1.12e-70 ## 2 BB 0.638 0.0344 18.5 1.35e-65 We can add other important summaries, such as confidence intervals:\ntidy(fit, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 2.20 0.113 19.4 1.12e-70 1.98 2.42 ## 2 BB 0.638 0.0344 18.5 1.35e-65 0.570 0.705 Because the outcome is a data frame, we can immediately use it with do to string together the commands that produce the table we are after. Because a data frame is returned, we can filter and select the rows and columns we want, which facilitates working with ggplot2:\ndat %\u0026gt;% group_by(HR) %\u0026gt;% do(tidy(lm(R ~ BB, data = .), conf.int = TRUE)) %\u0026gt;% filter(term == \u0026quot;BB\u0026quot;) %\u0026gt;% select(HR, estimate, conf.low, conf.high) %\u0026gt;% ggplot(aes(HR, y = estimate, ymin = conf.low, ymax = conf.high)) + geom_errorbar() + geom_point() Now we return to discussing our original task of determining if slopes changed. The plot we just made, using do and tidy, shows that the confidence intervals overlap, which provides a nice visual confirmation that our assumption that the slope does not change is safe.\nThe other functions provided by broom, glance, and augment, relate to model-specific and observation-specific outcomes, respectively. Here, we can see the model fit summaries glance returns:\nglance(fit) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0.266 0.265 0.454 343. 1.35e-65 1 -596. 1199. 1214. ## # … with 3 more variables: deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, nobs \u0026lt;int\u0026gt; You can learn more about these summaries in any regression text book.\nWe will see an example of augment in the next section.\n  Case study: Moneyball (continued) In trying to answer how well BBs predict runs, data exploration led us to a model:\n\\[ \\mbox{E}[R \\mid BB = x_1, HR = x_2] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\]\nHere, the data is approximately normal and conditional distributions were also normal. Thus, we are justified in using a linear model:\n\\[ Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\varepsilon_i \\]\nwith \\(Y_i\\) runs per game for team \\(i\\), \\(x_{i,1}\\) walks per game, and \\(x_{i,2}\\). To use lm here, we need to let the function know we have two predictor variables. So we use the + symbol as follows:\nfit \u0026lt;- Teams %\u0026gt;% filter(yearID %in% 1961:2001) %\u0026gt;% mutate(BB = BB/G, HR = HR/G, R = R/G) %\u0026gt;% lm(R ~ BB + HR, data = .) We can use tidy to see a nice summary:\ntidy(fit, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 1.74 0.0824 21.2 7.62e- 83 1.58 1.91 ## 2 BB 0.387 0.0270 14.3 1.20e- 42 0.334 0.440 ## 3 HR 1.56 0.0490 31.9 1.78e-155 1.47 1.66 When we fit the model with only one variable, the estimated slopes were 0.7353288 and 1.8448241 for BB and HR, respectively. Note that when fitting the multivariate model both go down, with the BB effect decreasing much more.\nNow we want to construct a metric to pick players, we need to consider singles, doubles, and triples as well. Can we build a model that predicts runs based on all these outcomes?\nWe now are going to take somewhat of a “leap of faith” and assume that these five variables are jointly normal. This means that if we pick any one of them, and hold the other four fixed, the relationship with the outcome is linear and the slope does not depend on the four values held constant. If this is true, then a linear model for our data is:\n\\[ Y_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3}+ \\beta_4 x_{i,4} + \\beta_5 x_{i,5} + \\varepsilon_i \\]\nwith \\(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\) representing BB, singles, doubles, triples, and HR respectively.\nUsing lm, we can quickly find the LSE for the parameters using:\nfit \u0026lt;- Teams %\u0026gt;% filter(yearID %in% 1961:2001) %\u0026gt;% mutate(BB = BB / G, singles = (H - X2B - X3B - HR) / G, doubles = X2B / G, triples = X3B / G, HR = HR / G, R = R / G) %\u0026gt;% lm(R ~ BB + singles + doubles + triples + HR, data = .) We can see the coefficients using tidy:\ncoefs \u0026lt;- tidy(fit, conf.int = TRUE) coefs ## # A tibble: 6 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) -2.77 0.0862 -32.1 4.76e-157 -2.94 -2.60 ## 2 BB 0.371 0.0117 31.6 1.87e-153 0.348 0.394 ## 3 singles 0.519 0.0127 40.8 8.67e-217 0.494 0.544 ## 4 doubles 0.771 0.0226 34.1 8.44e-171 0.727 0.816 ## 5 triples 1.24 0.0768 16.1 2.12e- 52 1.09 1.39 ## 6 HR 1.44 0.0243 59.3 0. 1.40 1.49 To see how well our metric actually predicts runs, we can predict the number of runs for each team in 2002 using the function predict, then make a plot:\nTeams %\u0026gt;% filter(yearID %in% 2002) %\u0026gt;% mutate(BB = BB/G, singles = (H-X2B-X3B-HR)/G, doubles = X2B/G, triples =X3B/G, HR=HR/G, R=R/G) %\u0026gt;% mutate(R_hat = predict(fit, newdata = .)) %\u0026gt;% ggplot(aes(R_hat, R, label = teamID)) + geom_point() + geom_text(nudge_x=0.1, cex = 2) + geom_abline() Our model does quite a good job as demonstrated by the fact that points from the observed versus predicted plot fall close to the identity line.\nSo instead of using batting average, or just number of HR, as a measure of picking players, we can use our fitted model to form a metric that relates more directly to run production. Specifically, to define a metric for player A, we imagine a team made up of players just like player A and use our fitted regression model to predict how many runs this team would produce. The formula would look like this: -2.7691857 + 0.3712147 \\(\\times\\) BB + 0.5193923 \\(\\times\\) singles + 0.7711444 \\(\\times\\) doubles + 1.2399696 \\(\\times\\) triples + 1.4433701 \\(\\times\\) HR.\nTo define a player-specific metric, we have a bit more work to do. A challenge here is that we derived the metric for teams, based on team-level summary statistics. For example, the HR value that is entered into the equation is HR per game for the entire team. If we compute the HR per game for a player, it will be much lower since the total is accumulated by 9 batters. Furthermore, if a player only plays part of the game and gets fewer opportunities than average, it is still considered a game played. For players, a rate that takes into account opportunities is the per-plate-appearance rate.\nTo make the per-game team rate comparable to the per-plate-appearance player rate, we compute the average number of team plate appearances per game:\npa_per_game \u0026lt;- Batting %\u0026gt;% filter(yearID == 2002) %\u0026gt;% group_by(teamID) %\u0026gt;% summarize(pa_per_game = sum(AB+BB)/max(G)) %\u0026gt;% pull(pa_per_game) %\u0026gt;% mean ## `summarise()` ungrouping output (override with `.groups` argument) We compute the per-plate-appearance rates for players available in 2002 on data from 1997-2001. To avoid small sample artifacts, we filter players with less than 200 plate appearances per year. Here is the entire calculation in one line:\nplayers \u0026lt;- Batting %\u0026gt;% filter(yearID %in% 1997:2001) %\u0026gt;% group_by(playerID) %\u0026gt;% mutate(PA = BB + AB) %\u0026gt;% summarize(G = sum(PA)/pa_per_game, BB = sum(BB)/G, singles = sum(H-X2B-X3B-HR)/G, doubles = sum(X2B)/G, triples = sum(X3B)/G, HR = sum(HR)/G, AVG = sum(H)/sum(AB), PA = sum(PA)) %\u0026gt;% filter(PA \u0026gt;= 1000) %\u0026gt;% select(-G) %\u0026gt;% mutate(R_hat = predict(fit, newdata = .)) ## `summarise()` ungrouping output (override with `.groups` argument) The player-specific predicted runs computed here can be interpreted as the number of runs we predict a team will score if all batters are exactly like that player. The distribution shows that there is wide variability across players:\nqplot(R_hat, data = players, binwidth = 0.5, color = I(\u0026quot;black\u0026quot;)) Adding salary and position information To actually build the team, we will need to know their salaries as well as their defensive position. For this, we join the players data frame we just created with the player information data frame included in some of the other Lahman data tables. We will learn more about the join function (and we will discuss this further in a later lecture).\nStart by adding the 2002 salary of each player:\nplayers \u0026lt;- Salaries %\u0026gt;% filter(yearID == 2002) %\u0026gt;% select(playerID, salary) %\u0026gt;% right_join(players, by=\u0026quot;playerID\u0026quot;) Next, we add their defensive position. This is a somewhat complicated task because players play more than one position each year. The Lahman package table Appearances tells how many games each player played in each position, so we can pick the position that was most played using which.max on each row. We use apply to do this. However, because some players are traded, they appear more than once on the table, so we first sum their appearances across teams. Here, we pick the one position the player most played using the top_n function. To make sure we only pick one position, in the case of ties, we pick the first row of the resulting data frame. We also remove the OF position which stands for outfielder, a generalization of three positions: left field (LF), center field (CF), and right field (RF). We also remove pitchers since they don’t bat in the league in which the A’s play.\nposition_names \u0026lt;- paste0(\u0026quot;G_\u0026quot;, c(\u0026quot;p\u0026quot;,\u0026quot;c\u0026quot;,\u0026quot;1b\u0026quot;,\u0026quot;2b\u0026quot;,\u0026quot;3b\u0026quot;,\u0026quot;ss\u0026quot;,\u0026quot;lf\u0026quot;,\u0026quot;cf\u0026quot;,\u0026quot;rf\u0026quot;, \u0026quot;dh\u0026quot;)) tmp \u0026lt;- Appearances %\u0026gt;% filter(yearID == 2002) %\u0026gt;% group_by(playerID) %\u0026gt;% summarize_at(position_names, sum) %\u0026gt;% ungroup() pos \u0026lt;- tmp %\u0026gt;% select(position_names) %\u0026gt;% apply(., 1, which.max) ## Note: Using an external vector in selections is ambiguous. ## ℹ Use `all_of(position_names)` instead of `position_names` to silence this message. ## ℹ See \u0026lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html\u0026gt;. ## This message is displayed once per session. players \u0026lt;- tibble(playerID = tmp$playerID, POS = position_names[pos]) %\u0026gt;% mutate(POS = str_to_upper(str_remove(POS, \u0026quot;G_\u0026quot;))) %\u0026gt;% filter(POS != \u0026quot;P\u0026quot;) %\u0026gt;% right_join(players, by=\u0026quot;playerID\u0026quot;) %\u0026gt;% filter(!is.na(POS) \u0026amp; !is.na(salary)) Finally, we add their first and last name:\nplayers \u0026lt;- Master %\u0026gt;% select(playerID, nameFirst, nameLast, debut) %\u0026gt;% mutate(debut = as.Date(debut)) %\u0026gt;% right_join(players, by=\u0026quot;playerID\u0026quot;) If you are a baseball fan, you will recognize the top 10 players:\nplayers %\u0026gt;% select(nameFirst, nameLast, POS, salary, R_hat) %\u0026gt;% arrange(desc(R_hat)) %\u0026gt;% top_n(10) ## Selecting by R_hat ## nameFirst nameLast POS salary R_hat ## 1 Barry Bonds LF 15000000 8.441480 ## 2 Larry Walker RF 12666667 8.344316 ## 3 Todd Helton 1B 5000000 7.764649 ## 4 Manny Ramirez LF 15462727 7.714582 ## 5 Sammy Sosa RF 15000000 7.559582 ## 6 Jeff Bagwell 1B 11000000 7.405572 ## 7 Mike Piazza C 10571429 7.343984 ## 8 Jason Giambi 1B 10428571 7.263690 ## 9 Edgar Martinez DH 7086668 7.259399 ## 10 Jim Thome 1B 8000000 7.231955  Picking nine players On average, players with a higher metric have higher salaries:\nplayers %\u0026gt;% ggplot(aes(salary, R_hat, color = POS)) + geom_point() + scale_x_log10() % filter(year(debut) % ggplot(aes(salary, R_hat, color = POS)) + geom_point() + scale_x_log10() ``` -- We can search for good deals by looking at players who produce many more runs than others with similar salaries. We can use this table to decide what players to pick and keep our total salary below the 40 million dollars Billy Beane had to work with. This can be done using what computer scientists call linear programming. This is not something we teach, but here are the position players selected with this approach:\n  nameFirst  nameLast  POS  salary  R_hat      Todd  Helton  1B  5000000  7.764649    Mike  Piazza  C  10571429  7.343984    Edgar  Martinez  DH  7086668  7.259399    Jim  Edmonds  CF  7333333  6.552456    Jeff  Kent  2B  6000000  6.391614    Phil  Nevin  3B  2600000  6.163936    Matt  Stairs  RF  500000  6.062372    Henry  Rodriguez  LF  300000  5.938315    John  Valentin  SS  550000  5.273441     We see that all these players have above average BB and most have above average HR rates, while the same is not true for singles. Here is a table with statistics standardized across players so that, for example, above average HR hitters have values above 0.\n  nameLast  BB  singles  doubles  triples  HR  AVG  R_hat      Helton  0.9088340  -0.2147828  2.6489997  -0.3105275  1.5221254  2.6704562  2.5316660    Piazza  0.3281058  0.4231217  0.2037161  -1.4181571  1.8253653  2.1990055  2.0890701    Martinez  2.1352215  -0.0051702  1.2649044  -1.2242578  0.8079817  2.2032836  2.0000756    Edmonds  1.0706548  -0.5579104  0.7912381  -1.1517126  0.9730052  0.8543566  1.2562767    Kent  0.2316321  -0.7322902  2.0113988  0.4483097  0.7658693  0.7871932  1.0870488    Nevin  0.3066863  -0.9051225  0.4787634  -1.1908955  1.1927055  0.1048721  0.8475017    Stairs  1.0996635  -1.5127562  -0.0460876  -1.1285395  1.1209081  -0.5608456  0.7406428    Rodriguez  0.2011513  -1.5963595  0.3324557  -0.7823620  1.3202734  -0.6723416  0.6101181    Valentin  0.1802855  -0.9287069  1.7940379  -0.4348410  -0.0452462  -0.4717038  -0.0894187       The regression fallacy Wikipedia defines the sophomore slump as:\n A sophomore slump or sophomore jinx or sophomore jitters refers to an instance in which a second, or sophomore, effort fails to live up to the standards of the first effort. It is commonly used to refer to the apathy of students (second year of high school, college or university), the performance of athletes (second season of play), singers/bands (second album), television shows (second seasons) and movies (sequels/prequels).\n In Major League Baseball, the rookie of the year (ROY) award is given to the first-year player who is judged to have performed the best. The sophmore slump phrase is used to describe the observation that ROY award winners don’t do as well during their second year. For example, this Fox Sports article9 asks “Will MLB’s tremendous rookie class of 2015 suffer a sophomore slump?”.\nDoes the data confirm the existence of a sophomore slump? Let’s take a look. Examining the data for batting average, we see that this observation holds true for the top performing ROYs:\n  nameFirst  nameLast  rookie_year  rookie  sophomore      Willie  McCovey  1959  0.3541667  0.2384615    Ichiro  Suzuki  2001  0.3497110  0.3214838    Al  Bumbry  1973  0.3370787  0.2333333    Fred  Lynn  1975  0.3314394  0.3136095    Albert  Pujols  2001  0.3288136  0.3135593     In fact, the proportion of players that have a lower batting average their sophomore year is 0.6862745.\nSo is it “jitters” or “jinx”? To answer this question, let’s turn our attention to all players that played the 2013 and 2014 seasons and batted more than 130 times (minimum to win Rookie of the Year).\n## `summarise()` regrouping output by \u0026#39;playerID\u0026#39; (override with `.groups` argument) The same pattern arises when we look at the top performers: batting averages go down for most of the top performers.\n  nameFirst  nameLast  2013  2014      Miguel  Cabrera  0.3477477  0.3126023    Hanley  Ramirez  0.3453947  0.2828508    Michael  Cuddyer  0.3312883  0.3315789    Scooter  Gennett  0.3239437  0.2886364    Joe  Mauer  0.3235955  0.2769231     But these are not rookies! Also, look at what happens to the worst performers of 2013:\n  nameFirst  nameLast  2013  2014      Danny  Espinosa  0.1582278  0.2192192    Dan  Uggla  0.1785714  0.1489362    Jeff  Mathis  0.1810345  0.2000000    B. J.  Upton  0.1841432  0.2080925    Adam  Rosales  0.1904762  0.2621951     Their batting averages mostly go up! Is this some sort of reverse sophomore slump? It is not. There is no such thing as the sophomore slump. This is all explained with a simple statistical fact: the correlation for performance in two separate years is high, but not perfect:\nThe correlation is 0.460254 and the data look very much like a bivariate normal distribution, which means we predict a 2014 batting average \\(Y\\) for any given player that had a 2013 batting average \\(X\\) with:\n\\[ \\frac{Y - .255}{.032} = 0.46 \\left( \\frac{X - .261}{.023}\\right) \\]\nBecause the correlation is not perfect, regression tells us that, on average, expect high performers from 2013 to do a bit worse in 2014. It’s not a jinx; it’s just due to chance. The ROY are selected from the top values of \\(X\\) so it is expected that \\(Y\\) will regress to the mean.\n Measurement error models Up to now, all our linear regression examples have been applied to two or more random variables. We assume the pairs are bivariate normal and use this to motivate a linear model. This approach covers most real-life examples of linear regression. The other major application comes from measurement errors models. In these applications, it is common to have a non-random covariate, such as time, and randomness is introduced from measurement error rather than sampling or natural variability.\nTo understand these models, imagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let’s simulate some data using the equations we know today and adding some measurement error. The dslabs function rfalling_object generates these simulations:\nlibrary(dslabs) falling_object \u0026lt;- rfalling_object() The assistants hand the data to Galileo and this is what he sees:\nfalling_object %\u0026gt;% ggplot(aes(time, observed_distance)) + geom_point() + ylab(\u0026quot;Distance in meters\u0026quot;) + xlab(\u0026quot;Time in seconds\u0026quot;) Galileo does not know the exact equation, but by looking at the plot above, he deduces that the position should follow a parabola, which we can write like this:\n\\[ f(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\]\nThe data does not fall exactly on a parabola. Galileo knows this is due to measurement error. His helpers make mistakes when measuring the distance. To account for this, he models the data with:\n\\[ Y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i, i=1,\\dots,n \\]\nwith \\(Y_i\\) representing distance in meters, \\(x_i\\) representing time in seconds, and \\(\\varepsilon\\) accounting for measurement error. The measurement error is assumed to be random, independent from each other, and having the same distribution for each \\(i\\). We also assume that there is no bias, which means the expected value \\(\\mbox{E}[\\varepsilon] = 0\\).\nNote that this is a linear model because it is a linear combination of known quantities (\\(x\\) and \\(x^2\\) are known) and unknown parameters (the \\(\\beta\\)s are unknown parameters to Galileo). Unlike our previous examples, here \\(x\\) is a fixed quantity; we are not conditioning.\nTo pose a new physical theory and start making predictions about other falling objects, Galileo needs actual numbers, rather than unknown parameters. Using LSE seems like a reasonable approach. How do we find the LSE?\nLSE calculations do not require the errors to be approximately normal. The lm function will find the \\(\\beta\\) s that will minimize the residual sum of squares:\nfit \u0026lt;- falling_object %\u0026gt;% mutate(time_sq = time^2) %\u0026gt;% lm(observed_distance~time+time_sq, data=.) tidy(fit) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 56.1 0.592 94.9 2.23e-17 ## 2 time -0.786 0.845 -0.930 3.72e- 1 ## 3 time_sq -4.53 0.251 -18.1 1.58e- 9 Let’s check if the estimated parabola fits the data. The broom function augment lets us do this easily:\naugment(fit) %\u0026gt;% ggplot() + geom_point(aes(time, observed_distance)) + geom_line(aes(time, .fitted), col = \u0026quot;blue\u0026quot;) Thanks to my high school physics teacher, I know that the equation for the trajectory of a falling object is:\n\\[d = h_0 + v_0 t - 0.5 \\times 9.8 t^2\\]\nwith \\(h_0\\) and \\(v_0\\) the starting height and velocity, respectively. The data we simulated above followed this equation and added measurement error to simulate n observations for dropping the ball \\((v_0=0)\\) from the tower of Pisa \\((h_0=55.86)\\).\nThese are consistent with the parameter estimates:\ntidy(fit, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 56.1 0.592 94.9 2.23e-17 54.8 57.4 ## 2 time -0.786 0.845 -0.930 3.72e- 1 -2.65 1.07 ## 3 time_sq -4.53 0.251 -18.1 1.58e- 9 -5.08 -3.98 The Tower of Pisa height is within the confidence interval for \\(\\beta_0\\), the initial velocity 0 is in the confidence interval for \\(\\beta_1\\) (note the p-value is larger than 0.05), and the acceleration constant is in a confidence interval for \\(-2 \\times \\beta_2\\).\nTRY IT\nSince the 1980s, sabermetricians have used a summary statistic different from batting average to evaluate players. They realized walks were important and that doubles, triples, and HRs, should be weighed more than singles. As a result, they proposed the following metric:\n\\[ \\frac{\\mbox{BB}}{\\mbox{PA}} + \\frac{\\mbox{Singles} + 2 \\mbox{Doubles} + 3 \\mbox{Triples} + 4\\mbox{HR}}{\\mbox{AB}} \\]\nThey called this on-base-percentage plus slugging percentage (OPS). Although the sabermetricians probably did not use regression, here we show how this metric is close to what one gets with regression.\nCompute the OPS for each team in the 2001 season. Then plot Runs per game versus OPS.\n For every year since 1961, compute the correlation between runs per game and OPS; then plot these correlations as a function of year.\n Note that we can rewrite OPS as a weighted average of BBs, singles, doubles, triples, and HRs. We know that the weights for doubles, triples, and HRs are 2, 3, and 4 times that of singles. But what about BB? What is the weight for BB relative to singles? Hint: the weight for BB relative to singles will be a function of AB and PA.\n Note that the weight for BB, \\(\\frac{\\mbox{AB}}{\\mbox{PA}}\\), will change from team to team. To see how variable it is, compute and plot this quantity for each team for each year since 1961. Then plot it again, but instead of computing it for every team, compute and plot the ratio for the entire year. Then, once you are convinced that there is not much of a time or team trend, report the overall average.\n So now we know that the formula for OPS is proportional to \\(0.91 \\times \\mbox{BB} + \\mbox{singles} + 2 \\times \\mbox{doubles} + 3 \\times \\mbox{triples} + 4 \\times \\mbox{HR}\\). Let’s see how these coefficients compare to those obtained with regression. Fit a regression model to the data after 1961, as done earlier: using per game statistics for each year for each team. After fitting this model, report the coefficients as weights relative to the coefficient for singles.\n We see that our linear regression model coefficients follow the same general trend as those used by OPS, but with slightly less weight for metrics other than singles. For each team in years after 1961, compute the OPS, the predicted runs with the regression model and compute the correlation between the two as well as the correlation with runs per game.\n      http://mlb.mlb.com/stats/league_leaders.jsp↩︎\n https://en.wikipedia.org/wiki/Bill_James↩︎\n https://en.wikipedia.org/wiki/Sabermetrics↩︎\n https://en.wikipedia.org/wiki/User:Cburnett↩︎\n https://creativecommons.org/licenses/by-sa/3.0/deed.en↩︎\n https://www.flickr.com/people/27003603@N00↩︎\n https://creativecommons.org/licenses/by-sa/2.0↩︎\n http://www.baseball-almanac.com/awards/lou_brock_award.shtml↩︎\n http://www.foxsports.com/mlb/story/kris-bryant-carlos-correa-rookies-of-year-award-matt-duffy-francisco-lindor-kang-sano-120715↩︎\n   ","date":1602547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603200224,"objectID":"4fb51266c58f677bf1e1454e5cd9c527","permalink":"/content/06-content/","publishdate":"2020-10-13T00:00:00Z","relpermalink":"/content/06-content/","section":"content","summary":"Required Reading  Supplemental Readings Guiding Questions Slides  Linear Models II  Case study: Moneyball  Sabermetics Baseball basics No awards for BB Base on balls or stolen bases? Regression applied to baseball statistics  Confounding  Understanding confounding through stratification Multivariate regression  Least squares estimates  Interpreting linear models Least Squares Estimates (LSE) The lm function LSE are random variables Predicted values are random variables  Exercises Linear regression in the tidyverse  The broom package  Case study: Moneyball (continued)  Adding salary and position information Picking nine players  The regression fallacy Measurement error models    Required Reading  This page.","tags":null,"title":"Linear Regression II","type":"docs"},{"authors":null,"categories":null,"content":"  Required Reading Supplemental Readings Guiding Questions Slides  Introduction to Linear Regression Case study: is height hereditary? The correlation coefficient Sample correlation is a random variable Correlation is not always a useful summary  Conditional expectations The regression line Regression improves precision Bivariate normal distribution (advanced) Variance explained Warning: there are two regression lines     Required Reading  This page.  Supplemental Readings  Coming soon.   Guiding Questions  Coming soon.   Slides Introduction       Today’s lecture will ask you to touch real data during the lecture. Please download the following dataset and load it into R.\n  Ames.csv  This dataset is from houses in Ames, Iowa. (Thrilling!) We will use this dataset during the lecture to illustrate some of the points discussed below.\n  Introduction to Linear Regression Up to this point, this class has focused mainly on single variables. However, in data analytics applications, it is very common to be interested in the relationship between two or more variables. For instance, in the coming days we will use a data-driven approach that examines the relationship between player statistics and success to guide the building of a baseball team with a limited budget. Before delving into this more complex example, we introduce necessary concepts needed to understand regression using a simpler illustration. We actually use the dataset from which regression was born.\nThe example is from genetics. Francis Galton1 studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity. While doing this, he developed the concepts of correlation and regression, as well as a connection to pairs of data that follow a normal distribution. Of course, at the time this data was collected our knowledge of genetics was quite limited compared to what we know today. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height? The technique he developed to answer this question, regression, can also be applied to our baseball question. Regression can be applied in many other circumstances as well.\nHistorical note: Galton made important contributions to statistics and genetics, but he was also one of the first proponents of eugenics, a scientifically flawed philosophical movement favored by many biologists of Galton’s time but with horrific historical consequences. These consequences still reverberate to this day, and form the basis for much of the Western world’s racist policies. You can read more about it here: https://pged.org/history-eugenics-and-genetics/.\nCase study: is height hereditary? We have access to Galton’s family height data through the HistData package. This data contains heights on several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:\nlibrary(tidyverse) library(HistData) data(\u0026quot;GaltonFamilies\u0026quot;) set.seed(1983) galton_heights \u0026lt;- GaltonFamilies %\u0026gt;% filter(gender == \u0026quot;male\u0026quot;) %\u0026gt;% group_by(family) %\u0026gt;% sample_n(1) %\u0026gt;% ungroup() %\u0026gt;% select(father, childHeight) %\u0026gt;% rename(son = childHeight) In the exercises, we will look at other relationships including mothers and daughters.\nSuppose we were asked to summarize the father and son data. Since both distributions are well approximated by the normal distribution, we could use the two averages and two standard deviations as summaries:\ngalton_heights %\u0026gt;% summarize(mean(father), sd(father), mean(son), sd(son)) ## # A tibble: 1 x 4 ## `mean(father)` `sd(father)` `mean(son)` `sd(son)` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 69.1 2.55 69.2 2.71 However, this summary fails to describe an important characteristic of the data: the trend that the taller the father, the taller the son.\ngalton_heights %\u0026gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) We will learn that the correlation coefficient is an informative summary of how two variables move together and then see how this can be used to predict one variable using the other.\n The correlation coefficient The correlation coefficient is defined for a list of pairs \\((x_1, y_1), \\dots, (x_n,y_n)\\) as the average of the product of the standardized values:\n\\[ \\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right) \\] with \\(\\mu_x, \\mu_y\\) the averages of \\(x_1,\\dots, x_n\\) and \\(y_1, \\dots, y_n\\), respectively, and \\(\\sigma_x, \\sigma_y\\) the standard deviations. The Greek letter \\(\\rho\\) is commonly used in statistics books to denote the correlation. The Greek letter for \\(r\\), \\(\\rho\\), because it is the first letter of regression. Soon we learn about the connection between correlation and regression. We can represent the formula above with R code using:\nrho \u0026lt;- mean(scale(x) * scale(y)) To understand why this equation does in fact summarize how two variables move together, consider the \\(i\\)-th entry of \\(x\\) is \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\) SDs away from the average. Similarly, the \\(y_i\\) that is paired with \\(x_i\\), is \\(\\left( \\frac{y_1-\\mu_y}{\\sigma_y} \\right)\\) SDs away from the average \\(y\\). If \\(x\\) and \\(y\\) are unrelated, the product \\(\\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)\\left( \\frac{y_i-\\mu_y}{\\sigma_y} \\right)\\) will be positive ( \\(+ \\times +\\) and \\(- \\times -\\) ) as often as negative (\\(+ \\times -\\) and \\(- \\times +\\)) and will average out to about 0. This correlation is the average and therefore unrelated variables will have 0 correlation. If instead the quantities vary together, then we are averaging mostly positive products ( \\(+ \\times +\\) and \\(- \\times -\\)) and we get a positive correlation. If they vary in opposite directions, we get a negative correlation.\nThe correlation coefficient is always between -1 and 1. We can show this mathematically: consider that we can’t have higher correlation than when we compare a list to itself (perfect correlation) and in this case the correlation is:\n\\[ \\rho = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i-\\mu_x}{\\sigma_x} \\right)^2 = \\frac{1}{\\sigma_x^2} \\frac{1}{n} \\sum_{i=1}^n \\left( x_i-\\mu_x \\right)^2 = \\frac{1}{\\sigma_x^2} \\sigma^2_x = 1 \\]\nA similar derivation, but with \\(x\\) and its exact opposite, proves the correlation has to be bigger or equal to -1.\nFor other pairs, the correlation is in between -1 and 1. The correlation between father and son’s heights is about 0.5:\ngalton_heights %\u0026gt;% summarize(r = cor(father, son)) %\u0026gt;% pull(r) ## [1] 0.4334102 To see what data looks like for different values of \\(\\rho\\), here are six examples of pairs with correlations ranging from -0.9 to 0.99:\nSample correlation is a random variable Before we continue connecting correlation to regression, let’s remind ourselves about random variability.\nIn most data science applications, we observe data that includes random variation. For example, in many cases, we do not observe data for the entire population of interest but rather for a random sample. As with the average and standard deviation, the sample correlation is the most commonly used estimate of the population correlation. This implies that the correlation we compute and use as a summary is a random variable.\nBy way of illustration, let’s assume that the 179 pairs of fathers and sons is our entire population. A less fortunate geneticist can only afford measurements from a random sample of 25 pairs. The sample correlation can be computed with:\nR \u0026lt;- sample_n(galton_heights, 25, replace = TRUE) %\u0026gt;% summarize(r = cor(father, son)) %\u0026gt;% pull(r) R is a random variable. We can run a Monte Carlo simulation to see its distribution:\nB \u0026lt;- 1000 N \u0026lt;- 25 R \u0026lt;- replicate(B, { sample_n(galton_heights, N, replace = TRUE) %\u0026gt;% summarize(r=cor(father, son)) %\u0026gt;% pull(r) }) qplot(R, geom = \u0026quot;histogram\u0026quot;, binwidth = 0.05, color = I(\u0026quot;black\u0026quot;)) We see that the expected value of R is the population correlation:\nmean(R) ## [1] 0.4307393 and that it has a relatively high standard error relative to the range of values R can take:\nsd(R) ## [1] 0.1609393 So, when interpreting correlations, remember that correlations derived from samples are estimates containing uncertainty.\nAlso, note that because the sample correlation is an average of independent draws, the central limit actually applies. Therefore, for large enough \\(N\\), the distribution of R is approximately normal with expected value \\(\\rho\\). The standard deviation, which is somewhat complex to derive, is \\(\\sqrt{\\frac{1-r^2}{N-2}}\\).\nIn our example, \\(N=25\\) does not seem to be large enough to make the approximation a good one:\nggplot(aes(sample=R), data = data.frame(R)) + stat_qq() + geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2))) If you increase \\(N\\), you will see the distribution converging to normal.\n Correlation is not always a useful summary Correlation is not always a good summary of the relationship between two variables. The following four artificial datasets, referred to as Anscombe’s quartet, famously illustrate this point. All these pairs have a correlation of 0.82:\n## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39; Correlation is only meaningful in a particular context. To help us understand when it is that correlation is meaningful as a summary statistic, we will return to the example of predicting a son’s height using his father’s height. This will help motivate and define linear regression. We start by demonstrating how correlation can be useful for prediction.\n  Conditional expectations Suppose we are asked to guess the height of a randomly selected son and we don’t know his father’s height. Because the distribution of sons’ heights is approximately normal, we know the average height, 69.2, is the value with the highest proportion and would be the prediction with the highest chance of minimizing the error. But what if we are told that the father is taller than average, say 72 inches tall, do we still guess 69.2 for the son?\nIt turns out that if we were able to collect data from a very large number of fathers that are 72 inches, the distribution of their sons’ heights would be normally distributed. This implies that the average of the distribution computed on this subset would be our best prediction.\nIn general, we call this approach conditioning. The general idea is that we stratify a population into groups and compute summaries in each group. To provide a mathematical description of conditioning, consider we have a population of pairs of values \\((x_1,y_1),\\dots,(x_n,y_n)\\), for example all father and son heights in England. In the previous week’s content, we learned that if you take a random pair \\((X,Y)\\), the expected value and best predictor of \\(Y\\) is \\(\\mbox{E}(Y) = \\mu_y\\), the population average \\(1/n\\sum_{i=1}^n y_i\\). However, we are no longer interested in the general population, instead we are interested in only the subset of a population with a specific \\(x_i\\) value, 72 inches in our example. This subset of the population, is also a population and thus the same principles and properties we have learned apply. The \\(y_i\\) in the subpopulation have a distribution, referred to as the conditional distribution, and this distribution has an expected value referred to as the conditional expectation. In our example, the conditional expectation is the average height of all sons in England with fathers that are 72 inches. The statistical notation for the conditional expectation is\n\\[ \\mbox{E}(Y \\mid X = x) \\]\nwith \\(x\\) representing the fixed value that defines that subset, for example 72 inches. Similarly, we denote the standard deviation of the strata with\n\\[ \\mbox{SD}(Y \\mid X = x) = \\sqrt{\\mbox{Var}(Y \\mid X = x)} \\]\nBecause the conditional expectation \\(E(Y\\mid X=x)\\) is the best predictor for the random variable \\(Y\\) for an individual in the strata defined by \\(X=x\\), many data science challenges reduce to estimating this quantity. The conditional standard deviation quantifies the precision of the prediction.\nIn the example we have been considering, we are interested in computing the average son height conditioned on the father being 72 inches tall. We want to estimate \\(E(Y|X=72)\\) using the sample collected by Galton. We previously learned that the sample average is the preferred approach to estimating the population average. However, a challenge when using this approach to estimating conditional expectations is that for continuous data we don’t have many data points matching exactly one value in our sample. For example, we have only:\nsum(galton_heights$father == 72) ## [1] 8 fathers that are exactly 72-inches. If we change the number to 72.5, we get even fewer data points:\nsum(galton_heights$father == 72.5) ## [1] 1 A practical way to improve these estimates of the conditional expectations, is to define strata of with similar values of \\(x\\). In our example, we can round father heights to the nearest inch and assume that they are all 72 inches. If we do this, we end up with the following prediction for the son of a father that is 72 inches tall:\nconditional_avg \u0026lt;- galton_heights %\u0026gt;% filter(round(father) == 72) %\u0026gt;% summarize(avg = mean(son)) %\u0026gt;% pull(avg) conditional_avg ## [1] 70.5 Note that a 72-inch father is taller than average – specifically, 72 - 69.1/2.5 = 1.1 standard deviations taller than the average father. Our prediction 70.5 is also taller than average, but only 0.49 standard deviations larger than the average son. The sons of 72-inch fathers have regressed some to the average height. We notice that the reduction in how many SDs taller is about 0.5, which happens to be the correlation. As we will see in a later lecture, this is not a coincidence.\nIf we want to make a prediction of any height, not just 72, we could apply the same approach to each strata. Stratification followed by boxplots lets us see the distribution of each group:\ngalton_heights %\u0026gt;% mutate(father_strata = factor(round(father))) %\u0026gt;% ggplot(aes(father_strata, son)) + geom_boxplot() + geom_point() Not surprisingly, the centers of the groups are increasing with height. Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables with standard errors, the data is consistent with these points following a straight line:\n## `summarise()` ungrouping output (override with `.groups` argument) The fact that these conditional averages follow a line is not a coincidence. In the next section, we explain that the line these averages follow is what we call the regression line, which improves the precision of our estimates. However, it is not always appropriate to estimate conditional expectations with the regression line so we also describe Galton’s theoretical justification for using the regression line.\n The regression line If we are predicting a random variable \\(Y\\) knowing the value of another \\(X=x\\) using a regression line, then we predict that for every standard deviation, \\(\\sigma_X\\), that \\(x\\) increases above the average \\(\\mu_X\\), \\(Y\\) increase \\(\\rho\\) standard deviations \\(\\sigma_Y\\) above the average \\(\\mu_Y\\) with \\(\\rho\\) the correlation between \\(X\\) and \\(Y\\). The formula for the regression is therefore:\n\\[ \\left( \\frac{Y-\\mu_Y}{\\sigma_Y} \\right) = \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\]\nWe can rewrite it like this:\n\\[ Y = \\mu_Y + \\rho \\left( \\frac{x-\\mu_X}{\\sigma_X} \\right) \\sigma_Y \\]\nIf there is perfect correlation, the regression line predicts an increase that is the same number of SDs. If there is 0 correlation, then we don’t use \\(x\\) at all for the prediction and simply predict the average \\(\\mu_Y\\). For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.\nNote that if the correlation is positive and lower than 1, our prediction is closer, in standard units, to the average height than the value used to predict, \\(x\\), is to the average of the \\(x\\)s. This is why we call it regression: the son regresses to the average height. In fact, the title of Galton’s paper was: Regression toward mediocrity in hereditary stature. To add regression lines to plots, we will need the above formula in the form:\n\\[ y= b + mx \\mbox{ with slope } m = \\rho \\frac{\\sigma_y}{\\sigma_x} \\mbox{ and intercept } b=\\mu_y - m \\mu_x \\]\nHere we add the regression line to the original data:\nmu_x \u0026lt;- mean(galton_heights$father) mu_y \u0026lt;- mean(galton_heights$son) s_x \u0026lt;- sd(galton_heights$father) s_y \u0026lt;- sd(galton_heights$son) r \u0026lt;- cor(galton_heights$father, galton_heights$son) galton_heights %\u0026gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) + geom_abline(slope = r * s_y/s_x, intercept = mu_y - r * s_y/s_x * mu_x) The regression formula implies that if we first standardize the variables, that is subtract the average and divide by the standard deviation, then the regression line has intercept 0 and slope equal to the correlation \\(\\rho\\). You can make same plot, but using standard units like this:\ngalton_heights %\u0026gt;% ggplot(aes(scale(father), scale(son))) + geom_point(alpha = 0.5) + geom_abline(intercept = 0, slope = r) Regression improves precision Let’s compare the two approaches to prediction that we have presented:\nRound fathers’ heights to closest inch, stratify, and then take the average. Compute the regression line and use it to predict.  We use a Monte Carlo simulation sampling \\(N=50\\) families:\nB \u0026lt;- 1000 N \u0026lt;- 50 set.seed(1983) conditional_avg \u0026lt;- replicate(B, { dat \u0026lt;- sample_n(galton_heights, N) dat %\u0026gt;% filter(round(father) == 72) %\u0026gt;% summarize(avg = mean(son)) %\u0026gt;% pull(avg) }) regression_prediction \u0026lt;- replicate(B, { dat \u0026lt;- sample_n(galton_heights, N) mu_x \u0026lt;- mean(dat$father) mu_y \u0026lt;- mean(dat$son) s_x \u0026lt;- sd(dat$father) s_y \u0026lt;- sd(dat$son) r \u0026lt;- cor(dat$father, dat$son) mu_y + r*(72 - mu_x)/s_x*s_y }) Although the expected value of these two random variables is about the same:\nmean(conditional_avg, na.rm = TRUE) ## [1] 70.49368 mean(regression_prediction) ## [1] 70.50941 The standard error for the regression prediction is substantially smaller:\nsd(conditional_avg, na.rm = TRUE) ## [1] 0.9635814 sd(regression_prediction) ## [1] 0.4520833 The regression line is therefore much more stable than the conditional mean. There is an intuitive reason for this. The conditional average is computed on a relatively small subset: the fathers that are about 72 inches tall. In fact, in some of the permutations we have no data, which is why we use na.rm=TRUE. The regression always uses all the data.\nSo why not always use the regression for prediction? Because it is not always appropriate. For example, Anscombe provided cases for which the data does not have a linear relationship. So are we justified in using the regression line to predict? Galton answered this in the positive for height data. The justification, which we include in the next section, is somewhat more advanced than the rest of the chapter.\n Bivariate normal distribution (advanced) Correlation and the regression slope are a widely used summary statistic, but they are often misused or misinterpreted. Anscombe’s examples provide over-simplified cases of dataset in which summarizing with correlation would be a mistake. But there are many more real-life examples.\nThe main way we motivate the use of correlation involves what is called the bivariate normal distribution.\nWhen a pair of random variables is approximated by the bivariate normal distribution, scatterplots look like ovals. These ovals can be thin (high correlation) or circle-shaped (no correlation).\n-- A more technical way to define the bivariate normal distribution is the following: if \\(X\\) is a normally distributed random variable, \\(Y\\) is also a normally distributed random variable, and the conditional distribution of \\(Y\\) for any \\(X=x\\) is approximately normal, then the pair is approximately bivariate normal.\nIf we think the height data is well approximated by the bivariate normal distribution, then we should see the normal approximation hold for each strata. Here we stratify the son heights by the standardized father heights and see that the assumption appears to hold:\ngalton_heights %\u0026gt;% mutate(z_father = round((father - mean(father)) / sd(father))) %\u0026gt;% filter(z_father %in% -2:2) %\u0026gt;% ggplot() + stat_qq(aes(sample = son)) + facet_wrap( ~ z_father) Now we come back to defining correlation. Galton used mathematical statistics to demonstrate that, when two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations. We don’t show the derivation here, but we can show that under this assumption, for any given value of \\(x\\), the expected value of the \\(Y\\) in pairs for which \\(X=x\\) is:\n\\[ \\mbox{E}(Y | X=x) = \\mu_Y + \\rho \\frac{X-\\mu_X}{\\sigma_X}\\sigma_Y \\]\nThis is the regression line, with slope \\[\\rho \\frac{\\sigma_Y}{\\sigma_X}\\] and intercept \\(\\mu_y - m\\mu_X\\). It is equivalent to the regression equation we showed earlier which can be written like this:\n\\[ \\frac{\\mbox{E}(Y \\mid X=x) - \\mu_Y}{\\sigma_Y} = \\rho \\frac{x-\\mu_X}{\\sigma_X} \\]\nThis implies that, if our data is approximately bivariate, the regression line gives the conditional probability. Therefore, we can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to predict.\nIn summary, if our data is approximately bivariate, then the conditional expectation, the best prediction of \\(Y\\) given we know the value of \\(X\\), is given by the regression line.\n Variance explained The bivariate normal theory also tells us that the standard deviation of the conditional distribution described above is:\n\\[ \\mbox{SD}(Y \\mid X=x ) = \\sigma_Y \\sqrt{1-\\rho^2} \\]\nTo see why this is intuitive, notice that without conditioning, \\(\\mbox{SD}(Y) = \\sigma_Y\\), we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with a tall, 72-inch, father. This group will all tend to be somewhat tall so the standard deviation is reduced.\nSpecifically, it is reduced to \\(\\sqrt{1-\\rho^2} = \\sqrt{1 - 0.25}\\) = 0.87 of what it was originally. We could say that father heights “explain” 13% of the variability observed in son heights.\nThe statement “\\(X\\) explains such and such percent of the variability” is commonly used in academic papers. In this case, this percent actually refers to the variance (the SD squared). So if the data is bivariate normal, the variance is reduced by \\(1-\\rho^2\\), so we say that \\(X\\) explains \\(1- (1-\\rho^2)=\\rho^2\\) (the correlation squared) of the variance.\nBut it is important to remember that the “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.\n Warning: there are two regression lines We computed a regression line to predict the son’s height from father’s height. We used these calculations:\nmu_x \u0026lt;- mean(galton_heights$father) mu_y \u0026lt;- mean(galton_heights$son) s_x \u0026lt;- sd(galton_heights$father) s_y \u0026lt;- sd(galton_heights$son) r \u0026lt;- cor(galton_heights$father, galton_heights$son) m_1 \u0026lt;- r * s_y / s_x b_1 \u0026lt;- mu_y - m_1*mu_x which gives us the function \\(\\mbox{E}(Y\\mid X=x) =\\) 37.3 + 0.46 \\(x\\).\nWhat if we want to predict the father’s height based on the son’s? It is important to know that this is not determined by computing the inverse function: \\(x = \\{ \\mbox{E}(Y\\mid X=x) -\\) 37.3 \\(\\} /\\) 0.5.\nWe need to compute \\(\\mbox{E}(X \\mid Y=y)\\). Since the data is approximately bivariate normal, the theory described above tells us that this conditional expectation will follow a line with slope and intercept:\nm_2 \u0026lt;- r * s_x / s_y b_2 \u0026lt;- mu_x - m_2 * mu_y So we get \\(\\mbox{E}(X \\mid Y=y) =\\) 40.9 + 0.41y. Again we see regression to the average: the prediction for the father is closer to the father average than the son heights \\(y\\) is to the son average.\nHere is a plot showing the two regression lines, with blue for the predicting son heights with father heights and red for predicting father heights with son heights:\ngalton_heights %\u0026gt;% ggplot(aes(father, son)) + geom_point(alpha = 0.5) + geom_abline(intercept = b_1, slope = m_1, col = \u0026quot;blue\u0026quot;) + geom_abline(intercept = -b_2/m_2, slope = 1/m_2, col = \u0026quot;red\u0026quot;) TRY IT\nLoad the GaltonFamilies data from the HistData. The children in each family are listed by gender and then by height. Create a dataset called galton_heights by picking a male and female at random.\n Make a scatterplot for heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.\n Compute the correlation in heights between mothers and daughters, mothers and sons, fathers and daughters, and fathers and sons.\n       https://en.wikipedia.org/wiki/Francis_Galton↩︎\n   ","date":1601942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601908673,"objectID":"744a9e81e6eae570052294b144f8d401","permalink":"/content/05-content/","publishdate":"2020-10-06T00:00:00Z","relpermalink":"/content/05-content/","section":"content","summary":"Required Reading Supplemental Readings Guiding Questions Slides  Introduction to Linear Regression Case study: is height hereditary? The correlation coefficient Sample correlation is a random variable Correlation is not always a useful summary  Conditional expectations The regression line Regression improves precision Bivariate normal distribution (advanced) Variance explained Warning: there are two regression lines     Required Reading  This page.  Supplemental Readings  Coming soon.","tags":null,"title":"Introduction to Regression","type":"docs"},{"authors":null,"categories":null,"content":"  Required Reading Supplemental Readings Guiding Questions Slides  Discrete probability Relative frequency Notation Probability distributions Monte Carlo simulations for categorical data Setting the random seed With and without replacement  Independence Conditional probabilities Addition and multiplication rules Multiplication rule Multiplication rule under independence Addition rule  Combinations and permutations Monte Carlo example  Examples Monty Hall problem Birthday problem  Infinity in practice Theoretical continuous distributions Theoretical distributions as approximations The probability density  Monte Carlo simulations for continuous variables Continuous distributions  Random variables Definition of Random variables Sampling models The probability distribution of a random variable Distributions versus probability distributions Notation for random variables The expected value and standard error Population SD versus the sample SD  Central Limit Theorem How large is large in the Central Limit Theorem?  Statistical properties of averages Law of large numbers Misinterpreting law of averages     Required Reading  This page.  Supplemental Readings   Why It’s So Hard for Us to Visualize Uncertainty  Amanda Cox’s keynote address at the 2017 OpenVis Conf  Communicating Uncertainty When Lives Are on the Line  Showing uncertainty during the live election forecast \u0026amp; Trolling the uncertainty dial   Guiding Questions  Why is uncertainty inherently a major part of data analytics? How have past attempts to visualize uncertainty failed? What is the right way to visualize election uncertainty?   Slides As with last week, today’s lecture will ask you to work with R during the lecture.\n  Discrete probability We start by covering some basic principles related to categorical data. The subset of probability is referred to as discrete probability. It will help us understand the probability theory we will later introduce for numeric and continuous data, which is much more common in data science applications. Discrete probability is more useful in card games and therefore we use these as examples.\nRelative frequency The word probability is used in everyday language. Answering questions about probability is often hard, if not impossible. Here we discuss a mathematical definition of probability that does permit us to give precise answers to certain questions.\nFor example, if I have 2 red beads and 3 blue beads inside an urn1 (most probability books use this archaic term, so we do too) and I pick one at random, what is the probability of picking a red one? Our intuition tells us that the answer is 2/5 or 40%. A precise definition can be given by noting that there are five possible outcomes of which two satisfy the condition necessary for the event “pick a red bead”. Since each of the five outcomes has the same chance of occurring, we conclude that the probability is .4 for red and .6 for blue.\nA more tangible way to think about the probability of an event is as the proportion of times the event occurs when we repeat the experiment an infinite number of times, independently, and under the same conditions.\n Notation We use the notation \\(\\mbox{Pr}(A)\\) to denote the probability of event \\(A\\) happening. We use the very general term event to refer to things that can happen when something occurs by chance. In our previous example, the event was “picking a red bead”. In a political poll in which we call 100 likely voters at random, an example of an event is “calling 48 Democrats and 52 Republicans”.\nIn data science applications, we will often deal with continuous variables. These events will often be things like “is this person taller than 6 feet”. In this case, we write events in a more mathematical form: \\(X \\geq 6\\). We will see more of these examples later. Here we focus on categorical data.\n Probability distributions If we know the relative frequency of the different categories, defining a distribution for categorical outcomes is relatively straightforward. We simply assign a probability to each category. In cases that can be thought of as beads in an urn, for each bead type, their proportion defines the distribution.\nIf we are randomly calling likely voters from a population that is 44% Democrat, 44% Republican, 10% undecided, and 2% Green Party, these proportions define the probability for each group. The probability distribution is:\n  Pr(picking a Republican) = 0.44  Pr(picking a Democrat) = 0.44  Pr(picking an undecided) = 0.10  Pr(picking a Green) = 0.02     Monte Carlo simulations for categorical data Computers provide a way to actually perform the simple random experiment described above: pick a bead at random from a bag that contains three blue beads and two red ones. Random number generators permit us to mimic the process of picking at random.\nAn example is the sample function in R. We demonstrate its use in the code below. First, we use the function rep to generate the urn:\nbeads \u0026lt;- rep(c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), times = c(2,3)) beads ## [1] \u0026quot;red\u0026quot; \u0026quot;red\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;blue\u0026quot; and then use sample to pick a bead at random:\nsample(beads, 1) ## [1] \u0026quot;blue\u0026quot; This line of code produces one random outcome. We want to repeat this experiment an infinite number of times, but it is impossible to repeat forever. Instead, we repeat the experiment a large enough number of times to make the results practically equivalent to repeating forever. This is an example of a Monte Carlo simulation.\nMuch of what mathematical and theoretical statisticians study, which we do not cover in this class, relates to providing rigorous definitions of “practically equivalent” as well as studying how close a large number of experiments gets us to what happens in the limit. Later in this lecture, we provide a practical approach to deciding what is “large enough”.\nTo perform our first Monte Carlo simulation, we use the replicate function, which permits us to repeat the same task any number of times. Here, we repeat the random event \\(B =\\) 10,000 times:\nB \u0026lt;- 10000 events \u0026lt;- replicate(B, sample(beads, 1)) We can now see if our definition actually is in agreement with this Monte Carlo simulation approximation. We can use table to see the distribution:\ntab \u0026lt;- table(events) tab ## events ## blue red ## 5952 4048 and prop.table gives us the proportions:\nprop.table(tab) ## events ## blue red ## 0.5952 0.4048 The numbers above are the estimated probabilities provided by this Monte Carlo simulation. Statistical theory, not covered here, tells us that as \\(B\\) gets larger, the estimates get closer to 3/5=.6 and 2/5=.4.\nAlthough this is a simple and not very useful example, we will use Monte Carlo simulations to estimate probabilities in cases in which it is harder to compute the exact ones. Before delving into more complex examples, we use simple ones to demonstrate the computing tools available in R.\nSetting the random seed Before we continue, we will briefly explain the following important line of code:\nset.seed(1986) Throughout this class, we use random number generators. This implies that many of the results presented can actually change by chance, which then suggests that a frozen version of the class may show a different result than what you obtain when you try to code as shown in the class. This is actually fine since the results are random and change from time to time. However, if you want to ensure that results are exactly the same every time you run them, you can set R’s random number generation seed to a specific number. Above we set it to 1986. We want to avoid using the same seed everytime. A popular way to pick the seed is the year - month - day. For example, we picked 1986 on December 20, 2018: \\(2018 - 12 - 20 = 1986\\).\nYou can learn more about setting the seed by looking at the documentation:\n?set.seed In the exercises, we may ask you to set the seed to assure that the results you obtain are exactly what we expect them to be.\n With and without replacement The function sample has an argument that permits us to pick more than one element from the urn. However, by default, this selection occurs without replacement: after a bead is selected, it is not put back in the bag. Notice what happens when we ask to randomly select five beads:\nsample(beads, 5) ## [1] \u0026quot;red\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;red\u0026quot; sample(beads, 5) ## [1] \u0026quot;red\u0026quot; \u0026quot;red\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;blue\u0026quot; sample(beads, 5) ## [1] \u0026quot;blue\u0026quot; \u0026quot;red\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;red\u0026quot; \u0026quot;blue\u0026quot; This results in rearrangements that always have three blue and two red beads. If we ask that six beads be selected, we get an error:\nsample(beads, 6) Error in sample.int(length(x), size, replace, prob) : cannot take a sample larger than the population when 'replace = FALSE'\nHowever, the sample function can be used directly, without the use of replicate, to repeat the same experiment of picking 1 out of the 5 beads, continually, under the same conditions. To do this, we sample with replacement: return the bead back to the urn after selecting it. We can tell sample to do this by changing the replace argument, which defaults to FALSE, to replace = TRUE:\nevents \u0026lt;- sample(beads, B, replace = TRUE) prop.table(table(events)) ## events ## blue red ## 0.6017 0.3983 Not surprisingly, we get results very similar to those previously obtained with replicate.\n  Independence We say two events are independent if the outcome of one does not affect the other. The classic example is coin tosses. Every time we toss a fair coin, the probability of seeing heads is 1/2 regardless of what previous tosses have revealed. The same is true when we pick beads from an urn with replacement. In the example above, the probability of red is 0.40 regardless of previous draws.\nMany examples of events that are not independent come from card games. When we deal the first card, the probability of getting a King is 1/13 since there are thirteen possibilities: Ace, Deuce, Three, \\(\\dots\\), Ten, Jack, Queen, King, and Ace. Now if we deal a King for the first card, and don’t replace it into the deck, the probabilities of a second card being a King is less because there are only three Kings left: the probability is 3 out of 51. These events are therefore not independent: the first outcome affected the next one.\nTo see an extreme case of non-independent events, consider our example of drawing five beads at random without replacement:\nx \u0026lt;- sample(beads, 5) If you have to guess the color of the first bead, you will predict blue since blue has a 60% chance. But if I show you the result of the last four outcomes:\nx[2:5] ## [1] \u0026quot;blue\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;blue\u0026quot; \u0026quot;red\u0026quot; would you still guess blue? Of course not. Now you know that the probability of red is 1 since the only bead left is red. The events are not independent, so the probabilities change.\n Conditional probabilities When events are not independent, conditional probabilities are useful. We already saw an example of a conditional probability: we computed the probability that a second dealt card is a King given that the first was a King. In probability, we use the following notation:\n\\[ \\mbox{Pr}(\\mbox{Card 2 is a king} \\mid \\mbox{Card 1 is a king}) = 3/51 \\]\nWe use the \\(\\mid\\) as shorthand for “given that” or “conditional on”.\nWhen two events, say \\(A\\) and \\(B\\), are independent, we have:\n\\[ \\mbox{Pr}(A \\mid B) = \\mbox{Pr}(A) \\]\nThis is the mathematical way of saying: the fact that \\(B\\) happened does not affect the probability of \\(A\\) happening. In fact, this can be considered the mathematical definition of independence.\n Addition and multiplication rules Multiplication rule If we want to know the probability of two events, say \\(A\\) and \\(B\\), occurring, we can use the multiplication rule:\n\\[ \\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A) \\] Let’s use Blackjack as an example. In Blackjack, you are assigned two random cards. After you see what you have, you can ask for more. The goal is to get closer to 21 than the dealer, without going over. Face cards are worth 10 points and Aces are worth 11 or 1 (you choose).\nSo, in a Blackjack game, to calculate the chances of getting a 21 by drawing an Ace and then a face card, we compute the probability of the first being an Ace and multiply by the probability of drawing a face card or a 10 given that the first was an Ace: \\(1/13 \\times 16/51 \\approx 0.025\\)\nThe multiplication rule also applies to more than two events. We can use induction to expand for more events:\n\\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B \\mid A)\\mbox{Pr}(C \\mid A \\mbox{ and } B) \\]\n Multiplication rule under independence When we have independent events, then the multiplication rule becomes simpler:\n\\[ \\mbox{Pr}(A \\mbox{ and } B \\mbox{ and } C) = \\mbox{Pr}(A)\\mbox{Pr}(B)\\mbox{Pr}(C) \\]\nBut we have to be very careful before using this since assuming independence can result in very different and incorrect probability calculations when we don’t actually have independence.\nAs an example, imagine a court case in which the suspect was described as having a mustache and a beard. The defendant has a mustache and a beard and the prosecution brings in an “expert” to testify that 1/10 men have beards and 1/5 have mustaches, so using the multiplication rule we conclude that only \\(1/10 \\times 1/5\\) or 0.02 have both.\nBut to multiply like this we need to assume independence! Say the conditional probability of a man having a mustache conditional on him having a beard is .95. So the correct calculation probability is much higher: \\(1/10 \\times 95/100 = 0.095\\).\nThe multiplication rule also gives us a general formula for computing conditional probabilities:\n\\[ \\mbox{Pr}(B \\mid A) = \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\]\nTo illustrate how we use these formulas and concepts in practice, we will use several examples related to card games.\n Addition rule The addition rule tells us that:\n\\[ \\mbox{Pr}(A \\mbox{ or } B) = \\mbox{Pr}(A) + \\mbox{Pr}(B) - \\mbox{Pr}(A \\mbox{ and } B) \\]\nThis rule is intuitive: think of a Venn diagram. If we simply add the probabilities, we count the intersection twice so we need to substract one instance.\n  Combinations and permutations In our very first example, we imagined an urn with five beads. As a reminder, to compute the probability distribution of one draw, we simply listed out all the possibilities. There were 5 and so then, for each event, we counted how many of these possibilities were associated with the event. The resulting probability of choosing a blue bead is 3/5 because out of the five possible outcomes, three were blue.\nFor more complicated cases, the computations are not as straightforward. For instance, what is the probability that if I draw five cards without replacement, I get all cards of the same suit, what is known as a “flush” in poker? In a discrete probability course you learn theory on how to make these computations. Here we focus on how to use R code to compute the answers.\nFirst, let’s construct a deck of cards. For this, we will use the expand.grid and paste functions. We use paste to create strings by joining smaller strings. To do this, we take the number and suit of a card and create the card name like this:\nnumber \u0026lt;- \u0026quot;Three\u0026quot; suit \u0026lt;- \u0026quot;Hearts\u0026quot; paste(number, suit) ## [1] \u0026quot;Three Hearts\u0026quot; paste also works on pairs of vectors performing the operation element-wise:\npaste(letters[1:5], as.character(1:5)) ## [1] \u0026quot;a 1\u0026quot; \u0026quot;b 2\u0026quot; \u0026quot;c 3\u0026quot; \u0026quot;d 4\u0026quot; \u0026quot;e 5\u0026quot; The function expand.grid gives us all the combinations of entries of two vectors. For example, if you have blue and black pants and white, grey, and plaid shirts, all your combinations are:\nexpand.grid(pants = c(\u0026quot;blue\u0026quot;, \u0026quot;black\u0026quot;), shirt = c(\u0026quot;white\u0026quot;, \u0026quot;grey\u0026quot;, \u0026quot;plaid\u0026quot;)) ## pants shirt ## 1 blue white ## 2 black white ## 3 blue grey ## 4 black grey ## 5 blue plaid ## 6 black plaid Here is how we generate a deck of cards:\nsuits \u0026lt;- c(\u0026quot;Diamonds\u0026quot;, \u0026quot;Clubs\u0026quot;, \u0026quot;Hearts\u0026quot;, \u0026quot;Spades\u0026quot;) numbers \u0026lt;- c(\u0026quot;Ace\u0026quot;, \u0026quot;Deuce\u0026quot;, \u0026quot;Three\u0026quot;, \u0026quot;Four\u0026quot;, \u0026quot;Five\u0026quot;, \u0026quot;Six\u0026quot;, \u0026quot;Seven\u0026quot;, \u0026quot;Eight\u0026quot;, \u0026quot;Nine\u0026quot;, \u0026quot;Ten\u0026quot;, \u0026quot;Jack\u0026quot;, \u0026quot;Queen\u0026quot;, \u0026quot;King\u0026quot;) deck \u0026lt;- expand.grid(number=numbers, suit=suits) deck \u0026lt;- paste(deck$number, deck$suit) With the deck constructed, we can double check that the probability of a King in the first card is 1/13 by computing the proportion of possible outcomes that satisfy our condition:\nkings \u0026lt;- paste(\u0026quot;King\u0026quot;, suits) mean(deck %in% kings) ## [1] 0.07692308 Now, how about the conditional probability of the second card being a King given that the first was a King? Earlier, we deduced that if one King is already out of the deck and there are 51 left, then this probability is 3/51. Let’s confirm by listing out all possible outcomes.\nTo do this, we can use the permutations function from the gtools package. For any list of size n, this function computes all the different combinations we can get when we select r items. Here are all the ways we can choose two numbers from a list consisting of 1,2,3:\nlibrary(gtools) permutations(3, 2) ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 3 ## [3,] 2 1 ## [4,] 2 3 ## [5,] 3 1 ## [6,] 3 2 Notice that the order matters here: 3,1 is different than 1,3. Also, note that (1,1), (2,2), and (3,3) do not appear because once we pick a number, it can’t appear again.\nOptionally, we can add a vector. If you want to see five random seven digit phone numbers out of all possible phone numbers (without repeats), you can type:\nall_phone_numbers \u0026lt;- permutations(10, 7, v = 0:9) n \u0026lt;- nrow(all_phone_numbers) index \u0026lt;- sample(n, 5) all_phone_numbers[index,] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 1 3 8 0 6 7 5 ## [2,] 2 9 1 6 4 8 0 ## [3,] 5 1 6 0 9 8 2 ## [4,] 7 4 6 0 2 8 1 ## [5,] 4 6 5 9 2 8 0 Instead of using the numbers 1 through 10, the default, it uses what we provided through v: the digits 0 through 9.\nTo compute all possible ways we can choose two cards when the order matters, we type:\nhands \u0026lt;- permutations(52, 2, v = deck) This is a matrix with two columns and 2652 rows. With a matrix we can get the first and second cards like this:\nfirst_card \u0026lt;- hands[,1] second_card \u0026lt;- hands[,2] Now the cases for which the first hand was a King can be computed like this:\nkings \u0026lt;- paste(\u0026quot;King\u0026quot;, suits) sum(first_card %in% kings) ## [1] 204 To get the conditional probability, we compute what fraction of these have a King in the second card:\nsum(first_card%in%kings \u0026amp; second_card%in%kings) / sum(first_card%in%kings) ## [1] 0.05882353 which is exactly 3/51, as we had already deduced. Notice that the code above is equivalent to:\nmean(first_card%in%kings \u0026amp; second_card%in%kings) / mean(first_card%in%kings) ## [1] 0.05882353 which uses mean instead of sum and is an R version of:\n\\[ \\frac{\\mbox{Pr}(A \\mbox{ and } B)}{ \\mbox{Pr}(A)} \\]\nHow about if the order doesn’t matter? For example, in Blackjack if you get an Ace and a face card in the first draw, it is called a Natural 21 and you win automatically. If we wanted to compute the probability of this happening, we would enumerate the combinations, not the permutations, since the order does not matter.\ncombinations(3,2) ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 3 ## [3,] 2 3 In the second line, the outcome does not include (2,1) because (1,2) already was enumerated. The same applies to (3,1) and (3,2).\nSo to compute the probability of a Natural 21 in Blackjack, we can do this:\naces \u0026lt;- paste(\u0026quot;Ace\u0026quot;, suits) facecard \u0026lt;- c(\u0026quot;King\u0026quot;, \u0026quot;Queen\u0026quot;, \u0026quot;Jack\u0026quot;, \u0026quot;Ten\u0026quot;) facecard \u0026lt;- expand.grid(number = facecard, suit = suits) facecard \u0026lt;- paste(facecard$number, facecard$suit) hands \u0026lt;- combinations(52, 2, v = deck) mean(hands[,1] %in% aces \u0026amp; hands[,2] %in% facecard) ## [1] 0.04826546 In the last line, we assume the Ace comes first. This is only because we know the way combination enumerates possibilities and it will list this case first. But to be safe, we could have written this and produced the same answer:\nmean((hands[,1] %in% aces \u0026amp; hands[,2] %in% facecard) | (hands[,2] %in% aces \u0026amp; hands[,1] %in% facecard)) ## [1] 0.04826546 Monte Carlo example Instead of using combinations to deduce the exact probability of a Natural 21, we can use a Monte Carlo to estimate this probability. In this case, we draw two cards over and over and keep track of how many 21s we get. We can use the function sample to draw two cards without replacements:\nhand \u0026lt;- sample(deck, 2) hand ## [1] \u0026quot;Queen Clubs\u0026quot; \u0026quot;Seven Spades\u0026quot; And then check if one card is an Ace and the other a face card or a 10. Going forward, we include 10 when we say face card. Now we need to check both possibilities:\n(hands[1] %in% aces \u0026amp; hands[2] %in% facecard) | (hands[2] %in% aces \u0026amp; hands[1] %in% facecard) ## [1] FALSE If we repeat this 10,000 times, we get a very good approximation of the probability of a Natural 21.\nLet’s start by writing a function that draws a hand and returns TRUE if we get a 21. The function does not need any arguments because it uses objects defined in the global environment.\nblackjack \u0026lt;- function(){ hand \u0026lt;- sample(deck, 2) (hand[1] %in% aces \u0026amp; hand[2] %in% facecard) | (hand[2] %in% aces \u0026amp; hand[1] %in% facecard) } Here we do have to check both possibilities: Ace first or Ace second because we are not using the combinations function. The function returns TRUE if we get a 21 and FALSE otherwise:\nblackjack() ## [1] FALSE Now we can play this game, say, 10,000 times:\nB \u0026lt;- 10000 results \u0026lt;- replicate(B, blackjack()) mean(results) ## [1] 0.0475   Examples In this section, we describe two discrete probability popular examples: the Monty Hall problem and the birthday problem. We use R to help illustrate the mathematical concepts.\nMonty Hall problem In the 1970s, there was a game show called “Let’s Make a Deal” and Monty Hall was the host. At some point in the game, contestants were asked to pick one of three doors. Behind one door there was a prize. The other doors had a goat behind them to show the contestant they had lost. After the contestant picked a door, before revealing whether the chosen door contained a prize, Monty Hall would open one of the two remaining doors and show the contestant there was no prize behind that door. Then he would ask “Do you want to switch doors?” What would you do?\nWe can use probability to show that if you stick with the original door choice, your chances of winning a prize remain 1 in 3. However, if you switch to the other door, your chances of winning double to 2 in 3! This seems counterintuitive. Many people incorrectly think both chances are 1 in 2 since you are choosing between 2 options. You can watch a detailed mathematical explanation on Khan Academy2 or read one on Wikipedia3. Below we use a Monte Carlo simulation to see which strategy is better. Note that this code is written longer than it should be for pedagogical purposes.\nLet’s start with the stick strategy:\nB \u0026lt;- 10000 monty_hall \u0026lt;- function(strategy){ doors \u0026lt;- as.character(1:3) prize \u0026lt;- sample(c(\u0026quot;car\u0026quot;, \u0026quot;goat\u0026quot;, \u0026quot;goat\u0026quot;)) prize_door \u0026lt;- doors[prize == \u0026quot;car\u0026quot;] my_pick \u0026lt;- sample(doors, 1) show \u0026lt;- sample(doors[!doors %in% c(my_pick, prize_door)],1) stick \u0026lt;- my_pick stick == prize_door switch \u0026lt;- doors[!doors%in%c(my_pick, show)] choice \u0026lt;- ifelse(strategy == \u0026quot;stick\u0026quot;, stick, switch) choice == prize_door } stick \u0026lt;- replicate(B, monty_hall(\u0026quot;stick\u0026quot;)) mean(stick) ## [1] 0.3416 switch \u0026lt;- replicate(B, monty_hall(\u0026quot;switch\u0026quot;)) mean(switch) ## [1] 0.6682 As we write the code, we note that the lines starting with my_pick and show have no influence on the last logical operation when we stick to our original choice anyway. From this we should realize that the chance is 1 in 3, what we began with. When we switch, the Monte Carlo estimate confirms the 2/3 calculation. This helps us gain some insight by showing that we are removing a door, show, that is definitely not a winner from our choices. We also see that unless we get it right when we first pick, you win: 1 - 1/3 = 2/3.\n Birthday problem Suppose you are in a classroom with 50 people. If we assume this is a randomly selected group of 50 people, what is the chance that at least two people have the same birthday? Although it is somewhat advanced, we can deduce this mathematically. We will do this later. Here we use a Monte Carlo simulation. For simplicity, we assume nobody was born on February 29. This actually doesn’t change the answer much.\nFirst, note that birthdays can be represented as numbers between 1 and 365, so a sample of 50 birthdays can be obtained like this:\nn \u0026lt;- 50 bdays \u0026lt;- sample(1:365, n, replace = TRUE) To check if in this particular set of 50 people we have at least two with the same birthday, we can use the function duplicated, which returns TRUE whenever an element of a vector is a duplicate. Here is an example:\nduplicated(c(1,2,3,1,4,3,5)) ## [1] FALSE FALSE FALSE TRUE FALSE TRUE FALSE The second time 1 and 3 appear, we get a TRUE. So to check if two birthdays were the same, we simply use the any and duplicated functions like this:\nany(duplicated(bdays)) ## [1] TRUE In this case, we see that it did happen. At least two people had the same birthday.\nTo estimate the probability of a shared birthday in the group, we repeat this experiment by sampling sets of 50 birthdays over and over:\nB \u0026lt;- 10000 same_birthday \u0026lt;- function(n){ bdays \u0026lt;- sample(1:365, n, replace=TRUE) any(duplicated(bdays)) } results \u0026lt;- replicate(B, same_birthday(50)) mean(results) ## [1] 0.9691 Were you expecting the probability to be this high?\nPeople tend to underestimate these probabilities. To get an intuition as to why it is so high, think about what happens when the group size is close to 365. At this stage, we run out of days and the probability is one.\nSay we want to use this knowledge to bet with friends about two people having the same birthday in a group of people. When are the chances larger than 50%? Larger than 75%?\nLet’s create a look-up table. We can quickly create a function to compute this for any group size:\ncompute_prob \u0026lt;- function(n, B=10000){ results \u0026lt;- replicate(B, same_birthday(n)) mean(results) } Using the function sapply, we can perform element-wise operations on any function:\nn \u0026lt;- seq(1,60) prob \u0026lt;- sapply(n, compute_prob) We can now make a plot of the estimated probabilities of two people having the same birthday in a group of size \\(n\\):\nlibrary(tidyverse) prob \u0026lt;- sapply(n, compute_prob) qplot(n, prob) Now let’s compute the exact probabilities rather than use Monte Carlo approximations. Not only do we get the exact answer using math, but the computations are much faster since we don’t have to generate experiments.\nTo make the math simpler, instead of computing the probability of it happening, we will compute the probability of it not happening. For this, we use the multiplication rule.\nLet’s start with the first person. The probability that person 1 has a unique birthday is 1. The probability that person 2 has a unique birthday, given that person 1 already took one, is 364/365. Then, given that the first two people have unique birthdays, person 3 is left with 363 days to choose from. We continue this way and find the chances of all 50 people having a unique birthday is:\n\\[ 1 \\times \\frac{364}{365}\\times\\frac{363}{365} \\dots \\frac{365-n + 1}{365} \\]\nWe can write a function that does this for any number:\nexact_prob \u0026lt;- function(n){ prob_unique \u0026lt;- seq(365,365-n+1)/365 1 - prod( prob_unique) } eprob \u0026lt;- sapply(n, exact_prob) qplot(n, prob) + geom_line(aes(n, eprob), col = \u0026quot;red\u0026quot;) This plot shows that the Monte Carlo simulation provided a very good estimate of the exact probability. Had it not been possible to compute the exact probabilities, we would have still been able to accurately estimate the probabilities.\n  Infinity in practice The theory described here requires repeating experiments over and over forever. In practice we can’t do this. In the examples above, we used \\(B=10,000\\) Monte Carlo experiments and it turned out that this provided accurate estimates. The larger this number, the more accurate the estimate becomes until the approximaton is so good that your computer can’t tell the difference. But in more complex calculations, 10,000 may not be nearly enough. Also, for some calculations, 10,000 experiments might not be computationally feasible. In practice, we won’t know what the answer is, so we won’t know if our Monte Carlo estimate is accurate. We know that the larger \\(B\\), the better the approximation. But how big do we need it to be? This is actually a challenging question and answering it often requires advanced theoretical statistics training.\nOne practical approach we will describe here is to check for the stability of the estimate. The following is an example with the birthday problem for a group of 25 people.\nB \u0026lt;- 10^seq(1, 5, len = 100) compute_prob \u0026lt;- function(B, n=25){ same_day \u0026lt;- replicate(B, same_birthday(n)) mean(same_day) } prob \u0026lt;- sapply(B, compute_prob) qplot(log10(B), prob, geom = \u0026quot;line\u0026quot;) In this plot, we can see that the values start to stabilize (that is, they vary less than .01) around 1000. Note that the exact probability, which we know in this case, is 0.5686997.\nTRY IT\nOne ball will be drawn at random from a box containing: 3 cyan balls, 5 magenta balls, and 7 yellow balls. What is the probability that the ball will be cyan?\n What is the probability that the ball will not be cyan?\n Instead of taking just one draw, consider taking two draws. You take the second draw without returning the first draw to the box. We call this sampling without replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\n Now repeat the experiment, but this time, after taking the first draw and recording the color, return it to the box and shake the box. We call this sampling with replacement. What is the probability that the first draw is cyan and that the second draw is not cyan?\n Two events \\(A\\) and \\(B\\) are independent if \\(\\mbox{Pr}(A \\mbox{ and } B) = \\mbox{Pr}(A) P(B)\\). Under which situation are the draws independent?\n  You don’t replace the draw. You replace the draw. Neither Both  Say you’ve drawn 5 balls from the box, with replacement, and all have been yellow. What is the probability that the next one is yellow?\n If you roll a 6-sided die six times, what is the probability of not seeing a 6?\n Two teams, say the Celtics and the Cavs, are playing a seven game series. The Cavs are a better team and have a 60% chance of winning each game. What is the probability that the Celtics win at least one game?\n Create a Monte Carlo simulation to confirm your answer to the previous problem. Use B \u0026lt;- 10000 simulations. Hint: use the following code to generate the results of the first four games:\n  celtic_wins \u0026lt;- sample(c(0,1), 4, replace = TRUE, prob = c(0.6, 0.4)) The Celtics must win one of these 4 games.\nTwo teams, say the Cavs and the Warriors, are playing a seven game championship series. The first to win four games, therefore, wins the series. The teams are equally good so they each have a 50-50 chance of winning each game. If the Cavs lose the first game, what is the probability that they win the series?\n Confirm the results of the previous question with a Monte Carlo simulation.\n Two teams, \\(A\\) and \\(B\\), are playing a seven game series. Team \\(A\\) is better than team \\(B\\) and has a \\(p\u0026gt;0.5\\) chance of winning each game. Given a value \\(p\\), the probability of winning the series for the underdog team \\(B\\) can be computed with the following function based on a Monte Carlo simulation:\n  prob_win \u0026lt;- function(p){ B \u0026lt;- 10000 result \u0026lt;- replicate(B, { b_win \u0026lt;- sample(c(1,0), 7, replace = TRUE, prob = c(1-p, p)) sum(b_win)\u0026gt;=4 }) mean(result) } Use the function sapply to compute the probability, call it Pr, of winning for p \u0026lt;- seq(0.5, 0.95, 0.025). Then plot the result.\nRepeat the exercise above, but now keep the probability fixed at p \u0026lt;- 0.75 and compute the probability for different series lengths: best of 1 game, 3 games, 5 games,… Specifically, N \u0026lt;- seq(1, 25, 2). Hint: use this function:  prob_win \u0026lt;- function(N, p=0.75){ B \u0026lt;- 10000 result \u0026lt;- replicate(B, { b_win \u0026lt;- sample(c(1,0), N, replace = TRUE, prob = c(1-p, p)) sum(b_win)\u0026gt;=(N+1)/2 }) mean(result) }  In previous lectures, we explained why when summarizing a list of numeric values, such as heights, it is not useful to construct a distribution that defines a proportion to each possible outcome. For example, if we measure every single person in a very large population of size \\(n\\) with extremely high precision, since no two people are exactly the same height, we need to assign the proportion \\(1/n\\) to each observed value and attain no useful summary at all. Similarly, when defining probability distributions, it is not useful to assign a very small probability to every single height.\nJust as when using distributions to summarize numeric data, it is much more practical to define a function that operates on intervals rather than single values. The standard way of doing this is using the cumulative distribution function (CDF).\nWe described empirical cumulative distribution function (eCDF) as a basic summary of a list of numeric values. As an example, we earlier defined the height distribution for adult male students. Here, we define the vector \\(x\\) to contain these heights:\nlibrary(tidyverse) library(dslabs) data(heights) x \u0026lt;- heights %\u0026gt;% filter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;% pull(height) We defined the empirical distribution function as:\nF \u0026lt;- function(a) mean(x\u0026lt;=a) which, for any value a, gives the proportion of values in the list x that are smaller or equal than a.\nKeep in mind that we have not yet introduced probability in the context of CDFs. Let’s do this by asking the following: if I pick one of the male students at random, what is the chance that he is taller than 70.5 inches? Because every student has the same chance of being picked, the answer to this is equivalent to the proportion of students that are taller than 70.5 inches. Using the CDF we obtain an answer by typing:\n1 - F(70) ## [1] 0.3768473 Once a CDF is defined, we can use this to compute the probability of any subset. For instance, the probability of a student being between height a and height b is:\nF(b)-F(a) Because we can compute the probability for any possible event this way, the cumulative probability function defines the probability distribution for picking a height at random from our vector of heights x.\n Theoretical continuous distributions The normal distribution is a useful approximation to many naturally occurring distributions, including that of height. The cumulative distribution for the normal distribution is defined by a mathematical formula which in R can be obtained with the function pnorm. We say that a random quantity is normally distributed with average m and standard deviation s if its probability distribution is defined by:\nF(a) = pnorm(a, m, s) This is useful because if we are willing to use the normal approximation for, say, height, we don’t need the entire dataset to answer questions such as: what is the probability that a randomly selected student is taller then 70 inches? We just need the average height and standard deviation:\nm \u0026lt;- mean(x) s \u0026lt;- sd(x) 1 - pnorm(70.5, m, s) ## [1] 0.371369 Theoretical distributions as approximations The normal distribution is derived mathematically: we do not need data to define it. For practicing data scientists, almost everything we do involves data. Data is always, technically speaking, discrete. For example, we could consider our height data categorical with each specific height a unique category. The probability distribution is defined by the proportion of students reporting each height. Here is a plot of that probability distribution:\nWhile most students rounded up their heights to the nearest inch, others reported values with more precision. One student reported his height to be 69.6850393700787, which is 177 centimeters. The probability assigned to this height is 0.0012315 or 1 in 812. The probability for 70 inches is much higher at 0.1059113, but does it really make sense to think of the probability of being exactly 70 inches as being different than 69.6850393700787? Clearly it is much more useful for data analytic purposes to treat this outcome as a continuous numeric variable, keeping in mind that very few people, or perhaps none, are exactly 70 inches, and that the reason we get more values at 70 is because people round to the nearest inch.\nWith continuous distributions, the probability of a singular value is not even defined. For example, it does not make sense to ask what is the probability that a normally distributed value is 70. Instead, we define probabilities for intervals. We thus could ask what is the probability that someone is between 69.5 and 70.5.\nIn cases like height, in which the data is rounded, the normal approximation is particularly useful if we deal with intervals that include exactly one round number. For example, the normal distribution is useful for approximating the proportion of students reporting values in intervals like the following three:\nmean(x \u0026lt;= 68.5) - mean(x \u0026lt;= 67.5) ## [1] 0.114532 mean(x \u0026lt;= 69.5) - mean(x \u0026lt;= 68.5) ## [1] 0.1194581 mean(x \u0026lt;= 70.5) - mean(x \u0026lt;= 69.5) ## [1] 0.1219212 Note how close we get with the normal approximation:\npnorm(68.5, m, s) - pnorm(67.5, m, s) ## [1] 0.1031077 pnorm(69.5, m, s) - pnorm(68.5, m, s) ## [1] 0.1097121 pnorm(70.5, m, s) - pnorm(69.5, m, s) ## [1] 0.1081743 However, the approximation is not as useful for other intervals. For instance, notice how the approximation breaks down when we try to estimate:\nmean(x \u0026lt;= 70.9) - mean(x\u0026lt;=70.1) ## [1] 0.02216749 with\npnorm(70.9, m, s) - pnorm(70.1, m, s) ## [1] 0.08359562 In general, we call this situation discretization. Although the true height distribution is continuous, the reported heights tend to be more common at discrete values, in this case, due to rounding. As long as we are aware of how to deal with this reality, the normal approximation can still be a very useful tool.\n The probability density For categorical distributions, we can define the probability of a category. For example, a roll of a die, let’s call it \\(X\\), can be 1,2,3,4,5 or 6. The probability of 4 is defined as:\n\\[ \\mbox{Pr}(X=4) = 1/6 \\]\nThe CDF can then easily be defined: \\[ F(4) = \\mbox{Pr}(X\\leq 4) = \\mbox{Pr}(X = 4) + \\mbox{Pr}(X = 3) + \\mbox{Pr}(X = 2) + \\mbox{Pr}(X = 1) \\]\nAlthough for continuous distributions the probability of a single value \\(\\mbox{Pr}(X=x)\\) is not defined, there is a theoretical definition that has a similar interpretation. The probability density at \\(x\\) is defined as the function \\(f(a)\\) such that:\n\\[ F(a) = \\mbox{Pr}(X\\leq a) = \\int_{-\\infty}^a f(x)\\, dx \\]\nFor those that know calculus, remember that the integral is related to a sum: it is the sum of bars with widths approximating 0. If you don’t know calculus, you can think of \\(f(x)\\) as a curve for which the area under that curve up to the value \\(a\\), gives you the probability \\(\\mbox{Pr}(X\\leq a)\\).\nFor example, to use the normal approximation to estimate the probability of someone being taller than 76 inches, we use:\n1 - pnorm(76, m, s) ## [1] 0.03206008 which mathematically is the grey area below:\nThe curve you see is the probability density for the normal distribution. In R, we get this using the function dnorm.\nAlthough it may not be immediately obvious why knowing about probability densities is useful, understanding this concept will be essential to those wanting to fit models to data for which predefined functions are not available.\n  Monte Carlo simulations for continuous variables R provides functions to generate normally distributed outcomes. Specifically, the rnorm function takes three arguments: size, average (defaults to 0), and standard deviation (defaults to 1) and produces random numbers. Here is an example of how we could generate data that looks like our reported heights:\nn \u0026lt;- length(x) m \u0026lt;- mean(x) s \u0026lt;- sd(x) simulated_heights \u0026lt;- rnorm(n, m, s) Not surprisingly, the distribution looks normal:\nThis is one of the most useful functions in R as it will permit us to generate data that mimics natural events and answers questions related to what could happen by chance by running Monte Carlo simulations.\nIf, for example, we pick 800 males at random, what is the distribution of the tallest person? How rare is a seven footer in a group of 800 males? The following Monte Carlo simulation helps us answer that question:\nB \u0026lt;- 10000 tallest \u0026lt;- replicate(B, { simulated_data \u0026lt;- rnorm(800, m, s) max(simulated_data) }) Having a seven footer is quite rare:\nmean(tallest \u0026gt;= 7*12) ## [1] 0.0172 Here is the resulting distribution:\nNote that it does not look normal.\n Continuous distributions The normal distribution is not the only useful theoretical distribution. Other continuous distributions that we may encounter are the student-t, Chi-square, exponential, gamma, beta, and beta-binomial. R provides functions to compute the density, the quantiles, the cumulative distribution functions and to generate Monte Carlo simulations. R uses a convention that lets us remember the names, namely using the letters d, q, p, and r in front of a shorthand for the distribution. We have already seen the functions dnorm, pnorm, and rnorm for the normal distribution. The functions qnorm gives us the quantiles. We can therefore draw a distribution like this:\nx \u0026lt;- seq(-4, 4, length.out = 100) qplot(x, f, geom = \u0026quot;line\u0026quot;, data = data.frame(x, f = dnorm(x))) For the student-t, described later as we move toward hypothesis testing, the shorthand t is used so the functions are dt for the density, qt for the quantiles, pt for the cumulative distribution function, and rt for Monte Carlo simulation.\nTRY IT\nAssume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 5 feet or shorter?\n Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is 6 feet or taller?\n Assume the distribution of female heights is approximated by a normal distribution with a mean of 64 inches and a standard deviation of 3 inches. If we pick a female at random, what is the probability that she is between 61 and 67 inches?\n Repeat the exercise above, but convert everything to centimeters. That is, multiply every height, including the standard deviation, by 2.54. What is the answer now?\n Notice that the answer to the question does not change when you change units. This makes sense since the answer to the question should not be affected by what units we use. In fact, if you look closely, you notice that 61 and 64 are both 1 SD away from the average. Compute the probability that a randomly picked, normally distributed random variable is within 1 SD from the average.\n To see the math that explains why the answers to questions 3, 4, and 5 are the same, suppose we have a random variable with average \\(m\\) and standard error \\(s\\). Suppose we ask the probability of \\(X\\) being smaller or equal to \\(a\\). Remember that, by definition, \\(a\\) is \\((a - m)/s\\) standard deviations \\(s\\) away from the average \\(m\\). The probability is:\n  \\[ \\mbox{Pr}(X \\leq a) \\]\nNow we subtract \\(\\mu\\) to both sides and then divide both sides by \\(\\sigma\\):\n\\[ \\mbox{Pr}\\left(\\frac{X-m}{s} \\leq \\frac{a-m}{s} \\right) \\]\nThe quantity on the left is a standard normal random variable. It has an average of 0 and a standard error of 1. We will call it \\(Z\\):\n\\[ \\mbox{Pr}\\left(Z \\leq \\frac{a-m}{s} \\right) \\]\nSo, no matter the units, the probability of \\(X\\leq a\\) is the same as the probability of a standard normal variable being less than \\((a - m)/s\\). If mu is the average and sigma the standard error, which of the following R code would give us the right answer in every situation:\nmean(X\u0026lt;=a) pnorm((a - m)/s) pnorm((a - m)/s, m, s) pnorm(a)  Imagine the distribution of male adults is approximately normal with an expected value of 69 and a standard deviation of 3. How tall is the male in the 99th percentile? Hint: use qnorm.\n The distribution of IQ scores is approximately normally distributed. The average is 100 and the standard deviation is 15. Suppose you want to know the distribution of the highest IQ across all graduating classes if 10,000 people are born each in your school district. Run a Monte Carlo simulation with B=1000 generating 10,000 IQ scores and keeping the highest. Make a histogram.\n     Random variables In data science, we often deal with data that is affected by chance in some way: the data comes from a random sample, the data is affected by measurement error, or the data measures some outcome that is random in nature. Being able to quantify the uncertainty introduced by randomness is one of the most important jobs of a data analyst. Statistical inference offers a framework, as well as several practical tools, for doing this. The first step is to learn how to mathematically describe random variables.\nIn this section, we introduce random variables and their properties starting with their application to games of chance. We then describe some of the events surrounding the financial crisis of 2007-20084 using probability theory. This financial crisis was in part caused by underestimating the risk of certain securities5 sold by financial institutions. Specifically, the risks of mortgage-backed securities (MBS) and collateralized debt obligations (CDO) were grossly underestimated. These assets were sold at prices that assumed most homeowners would make their monthly payments, and the probability of this not occurring was calculated as being low. A combination of factors resulted in many more defaults than were expected, which led to a price crash of these securities. As a consequence, banks lost so much money that they needed government bailouts to avoid closing down completely.\nDefinition of Random variables Random variables are numeric outcomes resulting from random processes. We can easily generate random variables using some of the simple examples we have shown. For example, define X to be 1 if a bead is blue and red otherwise:\nbeads \u0026lt;- rep( c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), times = c(2,3)) X \u0026lt;- ifelse(sample(beads, 1) == \u0026quot;blue\u0026quot;, 1, 0) Here X is a random variable: every time we select a new bead the outcome changes randomly. See below:\nifelse(sample(beads, 1) == \u0026quot;blue\u0026quot;, 1, 0) ## [1] 1 ifelse(sample(beads, 1) == \u0026quot;blue\u0026quot;, 1, 0) ## [1] 0 ifelse(sample(beads, 1) == \u0026quot;blue\u0026quot;, 1, 0) ## [1] 0 Sometimes it’s 1 and sometimes it’s 0.\n Sampling models Many data generation procedures, those that produce the data we study, can be modeled quite well as draws from an urn. For instance, we can model the process of polling likely voters as drawing 0s (Republicans) and 1s (Democrats) from an urn containing the 0 and 1 code for all likely voters. In epidemiological studies, we often assume that the subjects in our study are a random sample from the population of interest. The data related to a specific outcome can be modeled as a random sample from an urn containing the outcome for the entire population of interest. Similarly, in experimental research, we often assume that the individual organisms we are studying, for example worms, flies, or mice, are a random sample from a larger population. Randomized experiments can also be modeled by draws from an urn given the way individuals are assigned into groups: when getting assigned, you draw your group at random. Sampling models are therefore ubiquitous in data science. Casino games offer a plethora of examples of real-world situations in which sampling models are used to answer specific questions. We will therefore start with such examples.\nSuppose a very small casino hires you to consult on whether they should set up roulette wheels. To keep the example simple, we will assume that 1,000 people will play and that the only game you can play on the roulette wheel is to bet on red or black. The casino wants you to predict how much money they will make or lose. They want a range of values and, in particular, they want to know what’s the chance of losing money. If this probability is too high, they will pass on installing roulette wheels.\nWe are going to define a random variable \\(S\\) that will represent the casino’s total winnings. Let’s start by constructing the urn. A roulette wheel has 18 red pockets, 18 black pockets and 2 green ones. So playing a color in one game of roulette is equivalent to drawing from this urn:\ncolor \u0026lt;- rep(c(\u0026quot;Black\u0026quot;, \u0026quot;Red\u0026quot;, \u0026quot;Green\u0026quot;), c(18, 18, 2)) The 1,000 outcomes from 1,000 people playing are independent draws from this urn. If red comes up, the gambler wins and the casino loses a dollar, so we draw a -1. Otherwise, the casino wins a dollar and we draw a 1. To construct our random variable \\(S\\), we can use this code:\nn \u0026lt;- 1000 X \u0026lt;- sample(ifelse(color == \u0026quot;Red\u0026quot;, -1, 1), n, replace = TRUE) X[1:10] ## [1] -1 1 1 -1 -1 -1 1 1 1 1 Because we know the proportions of 1s and -1s, we can generate the draws with one line of code, without defining color:\nX \u0026lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) We call this a sampling model since we are modeling the random behavior of roulette with the sampling of draws from an urn. The total winnings \\(S\\) is simply the sum of these 1,000 independent draws:\nX \u0026lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) S \u0026lt;- sum(X) S ## [1] 22  The probability distribution of a random variable If you run the code above, you see that \\(S\\) changes every time. This is, of course, because \\(S\\) is a random variable. The probability distribution of a random variable tells us the probability of the observed value falling at any given interval. So, for example, if we want to know the probability that we lose money, we are asking the probability that \\(S\\) is in the interval \\(S\u0026lt;0\\).\nNote that if we can define a cumulative distribution function \\(F(a) = \\mbox{Pr}(S\\leq a)\\), then we will be able to answer any question related to the probability of events defined by our random variable \\(S\\), including the event \\(S\u0026lt;0\\). We call this \\(F\\) the random variable’s distribution function.\nWe can estimate the distribution function for the random variable \\(S\\) by using a Monte Carlo simulation to generate many realizations of the random variable. With this code, we run the experiment of having 1,000 people play roulette, over and over, specifically \\(B = 10,000\\) times:\nn \u0026lt;- 1000 B \u0026lt;- 10000 roulette_winnings \u0026lt;- function(n){ X \u0026lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) sum(X) } S \u0026lt;- replicate(B, roulette_winnings(n)) Now we can ask the following: in our simulations, how often did we get sums less than or equal to a?\nmean(S \u0026lt;= a) This will be a very good approximation of \\(F(a)\\) and we can easily answer the casino’s question: how likely is it that we will lose money? We can see it is quite low:\nmean(S\u0026lt;0) ## [1] 0.0456 We can visualize the distribution of \\(S\\) by creating a histogram showing the probability \\(F(b)-F(a)\\) for several intervals \\((a,b]\\):\nWe see that the distribution appears to be approximately normal. A qq-plot will confirm that the normal approximation is close to a perfect approximation for this distribution. If, in fact, the distribution is normal, then all we need to define the distribution is the average and the standard deviation. Because we have the original values from which the distribution is created, we can easily compute these with mean(S) and sd(S). The blue curve you see added to the histogram above is a normal density with this average and standard deviation.\nThis average and this standard deviation have special names. They are referred to as the expected value and standard error of the random variable \\(S\\). We will say more about these in the next section.\nStatistical theory provides a way to derive the distribution of random variables defined as independent random draws from an urn. Specifically, in our example above, we can show that \\((S+n)/2\\) follows a binomial distribution. We therefore do not need to run for Monte Carlo simulations to know the probability distribution of \\(S\\). We did this for illustrative purposes.\nWe can use the function dbinom and pbinom to compute the probabilities exactly. For example, to compute \\(\\mbox{Pr}(S \u0026lt; 0)\\) we note that:\n\\[\\mbox{Pr}(S \u0026lt; 0) = \\mbox{Pr}((S+n)/2 \u0026lt; (0+n)/2)\\]\nand we can use the pbinom to compute \\[\\mbox{Pr}(S \\leq 0)\\]\nn \u0026lt;- 1000 pbinom(n/2, size = n, prob = 10/19) ## [1] 0.05109794 Because this is a discrete probability function, to get \\(\\mbox{Pr}(S \u0026lt; 0)\\) rather than \\(\\mbox{Pr}(S \\leq 0)\\), we write:\npbinom(n/2-1, size = n, prob = 10/19) ## [1] 0.04479591 For the details of the binomial distribution, you can consult any basic probability book or even Wikipedia6.\nHere we do not cover these details. Instead, we will discuss an incredibly useful approximation provided by mathematical theory that applies generally to sums and averages of draws from any urn: the Central Limit Theorem (CLT).\n Distributions versus probability distributions Before we continue, let’s make an important distinction and connection between the distribution of a list of numbers and a probability distribution. In the visualization lectures, we described how any list of numbers \\(x_1,\\dots,x_n\\) has a distribution. The definition is quite straightforward. We define \\(F(a)\\) as the function that tells us what proportion of the list is less than or equal to \\(a\\). Because they are useful summaries when the distribution is approximately normal, we define the average and standard deviation. These are defined with a straightforward operation of the vector containing the list of numbers x:\nm \u0026lt;- sum(x)/length(x) s \u0026lt;- sqrt(sum((x - m)^2) / length(x)) A random variable \\(X\\) has a distribution function. To define this, we do not need a list of numbers. It is a theoretical concept. In this case, we define the distribution as the \\(F(a)\\) that answers the question: what is the probability that \\(X\\) is less than or equal to \\(a\\)? There is no list of numbers.\nHowever, if \\(X\\) is defined by drawing from an urn with numbers in it, then there is a list: the list of numbers inside the urn. In this case, the distribution of that list is the probability distribution of \\(X\\) and the average and standard deviation of that list are the expected value and standard error of the random variable.\nAnother way to think about it that does not involve an urn is to run a Monte Carlo simulation and generate a very large list of outcomes of \\(X\\). These outcomes are a list of numbers. The distribution of this list will be a very good approximation of the probability distribution of \\(X\\). The longer the list, the better the approximation. The average and standard deviation of this list will approximate the expected value and standard error of the random variable.\n Notation for random variables In statistical textbooks, upper case letters are used to denote random variables and we follow this convention here. Lower case letters are used for observed values. You will see some notation that includes both. For example, you will see events defined as \\(X \\leq x\\). Here \\(X\\) is a random variable, making it a random event, and \\(x\\) is an arbitrary value and not random. So, for example, \\(X\\) might represent the number on a die roll and \\(x\\) will represent an actual value we see 1, 2, 3, 4, 5, or 6. So in this case, the probability of \\(X=x\\) is 1/6 regardless of the observed value \\(x\\). This notation is a bit strange because, when we ask questions about probability, \\(X\\) is not an observed quantity. Instead, it’s a random quantity that we will see in the future. We can talk about what we expect it to be, what values are probable, but not what it is. But once we have data, we do see a realization of \\(X\\). So data scientists talk of what could have been after we see what actually happened.\n The expected value and standard error We have described sampling models for draws. We will now go over the mathematical theory that lets us approximate the probability distributions for the sum of draws. Once we do this, we will be able to help the casino predict how much money they will make. The same approach we use for the sum of draws will be useful for describing the distribution of averages and proportion which we will need to understand how polls work.\nThe first important concept to learn is the expected value. In statistics books, it is common to use letter \\(\\mbox{E}\\) like this:\n\\[\\mbox{E}[X]\\]\nto denote the expected value of the random variable \\(X\\).\nA random variable will vary around its expected value in a way that if you take the average of many, many draws, the average of the draws will approximate the expected value, getting closer and closer the more draws you take.\nTheoretical statistics provides techniques that facilitate the calculation of expected values in different circumstances. For example, a useful formula tells us that the expected value of a random variable defined by one draw is the average of the numbers in the urn. In the urn used to model betting on red in roulette, we have 20 one dollars and 18 negative one dollars. The expected value is thus:\n\\[ \\mbox{E}[X] = (20 + -18)/38 \\]\nwhich is about 5 cents. It is a bit counterintuitive to say that \\(X\\) varies around 0.05, when the only values it takes is 1 and -1. One way to make sense of the expected value in this context is by realizing that if we play the game over and over, the casino wins, on average, 5 cents per game. A Monte Carlo simulation confirms this:\nB \u0026lt;- 10^6 x \u0026lt;- sample(c(-1,1), B, replace = TRUE, prob=c(9/19, 10/19)) mean(x) ## [1] 0.05169 In general, if the urn has two possible outcomes, say \\(a\\) and \\(b\\), with proportions \\(p\\) and \\(1-p\\) respectively, the average is:\n\\[\\mbox{E}[X] = ap + b(1-p)\\]\nTo see this, notice that if there are \\(n\\) beads in the urn, then we have \\(np\\) \\(a\\)s and \\(n(1-p)\\) \\(b\\)s and because the average is the sum, \\(n\\times a \\times p + n\\times b \\times (1-p)\\), divided by the total \\(n\\), we get that the average is \\(ap + b(1-p)\\).\nNow the reason we define the expected value is because this mathematical definition turns out to be useful for approximating the probability distributions of sum, which then is useful for describing the distribution of averages and proportions. The first useful fact is that the expected value of the sum of the draws is:\n\\[ \\mbox{}\\mbox{number of draws } \\times \\mbox{ average of the numbers in the urn} \\]\nSo if 1,000 people play roulette, the casino expects to win, on average, about 1,000 \\(\\times\\) $0.05 = $50. But this is an expected value. How different can one observation be from the expected value? The casino really needs to know this. What is the range of possibilities? If negative numbers are too likely, they will not install roulette wheels. Statistical theory once again answers this question. The standard error (SE) gives us an idea of the size of the variation around the expected value. In statistics books, it’s common to use:\n\\[\\mbox{SE}[X]\\]\nto denote the standard error of a random variable.\nIf our draws are independent, then the standard error of the sum is given by the equation:\n\\[ \\sqrt{\\mbox{number of draws }} \\times \\mbox{ standard deviation of the numbers in the urn} \\]\nUsing the definition of standard deviation, we can derive, with a bit of math, that if an urn contains two values \\(a\\) and \\(b\\) with proportions \\(p\\) and \\((1-p)\\), respectively, the standard deviation is:\n\\[\\mid b - a \\mid \\sqrt{p(1-p)}.\\]\nSo in our roulette example, the standard deviation of the values inside the urn is: \\(\\mid 1 - (-1) \\mid \\sqrt{10/19 \\times 9/19}\\) or:\n2 * sqrt(90)/19 ## [1] 0.998614 The standard error tells us the typical difference between a random variable and its expectation. Since one draw is obviously the sum of just one draw, we can use the formula above to calculate that the random variable defined by one draw has an expected value of 0.05 and a standard error of about 1. This makes sense since we either get 1 or -1, with 1 slightly favored over -1.\nUsing the formula above, the sum of 1,000 people playing has standard error of about $32:\nn \u0026lt;- 1000 sqrt(n) * 2 * sqrt(90)/19 ## [1] 31.57895 As a result, when 1,000 people bet on red, the casino is expected to win $50 with a standard error of $32. It therefore seems like a safe bet. But we still haven’t answered the question: how likely is it to lose money? Here the CLT will help.\nAdvanced note: Before continuing we should point out that exact probability calculations for the casino winnings can be performed with the binomial distribution. However, here we focus on the CLT, which can be generally applied to sums of random variables in a way that the binomial distribution can’t.\nPopulation SD versus the sample SD The standard deviation of a list x (below we use heights as an example) is defined as the square root of the average of the squared differences:\nlibrary(dslabs) x \u0026lt;- heights$height m \u0026lt;- mean(x) s \u0026lt;- sqrt(mean((x-m)^2)) Using mathematical notation we write:\n\\[ \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i \\\\ \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2} \\]\nHowever, be aware that the sd function returns a slightly different result:\nidentical(s, sd(x)) ## [1] FALSE s-sd(x) ## [1] -0.001942661 This is because the sd function R does not return the sd of the list, but rather uses a formula that estimates standard deviations of a population from a random sample \\(X_1, \\dots, X_N\\) which, for reasons not discussed here, divide the sum of squares by the \\(N-1\\).\n\\[ \\bar{X} = \\frac{1}{N} \\sum_{i=1}^N X_i, \\,\\,\\,\\, s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (X_i - \\bar{X})^2} \\]\nYou can see that this is the case by typing:\nn \u0026lt;- length(x) s-sd(x)*sqrt((n-1) / n) ## [1] 0 For all the theory discussed here, you need to compute the actual standard deviation as defined:\nsqrt(mean((x-m)^2)) So be careful when using the sd function in R. However, keep in mind that throughout the book we sometimes use the sd function when we really want the actual SD. This is because when the list size is big, these two are practically equivalent since \\(\\sqrt{(N-1)/N} \\approx 1\\).\n  Central Limit Theorem The Central Limit Theorem (CLT) tells us that when the number of draws, also called the sample size, is large, the probability distribution of the sum of the independent draws is approximately normal. Because sampling models are used for so many data generation processes, the CLT is considered one of the most important mathematical insights in history.\nPreviously, we discussed that if we know that the distribution of a list of numbers is approximated by the normal distribution, all we need to describe the list are the average and standard deviation. We also know that the same applies to probability distributions. If a random variable has a probability distribution that is approximated with the normal distribution, then all we need to describe the probability distribution are the average and standard deviation, referred to as the expected value and standard error.\nWe previously ran this Monte Carlo simulation:\nn \u0026lt;- 1000 B \u0026lt;- 10000 roulette_winnings \u0026lt;- function(n){ X \u0026lt;- sample(c(-1,1), n, replace = TRUE, prob=c(9/19, 10/19)) sum(X) } S \u0026lt;- replicate(B, roulette_winnings(n)) The Central Limit Theorem (CLT) tells us that the sum \\(S\\) is approximated by a normal distribution. Using the formulas above, we know that the expected value and standard error are:\nn * (20-18)/38 ## [1] 52.63158 sqrt(n) * 2 * sqrt(90)/19 ## [1] 31.57895 The theoretical values above match those obtained with the Monte Carlo simulation:\nmean(S) ## [1] 52.2242 sd(S) ## [1] 31.65508 Using the CLT, we can skip the Monte Carlo simulation and instead compute the probability of the casino losing money using this approximation:\nmu \u0026lt;- n * (20-18)/38 se \u0026lt;- sqrt(n) * 2 * sqrt(90)/19 pnorm(0, mu, se) ## [1] 0.04779035 which is also in very good agreement with our Monte Carlo result:\nmean(S \u0026lt; 0) ## [1] 0.0458 How large is large in the Central Limit Theorem? The CLT works when the number of draws is large. But large is a relative term. In many circumstances as few as 30 draws is enough to make the CLT useful. In some specific instances, as few as 10 is enough. However, these should not be considered general rules. Note, for example, that when the probability of success is very small, we need much larger sample sizes.\nBy way of illustration, let’s consider the lottery. In the lottery, the chances of winning are less than 1 in a million. Thousands of people play so the number of draws is very large. Yet the number of winners, the sum of the draws, range between 0 and 4. This sum is certainly not well approximated by a normal distribution, so the CLT does not apply, even with the very large sample size. This is generally true when the probability of a success is very low. In these cases, the Poisson distribution is more appropriate.\nYou can examine the properties of the Poisson distribution using dpois and ppois. You can generate random variables following this distribution with rpois. However, we do not cover the theory here. You can learn about the Poisson distribution in any probability textbook and even Wikipedia7\n  Statistical properties of averages There are several useful mathematical results that we used above and often employ when working with data. We list them below.\n1. The expected value of the sum of random variables is the sum of each random variable’s expected value. We can write it like this:\n\\[ \\mbox{E}[X_1+X_2+\\dots+X_n] = \\mbox{E}[X_1] + \\mbox{E}[X_2]+\\dots+\\mbox{E}[X_n] \\]\nIf the \\(X\\) are independent draws from the urn, then they all have the same expected value. Let’s call it \\(\\mu\\) and thus:\n\\[ \\mbox{E}[X_1+X_2+\\dots+X_n]= n\\mu \\]\nwhich is another way of writing the result we show above for the sum of draws.\n2. The expected value of a non-random constant times a random variable is the non-random constant times the expected value of a random variable. This is easier to explain with symbols:\n\\[ \\mbox{E}[aX] = a\\times\\mbox{E}[X] \\]\nTo see why this is intuitive, consider change of units. If we change the units of a random variable, say from dollars to cents, the expectation should change in the same way. A consequence of the above two facts is that the expected value of the average of independent draws from the same urn is the expected value of the urn, call it \\(\\mu\\) again:\n\\[ \\mbox{E}[(X_1+X_2+\\dots+X_n) / n]= \\mbox{E}[X_1+X_2+\\dots+X_n] / n = n\\mu/n = \\mu \\]\n3. The square of the standard error of the sum of independent random variables is the sum of the square of the standard error of each random variable. This one is easier to understand in math form:\n\\[ \\mbox{SE}[X_1+X_2+\\dots+X_n] = \\sqrt{\\mbox{SE}[X_1]^2 + \\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2 } \\]\nThe square of the standard error is referred to as the variance in statistical textbooks. Note that this particular property is not as intuitive as the previous three and more in depth explanations can be found in statistics textbooks.\n4. The standard error of a non-random constant times a random variable is the non-random constant times the random variable’s standard error. As with the expectation: \\[ \\mbox{SE}[aX] = a \\times \\mbox{SE}[X] \\]\nTo see why this is intuitive, again think of units.\nA consequence of 3 and 4 is that the standard error of the average of independent draws from the same urn is the standard deviation of the urn divided by the square root of \\(n\\) (the number of draws), call it \\(\\sigma\\):\n\\[ \\begin{aligned} \\mbox{SE}[(X_1+X_2+\\dots+X_n) / n] \u0026amp;= \\mbox{SE}[X_1+X_2+\\dots+X_n]/n \\\\ \u0026amp;= \\sqrt{\\mbox{SE}[X_1]^2+\\mbox{SE}[X_2]^2+\\dots+\\mbox{SE}[X_n]^2}/n \\\\ \u0026amp;= \\sqrt{\\sigma^2+\\sigma^2+\\dots+\\sigma^2}/n\\\\ \u0026amp;= \\sqrt{n\\sigma^2}/n\\\\ \u0026amp;= \\sigma / \\sqrt{n} \\end{aligned} \\]\n5. If \\(X\\) is a normally distributed random variable, then if \\(a\\) and \\(b\\) are non-random constants, \\(aX + b\\) is also a normally distributed random variable. All we are doing is changing the units of the random variable by multiplying by \\(a\\), then shifting the center by \\(b\\).\nNote that statistical textbooks use the Greek letters \\(\\mu\\) and \\(\\sigma\\) to denote the expected value and standard error, respectively. This is because \\(\\mu\\) is the Greek letter for \\(m\\), the first letter of mean, which is another term used for expected value. Similarly, \\(\\sigma\\) is the Greek letter for \\(s\\), the first letter of standard error.\n Law of large numbers An important implication of the final result is that the standard error of the average becomes smaller and smaller as \\(n\\) grows larger. When \\(n\\) is very large, then the standard error is practically 0 and the average of the draws converges to the average of the urn. This is known in statistical textbooks as the law of large numbers or the law of averages.\nMisinterpreting law of averages The law of averages is sometimes misinterpreted. For example, if you toss a coin 5 times and see a head each time, you might hear someone argue that the next toss is probably a tail because of the law of averages: on average we should see 50% heads and 50% tails. A similar argument would be to say that red “is due” on the roulette wheel after seeing black come up five times in a row. These events are independent so the chance of a coin landing heads is 50% regardless of the previous 5. This is also the case for the roulette outcome. The law of averages applies only when the number of draws is very large and not in small samples. After a million tosses, you will definitely see about 50% heads regardless of the outcome of the first five tosses.\nAnother funny misuse of the law of averages is in sports when TV sportscasters predict a player is about to succeed because they have failed a few times in a row.\n    https://en.wikipedia.org/wiki/Urn_problem↩︎\n https://www.khanacademy.org/math/precalculus/prob-comb/dependent-events-precalc/v/monty-hall-problem↩︎\n https://en.wikipedia.org/wiki/Monty_Hall_problem↩︎\n https://en.wikipedia.org/w/index.php?title=Financial_crisis_of_2007%E2%80%932008↩︎\n https://en.wikipedia.org/w/index.php?title=Security_(finance)↩︎\n https://en.wikipedia.org/w/index.php?title=Binomial_distribution↩︎\n https://en.wikipedia.org/w/index.php?title=Poisson_distribution↩︎\n   ","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601913344,"objectID":"570f902af3e232a725f0155a46b1910b","permalink":"/content/04-content/","publishdate":"2020-09-29T00:00:00Z","relpermalink":"/content/04-content/","section":"content","summary":"Required Reading Supplemental Readings Guiding Questions Slides  Discrete probability Relative frequency Notation Probability distributions Monte Carlo simulations for categorical data Setting the random seed With and without replacement  Independence Conditional probabilities Addition and multiplication rules Multiplication rule Multiplication rule under independence Addition rule  Combinations and permutations Monte Carlo example  Examples Monty Hall problem Birthday problem  Infinity in practice Theoretical continuous distributions Theoretical distributions as approximations The probability density  Monte Carlo simulations for continuous variables Continuous distributions  Random variables Definition of Random variables Sampling models The probability distribution of a random variable Distributions versus probability distributions Notation for random variables The expected value and standard error Population SD versus the sample SD  Central Limit Theorem How large is large in the Central Limit Theorem?","tags":null,"title":"Probability and Statistics","type":"docs"},{"authors":null,"categories":null,"content":"   Required Reading Supplemental Readings For “Fun” Guiding Questions Slides  Data visualization in practice Case study: new insights on poverty Exploring the Data  Scatterplots Faceting facet_wrap Fixed scales for better comparisons  Time series plots Labels instead of legends  Data transformations Log transformation Which base? Transform the values or the scale?  Visualizing multimodal distributions Comparing multiple distributions with boxplots and ridge plots Boxplots Ridge plots Example: 1970 versus 2010 income distributions Accessing computed variables Weighted densities  The ecological fallacy and importance of showing the data Logistic transformation Show the data  Case study: vaccines and infectious diseases    Required Reading  This page.  Supplemental Readings   Chapter 6 in Claus Wilke, Fundamentals of Data Visualization [@Wilke:2018]  Chapter 6 in Alberto Cairo, The Truthful Art [@Cairo:2016]  Chapter 10 in Claus Wilke, Fundamentals of Data Visualization [@Wilke:2018]  Engaging Readers with Square Pie/Waffle Charts  Understanding Pie Charts  Square pie chart beats out the rest in perception study  Twitter thread from John Burn-Murdoch on why the Financial Times uses log scales in their COVID-19 tracking charts  Tweet and Twitter thread from John Burn-Murdoch on why the Financial Times doesn’t use population-adjusted numbers in their COVID-19 tracking charts   For “Fun”   See how to create your own COVID-19 tracking chart with R   Guiding Questions  How do these types of visualizations help or hinder our search for truth in data? What is the appropriate visualization technique for the pandemic? Should we use population-adjusted numbers? Why or why not?   Slides As with last week, today’s lecture will ask you to work with real data during the lecture. Moreover, we will again avoid slides entirely. To follow along, please have R open throughout lecture and work through this reading on your own time to lock in some of the key concepts.\n  Data visualization in practice In this chapter, we will demonstrate how relatively simple ggplot2 code can create insightful and aesthetically pleasing plots. As motivation we will create plots that help us better understand trends in world health and economics. We will implement what we learned in previous sections of the class and learn how to augment the code to perfect the plots. As we go through our case study, we will describe relevant general data visualization principles and learn concepts such as faceting, time series plots, transformations, and ridge plots.\nCase study: new insights on poverty Hans Rosling1 was the co-founder of the Gapminder Foundation2, an organization dedicated to educating the public by using data to dispel common myths about the so-called developing world. The organization uses data to show how actual trends in health and economics contradict the narratives that emanate from sensationalist media coverage of catastrophes, tragedies, and other unfortunate events. As stated in the Gapminder Foundation’s website:\n Journalists and lobbyists tell dramatic stories. That’s their job. They tell stories about extraordinary events and unusual people. The piles of dramatic stories pile up in peoples’ minds into an over-dramatic worldview and strong negative stress feelings: “The world is getting worse!”, “It’s we vs. them!”, “Other people are strange!”, “The population just keeps growing!” and “Nobody cares!”\n Hans Rosling conveyed actual data-based trends in a dramatic way of his own, using effective data visualization. This section is based on two talks that exemplify this approach to education: [New Insights on Poverty]3 and The Best Stats You’ve Ever Seen4. Specifically, in this section, we use data to attempt to answer the following two questions:\nIs it a fair characterization of today’s world to say it is divided into western rich nations and the developing world in Africa, Asia, and Latin America? Has income inequality across countries worsened during the last 40 years?  To answer these questions, we will be using the gapminder dataset provided in dslabs. This dataset was created using a number of spreadsheets available from the Gapminder Foundation. You can access the table like this:\nlibrary(tidyverse) library(dslabs) library(ggrepel) data(gapminder) gapminder %\u0026gt;% as_tibble() ## # A tibble: 10,545 x 9 ## country year infant_mortality life_expectancy fertility population gdp ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Albania 1960 115. 62.9 6.19 1636054 NA ## 2 Algeria 1960 148. 47.5 7.65 11124892 1.38e10 ## 3 Angola 1960 208 36.0 7.32 5270844 NA ## 4 Antigu… 1960 NA 63.0 4.43 54681 NA ## 5 Argent… 1960 59.9 65.4 3.11 20619075 1.08e11 ## 6 Armenia 1960 NA 66.9 4.55 1867396 NA ## 7 Aruba 1960 NA 65.7 4.82 54208 NA ## 8 Austra… 1960 20.3 70.9 3.45 10292328 9.67e10 ## 9 Austria 1960 37.3 68.8 2.7 7065525 5.24e10 ## 10 Azerba… 1960 NA 61.3 5.57 3897889 NA ## # … with 10,535 more rows, and 2 more variables: continent \u0026lt;fct\u0026gt;, region \u0026lt;fct\u0026gt; Exploring the Data Taking an exercise from the New Insights on Poverty video, we start by testing our knowledge regarding differences in child mortality across different countries. For each of the six pairs of countries below, which country do you think had the highest child mortality rates in 2015? Which pairs do you think are most similar?\nSri Lanka or Turkey Poland or South Korea Malaysia or Russia Pakistan or Vietnam Thailand or South Africa  When answering these questions without data, the non-European countries are typically picked as having higher child mortality rates: Sri Lanka over Turkey, South Korea over Poland, and Malaysia over Russia. It is also common to assume that countries considered to be part of the developing world: Pakistan, Vietnam, Thailand, and South Africa, have similarly high mortality rates.\nTo answer these questions with data, we can use dplyr. For example, for the first comparison we see that:\ngapminder %\u0026gt;% filter(year == 2015 \u0026amp; country %in% c(\u0026quot;Sri Lanka\u0026quot;,\u0026quot;Turkey\u0026quot;)) %\u0026gt;% select(country, infant_mortality) ## country infant_mortality ## 1 Sri Lanka 8.4 ## 2 Turkey 11.6 Turkey has the higher infant mortality rate.\nWe can use this code on all comparisons and find the following:\n## New names: ## * country -\u0026gt; country...1 ## * infant_mortality -\u0026gt; infant_mortality...2 ## * country -\u0026gt; country...3 ## * infant_mortality -\u0026gt; infant_mortality...4   country  infant mortality  country  infant mortality      Sri Lanka  8.4  Turkey  11.6    Poland  4.5  South Korea  2.9    Malaysia  6.0  Russia  8.2    Pakistan  65.8  Vietnam  17.3    Thailand  10.5  South Africa  33.6     We see that the European countries on this list have higher child mortality rates: Poland has a higher rate than South Korea, and Russia has a higher rate than Malaysia. We also see that Pakistan has a much higher rate than Vietnam, and South Africa has a much higher rate than Thailand. It turns out that when Hans Rosling gave this quiz to educated groups of people, the average score was less than 2.5 out of 5, worse than what they would have obtained had they guessed randomly. This implies that more than ignorant, we are misinformed. In this chapter we see how data visualization helps inform us.\n  Scatterplots The reason for this stems from the preconceived notion that the world is divided into two groups: the western world (Western Europe and North America), characterized by long life spans and small families, versus the developing world (Africa, Asia, and Latin America) characterized by short life spans and large families. But do the data support this dichotomous view?\nThe necessary data to answer this question is also available in our gapminder table. Using our newly learned data visualization skills, we will be able to tackle this challenge.\nIn order to analyze this world view, our first plot is a scatterplot of life expectancy versus fertility rates (average number of children per woman). We start by looking at data from about 50 years ago, when perhaps this view was first cemented in our minds.\nfilter(gapminder, year == 1962) %\u0026gt;% ggplot(aes(fertility, life_expectancy)) + geom_point() Most points fall into two distinct categories:\nLife expectancy around 70 years and 3 or fewer children per family. Life expectancy lower than 65 years and more than 5 children per family.  To confirm that indeed these countries are from the regions we expect, we can use color to represent continent.\nfilter(gapminder, year == 1962) %\u0026gt;% ggplot( aes(fertility, life_expectancy, color = continent)) + geom_point() In 1962, the notion of “the West versus developing world” view was grounded in some reality. Is this still the case 50 years later? How might visualizations help us learn something? Before continuing, make a note of your prior beliefs.\n Faceting We could easily plot the 2012 data in the same way we did for 1962. To make comparisons, however, side by side plots are preferable. In ggplot2, we can achieve this by faceting variables: we stratify the data by some variable and make the same plot for each strata.\nTo achieve faceting, we add a layer with the function facet_grid, which automatically separates the plots. This function lets you facet by up to two variables using columns to represent one variable and rows to represent the other. The function expects the row and column variables to be separated by a ~. Here is an example of a scatterplot with facet_grid added as the last layer:\nfilter(gapminder, year%in%c(1962, 2012)) %\u0026gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid(continent~year) We see a plot for each continent/year pair. However, this is just an example and more than what we want, which is simply to compare 1962 and 2012. In this case, there is just one variable and we use . to let facet know that we are not using one of the variables:\nfilter(gapminder, year%in%c(1962, 2012)) %\u0026gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_grid(. ~ year) This plot clearly shows that the majority of countries have moved from the developing world cluster to the western world one. In 2012, the western versus developing world view no longer makes sense. This is particularly clear when comparing Europe to Asia, the latter of which includes several countries that have made great improvements.\nfacet_wrap To explore how this transformation happened through the years, we can make the plot for several years. For example, we can add 1970, 1980, 1990, and 2000. If we do this, we will not want all the plots on the same row, the default behavior of facet_grid, since they will become too thin to show the data. Instead, we will want to use multiple rows and columns. The function facet_wrap permits us to do this by automatically wrapping the series of plots so that each display has viewable dimensions:\nyears \u0026lt;- c(1962, 1980, 1990, 2000, 2012) continents \u0026lt;- c(\u0026quot;Europe\u0026quot;, \u0026quot;Asia\u0026quot;) gapminder %\u0026gt;% filter(year %in% years \u0026amp; continent %in% continents) %\u0026gt;% ggplot( aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_wrap(~year) This plot clearly shows how most Asian countries have improved at a much faster rate than European ones.\n Fixed scales for better comparisons The default choice of the range of the axes is important. When not using facet, this range is determined by the data shown in the plot. When using facet, this range is determined by the data shown in all plots and therefore kept fixed across plots. This makes comparisons across plots much easier. For example, in the above plot, we can see that life expectancy has increased and the fertility has decreased across most countries. We see this because the cloud of points moves. This is not the case if we adjust the scales:\nfilter(gapminder, year%in%c(1962, 2012)) %\u0026gt;% ggplot(aes(fertility, life_expectancy, col = continent)) + geom_point() + facet_wrap(. ~ year, scales = \u0026quot;free\u0026quot;) In the plot above, we have to pay special attention to the range to notice that the plot on the right has a larger life expectancy.\n  Time series plots The visualizations above effectively illustrate that data no longer supports the western versus developing world view. Once we see these plots, new questions emerge. For example, which countries are improving more and which ones less? Was the improvement constant during the last 50 years or was it more accelerated during certain periods? For a closer look that may help answer these questions, we introduce time series plots.\nTime series plots have time in the x-axis and an outcome or measurement of interest on the y-axis. For example, here is a trend plot of United States fertility rates:\ngapminder %\u0026gt;% filter(country == \u0026quot;United States\u0026quot;) %\u0026gt;% ggplot(aes(year, fertility)) + geom_point() We see that the trend is not linear at all. Instead there is sharp drop during the 1960s and 1970s to below 2. Then the trend comes back to 2 and stabilizes during the 1990s.\nWhen the points are regularly and densely spaced, as they are here, we create curves by joining the points with lines, to convey that these data are from a single series, here a country. To do this, we use the geom_line function instead of geom_point.\ngapminder %\u0026gt;% filter(country == \u0026quot;United States\u0026quot;) %\u0026gt;% ggplot(aes(year, fertility)) + geom_line() This is particularly helpful when we look at two countries. If we subset the data to include two countries, one from Europe and one from Asia, then adapt the code above:\ncountries \u0026lt;- c(\u0026quot;South Korea\u0026quot;,\u0026quot;Germany\u0026quot;) gapminder %\u0026gt;% filter(country %in% countries) %\u0026gt;% ggplot(aes(year,fertility)) + geom_line() Unfortunately, this is not the plot that we want. Rather than a line for each country, the points for both countries are joined. This is actually expected since we have not told ggplot anything about wanting two separate lines. To let ggplot know that there are two curves that need to be made separately, we assign each point to a group, one for each country:\ncountries \u0026lt;- c(\u0026quot;South Korea\u0026quot;,\u0026quot;Germany\u0026quot;) gapminder %\u0026gt;% filter(country %in% countries \u0026amp; !is.na(fertility)) %\u0026gt;% ggplot(aes(year, fertility, group = country)) + geom_line() But which line goes with which country? We can assign colors to make this distinction. A useful side-effect of using the color argument to assign different colors to the different countries is that the data is automatically grouped:\ncountries \u0026lt;- c(\u0026quot;South Korea\u0026quot;,\u0026quot;Germany\u0026quot;) gapminder %\u0026gt;% filter(country %in% countries \u0026amp; !is.na(fertility)) %\u0026gt;% ggplot(aes(year,fertility, col = country)) + geom_line() The plot clearly shows how South Korea’s fertility rate dropped drastically during the 1960s and 1970s, and by 1990 had a similar rate to that of Germany.\nLabels instead of legends For trend plots we recommend labeling the lines rather than using legends since the viewer can quickly see which line is which country. This suggestion actually applies to most plots: labeling is usually preferred over legends.\nWe demonstrate how we can do this using the life expectancy data. We define a data table with the label locations and then use a second mapping just for these labels:\nlabels \u0026lt;- data.frame(country = countries, x = c(1975,1965), y = c(60,72)) gapminder %\u0026gt;% filter(country %in% countries) %\u0026gt;% ggplot(aes(year, life_expectancy, col = country)) + geom_line() + geom_text(data = labels, aes(x, y, label = country), size = 5) + theme(legend.position = \u0026quot;none\u0026quot;) The plot clearly shows how an improvement in life expectancy followed the drops in fertility rates. In 1960, Germans lived 15 years longer than South Koreans, although by 2010 the gap is completely closed. It exemplifies the improvement that many non-western countries have achieved in the last 40 years.\n  Data transformations We now shift our attention to the second question related to the commonly held notion that wealth distribution across the world has become worse during the last decades. When general audiences are asked if poor countries have become poorer and rich countries become richer, the majority answers yes. By using stratification, histograms, smooth densities, and boxplots, we will be able to understand if this is in fact the case. First we learn how transformations can sometimes help provide more informative summaries and plots.\nThe gapminder data table includes a column with the countries’ gross domestic product (GDP). GDP measures the market value of goods and services produced by a country in a year. The GDP per person is often used as a rough summary of a country’s wealth. Here we divide this quantity by 365 to obtain the more interpretable measure dollars per day. Using current US dollars as a unit, a person surviving on an income of less than $2 a day is defined to be living in absolute poverty. We add this variable to the data table:\ngapminder \u0026lt;- gapminder %\u0026gt;% mutate(dollars_per_day = gdp/population/365) The GDP values are adjusted for inflation and represent current US dollars, so these values are meant to be comparable across the years. Of course, these are country averages and within each country there is much variability. All the graphs and insights described below relate to country averages and not to individuals.\nLog transformation Here is a histogram of per day incomes from 1970:\npast_year \u0026lt;- 1970 gapminder %\u0026gt;% filter(year == past_year \u0026amp; !is.na(gdp)) %\u0026gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = \u0026quot;black\u0026quot;) We use the color = \"black\" argument to draw a boundary and clearly distinguish the bins.\nIn this plot, we see that for the majority of countries, averages are below $10 a day.\nHowever, the majority of the x-axis is dedicated to the 35 countries with averages above $10. So the plot is not very informative about countries with values below $10 a day.\nIt might be more informative to quickly be able to see how many countries have average daily incomes of about $1 (extremely poor), $2 (very poor), $4 (poor), $8 (middle), $16 (well off), $32 (rich), $64 (very rich) per day. These changes are multiplicative and log transformations convert multiplicative changes into additive ones: when using base 2, a doubling of a value turns into an increase by 1.\nHere is the distribution if we apply a log base 2 transform:\ngapminder %\u0026gt;% filter(year == past_year \u0026amp; !is.na(gdp)) %\u0026gt;% ggplot(aes(log2(dollars_per_day))) + geom_histogram(binwidth = 1, color = \u0026quot;black\u0026quot;) In a way this provides a close-up of the mid to lower income countries.\n Which base? In the case above, we used base 2 in the log transformations. Other common choices are base \\(\\mathrm{e}\\) (the natural log) and base 10.\nIn general, we do not recommend using the natural log for data exploration and visualization. This is because while \\(2^2, 2^3, 2^4, \\dots\\) or \\(10^2, 10^3, \\dots\\) are easy to compute in our heads, the same is not true for \\(\\mathrm{e}^2, \\mathrm{e}^3, \\dots\\), so the scale is not intuitive or easy to interpret.\nIn the dollars per day example, we used base 2 instead of base 10 because the resulting range is easier to interpret. The range of the values being plotted is 0.3269426, 48.8852142.\nIn base 10, this turns into a range that includes very few integers: just 0 and 1. With base two, our range includes -2, -1, 0, 1, 2, 3, 4, and 5. It is easier to compute \\(2^x\\) and \\(10^x\\) when \\(x\\) is an integer and between -10 and 10, so we prefer to have smaller integers in the scale. Another consequence of a limited range is that choosing the binwidth is more challenging. With log base 2, we know that a binwidth of 1 will translate to a bin with range \\(x\\) to \\(2x\\).\nFor an example in which base 10 makes more sense, consider population sizes. A log base 10 is preferable since the range for these is:\nfilter(gapminder, year == past_year) %\u0026gt;% summarize(min = min(population), max = max(population)) ## min max ## 1 46075 808510713 Here is the histogram of the transformed values:\ngapminder %\u0026gt;% filter(year == past_year) %\u0026gt;% ggplot(aes(log10(population))) + geom_histogram(binwidth = 0.5, color = \u0026quot;black\u0026quot;) In the above, we quickly see that country populations range between ten thousand and ten billion.\n Transform the values or the scale? There are two ways we can use log transformations in plots. We can log the values before plotting them or use log scales in the axes. Both approaches are useful and have different strengths. If we log the data, we can more easily interpret intermediate values in the scale. For example, if we see:\n----1----x----2--------3----\nfor log transformed data, we know that the value of \\(x\\) is about 1.5. If the scales are logged:\n----1----x----10------100---\nthen, to determine x, we need to compute \\(10^{1.5}\\), which is not easy to do in our heads. The advantage of using logged scales is that we see the original values on the axes. However, the advantage of showing logged scales is that the original values are displayed in the plot, which are easier to interpret. For example, we would see “32 dollars a day” instead of “5 log base 2 dollars a day”.\nAs we learned earlier, if we want to scale the axis with logs, we can use the scale_x_continuous function. Instead of logging the values first, we apply this layer:\ngapminder %\u0026gt;% filter(year == past_year \u0026amp; !is.na(gdp)) %\u0026gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = \u0026quot;black\u0026quot;) + scale_x_continuous(trans = \u0026quot;log2\u0026quot;) Note that the log base 10 transformation has its own function: scale_x_log10(), but currently base 2 does not, although we could easily define our own.\nThere are other transformations available through the trans argument. As we learn later on, the square root (sqrt) transformation is useful when considering counts. The logistic transformation (logit) is useful when plotting proportions between 0 and 1. The reverse transformation is useful when we want smaller values to be on the right or on top.\n  Visualizing multimodal distributions In the histogram above we see two bumps: one at about 4 and another at about 32. In statistics these bumps are sometimes referred to as modes. The mode of a distribution is the value with the highest frequency. The mode of the normal distribution is the average. When a distribution, like the one above, doesn’t monotonically decrease from the mode, we call the locations where it goes up and down again local modes and say that the distribution has multiple modes.\nThe histogram above suggests that the 1970 country income distribution has two modes: one at about 2 dollars per day (1 in the log 2 scale) and another at about 32 dollars per day (5 in the log 2 scale). This bimodality is consistent with a dichotomous world made up of countries with average incomes less than $8 (3 in the log 2 scale) a day and countries above that.\n Comparing multiple distributions with boxplots and ridge plots A histogram showed us that the 1970 income distribution values show a dichotomy. However, the histogram does not show us if the two groups of countries are west versus the developing world.\nLet’s start by quickly examining the data by region. We reorder the regions by the median value and use a log scale.\ngapminder %\u0026gt;% filter(year == past_year \u0026amp; !is.na(gdp)) %\u0026gt;% mutate(region = reorder(region, dollars_per_day, FUN = median)) %\u0026gt;% ggplot(aes(dollars_per_day, region)) + geom_point() + scale_x_continuous(trans = \u0026quot;log2\u0026quot;) We can already see that there is indeed a “west versus the rest” dichotomy: we see two clear groups, with the rich group composed of North America, Northern and Western Europe, New Zealand and Australia. We define groups based on this observation:\ngapminder \u0026lt;- gapminder %\u0026gt;% mutate(group = case_when( region %in% c(\u0026quot;Western Europe\u0026quot;, \u0026quot;Northern Europe\u0026quot;,\u0026quot;Southern Europe\u0026quot;, \u0026quot;Northern America\u0026quot;, \u0026quot;Australia and New Zealand\u0026quot;) ~ \u0026quot;West\u0026quot;, region %in% c(\u0026quot;Eastern Asia\u0026quot;, \u0026quot;South-Eastern Asia\u0026quot;) ~ \u0026quot;East Asia\u0026quot;, region %in% c(\u0026quot;Caribbean\u0026quot;, \u0026quot;Central America\u0026quot;, \u0026quot;South America\u0026quot;) ~ \u0026quot;Latin America\u0026quot;, continent == \u0026quot;Africa\u0026quot; \u0026amp; region != \u0026quot;Northern Africa\u0026quot; ~ \u0026quot;Sub-Saharan\u0026quot;, TRUE ~ \u0026quot;Others\u0026quot;)) We turn this group variable into a factor to control the order of the levels:\ngapminder \u0026lt;- gapminder %\u0026gt;% mutate(group = factor(group, levels = c(\u0026quot;Others\u0026quot;, \u0026quot;Latin America\u0026quot;, \u0026quot;East Asia\u0026quot;, \u0026quot;Sub-Saharan\u0026quot;, \u0026quot;West\u0026quot;))) In the next section we demonstrate how to visualize and compare distributions across groups.\nBoxplots The exploratory data analysis above has revealed two characteristics about average income distribution in 1970. Using a histogram, we found a bimodal distribution with the modes relating to poor and rich countries. We now want to compare the distribution across these five groups to confirm the “west versus the rest” dichotomy. The number of points in each category is large enough that a summary plot may be useful. We could generate five histograms or five density plots, but it may be more practical to have all the visual summaries in one plot. We therefore start by stacking boxplots next to each other. Note that we add the layer theme(axis.text.x = element_text(angle = 90, hjust = 1)) to turn the group labels vertical, since they do not fit if we show them horizontally, and remove the axis label to make space.\np \u0026lt;- gapminder %\u0026gt;% filter(year == past_year \u0026amp; !is.na(gdp)) %\u0026gt;% ggplot(aes(group, dollars_per_day)) + geom_boxplot() + scale_y_continuous(trans = \u0026quot;log2\u0026quot;) + xlab(\u0026quot;\u0026quot;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) p Boxplots have the limitation that by summarizing the data into five numbers, we might miss important characteristics of the data. One way to avoid this is by showing the data.\np + geom_point(alpha = 0.5)  Ridge plots Showing each individual point does not always reveal important characteristics of the distribution. Although not the case here, when the number of data points is so large that there is over-plotting, showing the data can be counterproductive. Boxplots help with this by providing a five-number summary, but this has limitations too. For example, boxplots will not permit us to discover bimodal distributions. To see this, note that the two plots below are summarizing the same dataset:\nIn cases in which we are concerned that the boxplot summary is too simplistic, we can show stacked smooth densities or histograms. We refer to these as ridge plots. Because we are used to visualizing densities with values in the x-axis, we stack them vertically. Also, because more space is needed in this approach, it is convenient to overlay them. The package ggridges provides a convenient function for doing this. Here is the income data shown above with boxplots but with a ridge plot.\nlibrary(ggridges) p \u0026lt;- gapminder %\u0026gt;% filter(year == past_year \u0026amp; !is.na(dollars_per_day)) %\u0026gt;% ggplot(aes(dollars_per_day, group)) + scale_x_continuous(trans = \u0026quot;log2\u0026quot;) p + geom_density_ridges() Note that we have to invert the x and y used for the boxplot. A useful geom_density_ridges parameter is scale, which lets you determine the amount of overlap, with scale = 1 meaning no overlap and larger values resulting in more overlap.\nIf the number of data points is small enough, we can add them to the ridge plot using the following code:\np + geom_density_ridges(jittered_points = TRUE) By default, the height of the points is jittered and should not be interpreted in any way. To show data points, but without using jitter we can use the following code to add what is referred to as a rug representation of the data.\np + geom_density_ridges(jittered_points = TRUE, position = position_points_jitter(height = 0), point_shape = \u0026#39;|\u0026#39;, point_size = 3, point_alpha = 1, alpha = 0.7)  Example: 1970 versus 2010 income distributions Data exploration clearly shows that in 1970 there was a “west versus the rest” dichotomy. But does this dichotomy persist? Let’s use facet_grid see how the distributions have changed. To start, we will focus on two groups: the west and the rest. We make four histograms.\npast_year \u0026lt;- 1970 present_year \u0026lt;- 2010 years \u0026lt;- c(past_year, present_year) gapminder %\u0026gt;% filter(year %in% years \u0026amp; !is.na(gdp)) %\u0026gt;% mutate(west = ifelse(group == \u0026quot;West\u0026quot;, \u0026quot;West\u0026quot;, \u0026quot;Developing\u0026quot;)) %\u0026gt;% ggplot(aes(dollars_per_day)) + geom_histogram(binwidth = 1, color = \u0026quot;black\u0026quot;) + scale_x_continuous(trans = \u0026quot;log2\u0026quot;) + facet_grid(year ~ west) Before we interpret the findings of this plot, we notice that there are more countries represented in the 2010 histograms than in 1970: the total counts are larger. One reason for this is that several countries were founded after 1970. For example, the Soviet Union divided into several countries during the 1990s. Another reason is that data was available for more countries in 2010.\nWe remake the plots using only countries with data available for both years. In the data wrangling part of this class (after Thanksgiving), we will learn tidyverse tools that permit us to write efficient code for this, but here we can use simple code using the intersect function:\ncountry_list_1 \u0026lt;- gapminder %\u0026gt;% filter(year == past_year \u0026amp; !is.na(dollars_per_day)) %\u0026gt;% pull(country) country_list_2 \u0026lt;- gapminder %\u0026gt;% filter(year == present_year \u0026amp; !is.na(dollars_per_day)) %\u0026gt;% pull(country) country_list \u0026lt;- intersect(country_list_1, country_list_2) These 108 account for 86% of the world population, so this subset should be representative.\nLet’s remake the plot, but only for this subset by simply adding country %in% country_list to the filter function:\nWe now see that the rich countries have become a bit richer, but percentage-wise, the poor countries appear to have improved more. In particular, we see that the proportion of developing countries earning more than $16 a day increased substantially.\nTo see which specific regions improved the most, we can remake the boxplots we made above, but now adding the year 2010 and then using facet to compare the two years.\ngapminder %\u0026gt;% filter(year %in% years \u0026amp; country %in% country_list) %\u0026gt;% ggplot(aes(group, dollars_per_day)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + scale_y_continuous(trans = \u0026quot;log2\u0026quot;) + xlab(\u0026quot;\u0026quot;) + facet_grid(. ~ year) Here, we pause to introduce another powerful ggplot2 feature. Because we want to compare each region before and after, it would be convenient to have the 1970 boxplot next to the 2010 boxplot for each region. In general, comparisons are easier when data are plotted next to each other.\nSo instead of faceting, we keep the data from each year together and ask to color (or fill) them depending on the year. Note that groups are automatically separated by year and each pair of boxplots drawn next to each other. Because year is a number, we turn it into a factor since ggplot2 automatically assigns a color to each category of a factor. Note that we have to convert the year columns from numeric to factor.\ngapminder %\u0026gt;% filter(year %in% years \u0026amp; country %in% country_list) %\u0026gt;% mutate(year = factor(year)) %\u0026gt;% ggplot(aes(group, dollars_per_day, fill = year)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + scale_y_continuous(trans = \u0026quot;log2\u0026quot;) + xlab(\u0026quot;\u0026quot;) Finally, we point out that if what we are most interested in is comparing before and after values, it might make more sense to plot the percentage increases. We are still not ready to learn to code this, but here is what the plot would look like:\nThe previous data exploration suggested that the income gap between rich and poor countries has narrowed considerably during the last 40 years. We used a series of histograms and boxplots to see this. We suggest a succinct way to convey this message with just one plot.\nLet’s start by noting that density plots for income distribution in 1970 and 2010 deliver the message that the gap is closing:\ngapminder %\u0026gt;% filter(year %in% years \u0026amp; country %in% country_list) %\u0026gt;% ggplot(aes(dollars_per_day)) + geom_density(fill = \u0026quot;grey\u0026quot;) + scale_x_continuous(trans = \u0026quot;log2\u0026quot;) + facet_grid(. ~ year) In the 1970 plot, we see two clear modes: poor and rich countries. In 2010, it appears that some of the poor countries have shifted towards the right, closing the gap.\nThe next message we need to convey is that the reason for this change in distribution is that several poor countries became richer, rather than some rich countries becoming poorer. To do this, we can assign a color to the groups we identified during data exploration.\nHowever, we first need to learn how to make these smooth densities in a way that preserves information on the number of countries in each group. To understand why we need this, note the discrepancy in the size of each group:\n## `summarise()` ungrouping output (override with `.groups` argument)   Developing  West      87  21     But when we overlay two densities, the default is to have the area represented by each distribution add up to 1, regardless of the size of each group:\ngapminder %\u0026gt;% filter(year %in% years \u0026amp; country %in% country_list) %\u0026gt;% mutate(group = ifelse(group == \u0026quot;West\u0026quot;, \u0026quot;West\u0026quot;, \u0026quot;Developing\u0026quot;)) %\u0026gt;% ggplot(aes(dollars_per_day, fill = group)) + scale_x_continuous(trans = \u0026quot;log2\u0026quot;) + geom_density(alpha = 0.2) + facet_grid(year ~ .) This makes it appear as if there are the same number of countries in each group. To change this, we will need to learn to access computed variables with geom_density function.\n Accessing computed variables To have the areas of these densities be proportional to the size of the groups, we can simply multiply the y-axis values by the size of the group. From the geom_density help file, we see that the functions compute a variable called count that does exactly this. We want this variable to be on the y-axis rather than the density.\nIn ggplot2, we access these variables by surrounding the name with two dots. We will therefore use the following mapping:\naes(x = dollars_per_day, y = ..count..) We can now create the desired plot by simply changing the mapping in the previous code chunk. We will also expand the limits of the x-axis.\np \u0026lt;- gapminder %\u0026gt;% filter(year %in% years \u0026amp; country %in% country_list) %\u0026gt;% mutate(group = ifelse(group == \u0026quot;West\u0026quot;, \u0026quot;West\u0026quot;, \u0026quot;Developing\u0026quot;)) %\u0026gt;% ggplot(aes(dollars_per_day, y = ..count.., fill = group)) + scale_x_continuous(trans = \u0026quot;log2\u0026quot;, limit = c(0.125, 300)) p + geom_density(alpha = 0.2) + facet_grid(year ~ .) If we want the densities to be smoother, we use the bw argument so that the same bandwidth is used in each density. We selected 0.75 after trying out several values.\np + geom_density(alpha = 0.2, bw = 0.75) + facet_grid(year ~ .) This plot now shows what is happening very clearly. The developing world distribution is changing. A third mode appears consisting of the countries that most narrowed the gap.\nTo visualize if any of the groups defined above are driving this we can quickly make a ridge plot:\ngapminder %\u0026gt;% filter(year %in% years \u0026amp; !is.na(dollars_per_day)) %\u0026gt;% ggplot(aes(dollars_per_day, group)) + scale_x_continuous(trans = \u0026quot;log2\u0026quot;) + geom_density_ridges(adjust = 1.5) + facet_grid(. ~ year) Another way to achieve this is by stacking the densities on top of each other:\ngapminder %\u0026gt;% filter(year %in% years \u0026amp; country %in% country_list) %\u0026gt;% group_by(year) %\u0026gt;% mutate(weight = population/sum(population)*2) %\u0026gt;% ungroup() %\u0026gt;% ggplot(aes(dollars_per_day, fill = group)) + scale_x_continuous(trans = \u0026quot;log2\u0026quot;, limit = c(0.125, 300)) + geom_density(alpha = 0.2, bw = 0.75, position = \u0026quot;stack\u0026quot;) + facet_grid(year ~ .) Here we can clearly see how the distributions for East Asia, Latin America, and others shift markedly to the right. While Sub-Saharan Africa remains stagnant.\nNotice that we order the levels of the group so that the West’s density is plotted first, then Sub-Saharan Africa. Having the two extremes plotted first allows us to see the remaining bimodality better.\n Weighted densities As a final point, we note that these distributions weigh every country the same. So if most of the population is improving, but living in a very large country, such as China, we might not appreciate this. We can actually weight the smooth densities using the weight mapping argument. The plot then looks like this:\nThis particular figure shows very clearly how the income distribution gap is closing with most of the poor remaining in Sub-Saharan Africa.\n  The ecological fallacy and importance of showing the data Throughout this section, we have been comparing regions of the world. We have seen that, on average, some regions do better than others. In this section, we focus on describing the importance of variability within the groups when examining the relationship between a country’s infant mortality rates and average income.\nWe define a few more regions and compare the averages across regions:\n## `summarise()` ungrouping output (override with `.groups` argument) The relationship between these two variables is almost perfectly linear and the graph shows a dramatic difference. While in the West less than 0.5% of infants die, in Sub-Saharan Africa the rate is higher than 6%!\nNote that the plot uses a new transformation, the logistic transformation.\nLogistic transformation The logistic or logit transformation for a proportion or rate \\(p\\) is defined as:\n\\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\]\nWhen \\(p\\) is a proportion or probability, the quantity that is being logged, \\(p/(1-p)\\), is called the odds. In this case \\(p\\) is the proportion of infants that survived. The odds tell us how many more infants are expected to survive than to die. The log transformation makes this symmetric. If the rates are the same, then the log odds is 0. Fold increases or decreases turn into positive and negative increments, respectively.\nThis scale is useful when we want to highlight differences near 0 or 1. For survival rates this is important because a survival rate of 90% is unacceptable, while a survival of 99% is relatively good. We would much prefer a survival rate closer to 99.9%. We want our scale to highlight these difference and the logit does this. Note that 99.9/0.1 is about 10 times bigger than 99/1 which is about 10 times larger than 90/10. By using the log, these fold changes turn into constant increases.\n Show the data Now, back to our plot. Based on the plot above, do we conclude that a country with a low income is destined to have low survival rate? Do we conclude that survival rates in Sub-Saharan Africa are all lower than in Southern Asia, which in turn are lower than in the Pacific Islands, and so on?\nJumping to this conclusion based on a plot showing averages is referred to as the ecological fallacy. The almost perfect relationship between survival rates and income is only observed for the averages at the region level. Once we show all the data, we see a somewhat more complicated story:\nSpecifically, we see that there is a large amount of variability. We see that countries from the same regions can be quite different and that countries with the same income can have different survival rates. For example, while on average Sub-Saharan Africa had the worse health and economic outcomes, there is wide variability within that group. Mauritius and Botswana are doing better than Angola and Sierra Leone, with Mauritius comparable to Western countries.\n  Case study: vaccines and infectious diseases Vaccines have helped save millions of lives. In the 19th century, before herd immunization was achieved through vaccination programs, deaths from infectious diseases, such as smallpox and polio, were common. However, today vaccination programs have become somewhat controversial despite all the scientific evidence for their importance.\nThe controversy started with a paper5 published in 1988 and led by Andrew Wakefield claiming there was a link between the administration of the measles, mumps, and rubella (MMR) vaccine and the appearance of autism and bowel disease. Despite much scientific evidence contradicting this finding, sensationalist media reports and fear-mongering from conspiracy theorists led parts of the public into believing that vaccines were harmful. As a result, many parents ceased to vaccinate their children. This dangerous practice can be potentially disastrous given that the Centers for Disease Control (CDC) estimates that vaccinations will prevent more than 21 million hospitalizations and 732,000 deaths among children born in the last 20 years (see Benefits from Immunization during the Vaccines for Children Program Era — United States, 1994-2013, MMWR6). The 1988 paper has since been retracted and Andrew Wakefield was eventually “struck off the UK medical register, with a statement identifying deliberate falsification in the research published in The Lancet, and was thereby barred from practicing medicine in the UK.” (source: Wikipedia7). Yet misconceptions persist, in part due to self-proclaimed activists who continue to disseminate misinformation about vaccines.\nEffective communication of data is a strong antidote to misinformation and fear-mongering. Earlier we used an example provided by a Wall Street Journal article8 showing data related to the impact of vaccines on battling infectious diseases. Here we reconstruct that example.\nThe data used for these plots were collected, organized, and distributed by the Tycho Project9. They include weekly reported counts for seven diseases from 1928 to 2011, from all fifty states. The yearly totals are helpfully included in the dslabs package:\nlibrary(tidyverse) library(RColorBrewer) library(dslabs) data(us_contagious_diseases) names(us_contagious_diseases) ## [1] \u0026quot;disease\u0026quot; \u0026quot;state\u0026quot; \u0026quot;year\u0026quot; \u0026quot;weeks_reporting\u0026quot; ## [5] \u0026quot;count\u0026quot; \u0026quot;population\u0026quot; We create a temporary object dat that stores only the measles data, includes a per 100,000 rate, orders states by average value of disease and removes Alaska and Hawaii since they only became states in the late 1950s. Note that there is a weeks_reporting column that tells us for how many weeks of the year data was reported. We have to adjust for that value when computing the rate.\nthe_disease \u0026lt;- \u0026quot;Measles\u0026quot; dat \u0026lt;- us_contagious_diseases %\u0026gt;% filter(!state%in%c(\u0026quot;Hawaii\u0026quot;,\u0026quot;Alaska\u0026quot;) \u0026amp; disease == the_disease) %\u0026gt;% mutate(rate = count / population * 10000 * 52 / weeks_reporting) %\u0026gt;% mutate(state = reorder(state, rate)) We can now easily plot disease rates per year. Here are the measles data from California:\ndat %\u0026gt;% filter(state == \u0026quot;California\u0026quot; \u0026amp; !is.na(rate)) %\u0026gt;% ggplot(aes(year, rate)) + geom_line() + ylab(\u0026quot;Cases per 10,000\u0026quot;) + geom_vline(xintercept=1963, col = \u0026quot;blue\u0026quot;) We add a vertical line at 1963 since this is when the vaccine was introduced [Control, Centers for Disease; Prevention (2014). CDC health information for international travel 2014 (the yellow book). p. 250. ISBN 9780199948505].\nNow can we show data for all states in one plot? We have three variables to show: year, state, and rate. In the WSJ figure, they use the x-axis for year, the y-axis for state, and color hue to represent rates. However, the color scale they use, which goes from yellow to blue to green to orange to red, can be improved.\nIn our example, we want to use a sequential palette since there is no meaningful center, just low and high rates.\nWe use the geometry geom_tile to tile the region with colors representing disease rates. We use a square root transformation to avoid having the really high counts dominate the plot. Notice that missing values are shown in grey. Note that once a disease was pretty much eradicated, some states stopped reporting cases all together. This is why we see so much grey after 1980.\ndat %\u0026gt;% ggplot(aes(year, state, fill = rate)) + geom_tile(color = \u0026quot;grey50\u0026quot;) + scale_x_continuous(expand=c(0,0)) + scale_fill_gradientn(colors = brewer.pal(9, \u0026quot;Reds\u0026quot;), trans = \u0026quot;sqrt\u0026quot;) + geom_vline(xintercept=1963, col = \u0026quot;blue\u0026quot;) + theme_minimal() + theme(panel.grid = element_blank(), legend.position=\u0026quot;bottom\u0026quot;, text = element_text(size = 8)) + ggtitle(the_disease) + ylab(\u0026quot;\u0026quot;) + xlab(\u0026quot;\u0026quot;) This plot makes a very striking argument for the contribution of vaccines. However, one limitation of this plot is that it uses color to represent quantity, which we earlier explained makes it harder to know exactly how high values are going. Position and lengths are better cues. If we are willing to lose state information, we can make a version of the plot that shows the values with position. We can also show the average for the US, which we compute like this:\navg \u0026lt;- us_contagious_diseases %\u0026gt;% filter(disease==the_disease) %\u0026gt;% group_by(year) %\u0026gt;% summarize(us_rate = sum(count, na.rm = TRUE) / sum(population, na.rm = TRUE) * 10000) ## `summarise()` ungrouping output (override with `.groups` argument) Now to make the plot we simply use the geom_line geometry:\ndat %\u0026gt;% filter(!is.na(rate)) %\u0026gt;% ggplot() + geom_line(aes(year, rate, group = state), color = \u0026quot;grey50\u0026quot;, show.legend = FALSE, alpha = 0.2, size = 1) + geom_line(mapping = aes(year, us_rate), data = avg, size = 1) + scale_y_continuous(trans = \u0026quot;sqrt\u0026quot;, breaks = c(5, 25, 125, 300)) + ggtitle(\u0026quot;Cases per 10,000 by state\u0026quot;) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;\u0026quot;) + geom_text(data = data.frame(x = 1955, y = 50), mapping = aes(x, y, label=\u0026quot;US average\u0026quot;), color=\u0026quot;black\u0026quot;) + geom_vline(xintercept=1963, col = \u0026quot;blue\u0026quot;) In theory, we could use color to represent the categorical value state, but it is hard to pick 50 distinct colors.\nTRY IT\nReproduce the image plot we previously made but for smallpox. For this plot, do not include years in which cases were not reported in 10 or more weeks.\n Now reproduce the time series plot we previously made, but this time following the instructions of the previous question for smallpox.\n For the state of California, make a time series plot showing rates for all diseases. Include only years with 10 or more weeks reporting. Use a different color for each disease.\n Now do the same for the rates for the US. Hint: compute the US rate by using summarize: the total divided by total population.\n      https://en.wikipedia.org/wiki/Hans_Rosling↩︎\n http://www.gapminder.org/↩︎\n https://www.ted.com/talks/hans_rosling_reveals_new_insights_on_poverty?language=en↩︎\n https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen↩︎\n http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(97)11096-0/abstract↩︎\n https://www.cdc.gov/mmwr/preview/mmwrhtml/mm6316a4.htm↩︎\n https://en.wikipedia.org/wiki/Andrew_Wakefield↩︎\n http://graphics.wsj.com/infectious-diseases-and-vaccines/↩︎\n http://www.tycho.pitt.edu/↩︎\n   ","date":1600732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600782994,"objectID":"267a7446c3b4fe31bb41b58ba3b49b11","permalink":"/content/03-content/","publishdate":"2020-09-22T00:00:00Z","relpermalink":"/content/03-content/","section":"content","summary":"Required Reading Supplemental Readings For “Fun” Guiding Questions Slides  Data visualization in practice Case study: new insights on poverty Exploring the Data  Scatterplots Faceting facet_wrap Fixed scales for better comparisons  Time series plots Labels instead of legends  Data transformations Log transformation Which base? Transform the values or the scale?  Visualizing multimodal distributions Comparing multiple distributions with boxplots and ridge plots Boxplots Ridge plots Example: 1970 versus 2010 income distributions Accessing computed variables Weighted densities  The ecological fallacy and importance of showing the data Logistic transformation Show the data  Case study: vaccines and infectious diseases    Required Reading  This page.","tags":null,"title":"Visualizations in Practice","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Guiding Questions Slides  Visualizing data distributions Variable types Case study: describing student heights Distribution function Cumulative distribution functions Histograms Smoothed density Interpreting the y-axis Densities permit stratification  The normal distribution Standard units Quantile-quantile plots Percentiles Boxplots Stratification Case study: describing student heights (continued) ggplot2 geometries Barplots Histograms Density plots Boxplots QQ-plots   Data visualization principles Encoding data using visual cues Know when to include 0 Do not distort quantities Order categories by a meaningful value Show the data Ease comparisons Use common axes Align plots vertically to see horizontal changes and horizontally to see vertical changes Consider transformations Visual cues to be compared should be adjacent Use color  Think of the color blind Plots for two variables Slope charts Bland-Altman plot  Encoding a third variable Avoid pseudo-three-dimensional plots Avoid too many significant digits Know your audience    Readings  This page.  Guiding Questions  Why do we create visualizations? What types of data are best suited for visuals? How do we best visualize the variability in our data? What makes a visual compelling? What are the worst visuals? Which of these are most frequently used? Why?   Slides As with last week’s content, the technical aspects of this lecture will be explored in greater detail in the Thursday practical lecture. Today, we will focus on some principles. We will also avoid slides entirely.\n “The greatest value of a picture is when it forces us to notice what we never expected to see.” – John Tukey\n Today’s lecture will ask you to touch real data during the lecture. Please download the following dataset and load it into R.\n  Ames.csv  This dataset is from houses in Ames, Iowa. (Thrilling!) We will use this dataset during the lecture to illustrate some of the points discussed below.\n  Visualizing data distributions Throughout your education, you may have noticed that numerical data is often summarized with the average value. For example, the quality of a high school is sometimes summarized with one number: the average score on a standardized test. Occasionally, a second number is reported: the standard deviation. For example, you might read a report stating that scores were 680 plus or minus 50 (the standard deviation). The report has summarized an entire vector of scores with just two numbers. Is this appropriate? Is there any important piece of information that we are missing by only looking at this summary rather than the entire list?\nOur first data visualization building block is learning to summarize lists of factors or numeric vectors—the two primary data types that we encounter in data analytics. More often than not, the best way to share or explore this summary is through data visualization. The most basic statistical summary of a list of objects or numbers is its distribution. Once a vector has been summarized as a distribution, there are several data visualization techniques to effectively relay this information.\nIn this section, we first discuss properties of a variety of distributions and how to visualize distributions using a motivating example of student heights. We then discuss some principles of data visualizations more broadly.\nVariable types We will be working with two types of variables: categorical and numeric. Each can be divided into two other groups: categorical can be ordinal or not, whereas numerical variables can be discrete or continuous.\nWhen each entry in a vector comes from one of a small number of groups, we refer to the data as categorical data. Two simple examples are sex (male or female) and regions (Northeast, South, North Central, West). Some categorical data can be ordered even if they are not numbers per se, such as spiciness (mild, medium, hot). In statistics textbooks, ordered categorical data are referred to as ordinal data. In psychology, a number of different terms are used for this same idea.\nExamples of numerical data are population sizes, murder rates, and heights. Some numerical data can be treated as ordered categorical. We can further divide numerical data into continuous and discrete. Continuous variables are those that can take any value, such as heights, if measured with enough precision. For example, a pair of twins may be 68.12 and 68.11 inches, respectively. Counts, such as population sizes, are discrete because they have to be integers—that’s how we count.1\n Case study: describing student heights Here we consider an artificial problem to help us illustrate the underlying concepts.\nPretend that we have to describe the heights of our classmates to ET, an extraterrestrial that has never seen humans. As a first step, we need to collect data. To do this, we ask students to report their heights in inches. We ask them to provide sex information because we know there are two different distributions by sex. We collect the data and save it in the heights data frame:\nlibrary(tidyverse) library(dslabs) data(heights) One way to convey the heights to ET is to simply send him this list of 1050 heights. But there are much more effective ways to convey this information, and understanding the concept of a distribution will help. To simplify the explanation, we first focus on male heights.\n Distribution function It turns out that, in some cases, the average and the standard deviation are pretty much all we need to understand the data. We will learn data visualization techniques that will help us determine when this two number summary is appropriate. These same techniques will serve as an alternative for when two numbers are not enough.\nThe most basic statistical summary of a list of objects or numbers is its distribution. The simplest way to think of a distribution is as a compact description of a list with many entries. This concept should not be new for readers of this book. For example, with categorical data, the distribution simply describes the proportion of each unique category. The sex represented in the heights dataset is:\n## ## Female Male ## 0.2266667 0.7733333 This two-category frequency table is the simplest form of a distribution. We don’t really need to visualize it since one number describes everything we need to know: 23% are females and the rest are males. When there are more categories, then a simple barplot describes the distribution. Here is an example with US state regions:\nmurders %\u0026gt;% group_by(region) %\u0026gt;% summarize(n = n()) %\u0026gt;% mutate(Proportion = n/sum(n), region = reorder(region, Proportion)) %\u0026gt;% ggplot(aes(x=region, y=Proportion, fill=region)) + geom_bar(stat = \u0026quot;identity\u0026quot;, show.legend = FALSE) + xlab(\u0026quot;\u0026quot;) This particular plot simply shows us four numbers, one for each category. We usually use barplots to display a few numbers. Although this particular plot does not provide much more insight than a frequency table itself, it is a first example of how we convert a vector into a plot that succinctly summarizes all the information in the vector. When the data is numerical, the task of displaying distributions is more challenging.\n Cumulative distribution functions Numerical data that are not categorical also have distributions. In general, when data is not categorical, reporting the frequency of each entry is not an effective summary since most entries are unique. In our case study, while several students reported a height of 68 inches, only one student reported a height of 68.503937007874 inches and only one student reported a height 68.8976377952756 inches. We assume that they converted from 174 and 175 centimeters, respectively.\nStatistics textbooks teach us that a more useful way to define a distribution for numeric data is to define a function that reports the proportion of the data below \\(a\\) for all possible values of \\(a\\). This function is called the cumulative distribution function (CDF). In statistics, the following notation is used:\n\\[ F(a) = \\mbox{Pr}(x \\leq a) \\]\nHere is a plot of \\(F\\) for the male height data:\nSimilar to what the frequency table does for categorical data, the CDF defines the distribution for numerical data. From the plot, we can see that 16% of the values are below 65, since \\(F(66)=\\) 0.1637931, or that 84% of the values are below 72, since \\(F(72)=\\) 0.841133, and so on. In fact, we can report the proportion of values between any two heights, say \\(a\\) and \\(b\\), by computing \\(F(b) - F(a)\\). This means that if we send this plot above to ET, he will have all the information needed to reconstruct the entire list. Paraphrasing the expression “a picture is worth a thousand words”, in this case, a picture is as informative as 812 numbers.\nA final note: because CDFs can be defined mathematically—and absent any data—the word empirical is added to make the distinction when data is used. We therefore use the term empirical CDF (eCDF).\n Histograms Although the CDF concept is widely discussed in statistics textbooks, the plot is actually not very popular in practice. The main reason is that it does not easily convey characteristics of interest such as: at what value is the distribution centered? Is the distribution symmetric? What ranges contain 95% of the values? I doubt you can figure these out from glancing at the plot above. Histograms are much preferred because they greatly facilitate answering such questions. Histograms sacrifice just a bit of information to produce plots that are much easier to interpret.\nThe simplest way to make a histogram is to divide the span of our data into non-overlapping bins of the same size. Then, for each bin, we count the number of values that fall in that interval. The histogram plots these counts as bars with the base of the bar defined by the intervals. Here is the histogram for the height data splitting the range of values into one inch intervals: \\((49.5, 50.5],(50.5, 51.5],(51.5,52.5],(52.5,53.5],...,(82.5,83.5]\\)\nIf we send this plot to some uninformed reader, she will immediately learn some important properties about our data. First, the range of the data is from 50 to 84 with the majority (more than 95%) between 63 and 75 inches. Second, the heights are close to symmetric around 69 inches. Also, by adding up counts, this reader could obtain a very good approximation of the proportion of the data in any interval. Therefore, the histogram above is not only easy to interpret, but also provides almost all the information contained in the raw list of 812 heights with about 30 bin counts.\nWhat information do we lose? Note that all values in each interval are treated the same when computing bin heights. So, for example, the histogram does not distinguish between 64, 64.1, and 64.2 inches. Given that these differences are almost unnoticeable to the eye, the practical implications are negligible and we were able to summarize the data to just 23 numbers.\n Smoothed density Smooth density plots are aesthetically more appealing than histograms. Here is what a smooth density plot looks like for our heights data:\nIn this plot, we no longer have sharp edges at the interval boundaries and many of the local peaks have been removed. Also, the scale of the y-axis changed from counts to density.\nTo understand the smooth densities, we have to understand estimates, a topic we don’t cover until later. However, we provide a heuristic explanation to help you understand the basics so you can use this useful data visualization tool.\nThe main new concept you must understand is that we assume that our list of observed values is a subset of a much larger list of unobserved values. In the case of heights, you can imagine that our list of 812 male students comes from a hypothetical list containing all the heights of all the male students in all the world measured very precisely. Let’s say there are 1,000,000 of these measurements. This list of values has a distribution, like any list of values, and this larger distribution is really what we want to report to ET since it is much more general. Unfortunately, we don’t get to see it.\nHowever, we make an assumption that helps us perhaps approximate it. If we had 1,000,000 values, measured very precisely, we could make a histogram with very, very small bins. The assumption is that if we show this, the height of consecutive bins will be similar. This is what we mean by smooth: we don’t have big jumps in the heights of consecutive bins. Below we have a hypothetical histogram with bins of size 1:\nThe smaller we make the bins, the smoother the histogram gets. Here are the histograms with bin width of 1, 0.5, and 0.1:\nThe smooth density is basically the curve that goes through the top of the histogram bars when the bins are very, very small. To make the curve not depend on the hypothetical size of the hypothetical list, we compute the curve on frequencies rather than counts:\nNow, back to reality. We don’t have millions of measurements. In this concrete example, we have 812 and we can’t make a histogram with very small bins.\nWe therefore make a histogram, using bin sizes appropriate for our data and computing frequencies rather than counts, and we draw a smooth curve that goes through the tops of the histogram bars. The following plots (loosely) demonstrate the steps that the computer goes through to ultimately create a smooth density:\nHowever, remember that smooth is a relative term. We can actually control the smoothness of the curve that defines the smooth density through an option in the function that computes the smooth density curve. Here are two examples using different degrees of smoothness on the same histogram:\nWe need to make this choice with care as the resulting visualizations can change our interpretation of the data. We should select a degree of smoothness that we can defend as being representative of the underlying data. In the case of height, we really do have reason to believe that the proportion of people with similar heights should be the same. For example, the proportion that is 72 inches should be more similar to the proportion that is 71 than to the proportion that is 78 or 65. This implies that the curve should be pretty smooth; that is, the curve should look more like the example on the right than on the left.\nWhile the histogram is an assumption-free summary, the smoothed density is based on some assumptions.\nInterpreting the y-axis Note that interpreting the y-axis of a smooth density plot is not straightforward. It is scaled so that the area under the density curve adds up to 1. If you imagine we form a bin with a base 1 unit in length, the y-axis value tells us the proportion of values in that bin. However, this is only true for bins of size 1. For other size intervals, the best way to determine the proportion of data in that interval is by computing the proportion of the total area contained in that interval. For example, here are the proportion of values between 65 and 68:\nThe proportion of this area is about 0.3, meaning that about 30% of male heights are between 65 and 68 inches.\nBy understanding this, we are ready to use the smooth density as a summary. For this dataset, we would feel quite comfortable with the smoothness assumption, and therefore with sharing this aesthetically pleasing figure with ET, which he could use to understand our male heights data:\n Densities permit stratification As a final note, we point out that an advantage of smooth densities over histograms for visualization purposes is that densities make it easier to compare two distributions. This is in large part because the jagged edges of the histogram add clutter. Here is an example comparing male and female heights:\nWith the right argument, ggplot automatically shades the intersecting region with a different color. We will show examples of ggplot2 code in the coming Example later this week.\n  The normal distribution Histograms and density plots provide excellent summaries of a distribution. But can we summarize even further? We often see the average and standard deviation used as summary statistics: a two-number summary! To understand what these summaries are and why they are so widely used, we need to understand the normal distribution.\nThe normal distribution, also known as the bell curve and as the Gaussian distribution, is one of the most famous mathematical concepts in history. A reason for this is that approximately normal distributions occur in many situations, including gambling winnings, heights, weights, blood pressure, standardized test scores, and experimental measurement errors. There are explanations for this, but we describe these later. Here we focus on how the normal distribution helps us summarize data.\nRather than using data, the normal distribution is defined with a mathematical formula. For any interval \\((a,b)\\), the proportion of values in that interval can be computed using this formula:\n\\[\\mbox{Pr}(a \u0026lt; x \u0026lt; b) = \\int_a^b \\frac{1}{\\sqrt{2\\pi}s} e^{-\\frac{1}{2}\\left( \\frac{x-m}{s} \\right)^2} \\, dx\\]\nYou don’t need to memorize or understand the details of the formula. But note that it is completely defined by just two parameters: \\(m\\) and \\(s\\). The rest of the symbols in the formula represent the interval ends that we determine, \\(a\\) and \\(b\\), and known mathematical constants \\(\\pi\\) and \\(e\\). These two parameters, \\(m\\) and \\(s\\), are referred to as the average (also called the mean) and the standard deviation (SD) of the distribution, respectively.\nThe distribution is symmetric, centered at the average, and most values (about 95%) are within 2 SDs from the average. Here is what the normal distribution looks like when the average is 0 and the SD is 1:\nThe fact that the distribution is defined by just two parameters implies that if a dataset is approximated by a normal distribution, all the information needed to describe the distribution can be encoded in just two numbers: the average and the standard deviation. We now define these values for an arbitrary list of numbers.\nFor a list of numbers contained in a vector x, the average is defined as:\nm \u0026lt;- sum(x) / length(x) and the SD is defined as:\ns \u0026lt;- sqrt(sum((x-mu)^2) / length(x)) which can be interpreted as the average distance between values and their average.\nLet’s compute the values for the height for males which we will store in the object \\(x\\):\nindex \u0026lt;- heights$sex == \u0026quot;Male\u0026quot; x \u0026lt;- heights$height[index] The pre-built functions mean and sd (note that for reasons explained in Section ??, sd divides by length(x)-1 rather than length(x)) can be used here:\nm \u0026lt;- mean(x) s \u0026lt;- sd(x) c(average = m, sd = s) ## average sd ## 69.314755 3.611024 Here is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:\nThe normal distribution does appear to be quite a good approximation here. We now will see how well this approximation works at predicting the proportion of values within intervals.\n Standard units For data that is approximately normally distributed, it is convenient to think in terms of standard units. The standard unit of a value tells us how many standard deviations away from the average it is. Specifically, for a value x from a vector X, we define the value of x in standard units as z = (x - m)/s with m and s the average and standard deviation of X, respectively. Why is this convenient?\nFirst look back at the formula for the normal distribution and note that what is being exponentiated is \\(-z^2/2\\) with \\(z\\) equivalent to \\(x\\) in standard units. Because the maximum of \\(e^{-z^2/2}\\) is when \\(z=0\\), this explains why the maximum of the distribution occurs at the average. It also explains the symmetry since \\(- z^2/2\\) is symmetric around 0. Second, note that if we convert the normally distributed data to standard units, we can quickly know if, for example, a person is about average (\\(z=0\\)), one of the largest (\\(z \\approx 2\\)), one of the smallest (\\(z \\approx -2\\)), or an extremely rare occurrence (\\(z \u0026gt; 3\\) or \\(z \u0026lt; -3\\)). Remember that it does not matter what the original units are, these rules apply to any data that is approximately normal.\nIn R, we can obtain standard units using the function scale:\nz \u0026lt;- scale(x) Now to see how many men are within 2 SDs from the average, we simply type:\nmean(abs(z) \u0026lt; 2) ## [1] 0.9495074 The proportion is about 95%, which is what the normal distribution predicts! To further confirm that, in fact, the approximation is a good one, we can use quantile-quantile plots.\n Quantile-quantile plots A systematic way to assess how well the normal distribution fits the data is to check if the observed and predicted proportions match. In general, this is the approach of the quantile-quantile plot (QQ-plot).\nFirst let’s define the theoretical quantiles for the normal distribution. In statistics books we use the symbol \\(\\Phi(x)\\) to define the function that gives us the probability of a standard normal distribution being smaller than \\(x\\). So, for example, \\(\\Phi(-1.96) = 0.025\\) and \\(\\Phi(1.96) = 0.975\\). In R, we can evaluate \\(\\Phi\\) using the pnorm function:\npnorm(-1.96) ## [1] 0.0249979 The inverse function \\(\\Phi^{-1}(x)\\) gives us the theoretical quantiles for the normal distribution. So, for example, \\(\\Phi^{-1}(0.975) = 1.96\\). In R, we can evaluate the inverse of \\(\\Phi\\) using the qnorm function.\nqnorm(0.975) ## [1] 1.959964 Note that these calculations are for the standard normal distribution by default (mean = 0, standard deviation = 1), but we can also define these for any normal distribution. We can do this using the mean and sd arguments in the pnorm and qnorm function. For example, we can use qnorm to determine quantiles of a distribution with a specific average and standard deviation\nqnorm(0.975, mean = 5, sd = 2) ## [1] 8.919928 For the normal distribution, all the calculations related to quantiles are done without data, thus the name theoretical quantiles. But quantiles can be defined for any distribution, including an empirical one. So if we have data in a vector \\(x\\), we can define the quantile associated with any proportion \\(p\\) as the \\(q\\) for which the proportion of values below \\(q\\) is \\(p\\). Using R code, we can define q as the value for which mean(x \u0026lt;= q) = p. Notice that not all \\(p\\) have a \\(q\\) for which the proportion is exactly \\(p\\). There are several ways of defining the best \\(q\\) as discussed in the help for the quantile function.\nTo give a quick example, for the male heights data, we have that:\nmean(x \u0026lt;= 69.5) ## [1] 0.5147783 So about 50% are shorter or equal to 69 inches. This implies that if \\(p=0.50\\) then \\(q=69.5\\).\nThe idea of a QQ-plot is that if your data is well approximated by normal distribution then the quantiles of your data should be similar to the quantiles of a normal distribution. To construct a QQ-plot, we do the following:\nDefine a vector of \\(m\\) proportions \\(p_1, p_2, \\dots, p_m\\). Define a vector of quantiles \\(q_1, \\dots, q_m\\) for your data for the proportions \\(p_1, \\dots, p_m\\). We refer to these as the sample quantiles. Define a vector of theoretical quantiles for the proportions \\(p_1, \\dots, p_m\\) for a normal distribution with the same average and standard deviation as the data. Plot the sample quantiles versus the theoretical quantiles.  Let’s construct a QQ-plot using R code. Start by defining the vector of proportions.\np \u0026lt;- seq(0.05, 0.95, 0.05) To obtain the quantiles from the data, we can use the quantile function like this:\nsample_quantiles \u0026lt;- quantile(x, p) To obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the qnorm function:\ntheoretical_quantiles \u0026lt;- qnorm(p, mean = mean(x), sd = sd(x)) To see if they match or not, we plot them against each other and draw the identity line:\nqplot(theoretical_quantiles, sample_quantiles) + geom_abline() Notice that this code becomes much cleaner if we use standard units:\nsample_quantiles \u0026lt;- quantile(z, p) theoretical_quantiles \u0026lt;- qnorm(p) qplot(theoretical_quantiles, sample_quantiles) + geom_abline() The above code is included to help describe QQ-plots. However, in practice it is easier to use the ggplot2 code described in Section ??:\nheights %\u0026gt;% filter(sex == \u0026quot;Male\u0026quot;) %\u0026gt;% ggplot(aes(sample = scale(height))) + geom_qq() + geom_abline() While for the illustration above we used 20 quantiles, the default from the geom_qq function is to use as many quantiles as data points.\n Percentiles Before we move on, let’s define some terms that are commonly used in exploratory data analysis.\nPercentiles are special cases of quantiles that are commonly used. The percentiles are the quantiles you obtain when setting the \\(p\\) at \\(0.01, 0.02, ..., 0.99\\). We call, for example, the case of \\(p=0.25\\) the 25th percentile, which gives us a number for which 25% of the data is below. The most famous percentile is the 50th, also known as the median.\nFor the normal distribution the median and average are the same, but this is generally not the case.\nAnother special case that receives a name are the quartiles, which are obtained when setting \\(p=0.25,0.50\\), and \\(0.75\\).\n Boxplots To introduce boxplots we will go back to the US murder data. Suppose we want to summarize the murder rate distribution. Using the data visualization technique we have learned, we can quickly see that the normal approximation does not apply here:\nIn this case, the histogram above or a smooth density plot would serve as a relatively succinct summary.\nNow suppose those used to receiving just two numbers as summaries ask us for a more compact numerical summary.\nHere, some of our wise predecessors have offered their advice. In short, the standard methodology is to provide a five-number summary composed of the range along with the quartiles (the 25th, 50th, and 75th percentiles). Further, ignore outliers when computing the range and instead plot these as independent points.2 Finally, plot these numbers as a “box” with “whiskers” like this:\nwith the box defined by the 25% and 75% percentile and the whiskers showing the range. The distance between these two is called the interquartile range. The median is shown with a horizontal line. Today, we call these boxplots.\nFrom just this simple plot, we know that the median is about 2.5, that the distribution is not symmetric, and that the range is 0 to 5 for the great majority of states with two exceptions.\n Stratification In data analysis we often divide observations into groups based on the values of one or more variables associated with those observations. For example in the next section we divide the height values into groups based on a sex variable: females and males. We call this procedure stratification and refer to the resulting groups as strata.\nStratification is common in data visualization because we are often interested in how the distribution of variables differs across different subgroups. We will see several examples throughout this part of the book. We will revisit the concept of stratification when we learn regression in Chapter ?? and in the Machine Learning part of the book.\n Case study: describing student heights (continued) Using the histogram, density plots, and QQ-plots, we have become convinced that the male height data is well approximated with a normal distribution. In this case, we report back to ET a very succinct summary: male heights follow a normal distribution with an average of 69.3 inches and a SD of 3.6 inches. With this information, ET will have a good idea of what to expect when he meets our male students. However, to provide a complete picture we need to also provide a summary of the female heights.\nWe learned that boxplots are useful when we want to quickly compare two or more distributions. Here are the heights for men and women:\nThe plot immediately reveals that males are, on average, taller than females. The standard deviations appear to be similar. But does the normal approximation also work for the female height data collected by the survey? We expect that they will follow a normal distribution, just like males. However, exploratory plots reveal that the approximation is not as useful:\nWe see something we did not see for the males: the density plot has a second “bump”. Also, the QQ-plot shows that the highest points tend to be taller than expected by the normal distribution. Finally, we also see five points in the QQ-plot that suggest shorter than expected heights for a normal distribution. When reporting back to ET, we might need to provide a histogram rather than just the average and standard deviation for the female heights.\nHowever, go back and read Tukey’s quote. We have noticed what we didn’t expect to see. If we look at other female height distributions, we do find that they are well approximated with a normal distribution. So why are our female students different? Is our class a requirement for the female basketball team? Are small proportions of females claiming to be taller than they are? Another, perhaps more likely, explanation is that in the form students used to enter their heights, FEMALE was the default sex and some males entered their heights, but forgot to change the sex variable. In any case, data visualization has helped discover a potential flaw in our data.\nRegarding the five smallest values, note that these values are:\nheights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% top_n(5, desc(height)) %\u0026gt;% pull(height) ## [1] 51 53 55 52 52 Because these are reported heights, a possibility is that the student meant to enter 5'1\", 5'2\", 5'3\" or 5'5\".\nTRY IT\nDefine variables containing the heights of males and females like this:  library(dslabs) data(heights) male \u0026lt;- heights$height[heights$sex == \u0026quot;Male\u0026quot;] female \u0026lt;- heights$height[heights$sex == \u0026quot;Female\u0026quot;] How many measurements do we have for each?\nSuppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing female_percentiles and male_percentiles with the 10th, 30th, 50th, 70th, \u0026amp; 90th percentiles for each sex. Then create a data frame with these two as columns.\n Study the following boxplots showing population sizes by country:\n  Which continent has the country with the biggest population size?\nWhat continent has the largest median population size?\n What is median population size for Africa to the nearest million?\n What proportion of countries in Europe have populations below 14 million?\n If we use a log transformation, which continent shown above has the largest interquartile range?\n Load the height data set and create a vector x with just the male heights:\n  library(dslabs) data(heights) x \u0026lt;- heights$height[heights$sex==\u0026quot;Male\u0026quot;] What proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and mean.\nSuppose all you know about the data is the average and the standard deviation. Use the normal approximation to estimate the proportion you just calculated. Hint: start by computing the average and standard deviation. Then use the pnorm function to predict the proportions.\n Notice that the approximation calculated in question nine is very close to the exact calculation in the first question. Now perform the same task for more extreme values. Compare the exact calculation and the normal approximation for the interval (79,81]. How many times bigger is the actual proportion than the approximation?\n Approximate the distribution of adult men in the world as normally distributed with an average of 69 inches and a standard deviation of 3 inches. Using this approximation, estimate the proportion of adult men that are 7 feet tall or taller, referred to as seven footers. Hint: use the pnorm function.\n There are about 1 billion men between the ages of 18 and 40 in the world. Use your answer to the previous question to estimate how many of these men (18-40 year olds) are seven feet tall or taller in the world?\n There are about 10 National Basketball Association (NBA) players that are 7 feet tall or higher. Using the answer to the previous two questions, what proportion of the world’s 18-to-40-year-old seven footers are in the NBA?\n Repeat the calculations performed in the previous question for Lebron James’ height: 6 feet 8 inches. There are about 150 players that are at least that tall.\n In answering the previous questions, we found that it is not at all rare for a seven footer to become an NBA player. What would be a fair critique of our calculations:\n  Practice and talent are what make a great basketball player, not height. The normal approximation is not appropriate for heights. As seen in question 10, the normal approximation tends to underestimate the extreme values. It’s possible that there are more seven footers than we predicted. As seen in question 10, the normal approximation tends to overestimate the extreme values. It’s possible that there are fewer seven footers than we predicted.    ggplot2 geometries Alhough we haven’t gone into detain about the ggplot2 package for data visualization, we now will briefly discuss some of the geometries involved in the plots above. We will discuss ggplot2 in (excruciating) detail later this week. For now, we will briefly demonstrate how to generate plots related to distributions.\nBarplots To generate a barplot we can use the geom_bar geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.\nmurders %\u0026gt;% ggplot(aes(region)) + geom_bar() We often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:\ndata(murders) tab \u0026lt;- murders %\u0026gt;% count(region) %\u0026gt;% mutate(proportion = n/sum(n)) tab ## region n proportion ## 1 Northeast 9 0.1764706 ## 2 South 17 0.3333333 ## 3 North Central 12 0.2352941 ## 4 West 13 0.2549020 We no longer want geom_bar to count, but rather just plot a bar to the height provided by the proportion variable. For this we need to provide x (the categories) and y (the values) and use the stat=\"identity\" option.\ntab %\u0026gt;% ggplot(aes(region, proportion)) + geom_bar(stat = \u0026quot;identity\u0026quot;)  Histograms To generate histograms we use geom_histogram. By looking at the help file for this function, we learn that the only required argument is x, the variable for which we will construct a histogram. We dropped the x because we know it is the first argument. The code looks like this:\nheights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% ggplot(aes(height)) + geom_histogram() If we run the code above, it gives us a message:\n stat_bin() using bins = 30. Pick better value with binwidth.\n We previously used a bin size of 1 inch, so the code looks like this:\nheights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% ggplot(aes(height)) + geom_histogram(binwidth = 1) Finally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:\nheights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% ggplot(aes(height)) + geom_histogram(binwidth = 1, fill = \u0026quot;blue\u0026quot;, col = \u0026quot;black\u0026quot;) + xlab(\u0026quot;Male heights in inches\u0026quot;) + ggtitle(\u0026quot;Histogram\u0026quot;)  Density plots To create a smooth density, we use the geom_density. To make a smooth density plot with the data previously shown as a histogram we can use this code:\nheights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% ggplot(aes(height)) + geom_density() To fill in with color, we can use the fill argument.\nheights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% ggplot(aes(height)) + geom_density(fill=\u0026quot;blue\u0026quot;) To change the smoothness of the density, we use the adjust argument to multiply the default value by that adjust. For example, if we want the bandwidth to be twice as big we use:\nheights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) + geom_density(fill=\u0026quot;blue\u0026quot;, adjust = 2)  Boxplots The geometry for boxplot is geom_boxplot. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments x as the categories, and y as the values.\n QQ-plots For qq-plots we use the geom_qq geometry. From the help file, we learn that we need to specify the sample (we will learn about samples in a later bit of the course). Here is the qqplot for men heights.\nheights %\u0026gt;% filter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;% ggplot(aes(sample = height)) + geom_qq() By default, the sample variable is compared to a normal distribution with average 0 and standard deviation 1. To change this, we use the dparams arguments based on the help file. Adding an identity line is as simple as assigning another layer. For straight lines, we use the geom_abline function. The default line is the identity line (slope = 1, intercept = 0).\nparams \u0026lt;- heights %\u0026gt;% filter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;% summarize(mean = mean(height), sd = sd(height)) heights %\u0026gt;% filter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;% ggplot(aes(sample = height)) + geom_qq(dparams = params) + geom_abline() Another option here is to scale the data first and then make a qqplot against the standard normal.\nheights %\u0026gt;% filter(sex==\u0026quot;Male\u0026quot;) %\u0026gt;% ggplot(aes(sample = scale(height))) + geom_qq() + geom_abline()    Data visualization principles We have already provided some rules to follow as we created plots for our examples. Here, we aim to provide some general principles we can use as a guide for effective data visualization. Much of this section is based on a talk by Karl Broman3 titled “Creating Effective Figures and Tables”4 and includes some of the figures which were made with code that Karl makes available on his GitHub repository5, as well as class notes from Peter Aldhous’ Introduction to Data Visualization course6. Following Karl’s approach, we show some examples of plot styles we should avoid, explain how to improve them, and use these as motivation for a list of principles. We compare and contrast plots that follow these principles to those that don’t.\nThe principles are mostly based on research related to how humans detect patterns and make visual comparisons. The preferred approaches are those that best fit the way our brains process visual information. When deciding on a visualization approach, it is also important to keep our goal in mind. We may be comparing a viewable number of quantities, describing distributions for categories or numeric values, comparing the data from two groups, or describing the relationship between two variables. As a final note, we want to emphasize that for a data scientist it is important to adapt and optimize graphs to the audience. For example, an exploratory plot made for ourselves will be different than a chart intended to communicate a finding to a general audience.\nAs with the discussion above, we will be using these libraries—note the addition of gridExtra:\nlibrary(tidyverse) library(dslabs) library(gridExtra) Encoding data using visual cues We start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue.\nTo illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:\nHere we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. The donut chart is an example of a plot that uses only area:\nTo see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In fact, the pie R function help file states that:\n Pie charts are a very bad way of displaying information. The eye is good at judging linear measures and bad at judging relative areas. A bar chart or dot chart is a preferable way of displaying this type of data.\n In this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy:\n  Browser  2000  2015      Opera  3  2    Safari  21  22    Firefox  23  21    Chrome  26  29    IE  28  27     The preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.\nNotice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.\nIf for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:\nIn general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once.\n Know when to include 0 When using barplots, it is misinformative not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference. Below is an illustrative example used by Peter Aldhous in this lecture: http://paldhous.github.io/ucb/2016/dataviz/week2.html.\n(Source: Fox News, via Media Matters7.)\nFrom the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:\nHere is another example, described in detail in a Flowing Data blog post:\n(Source: Fox News, via Flowing Data8.)\nThis plot makes a 13% increase look like a five fold change. Here is the appropriate plot:\nFinally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change:\n(Source: Venezolana de Televisión via Pakistan Today9 and Diego Mariano.)\nHere is the appropriate plot:\nWhen using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:\nNote that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability.\n Do not distort quantities During President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations:\n(Source: The 2011 State of the Union Address10)\nJudging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:\ngdp \u0026lt;- c(14.6, 5.7, 5.3, 3.3, 2.5) gdp_data \u0026lt;- data.frame(Country = rep(c(\u0026quot;United States\u0026quot;, \u0026quot;China\u0026quot;, \u0026quot;Japan\u0026quot;, \u0026quot;Germany\u0026quot;, \u0026quot;France\u0026quot;),2), y = factor(rep(c(\u0026quot;Radius\u0026quot;,\u0026quot;Area\u0026quot;),each=5), levels = c(\u0026quot;Radius\u0026quot;, \u0026quot;Area\u0026quot;)), GDP= c(gdp^2/min(gdp^2), gdp/min(gdp))) %\u0026gt;% mutate(Country = reorder(Country, GDP)) gdp_data %\u0026gt;% ggplot(aes(Country, y, size = GDP)) + geom_point(show.legend = FALSE, color = \u0026quot;blue\u0026quot;) + scale_size(range = c(2,25)) + coord_flip() + ylab(\u0026quot;\u0026quot;) + xlab(\u0026quot;\u0026quot;) Not surprisingly, ggplot2 defaults to using area rather than radius. Of course, in this case, we really should not be using area at all since we can use position and length:\ngdp_data %\u0026gt;% filter(y == \u0026quot;Area\u0026quot;) %\u0026gt;% ggplot(aes(Country, GDP)) + geom_bar(stat = \u0026quot;identity\u0026quot;, width = 0.5) + ylab(\u0026quot;GDP in trillions of US dollars\u0026quot;)  Order categories by a meaningful value When one of the axes is used to show categories, as is done in barplots, the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.\nWe previously learned how to use the reorder function, which helps us achieve this goal. To appreciate how the right order can help convey a message, suppose we want to create a plot to compare the murder rate across states. We are particularly interested in the most dangerous and safest states. Note the difference when we order alphabetically (the default) versus when we order by the actual rate:\ndata(murders) p1 \u0026lt;- murders %\u0026gt;% mutate(murder_rate = total / population * 100000) %\u0026gt;% ggplot(aes(state, murder_rate)) + geom_bar(stat=\u0026quot;identity\u0026quot;) + coord_flip() + theme(axis.text.y = element_text(size = 8)) + xlab(\u0026quot;\u0026quot;) p2 \u0026lt;- murders %\u0026gt;% mutate(murder_rate = total / population * 100000) %\u0026gt;% mutate(state = reorder(state, murder_rate)) %\u0026gt;% ggplot(aes(state, murder_rate)) + geom_bar(stat=\u0026quot;identity\u0026quot;) + coord_flip() + theme(axis.text.y = element_text(size = 8)) + xlab(\u0026quot;\u0026quot;) grid.arrange(p1, p2, ncol = 2) We can make the second plot like this:\ndata(murders) murders %\u0026gt;% mutate(murder_rate = total / population * 100000) %\u0026gt;% mutate(state = reorder(state, murder_rate)) %\u0026gt;% ggplot(aes(state, murder_rate)) + geom_bar(stat=\u0026quot;identity\u0026quot;) + coord_flip() + theme(axis.text.y = element_text(size = 6)) + xlab(\u0026quot;\u0026quot;) The reorder function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other:\nThe first orders the regions alphabetically, while the second orders them by the group’s median.\n Show the data We have focused on displaying single quantities across categories. We now shift our attention to displaying data, with a focus on comparing groups.\nTo motivate our first principle, “show the data”, we go back to our artificial example of describing heights to a person who is unaware of some basic facts about the population of interest (and is otherwise unsophisticated). This time let’s assume that this person is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors.11 The plot looks like this:\n## `summarise()` ungrouping output (override with `.groups` argument) The average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.\nThis brings us to our first principle: show the data. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points:\nheights %\u0026gt;% ggplot(aes(sex, height)) + geom_point() For example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.\nThe first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:\nheights %\u0026gt;% ggplot(aes(sex, height)) + geom_jitter(width = 0.1, alpha = 0.2) Now we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer.\n Ease comparisons Use common axes Since there are so many points, it is more effective to show distributions rather than individual points. We therefore show histograms for each group:\nHowever, from this plot it is not immediately obvious that males are, on average, taller than females. We have to look carefully to notice that the x-axis has a higher range of values in the male histogram. An important principle here is to keep the axes the same when comparing data across two plots. Below we see how the comparison becomes easier:\n Align plots vertically to see horizontal changes and horizontally to see vertical changes In these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:\nheights %\u0026gt;% ggplot(aes(height, ..density..)) + geom_histogram(binwidth = 1, color=\u0026quot;black\u0026quot;) + facet_grid(sex~.) This plot makes it much easier to notice that men are, on average, taller.\nIf , we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points:\n heights %\u0026gt;% ggplot(aes(sex, height)) + geom_boxplot(coef=3) + geom_jitter(width = 0.1, alpha = 0.2) + ylab(\u0026quot;Height in inches\u0026quot;) Now contrast and compare these three plots, based on exactly the same data:\nNotice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions.\n Consider transformations We have motivated the use of the log transformation in cases where the changes are multiplicative. Population size was an example in which we found a log transformation to yield a more informative transformation.\nThe combination of an incorrectly chosen barplot and a failure to use a log transformation when one is merited can be particularly distorting. As an example, consider this barplot showing the average population sizes for each continent in 2015:\n## `summarise()` ungrouping output (override with `.groups` argument) From this plot, one would conclude that countries in Asia are much more populous than in other continents. Following the show the data principle, we quickly notice that this is due to two very large countries, which we assume are India and China:\nUsing a log transformation here provides a much more informative plot. We compare the original barplot to a boxplot using the log scale transformation for the y-axis:\nWith the new plot, we realize that countries in Africa actually have a larger median population size than those in Asia.\nOther transformations you should consider are the logistic transformation (logit), useful to better see fold changes in odds, and the square root transformation (sqrt), useful for count data.\n Visual cues to be compared should be adjacent For each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.\nThe default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:\n Use color The comparison becomes even easier to make if we use color to denote the two things we want to compare:\n  Think of the color blind About 10% of the population is color blind. Unfortunately, the default colors used in ggplot2 are not optimal for this group. However, ggplot2 does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette:\ncolor_blind_friendly_cols \u0026lt;- c(\u0026quot;#999999\u0026quot;, \u0026quot;#E69F00\u0026quot;, \u0026quot;#56B4E9\u0026quot;, \u0026quot;#009E73\u0026quot;, \u0026quot;#F0E442\u0026quot;, \u0026quot;#0072B2\u0026quot;, \u0026quot;#D55E00\u0026quot;, \u0026quot;#CC79A7\u0026quot;) Here are the colors There are several resources that can help you select colors, for example this one: http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/.\n Plots for two variables In general, you should use scatterplots to visualize the relationship between two variables. In every single instance in which we have examined the relationship between two variables, including total murders versus population size, life expectancy versus fertility rates, and infant mortality versus income, we have used scatterplots. This is the plot we generally recommend. However, there are some exceptions and we describe two alternative plots here: the slope chart and the Bland-Altman plot.\nSlope charts One exception where another type of plot may be more informative is when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a slope chart.\nThere is no geometry for slope charts in ggplot2, but we can construct one using geom_line. We need to do some tinkering to add labels. Below is an example comparing 2010 to 2015 for large western countries:\nwest \u0026lt;- c(\u0026quot;Western Europe\u0026quot;,\u0026quot;Northern Europe\u0026quot;,\u0026quot;Southern Europe\u0026quot;, \u0026quot;Northern America\u0026quot;,\u0026quot;Australia and New Zealand\u0026quot;) dat \u0026lt;- gapminder %\u0026gt;% filter(year%in% c(2010, 2015) \u0026amp; region %in% west \u0026amp; !is.na(life_expectancy) \u0026amp; population \u0026gt; 10^7) dat %\u0026gt;% mutate(location = ifelse(year == 2010, 1, 2), location = ifelse(year == 2015 \u0026amp; country %in% c(\u0026quot;United Kingdom\u0026quot;, \u0026quot;Portugal\u0026quot;), location+0.22, location), hjust = ifelse(year == 2010, 1, 0)) %\u0026gt;% mutate(year = as.factor(year)) %\u0026gt;% ggplot(aes(year, life_expectancy, group = country)) + geom_line(aes(color = country), show.legend = FALSE) + geom_text(aes(x = location, label = country, hjust = hjust), show.legend = FALSE) + xlab(\u0026quot;\u0026quot;) + ylab(\u0026quot;Life Expectancy\u0026quot;) An advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot:\nIn the scatterplot, we have followed the principle use common axes since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines.\n Bland-Altman plot Since we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also known as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average:\nlibrary(ggrepel) dat %\u0026gt;% mutate(year = paste0(\u0026quot;life_expectancy_\u0026quot;, year)) %\u0026gt;% select(country, year, life_expectancy) %\u0026gt;% spread(year, life_expectancy) %\u0026gt;% mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2, difference = life_expectancy_2015 - life_expectancy_2010) %\u0026gt;% ggplot(aes(average, difference, label = country)) + geom_point() + geom_text_repel() + geom_abline(lty = 2) + xlab(\u0026quot;Average of 2010 and 2015\u0026quot;) + ylab(\u0026quot;Difference between 2015 and 2010\u0026quot;) Here, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis.\n  Encoding a third variable An earlier scatterplot showed the relationship between infant survival and average income. Below is a version of this plot that encodes three variables: OPEC membership, region, and population.\nWe encode categorical variables with color and shape. These shapes can be controlled with shape argument. Below are the shapes available for use in R. For the last five, the color goes inside.\nFor continuous variables, we can use color, intensity, or size. We now show an example of how we do this with a case study.\nWhen selecting colors to quantify a numeric variable, we choose between two options: sequential and diverging. Sequential colors are suited for data that goes from high to low. High values are clearly distinguished from low values. Here are some examples offered by the package RColorBrewer:\nlibrary(RColorBrewer) display.brewer.all(type=\u0026quot;seq\u0026quot;) Diverging colors are used to represent values that diverge from a center. We put equal emphasis on both ends of the data range: higher than the center and lower than the center. An example of when we would use a divergent pattern would be if we were to show height in standard deviations away from the average. Here are some examples of divergent patterns:\nlibrary(RColorBrewer) display.brewer.all(type=\u0026quot;div\u0026quot;)  Avoid pseudo-three-dimensional plots The figure below, taken from the scientific literature12, shows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.\n(Image courtesy of Karl Broman)\nHumans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:\n##First read data url \u0026lt;- \u0026quot;https://github.com/kbroman/Talk_Graphs/raw/master/R/fig8dat.csv\u0026quot; dat \u0026lt;- read.csv(url) ##Now make alternative plot dat %\u0026gt;% gather(drug, survival, -log.dose) %\u0026gt;% mutate(drug = gsub(\u0026quot;Drug.\u0026quot;,\u0026quot;\u0026quot;,drug)) %\u0026gt;% ggplot(aes(log.dose, survival, color = drug)) + geom_line() Notice how much easier it is to determine the survival values.\nPseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:\n(Images courtesy of Karl Broman)\n Avoid too many significant digits By default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:\n  state  year  Measles  Pertussis  Polio      California  1940  37.8826320  18.3397861  0.8266512    California  1950  13.9124205  4.7467350  1.9742639    California  1960  14.1386471  NA  0.2640419    California  1970  0.9767889  NA  NA    California  1980  0.3743467  0.0515466  NA     We are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:\n  state  year  Measles  Pertussis  Polio      California  1940  37.9  18.3  0.8    California  1950  13.9  4.7  2.0    California  1960  14.1  NA  0.3    California  1970  1.0  NA  NA    California  1980  0.4  0.1  NA     Useful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3).\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:\n  state  disease  1940  1950  1960  1970  1980      California  Measles  37.9  13.9  14.1  1  0.4    California  Pertussis  18.3  4.7  NA  NA  0.1    California  Polio  0.8  2.0  0.3  NA  NA      Know your audience Graphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot.\nAs a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest.\nTRY IT\nFor these exercises, we will be using the vaccines data in the dslabs package:\nlibrary(dslabs) data(us_contagious_diseases) Pie charts are appropriate:  When we want to display percentages. When ggplot2 is not available. When I am in a bakery. Never. Barplots and tables are always better.  What is the problem with the plot below:  The values are wrong. The final vote was 306 to 232. The axis does not start at 0. Judging by the length, it appears Trump received 3 times as many votes when, in fact, it was about 30% more. The colors should be the same. Percentages should be shown as a pie chart.  Take a look at the following two plots. They show the same information: 1928 rates of measles across the 50 states.  Which plot is easier to read if you are interested in determining which are the best and worst states in terms of rates, and why?\nThey provide the same information, so they are both equally as good. The plot on the right is better because it orders the states alphabetically. The plot on the right is better because alphabetical order has nothing to do with the disease and by ordering according to actual rate, we quickly see the states with most and least rates. Both plots should be a pie chart.  To make the plot on the left, we have to reorder the levels of the states’ variables.  dat \u0026lt;- us_contagious_diseases %\u0026gt;% filter(year == 1967 \u0026amp; disease==\u0026quot;Measles\u0026quot; \u0026amp; !is.na(population)) %\u0026gt;% mutate(rate = count / population * 10000 * 52 / weeks_reporting) Note what happens when we make a barplot:\ndat %\u0026gt;% ggplot(aes(state, rate)) + geom_bar(stat=\u0026quot;identity\u0026quot;) + coord_flip() Define these objects:\nstate \u0026lt;- dat$state rate \u0026lt;- dat$count/dat$population*10000*52/dat$weeks_reporting Redefine the state object so that the levels are re-ordered. Print the new object state and its levels so you can see that the vector is not re-ordered by the levels.\nNow with one line of code, define the dat table as done above, but change the use mutate to create a rate variable and re-order the state variable so that the levels are re-ordered by this variable. Then make a barplot using the code above, but for this new dat.\n Say we are interested in comparing gun homicide rates across regions of the US. We see this plot:\n  library(dslabs) data(\u0026quot;murders\u0026quot;) murders %\u0026gt;% mutate(rate = total/population*100000) %\u0026gt;% group_by(region) %\u0026gt;% summarize(avg = mean(rate)) %\u0026gt;% mutate(region = factor(region)) %\u0026gt;% ggplot(aes(region, avg)) + geom_bar(stat=\u0026quot;identity\u0026quot;) + ylab(\u0026quot;Murder Rate Average\u0026quot;) ## `summarise()` ungrouping output (override with `.groups` argument) and decide to move to a state in the western region. What is the main problem with this interpretation?\nThe categories are ordered alphabetically. The graph does not show standarad errors. It does not show all the data. We do not see the variability within a region and it’s possible that the safest states are not in the West. The Northeast has the lowest average.  Make a boxplot of the murder rates defined as  data(\u0026quot;murders\u0026quot;) murders %\u0026gt;% mutate(rate = total/population*100000) by region, showing all the points and ordering the regions by their median rate.\nThe plots below show three continuous variables.  The line \\(x=2\\) appears to separate the points. But it is actually not the case, which we can see by plotting the data in a couple of two-dimensional points.\nWhy is this happening?\nHumans are not good at reading pseudo-3D plots. There must be an error in the code. The colors confuse us. Scatterplots should not be used to compare two variables when we have access to 3.      Keep in mind that discrete numeric data can be considered ordinal. Although this is technically true, we usually reserve the term ordinal data for variables belonging to a small number of different groups, with each group having many members. In contrast, when we have many groups with few cases in each group, we typically refer to them as discrete numerical variables. So, for example, the number of packs of cigarettes a person smokes a day, rounded to the closest pack, would be considered ordinal, while the actual number of cigarettes would be considered a numerical variable. But, indeed, there are examples that can be considered both numerical and ordinal when it comes to visualizing data.↩︎\n We provide a detailed explanation of outliers later.↩︎\n http://kbroman.org/↩︎\n https://www.biostat.wisc.edu/~kbroman/presentations/graphs2017.pdf↩︎\n https://github.com/kbroman/Talk_Graphs↩︎\n http://paldhous.github.io/ucb/2016/dataviz/index.html↩︎\n http://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507↩︎\n http://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/↩︎\n https://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote↩︎\n https://www.youtube.com/watch?v=kl2g40GoRxg↩︎\n If you’re unfamiliar, standard errors are defined later in the course—do not confuse them with the standard deviation of the data.↩︎\n https://projecteuclid.org/download/pdf_1/euclid.ss/1177010488↩︎\n   ","date":1600128000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600176491,"objectID":"f4445f225dbc16ad343f16f0677c881d","permalink":"/content/02-content/","publishdate":"2020-09-15T00:00:00Z","relpermalink":"/content/02-content/","section":"content","summary":"Readings Guiding Questions Slides  Visualizing data distributions Variable types Case study: describing student heights Distribution function Cumulative distribution functions Histograms Smoothed density Interpreting the y-axis Densities permit stratification  The normal distribution Standard units Quantile-quantile plots Percentiles Boxplots Stratification Case study: describing student heights (continued) ggplot2 geometries Barplots Histograms Density plots Boxplots QQ-plots   Data visualization principles Encoding data using visual cues Know when to include 0 Do not distort quantities Order categories by a meaningful value Show the data Ease comparisons Use common axes Align plots vertically to see horizontal changes and horizontally to see vertical changes Consider transformations Visual cues to be compared should be adjacent Use color  Think of the color blind Plots for two variables Slope charts Bland-Altman plot  Encoding a third variable Avoid pseudo-three-dimensional plots Avoid too many significant digits Know your audience    Readings  This page.","tags":null,"title":"Effective Visualizations","type":"docs"},{"authors":null,"categories":null,"content":"  Readings Guiding Question  Slides The tidyverse Tidy data Manipulating data frames Adding a column with mutate Subsetting with filter Selecting columns with select  The pipe: %\u0026gt;% Summarizing data summarize pull Group then summarize with group_by  Sorting data frames Nested sorting The top \\(n\\)  Tibbles Tibbles display better Subsets of tibbles are tibbles Tibbles can have complex entries Tibbles can be grouped Create a tibble using tibble instead of data.frame  The dot operator do The purrr package Tidyverse conditionals case_when between   Videos   Readings  This page. Link to survey about teams. Chapter 1 of Introduction to Statistical Learning, available here.  Guiding Question For future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n Why do we want tidy data? What are the challenges associated with shaping things into a tidy format?    Slides The slides below are a much broader overview about goals for the class and thinking about relationships that we might explore with data. The technical aspects of this lecture will be explored in greater detail in the Thursday practical lecture.\nOverview       NOTE\nThis week’s content is split into two “halves”: the critical data manipulation information contained below and a more-entertaining discussion of visualization included in the Exercises.\n  The tidyverse In the first weeks’ content, we demonstrated how to manipulate vectors by reordering and subsetting them through indexing. However, once we start more advanced analyses, the preferred unit for data storage is not the vector but the data frame. In this lecture, we learn to work directly with data frames, which greatly facilitate the organization of information. We will be using data frames for the majority of this class and you will use them for the majority of your data science life (however long that might be). We will focus on a specific data format referred to as tidy and on specific collection of packages that are particularly helpful for working with tidy data referred to as the tidyverse.\nWe can load all the tidyverse packages at once by installing and loading the tidyverse package:1\nlibrary(tidyverse) We will learn how to implement the tidyverse approach throughout the book, but before delving into the details, in this chapter we introduce some of the most widely used tidyverse functionality, starting with the dplyr package for manipulating data frames and the purrr package for working with functions. Note that the tidyverse also includes a graphing package, ggplot2, which we introduce later in Chapter ?? in the Data Visualization part of the book; the readr package discussed in Chapter ??; and many others. In this chapter, we first introduce the concept of tidy data and then demonstrate how we use the tidyverse to work with data frames in this format.\nTidy data We say that a data table is in tidy format if each row represents one observation and columns represent the different variables available for each of these observations. The murders dataset is an example of a tidy data frame.\nlibrary(dslabs) data(murders) head(murders) ## state abb region population total ## 1 Alabama AL South 4779736 135 ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232 ## 4 Arkansas AR South 2915918 93 ## 5 California CA West 37253956 1257 ## 6 Colorado CO West 5029196 65 Each row represent a state with each of the five columns providing a different variable related to these states: name, abbreviation, region, population, and total murders.\nTo see how the same information can be provided in different formats, consider the following example:\nlibrary(dslabs) data(\u0026quot;gapminder\u0026quot;) tidy_data \u0026lt;- gapminder %\u0026gt;% filter(country %in% c(\u0026quot;South Korea\u0026quot;, \u0026quot;Germany\u0026quot;) \u0026amp; !is.na(fertility)) %\u0026gt;% select(country, year, fertility) head(tidy_data, 6) ## country year fertility ## 1 Germany 1960 2.41 ## 2 South Korea 1960 6.16 ## 3 Germany 1961 2.44 ## 4 South Korea 1961 5.99 ## 5 Germany 1962 2.47 ## 6 South Korea 1962 5.79 This tidy dataset provides fertility rates for two countries across the years. This is a tidy dataset because each row presents one observation with the three variables being country, year, and fertility rate. However, this dataset originally came in another format and was reshaped for the dslabs package. Originally, the data was in the following format:\n## country 1960 1961 1962 ## 1 Germany 2.41 2.44 2.47 ## 2 South Korea 6.16 5.99 5.79 The same information is provided, but there are two important differences in the format: 1) each row includes several observations and 2) one of the variables, year, is stored in the header. For the tidyverse packages to be optimally used, data need to be reshaped into tidy format, which you will learn to do throughout this course. For starters, though, we will use example datasets that are already in tidy format.\nAlthough not immediately obvious, as you go through the book you will start to appreciate the advantages of working in a framework in which functions use tidy formats for both inputs and outputs. You will see how this permits the data analyst to focus on more important aspects of the analysis rather than the format of the data.\nTRY IT\nExamine the built-in dataset co2. Which of the following is true:  co2 is tidy data: it has one year for each row. co2 is not tidy: we need at least one column with a character vector. co2 is not tidy: it is a matrix instead of a data frame. co2 is not tidy: to be tidy we would have to wrangle it to have three columns (year, month and value), then each co2 observation would have a row.  Examine the built-in dataset ChickWeight. Which of the following is true:  ChickWeight is not tidy: each chick has more than one row. ChickWeight is tidy: each observation (a weight) is represented by one row. The chick from which this measurement came is one of the variables. ChickWeight is not tidy: we are missing the year column. ChickWeight is tidy: it is stored in a data frame.  Examine the built-in dataset BOD. Which of the following is true:  BOD is not tidy: it only has six rows. BOD is not tidy: the first column is just an index. BOD is tidy: each row is an observation with two values (time and demand) BOD is tidy: all small datasets are tidy by definition.  Which of the following built-in datasets is tidy (you can pick more than one):  BJsales EuStockMarkets DNase Formaldehyde Orange UCBAdmissions    Manipulating data frames The dplyr package from the tidyverse introduces functions that perform some of the most common operations when working with data frames and uses names for these functions that are relatively easy to remember. For instance, to change the data table by adding a new column, we use mutate. To filter the data table to a subset of rows, we use filter. Finally, to subset the data by selecting specific columns, we use select.\nAdding a column with mutate We want all the necessary information for our analysis to be included in the data table. So the first task is to add the murder rates to our murders data frame. The function mutate takes the data frame as a first argument and the name and values of the variable as a second argument using the convention name = values. So, to add murder rates, we use:\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;) murders \u0026lt;- mutate(murders, rate = total / population * 100000) Notice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error?\nThis is one of dplyr’s main features. Functions in this package, such as mutate, know to look for variables in the data frame provided in the first argument. In the call to mutate above, total will have the values in murders$total. This approach makes the code much more readable.\nWe can see that the new column is added:\nhead(murders) ## state abb region population total rate ## 1 Alabama AL South 4779736 135 2.824424 ## 2 Alaska AK West 710231 19 2.675186 ## 3 Arizona AZ West 6392017 232 3.629527 ## 4 Arkansas AR South 2915918 93 3.189390 ## 5 California CA West 37253956 1257 3.374138 ## 6 Colorado CO West 5029196 65 1.292453 Although we have overwritten the original murders object, this does not change the object that loaded with data(murders). If we load the murders data again, the original will overwrite our mutated version.\n Subsetting with filter Now suppose that we want to filter the data table to only show the entries for which the murder rate is lower than 0.71. To do this we use the filter function, which takes the data table as the first argument and then the conditional statement as the second. Like mutate, we can use the unquoted variable names from murders inside the function and it will know we mean the columns and not objects in the workspace.\nfilter(murders, rate \u0026lt;= 0.71) ## state abb region population total rate ## 1 Hawaii HI West 1360301 7 0.5145920 ## 2 Iowa IA North Central 3046355 21 0.6893484 ## 3 New Hampshire NH Northeast 1316470 5 0.3798036 ## 4 North Dakota ND North Central 672591 4 0.5947151 ## 5 Vermont VT Northeast 625741 2 0.3196211  Selecting columns with select Although our data table only has six columns, some data tables include hundreds. If we want to view just a few, we can use the dplyr select function. In the code below we select three columns, assign this to a new object and then filter the new object:\nnew_table \u0026lt;- select(murders, state, region, rate) filter(new_table, rate \u0026lt;= 0.71) ## state region rate ## 1 Hawaii West 0.5145920 ## 2 Iowa North Central 0.6893484 ## 3 New Hampshire Northeast 0.3798036 ## 4 North Dakota North Central 0.5947151 ## 5 Vermont Northeast 0.3196211 In the call to select, the first argument murders is an object, but state, region, and rate are variable names.\nTRY IT\nLoad the dplyr package and the murders dataset.  library(dplyr) library(dslabs) data(murders) You can add columns using the dplyr function mutate. This function is aware of the column names and inside the function you can call them unquoted:\nmurders \u0026lt;- mutate(murders, population_in_millions = population / 10^6) We can write population rather than murders$population. The function mutate knows we are grabbing columns from murders.\nUse the function mutate to add a murders column named rate with the per 100,000 murder rate as in the example code above. Make sure you redefine murders as done in the example code above ( murders \u0026lt;- [your code]) so we can keep using this variable.\nIf rank(x) gives you the ranks of x from lowest to highest, rank(-x) gives you the ranks from highest to lowest. Use the function mutate to add a column rank containing the rank, from highest to lowest murder rate. Make sure you redefine murders so we can keep using this variable.\n With dplyr, we can use select to show only certain columns. For example, with this code we would only show the states and population sizes:\n  select(murders, state, population) %\u0026gt;% head() Use select to show the state names and abbreviations in murders. Do not redefine murders, just show the results.\nThe dplyr function filter is used to choose specific rows of the data frame to keep. Unlike select which is for columns, filter is for rows. For example, you can show just the New York row like this:  filter(murders, state == \u0026quot;New York\u0026quot;) You can use other logical vectors to filter rows.\nUse filter to show the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:  no_florida \u0026lt;- filter(murders, state != \u0026quot;Florida\u0026quot;) Create a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\nWe can also use %in% to filter with dplyr. You can therefore see the data from New York and Texas like this:  filter(murders, state %in% c(\u0026quot;New York\u0026quot;, \u0026quot;Texas\u0026quot;)) Create a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.  filter(murders, population \u0026lt; 5000000 \u0026amp; region == \u0026quot;Northeast\u0026quot;) Make sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: it is in the Northeast or West and the murder rate is less than 1. Use select to show only the state name, the rate, and the rank.\n   The pipe: %\u0026gt;% With dplyr we can perform a series of operations, for example select and then filter, by sending the results of one function to another using what is called the pipe operator: %\u0026gt;%. Some details are included below.\nWe wrote code above to show three variables (state, region, rate) for states that have murder rates below 0.71. To do this, we defined the intermediate object new_table. In dplyr we can write code that looks more like a description of what we want to do without intermediate objects:\n\\[ \\mbox{original data } \\rightarrow \\mbox{ select } \\rightarrow \\mbox{ filter } \\]\nFor such an operation, we can use the pipe %\u0026gt;%. The code looks like this:\nmurders %\u0026gt;% select(state, region, rate) %\u0026gt;% filter(rate \u0026lt;= 0.71) ## state region rate ## 1 Hawaii West 0.5145920 ## 2 Iowa North Central 0.6893484 ## 3 New Hampshire Northeast 0.3798036 ## 4 North Dakota North Central 0.5947151 ## 5 Vermont Northeast 0.3196211 This line of code is equivalent to the two lines of code above. What is going on here?\nIn general, the pipe sends the result of the left side of the pipe to be the first argument of the function on the right side of the pipe. Here is a very simple example:\n16 %\u0026gt;% sqrt() ## [1] 4 We can continue to pipe values along:\n16 %\u0026gt;% sqrt() %\u0026gt;% log2() ## [1] 2 The above statement is equivalent to log2(sqrt(16)).\nRemember that the pipe sends values to the first argument, so we can define other arguments as if the first argument is already defined:\n16 %\u0026gt;% sqrt() %\u0026gt;% log(base = 2) ## [1] 2 Therefore, when using the pipe with data frames and dplyr, we no longer need to specify the required first argument since the dplyr functions we have described all take the data as the first argument. In the code we wrote:\nmurders %\u0026gt;% select(state, region, rate) %\u0026gt;% filter(rate \u0026lt;= 0.71) murders is the first argument of the select function, and the new data frame (formerly new_table) is the first argument of the filter function.\nNote that the pipe works well with functions where the first argument is the input data. Functions in tidyverse packages like dplyr have this format and can be used easily with the pipe.\nTRY IT\nThe pipe %\u0026gt;% can be used to perform operations sequentially without having to define intermediate objects. Start by redefining murder to include rate and rank.  murders \u0026lt;- mutate(murders, rate = total / population * 100000, rank = rank(-rate)) In the solution to the previous exercise, we did the following:\nmy_states \u0026lt;- filter(murders, region %in% c(\u0026quot;Northeast\u0026quot;, \u0026quot;West\u0026quot;) \u0026amp; rate \u0026lt; 1) select(my_states, state, rate, rank) The pipe %\u0026gt;% permits us to perform both operations sequentially without having to define an intermediate variable my_states. We therefore could have mutated and selected in the same line like this:\nmutate(murders, rate = total / population * 100000, rank = rank(-rate)) %\u0026gt;% select(state, rate, rank) Notice that select no longer has a data frame as the first argument. The first argument is assumed to be the result of the operation conducted right before the %\u0026gt;%.\nRepeat the previous exercise, but now instead of creating a new object, show the result and only include the state, rate, and rank columns. Use a pipe %\u0026gt;% to do this in just one line.\nReset murders to the original table by using data(murders). Use a pipe to create a new data frame called my_states that considers only states in the Northeast or West which have a murder rate lower than 1, and contains only the state, rate and rank columns. The pipe should also have four components separated by three %\u0026gt;%. The code should look something like this:  my_states \u0026lt;- murders %\u0026gt;% mutate SOMETHING %\u0026gt;% filter SOMETHING %\u0026gt;% select SOMETHING   Summarizing data An important part of exploratory data analysis is summarizing data. The average and standard deviation are two examples of widely used summary statistics. More informative summaries can often be achieved by first splitting data into groups. In this section, we cover two new dplyr verbs that make these computations easier: summarize and group_by. We learn to access resulting values using the pull function.\nsummarize The summarize function in dplyr provides a way to compute summary statistics with intuitive and readable code. We start with a simple example based on heights. The heights dataset includes heights and sex reported by students in an in-class survey.\nlibrary(dplyr) library(dslabs) data(heights) The following code computes the average and standard deviation for females:\ns \u0026lt;- heights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% summarize(average = mean(height), standard_deviation = sd(height)) s ## average standard_deviation ## 1 64.93942 3.760656 This takes our original data table as input, filters it to keep only females, and then produces a new summarized table with just the average and the standard deviation of heights. We get to choose the names of the columns of the resulting table. For example, above we decided to use average and standard_deviation, but we could have used other names just the same.\nBecause the resulting table stored in s is a data frame, we can access the components with the accessor $:\ns$average ## [1] 64.93942 s$standard_deviation ## [1] 3.760656 As with most other dplyr functions, summarize is aware of the variable names and we can use them directly. So when inside the call to the summarize function we write mean(height), the function is accessing the column with the name “height” and then computing the average of the resulting numeric vector. We can compute any other summary that operates on vectors and returns a single value. For example, we can add the median, minimum, and maximum heights like this:\nheights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% summarize(median = median(height), minimum = min(height), maximum = max(height)) ## median minimum maximum ## 1 64.98031 51 79 We can obtain these three values with just one line using the quantile function: for example, quantile(x, c(0,0.5,1)) returns the min (0th percentile), median (50th percentile), and max (100th percentile) of the vector x. However, if we attempt to use a function like this that returns two or more values inside summarize:\nheights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% summarize(range = quantile(height, c(0, 0.5, 1))) we will receive an error: Error: expecting result of length one, got : 2. With the function summarize, we can only call functions that return a single value. In Section ??, we will learn how to deal with functions that return more than one value.\nFor another example of how we can use the summarize function, let’s compute the average murder rate for the United States. Remember our data table includes total murders and population size for each state and we have already used dplyr to add a murder rate column:\nmurders \u0026lt;- murders %\u0026gt;% mutate(rate = total/population*100000) Remember that the US murder rate is not the average of the state murder rates:\nsummarize(murders, mean(rate)) ## mean(rate) ## 1 2.779125 This is because in the computation above the small states are given the same weight as the large ones. The US murder rate is the total number of murders in the US divided by the total US population. So the correct computation is:\nus_murder_rate \u0026lt;- murders %\u0026gt;% summarize(rate = sum(total) / sum(population) * 100000) us_murder_rate ## rate ## 1 3.034555 This computation counts larger states proportionally to their size which results in a larger value.\n pull The us_murder_rate object defined above represents just one number. Yet we are storing it in a data frame:\nclass(us_murder_rate) ## [1] \u0026quot;data.frame\u0026quot; since, as most dplyr functions, summarize always returns a data frame.\nThis might be problematic if we want to use this result with functions that require a numeric value. Here we show a useful trick for accessing values stored in data when using pipes: when a data object is piped that object and its columns can be accessed using the pull function. To understand what we mean take a look at this line of code:\nus_murder_rate %\u0026gt;% pull(rate) ## [1] 3.034555 This returns the value in the rate column of us_murder_rate making it equivalent to us_murder_rate$rate.\nTo get a number from the original data table with one line of code we can type:\nus_murder_rate \u0026lt;- murders %\u0026gt;% summarize(rate = sum(total) / sum(population) * 100000) %\u0026gt;% pull(rate) us_murder_rate ## [1] 3.034555 which is now a numeric:\nclass(us_murder_rate) ## [1] \u0026quot;numeric\u0026quot;  Group then summarize with group_by A common operation in data exploration is to first split data into groups and then compute summaries for each group. For example, we may want to compute the average and standard deviation for men’s and women’s heights separately. The group_by function helps us do this.\nIf we type this:\nheights %\u0026gt;% group_by(sex) ## # A tibble: 1,050 x 2 ## # Groups: sex [2] ## sex height ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Male 75 ## 2 Male 70 ## 3 Male 68 ## 4 Male 74 ## 5 Male 61 ## 6 Female 65 ## 7 Female 66 ## 8 Female 62 ## 9 Female 66 ## 10 Male 67 ## # … with 1,040 more rows The result does not look very different from heights, except we see Groups: sex [2] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object. Conceptually, you can think of this table as many tables, with the same columns but not necessarily the same number of rows, stacked together in one object. When we summarize the data after grouping, this is what happens:\nheights %\u0026gt;% group_by(sex) %\u0026gt;% summarize(average = mean(height), standard_deviation = sd(height)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 3 ## sex average standard_deviation ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Female 64.9 3.76 ## 2 Male 69.3 3.61 The summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median murder rate in the four regions of the country:\nmurders %\u0026gt;% group_by(region) %\u0026gt;% summarize(median_rate = median(rate)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 2 ## region median_rate ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Northeast 1.80 ## 2 South 3.40 ## 3 North Central 1.97 ## 4 West 1.29   Sorting data frames When examining a dataset, it is often convenient to sort the table by the different columns. We know about the order and sort function, but for ordering entire tables, the dplyr function arrange is useful. For example, here we order the states by population size:\nmurders %\u0026gt;% arrange(population) %\u0026gt;% head() ## state abb region population total rate ## 1 Wyoming WY West 563626 5 0.8871131 ## 2 District of Columbia DC South 601723 99 16.4527532 ## 3 Vermont VT Northeast 625741 2 0.3196211 ## 4 North Dakota ND North Central 672591 4 0.5947151 ## 5 Alaska AK West 710231 19 2.6751860 ## 6 South Dakota SD North Central 814180 8 0.9825837 With arrange we get to decide which column to sort by. To see the states by murder rate, from lowest to highest, we arrange by rate instead:\nmurders %\u0026gt;% arrange(rate) %\u0026gt;% head() ## state abb region population total rate ## 1 Vermont VT Northeast 625741 2 0.3196211 ## 2 New Hampshire NH Northeast 1316470 5 0.3798036 ## 3 Hawaii HI West 1360301 7 0.5145920 ## 4 North Dakota ND North Central 672591 4 0.5947151 ## 5 Iowa IA North Central 3046355 21 0.6893484 ## 6 Idaho ID West 1567582 12 0.7655102 Note that the default behavior is to order in ascending order. In dplyr, the function desc transforms a vector so that it is in descending order. To sort the table in descending order, we can type:\nmurders %\u0026gt;% arrange(desc(rate)) Nested sorting If we are ordering by a column with ties, we can use a second column to break the tie. Similarly, a third column can be used to break ties between first and second and so on. Here we order by region, then within region we order by murder rate:\nmurders %\u0026gt;% arrange(region, rate) %\u0026gt;% head() ## state abb region population total rate ## 1 Vermont VT Northeast 625741 2 0.3196211 ## 2 New Hampshire NH Northeast 1316470 5 0.3798036 ## 3 Maine ME Northeast 1328361 11 0.8280881 ## 4 Rhode Island RI Northeast 1052567 16 1.5200933 ## 5 Massachusetts MA Northeast 6547629 118 1.8021791 ## 6 New York NY Northeast 19378102 517 2.6679599  The top \\(n\\) In the code above, we have used the function head to avoid having the page fill up with the entire dataset. If we want to see a larger proportion, we can use the top_n function. This function takes a data frame as it’s first argument, the number of rows to show in the second, and the variable to filter by in the third. Here is an example of how to see the top 5 rows:\nmurders %\u0026gt;% top_n(5, rate) ## state abb region population total rate ## 1 District of Columbia DC South 601723 99 16.452753 ## 2 Louisiana LA South 4533372 351 7.742581 ## 3 Maryland MD South 5773552 293 5.074866 ## 4 Missouri MO North Central 5988927 321 5.359892 ## 5 South Carolina SC South 4625364 207 4.475323 Note that rows are not sorted by rate, only filtered. If we want to sort, we need to use arrange. Note that if the third argument is left blank, top_n filters by the last column.\nTRY IT\nFor these exercises, we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\nlibrary(NHANES) data(NHANES) The NHANES data has many missing values. The mean and sd functions in R will return NA if any of the entries of the input vector is an NA. Here is an example:\nlibrary(dslabs) data(na_example) mean(na_example) ## [1] NA sd(na_example) ## [1] NA To ignore the NAs we can use the na.rm argument:\nmean(na_example, na.rm = TRUE) ## [1] 2.301754 sd(na_example, na.rm = TRUE) ## [1] 1.22338 Let’s now explore the NHANES data.\nWe will provide some basic facts about blood pressure. First let’s select a group to set the standard. We will use 20-to-29-year-old females. AgeDecade is a categorical variable with these ages. Note that the category is coded like \" 20-29\", with a space in front! What is the average and standard deviation of systolic blood pressure as saved in the BPSysAve variable? Save it to a variable called ref.  Hint: Use filter and summarize and use the na.rm = TRUE argument when computing the average and standard deviation. You can also filter the NA values using filter.\nUsing a pipe, assign the average to a numeric variable ref_avg. Hint: Use the code similar to above and then pull.\n Now report the min and max values for the same group.\n Compute the average and standard deviation for females, but for each age group separately rather than a selected decade as in question 1. Note that the age groups are defined by AgeDecade. Hint: rather than filtering by age and gender, filter by Gender and then use group_by.\n Repeat exercise 4 for males.\n We can actually combine both summaries for exercises 4 and 5 into one line of code. This is because group_by permits us to group by more than one variable. Obtain one big summary table using group_by(AgeDecade, Gender).\n For males between the ages of 40-49, compare systolic blood pressure across race as reported in the Race1 variable. Order the resulting table from lowest to highest average systolic blood pressure.\n     Tibbles Tidy data must be stored in data frames. We introduced the data frame in Section ?? and have been using the murders data frame throughout the book. In Section ?? we introduced the group_by function, which permits stratifying data before computing summary statistics. But where is the group information stored in the data frame?\nmurders %\u0026gt;% group_by(region) ## # A tibble: 51 x 6 ## # Groups: region [4] ## state abb region population total rate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Alabama AL South 4779736 135 2.82 ## 2 Alaska AK West 710231 19 2.68 ## 3 Arizona AZ West 6392017 232 3.63 ## 4 Arkansas AR South 2915918 93 3.19 ## 5 California CA West 37253956 1257 3.37 ## 6 Colorado CO West 5029196 65 1.29 ## 7 Connecticut CT Northeast 3574097 97 2.71 ## 8 Delaware DE South 897934 38 4.23 ## 9 District of Columbia DC South 601723 99 16.5 ## 10 Florida FL South 19687653 669 3.40 ## # … with 41 more rows Notice that there are no columns with this information. But, if you look closely at the output above, you see the line A tibble followd by dimensions. We can learn the class of the returned object using:\nmurders %\u0026gt;% group_by(region) %\u0026gt;% class() ## [1] \u0026quot;grouped_df\u0026quot; \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot; The tbl, pronounced tibble, is a special kind of data frame. The functions group_by and summarize always return this type of data frame. The group_by function returns a special kind of tbl, the grouped_df. We will say more about these later. For consistency, the dplyr manipulation verbs (select, filter, mutate, and arrange) preserve the class of the input: if they receive a regular data frame they return a regular data frame, while if they receive a tibble they return a tibble. But tibbles are the preferred format in the tidyverse and as a result tidyverse functions that produce a data frame from scratch return a tibble. For example, in Chapter ?? we will see that tidyverse functions used to import data create tibbles.\nTibbles are very similar to data frames. In fact, you can think of them as a modern version of data frames. Nonetheless there are three important differences which we describe next.\nTibbles display better The print method for tibbles is more readable than that of a data frame. To see this, compare the outputs of typing murders and the output of murders if we convert it to a tibble. We can do this using as_tibble(murders). If using RStudio, output for a tibble adjusts to your window size. To see this, change the width of your R console and notice how more/less columns are shown.\n Subsets of tibbles are tibbles If you subset the columns of a data frame, you may get back an object that is not a data frame, such as a vector or scalar. For example:\nclass(murders[,4]) ## [1] \u0026quot;numeric\u0026quot; is not a data frame. With tibbles this does not happen:\nclass(as_tibble(murders)[,4]) ## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot; This is useful in the tidyverse since functions require data frames as input.\nWith tibbles, if you want to access the vector that defines a column, and not get back a data frame, you need to use the accessor $:\nclass(as_tibble(murders)$population) ## [1] \u0026quot;numeric\u0026quot; A related feature is that tibbles will give you a warning if you try to access a column that does not exist. If we accidentally write Population instead of population this:\nmurders$Population ## NULL returns a NULL with no warning, which can make it harder to debug. In contrast, if we try this with a tibble we get an informative warning:\nas_tibble(murders)$Population ## Warning: Unknown or uninitialised column: `Population`. ## NULL  Tibbles can have complex entries While data frame columns need to be vectors of numbers, strings, or logical values, tibbles can have more complex objects, such as lists or functions. Also, we can create tibbles with functions:\ntibble(id = c(1, 2, 3), func = c(mean, median, sd)) ## # A tibble: 3 x 2 ## id func ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt; ## 1 1 \u0026lt;fn\u0026gt; ## 2 2 \u0026lt;fn\u0026gt; ## 3 3 \u0026lt;fn\u0026gt;  Tibbles can be grouped The function group_by returns a special kind of tibble: a grouped tibble. This class stores information that lets you know which rows are in which groups. The tidyverse functions, in particular the summarize function, are aware of the group information.\n Create a tibble using tibble instead of data.frame It is sometimes useful for us to create our own data frames. To create a data frame in the tibble format, you can do this by using the tibble function.\ngrades \u0026lt;- tibble(names = c(\u0026quot;John\u0026quot;, \u0026quot;Juan\u0026quot;, \u0026quot;Jean\u0026quot;, \u0026quot;Yao\u0026quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90)) Note that base R (without packages loaded) has a function with a very similar name, data.frame, that can be used to create a regular data frame rather than a tibble. One other important difference is that by default data.frame coerces characters into factors without providing a warning or message:\ngrades \u0026lt;- data.frame(names = c(\u0026quot;John\u0026quot;, \u0026quot;Juan\u0026quot;, \u0026quot;Jean\u0026quot;, \u0026quot;Yao\u0026quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90)) class(grades$names) ## [1] \u0026quot;factor\u0026quot; To avoid this, we use the rather cumbersome argument stringsAsFactors:\ngrades \u0026lt;- data.frame(names = c(\u0026quot;John\u0026quot;, \u0026quot;Juan\u0026quot;, \u0026quot;Jean\u0026quot;, \u0026quot;Yao\u0026quot;), exam_1 = c(95, 80, 90, 85), exam_2 = c(90, 85, 85, 90), stringsAsFactors = FALSE) class(grades$names) ## [1] \u0026quot;character\u0026quot; To convert a regular data frame to a tibble, you can use the as_tibble function.\nas_tibble(grades) %\u0026gt;% class() ## [1] \u0026quot;tbl_df\u0026quot; \u0026quot;tbl\u0026quot; \u0026quot;data.frame\u0026quot;   The dot operator One of the advantages of using the pipe %\u0026gt;% is that we do not have to keep naming new objects as we manipulate the data frame. As a quick reminder, if we want to compute the median murder rate for states in the southern states, instead of typing:\ntab_1 \u0026lt;- filter(murders, region == \u0026quot;South\u0026quot;) tab_2 \u0026lt;- mutate(tab_1, rate = total / population * 10^5) rates \u0026lt;- tab_2$rate median(rates) ## [1] 3.398069 We can avoid defining any new intermediate objects by instead typing:\nfilter(murders, region == \u0026quot;South\u0026quot;) %\u0026gt;% mutate(rate = total / population * 10^5) %\u0026gt;% summarize(median = median(rate)) %\u0026gt;% pull(median) ## [1] 3.398069 We can do this because each of these functions takes a data frame as the first argument. But what if we want to access a component of the data frame. For example, what if the pull function was not available and we wanted to access tab_2$rate? What data frame name would we use? The answer is the dot operator.\nFor example to access the rate vector without the pull function we could use\nrates \u0026lt;- filter(murders, region == \u0026quot;South\u0026quot;) %\u0026gt;% mutate(rate = total / population * 10^5) %\u0026gt;% .$rate median(rates) ## [1] 3.398069 In the next section, we will see other instances in which using the . is useful.\n do The tidyverse functions know how to interpret grouped tibbles. Furthermore, to facilitate stringing commands through the pipe %\u0026gt;%, tidyverse functions consistently return data frames, since this assures that the output of a function is accepted as the input of another. But most R functions do not recognize grouped tibbles nor do they return data frames. The quantile function is an example we described in Section ??. The do function serves as a bridge between R functions such as quantile and the tidyverse. The do function understands grouped tibbles and always returns a data frame.\nIn Section ??, we noted that if we attempt to use quantile to obtain the min, median and max in one call, we will receive an error: Error: expecting result of length one, got : 2.\ndata(heights) heights %\u0026gt;% filter(sex == \u0026quot;Female\u0026quot;) %\u0026gt;% summarize(range = quantile(height, c(0, 0.5, 1))) We can use the do function to fix this.\nFirst we have to write a function that fits into the tidyverse approach: that is, it receives a data frame and returns a data frame.\nmy_summary \u0026lt;- function(dat){ x \u0026lt;- quantile(dat$height, c(0, 0.5, 1)) tibble(min = x[1], median = x[2], max = x[3]) } We can now apply the function to the heights dataset to obtain the summaries:\nheights %\u0026gt;% group_by(sex) %\u0026gt;% my_summary ## # A tibble: 1 x 3 ## min median max ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 50 68.5 82.7 But this is not what we want. We want a summary for each sex and the code returned just one summary. This is because my_summary is not part of the tidyverse and does not know how to handled grouped tibbles. do makes this connection:\nheights %\u0026gt;% group_by(sex) %\u0026gt;% do(my_summary(.)) ## # A tibble: 2 x 4 ## # Groups: sex [2] ## sex min median max ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Female 51 65.0 79 ## 2 Male 50 69 82.7 Note that here we need to use the dot operator. The tibble created by group_by is piped to do. Within the call to do, the name of this tibble is . and we want to send it to my_summary. If you do not use the dot, then my_summary has no argument and returns an error telling us that argument \"dat\" is missing. You can see the error by typing:\nheights %\u0026gt;% group_by(sex) %\u0026gt;% do(my_summary()) If you do not use the parenthesis, then the function is not executed and instead do tries to return the function. This gives an error because do must always return a data frame. You can see the error by typing:\nheights %\u0026gt;% group_by(sex) %\u0026gt;% do(my_summary)  The purrr package In Section ?? we learned about the sapply function, which permitted us to apply the same function to each element of a vector. We constructed a function and used sapply to compute the sum of the first n integers for several values of n like this:\ncompute_s_n \u0026lt;- function(n){ x \u0026lt;- 1:n sum(x) } n \u0026lt;- 1:25 s_n \u0026lt;- sapply(n, compute_s_n) This type of operation, applying the same function or procedure to elements of an object, is quite common in data analysis. The purrr package includes functions similar to sapply but that better interact with other tidyverse functions. The main advantage is that we can better control the output type of functions. In contrast, sapply can return several different object types; for example, we might expect a numeric result from a line of code, but sapply might convert our result to character under some circumstances. purrr functions will never do this: they will return objects of a specified type or return an error if this is not possible.\nThe first purrr function we will learn is map, which works very similar to sapply but always, without exception, returns a list:\nlibrary(purrr) s_n \u0026lt;- map(n, compute_s_n) class(s_n) ## [1] \u0026quot;list\u0026quot; If we want a numeric vector, we can instead use map_dbl which always returns a vector of numeric values.\ns_n \u0026lt;- map_dbl(n, compute_s_n) class(s_n) ## [1] \u0026quot;numeric\u0026quot; This produces the same results as the sapply call shown above.\nA particularly useful purrr function for interacting with the rest of the tidyverse is map_df, which always returns a tibble data frame. However, the function being called needs to return a vector or a list with names. For this reason, the following code would result in a Argument 1 must have names error:\ns_n \u0026lt;- map_df(n, compute_s_n) We need to change the function to make this work:\ncompute_s_n \u0026lt;- function(n){ x \u0026lt;- 1:n tibble(sum = sum(x)) } s_n \u0026lt;- map_df(n, compute_s_n) The purrr package provides much more functionality not covered here. For more details you can consult this online resource.\n Tidyverse conditionals A typical data analysis will often involve one or more conditional operations. In Section ?? we described the ifelse function, which we will use extensively in this book. In this section we present two dplyr functions that provide further functionality for performing conditional operations.\ncase_when The case_when function is useful for vectorizing conditional statements. It is similar to ifelse but can output any number of values, as opposed to just TRUE or FALSE. Here is an example splitting numbers into negative, positive, and 0:\nx \u0026lt;- c(-2, -1, 0, 1, 2) case_when(x \u0026lt; 0 ~ \u0026quot;Negative\u0026quot;, x \u0026gt; 0 ~ \u0026quot;Positive\u0026quot;, TRUE ~ \u0026quot;Zero\u0026quot;) ## [1] \u0026quot;Negative\u0026quot; \u0026quot;Negative\u0026quot; \u0026quot;Zero\u0026quot; \u0026quot;Positive\u0026quot; \u0026quot;Positive\u0026quot; A common use for this function is to define categorical variables based on existing variables. For example, suppose we want to compare the murder rates in four groups of states: New England, West Coast, South, and other. For each state, we need to ask if it is in New England, if it is not we ask if it is in the West Coast, if not we ask if it is in the South, and if not we assign other. Here is how we use case_when to do this:\nmurders %\u0026gt;% mutate(group = case_when( abb %in% c(\u0026quot;ME\u0026quot;, \u0026quot;NH\u0026quot;, \u0026quot;VT\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;RI\u0026quot;, \u0026quot;CT\u0026quot;) ~ \u0026quot;New England\u0026quot;, abb %in% c(\u0026quot;WA\u0026quot;, \u0026quot;OR\u0026quot;, \u0026quot;CA\u0026quot;) ~ \u0026quot;West Coast\u0026quot;, region == \u0026quot;South\u0026quot; ~ \u0026quot;South\u0026quot;, TRUE ~ \u0026quot;Other\u0026quot;)) %\u0026gt;% group_by(group) %\u0026gt;% summarize(rate = sum(total) / sum(population) * 10^5) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 2 ## group rate ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 New England 1.72 ## 2 Other 2.71 ## 3 South 3.63 ## 4 West Coast 2.90  between A common operation in data analysis is to determine if a value falls inside an interval. We can check this using conditionals. For example, to check if the elements of a vector x are between a and b we can type\nx \u0026gt;= a \u0026amp; x \u0026lt;= b However, this can become cumbersome, especially within the tidyverse approach. The between function performs the same operation.\nbetween(x, a, b) TRY IT\nLoad the murders dataset. Which of the following is true?  murders is in tidy format and is stored in a tibble. murders is in tidy format and is stored in a data frame. murders is not in tidy format and is stored in a tibble. murders is not in tidy format and is stored in a data frame.  Use as_tibble to convert the murders data table into a tibble and save it in an object called murders_tibble.\n Use the group_by function to convert murders into a tibble that is grouped by region.\n Write tidyverse code that is equivalent to this code:\n  exp(mean(log(murders$population))) Write it using the pipe so that each function is called without arguments. Use the dot operator to access the population. Hint: The code should start with murders %\u0026gt;%.\nUse the map_df to create a data frame with three columns named n, s_n, and s_n_2. The first column should contain the numbers 1 through 100. The second and third columns should each contain the sum of 1 through \\(n\\) with \\(n\\) the row number.      Videos     If you have not installed this package already, you must use install.packages(\"tidyverse\") prior to the library() call you see below.↩︎\n   ","date":1599523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600098198,"objectID":"fbbae951c935dd3d35f82710943d5efd","permalink":"/content/01-content/","publishdate":"2020-09-08T00:00:00Z","relpermalink":"/content/01-content/","section":"content","summary":"Readings Guiding Question  Slides The tidyverse Tidy data Manipulating data frames Adding a column with mutate Subsetting with filter Selecting columns with select  The pipe: %\u0026gt;% Summarizing data summarize pull Group then summarize with group_by  Sorting data frames Nested sorting The top \\(n\\)  Tibbles Tibbles display better Subsets of tibbles are tibbles Tibbles can have complex entries Tibbles can be grouped Create a tibble using tibble instead of data.","tags":null,"title":"Introduction to the tidyverse","type":"docs"},{"authors":null,"categories":null,"content":"   Readings Guiding Question  Slides R basics Case study: US homicides by firearm The (very) basics Objects The workspace Functions Other prebuilt objects Variable names Saving your workspace Motivating scripts Commenting your code  Data types Data frames Examining an object The accessor: $ Vectors: numerics, characters, and logical Factors Lists Matrices  Vectors Creating vectors Names Sequences Subsetting  Coercion Not availables (NA)  Sorting sort order max and which.max rank Beware of recycling  Vector arithmetics Rescaling a vector Two vectors  Indexing Subsetting with logicals Logical operators which match %in%  Videos    Readings As noted in the syllabus, your readings will be assigned each week in this area. For this initial week, please read the course content. Read closely the following:\n The syllabus, content, examples, and labs pages for this class. This page. Yes, the whole thing.  Guiding Question For future lectures, the guiding questions will be more pointed and at a higher level to help steer your thinking. Here, we want to ensure you remember some basics and accordingly the questions are straightforward.\n Do you remember anything about R? What are the different data types in R? How do you index specific elements of a vector? Why might you want to do that?    Slides There are no direct slides for this lesson; the content below is intended to serve as a refresher for those who may have forgotten some of the basics of R. This content will form the first week of the course and will include the Example 0. The slides below cover introductory material from the first lecture; some kinks to be worked out.\nIntroduction       ALERT\nThe course content below should be considered a prerequisite for success. For those concerned about basics of R, you absolutely must read this content and attempt the coding exercises. If you struggle to follow the content, please contact the professor or TA.\n  R basics In this class, we will be using R software environment for all our analyses. You will learn R and data analysis techniques simultaneously. To follow along you will therefore need access to R. We also recommend the use of an integrated development environment (IDE), such as RStudio, to save your work. Note that it is common for a course or workshop to offer access to an R environment and an IDE through your web browser, as done by RStudio cloud1. If you have access to such a resource, you don’t need to install R and RStudio. However, if you intend on becoming a practicing data analyst, we highly recommend installing these tools on your computer2. This is not hard.\nBoth R and RStudio are free and available online.\nCase study: US homicides by firearm Imagine you live in Europe (if only!) and are offered a job in a US company with many locations in every state. It is a great job, but headlines such as US Gun Homicide Rate Higher Than Other Developed Countries3 have you worried. Fox News runs a scary looking graphic, and charts like the one below only add to that concern:\nOr even worse, this version from everytown.org: But then you remember that (1) this is a hypothetical exercise; (2) you’ll take literally any job at this point; and (3) Geographic diversity matters – the United States is a large and diverse country with 50 very different states (plus the District of Columbia and some lovely territories).4\nCalifornia, for example, has a larger population than Canada, and 20 US states have populations larger than that of Norway. In some respects, the variability across states in the US is akin to the variability across countries in Europe. Furthermore, although not included in the charts above, the murder rates in Lithuania, Ukraine, and Russia are higher than 4 per 100,000. So perhaps the news reports that worried you are too superficial.\nThis is a relatively simple and straightforward problem in social science: you have options of where to live, and want to determine the safety of the various states. Your “research” is clearly policy-relevant: you will eventually have to live somewhere. We will begin to tackle the problem by examining data related to gun homicides in the US during 2010 using R.\nBefore we get started with our example, we need to cover logistics as well as some of the very basic building blocks that are required to gain more advanced R skills. Ideally, this is a refresher. However, we are aware that your preparation in previously courses varies greatly from student to student. Moreover, we want you to be aware that the usefulness of some of these early building blocks may not be immediately obvious. Later in the class you will appreciate having these skills. Mastery will be rewarded both in this class and (of course) in life.\n The (very) basics Before we get started with the motivating dataset, we need to cover the very basics of R.\nObjects Suppose a relatively math unsavvy student asks us for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). You—a savvy student—recall that the quadratic formula gives us the solutions:\n\\[ \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}\\,\\, \\mbox{ and } \\frac{-b + \\sqrt{b^2 - 4ac}}{2a} \\]\nwhich of course depend on the values of \\(a\\), \\(b\\), and \\(c\\). That is, the quadratic equation represents a function with three arguments.\nOne advantage of programming languages is that we can define variables and write expressions with these variables, similar to how we do so in math, but obtain a numeric solution. We will write out general code for the quadratic equation below, but if we are asked to solve \\(x^2 + x -1 = 0\\), then we define:\na \u0026lt;- 1 b \u0026lt;- 1 c \u0026lt;- -1 which stores the values for later use. We use \u0026lt;- to assign values to the variables.\nWe can also assign values using = instead of \u0026lt;-, but we recommend against using = to avoid confusion.5\nTRY IT\nCopy and paste the code above into your console to define the three variables. Note that R does not print anything when we make this assignment. This means the objects were defined successfully. Had you made a mistake, you would have received an error message. Throughout these written notes, you’ll have the most success if you continue to copy code into your own console.\n To see the value stored in a variable, we simply ask R to evaluate a and it shows the stored value:\na ## [1] 1 A more explicit way to ask R to show us the value stored in a is using print like this:\nprint(a) ## [1] 1 We use the term object to describe stuff that is stored in R. Variables are examples, but objects can also be more complicated entities such as functions, which are described later.\n The workspace As we define objects in the console, we are actually changing the workspace. You can see all the variables saved in your workspace by typing:\nls() ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;c\u0026quot; \u0026quot;dat\u0026quot; \u0026quot;murders\u0026quot; ## [6] \u0026quot;sections\u0026quot; \u0026quot;slide_tabs\u0026quot; (Note that one of my variables listed above comes from generating the graphs above). In RStudio, the Environment tab shows the values:\nWe should see a, b, and c. If you try to recover the value of a variable that is not in your workspace, you receive an error. For example, if you type x you will receive the following message: Error: object 'x' not found.\nNow since these values are saved in variables, to obtain a solution to our equation, we use the quadratic formula:\n(-b + sqrt(b^2 - 4*a*c) ) / ( 2*a ) ## [1] 0.618034 (-b - sqrt(b^2 - 4*a*c) ) / ( 2*a ) ## [1] -1.618034  Functions Once you define variables, the data analysis process can usually be described as a series of functions applied to the data. R includes several zillion predefined functions and most of the analysis pipelines we construct make extensive use of the built-in functions. But R’s power comes from its scalability. We have access to (nearly) infinite functions via install.packages and library. As we go through the course, we will carefully note new functions we bring to each problem. For now, though, we will stick to the basics.\nNote that you’ve used a function already: you used the function sqrt to solve the quadratic equation above. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nIn general, we need to use parentheses to evaluate a function. If you type ls, the function is not evaluated and instead R shows you the code that defines the function. If you type ls() the function is evaluated and, as seen above, we see objects in the workspace.\nUnlike ls, most functions require one or more arguments. Below is an example of how we assign an object to the argument of the function log. Remember that we earlier defined a to be 1:\nlog(8) ## [1] 2.079442 log(a) ## [1] 0 You can find out what the function expects and what it does by reviewing the very useful manuals included in R. You can get help by using the help function like this:\nhelp(\u0026quot;log\u0026quot;) For most functions, we can also use this shorthand:\n?log The help page will show you what arguments the function is expecting. For example, log needs x and base to run. However, some arguments are required and others are optional. You can determine which arguments are optional by noting in the help document that a default value is assigned with =. Defining these is optional.6 For example, the base of the function log defaults to base = exp(1)—that is, log evaluates the natural log by default.\nIf you want a quick look at the arguments without opening the help system, you can type:\nargs(log) ## function (x, base = exp(1)) ## NULL You can change the default values by simply assigning another object:\nlog(8, base = 2) ## [1] 3 Note that we have not been specifying the argument x as such:\nlog(x = 8, base = 2) ## [1] 3 The above code works, but we can save ourselves some typing: if no argument name is used, R assumes you are entering arguments in the order shown in the help file or by args. So by not using the names, it assumes the arguments are x followed by base:\nlog(8,2) ## [1] 3 If using the arguments’ names, then we can include them in whatever order we want:\nlog(base = 2, x = 8) ## [1] 3 To specify arguments, we must use =, and cannot use \u0026lt;-.\nThere are some exceptions to the rule that functions need the parentheses to be evaluated. Among these, the most commonly used are the arithmetic and relational operators. For example:\n2 ^ 3 ## [1] 8 You can see the arithmetic operators by typing:\nhelp(\u0026quot;+\u0026quot;) or\n?\u0026quot;+\u0026quot; and the relational operators by typing:\nhelp(\u0026quot;\u0026gt;\u0026quot;) or\n?\u0026quot;\u0026gt;\u0026quot;  Other prebuilt objects There are several datasets that are included for users to practice and test out functions. You can see all the available datasets by typing:\ndata() This shows you the object name for these datasets. These datasets are objects that can be used by simply typing the name. For example, if you type:\nco2 R will show you Mauna Loa atmospheric \\(CO^2\\) concentration data.\nOther prebuilt objects are mathematical quantities, such as the constant \\(\\pi\\) and \\(\\infty\\):\npi ## [1] 3.141593 Inf+1 ## [1] Inf  Variable names We have used the letters a, b, and c as variable names, but variable names can be almost anything. Some basic rules in R are that variable names have to start with a letter, can’t contain spaces, and should not be variables that are predefined in R. For example, don’t name one of your variables install.packages by typing something like install.packages \u0026lt;- 2. Usually, R is smart enough to prevent you from doing such nonsense, but it’s important to develop good habits.\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces. For the quadratic equations, we could use something like this:\nsolution_1 \u0026lt;- (-b + sqrt(b^2 - 4*a*c)) / (2*a) solution_2 \u0026lt;- (-b - sqrt(b^2 - 4*a*c)) / (2*a) For more advice, we highly recommend studying (Hadley Wickham’s style guide)[http://adv-r.had.co.nz/Style.html].\n Saving your workspace Values remain in the workspace until you end your session or erase them with the function rm. But workspaces also can be saved for later use. In fact, when you quit R, the program asks you if you want to save your workspace. If you do save it, the next time you start R, the program will restore the workspace.\nWe actually recommend against saving the workspace this way because, as you start working on different projects, it will become harder to keep track of what is saved. Instead, we recommend you assign the workspace a specific name. You can do this by using the function save or save.image. To load, use the function load. When saving a workspace, we recommend the suffix rda or RData. In RStudio, you can also do this by navigating to the Session tab and choosing Save Workspace as. You can later load it using the Load Workspace options in the same tab. You can read the help pages on save, save.image, and load to learn more.\n Motivating scripts To solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\na \u0026lt;- 3 b \u0026lt;- 2 c \u0026lt;- -1 (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a) By creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\n Commenting your code If a line of R code starts with the symbol #, it is not evaluated. We can use this to write reminders of why we wrote particular code. For example, in the script above we could add:\n## Code to compute solution to quadratic equation of the form ax^2 + bx + c ## define the variables a \u0026lt;- 3 b \u0026lt;- 2 c \u0026lt;- -1 ## now compute the solution (-b + sqrt(b^2 - 4*a*c)) / (2*a) (-b - sqrt(b^2 - 4*a*c)) / (2*a) TRY IT\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\n Now use the same formula to compute the sum of the integers from 1 through 1,000.\n Look at the result of typing the following code into R:\n  n \u0026lt;- 1000 x \u0026lt;- seq(1, n) sum(x) Based on the result, what do you think the functions seq and sum do? You can use help.\nsum creates a list of numbers and seq adds them up. seq creates a list of numbers and sum adds them up. seq creates a random list and sum computes the sum of 1 through 1,000. sum always returns the same number.  In math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\n Which of the following will always return the numeric value stored in x? You can try out examples and use the help system if you want.\n  log(10^x) log10(x^10) log(exp(x)) exp(log(x, base = 2))     Data types Variables in R can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The function class helps us determine what type of object we have:\na \u0026lt;- 2 class(a) ## [1] \u0026quot;numeric\u0026quot; To work efficiently in R, it is important to learn the different types of variables and what we can do with these.\nData frames Up to now, the variables we have defined are just one number. This is not very useful for storing data. The most common way of storing a dataset in R is in a data frame. Conceptually, we can think of a data frame as a table with rows representing observations and the different variables reported for each observation defining the columns. Data frames are particularly useful for datasets because we can combine different data types into one object.\nA large proportion of data analysis challenges start with data stored in a data frame. For example, we stored the data for our motivating example in a data frame. You can access this dataset by loading the dslabs library and loading the murders dataset using the data function:\nlibrary(dslabs) data(murders) To see that this is in fact a data frame, we type:\nclass(murders) ## [1] \u0026quot;data.frame\u0026quot;  Examining an object The function str is useful for finding out more about the structure of an object:\nstr(murders) ## \u0026#39;data.frame\u0026#39;: 51 obs. of 5 variables: ## $ state : chr \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; ... ## $ abb : chr \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; ... ## $ region : Factor w/ 4 levels \u0026quot;Northeast\u0026quot;,\u0026quot;South\u0026quot;,..: 2 4 4 2 4 4 1 2 2 2 ... ## $ population: num 4779736 710231 6392017 2915918 37253956 ... ## $ total : num 135 19 232 93 1257 ... This tells us much more about the object. We see that the table has 51 rows (50 states plus DC) and five variables. We can show the first six lines using the function head:\nhead(murders) ## state abb region population total ## 1 Alabama AL South 4779736 135 ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232 ## 4 Arkansas AR South 2915918 93 ## 5 California CA West 37253956 1257 ## 6 Colorado CO West 5029196 65 In this dataset, each state is considered an observation and five variables are reported for each state.\nBefore we go any further in answering our original question about different states, let’s learn more about the components of this object.\n The accessor: $ For our analysis, we will need to access the different variables represented by columns included in this data frame. To do this, we use the accessor operator $ in the following way:\nmurders$population ## [1] 4779736 710231 6392017 2915918 37253956 5029196 3574097 897934 ## [9] 601723 19687653 9920000 1360301 1567582 12830632 6483802 3046355 ## [17] 2853118 4339367 4533372 1328361 5773552 6547629 9883640 5303925 ## [25] 2967297 5988927 989415 1826341 2700551 1316470 8791894 2059179 ## [33] 19378102 9535483 672591 11536504 3751351 3831074 12702379 1052567 ## [41] 4625364 814180 6346105 25145561 2763885 625741 8001024 6724540 ## [49] 1852994 5686986 563626 But how did we know to use population? Previously, by applying the function str to the object murders, we revealed the names for each of the five variables stored in this table. We can quickly access the variable names using:\nnames(murders) ## [1] \u0026quot;state\u0026quot; \u0026quot;abb\u0026quot; \u0026quot;region\u0026quot; \u0026quot;population\u0026quot; \u0026quot;total\u0026quot; It is important to know that the order of the entries in murders$population preserves the order of the rows in our data table. This will later permit us to manipulate one variable based on the results of another. For example, we will be able to order the state names by the number of murders.\nTip: R comes with a very nice auto-complete functionality that saves us the trouble of typing out all the names. Try typing murders$p then hitting the tab key on your keyboard. This functionality and many other useful auto-complete features are available when working in RStudio.\n Vectors: numerics, characters, and logical The object murders$population is not one number but several. We call these types of objects vectors. A single number is technically a vector of length 1, but in general we use the term vectors to refer to objects with several entries. The function length tells you how many entries are in the vector:\npop \u0026lt;- murders$population length(pop) ## [1] 51 This particular vector is numeric since population sizes are numbers:\nclass(pop) ## [1] \u0026quot;numeric\u0026quot; In a numeric vector, every entry must be a number.\nTo store character strings, vectors can also be of class character. For example, the state names are characters:\nclass(murders$state) ## [1] \u0026quot;character\u0026quot; As with numeric vectors, all entries in a character vector need to be a character.\nAnother important type of vectors are logical vectors. These must be either TRUE or FALSE.\nz \u0026lt;- 3 == 2 z ## [1] FALSE class(z) ## [1] \u0026quot;logical\u0026quot; Here the == is a relational operator asking if 3 is equal to 2. In R, if you just use one =, you actually assign a variable, but if you use two == you test for equality. Yet another reason to avoid assigning via =… it can get confusing and typos can really mess things up.\nYou can see the other relational operators by typing:\n?Comparison In future sections, you will see how useful relational operators can be.\nWe discuss more important features of vectors after the next set of exercises.\nAdvanced: Mathematically, the values in pop are integers and there is an integer class in R. However, by default, numbers are assigned class numeric even when they are round integers. For example, class(1) returns numeric. You can turn them into class integer with the as.integer() function or by adding an L like this: 1L. Note the class by typing: class(1L)\n Factors In the murders dataset, we might expect the region to also be a character vector. However, it is not:\nclass(murders$region) ## [1] \u0026quot;factor\u0026quot; It is a factor. Factors are useful for storing categorical data. We can see that there are only 4 regions by using the levels function:\nlevels(murders$region) ## [1] \u0026quot;Northeast\u0026quot; \u0026quot;South\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot; In the background, R stores these levels as integers and keeps a map to keep track of the labels. This is more memory efficient than storing all the characters. It is also useful for computational reasons we’ll explore later.\nNote that the levels have an order that is different from the order of appearance in the factor object. The default in R is for the levels to follow alphabetical order. However, often we want the levels to follow a different order. You can specify an order through the levels argument when creating the factor with the factor function. For example, in the murders dataset regions are ordered from east to west. The function reorder lets us change the order of the levels of a factor variable based on a summary computed on a numeric vector. We will demonstrate this with a simple example, and will see more advanced ones in the Data Visualization part of the book.\nSuppose we want the levels of the region by the total number of murders rather than alphabetical order. If there are values associated with each level, we can use the reorder and specify a data summary to determine the order. The following code takes the sum of the total murders in each region, and reorders the factor following these sums.\nregion \u0026lt;- murders$region value \u0026lt;- murders$total region \u0026lt;- reorder(region, value, FUN = sum) levels(region) ## [1] \u0026quot;Northeast\u0026quot; \u0026quot;North Central\u0026quot; \u0026quot;West\u0026quot; \u0026quot;South\u0026quot; The new order is in agreement with the fact that the Northeast has the least murders and the South has the most.\nWarning: Factors can be a source of confusion since sometimes they behave like characters and sometimes they do not. As a result, confusing factors and characters are a common source of bugs.\n Lists Data frames are a special case of lists. We will cover lists in more detail later, but know that they are useful because you can store any combination of different types. Below is an example of a list we created for you:\nrecord ## $name ## [1] \u0026quot;John Doe\u0026quot; ## ## $student_id ## [1] 1234 ## ## $grades ## [1] 95 82 91 97 93 ## ## $final_grade ## [1] \u0026quot;A\u0026quot; class(record) ## [1] \u0026quot;list\u0026quot; As with data frames, you can extract the components of a list with the accessor $. In fact, data frames are a type of list.\nrecord$student_id ## [1] 1234 We can also use double square brackets ([[) like this:\nrecord[[\u0026quot;student_id\u0026quot;]] ## [1] 1234 You should get used to the fact that in R there are often several ways to do the same thing. such as accessing entries.7\nYou might also encounter lists without variable names.\nrecord2 ## [[1]] ## [1] \u0026quot;John Doe\u0026quot; ## ## [[2]] ## [1] 1234 If a list does not have names, you cannot extract the elements with $, but you can still use the brackets method and instead of providing the variable name, you provide the list index, like this:\nrecord2[[1]] ## [1] \u0026quot;John Doe\u0026quot; We won’t be using lists until later, but you might encounter one in your own exploration of R. For this reason, we show you some basics here.\n Matrices Matrices are another type of object that are common in R. Matrices are similar to data frames in that they are two-dimensional: they have rows and columns. However, like numeric, character and logical vectors, entries in matrices have to be all the same type. For this reason data frames are much more useful for storing data, since we can have characters, factors, and numbers in them.\nYet matrices have a major advantage over data frames: we can perform matrix algebra operations, a powerful type of mathematical technique. We do not describe these operations in this class, but much of what happens in the background when you perform a data analysis involves matrices. We describe them briefly here since some of the functions we will learn return matrices.\nWe can define a matrix using the matrix function. We need to specify the number of rows and columns.\nmat \u0026lt;- matrix(1:12, 4, 3) mat ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 You can access specific entries in a matrix using square brackets ([). If you want the second row, third column, you use:\nmat[2, 3] ## [1] 10 If you want the entire second row, you leave the column spot empty:\nmat[2, ] ## [1] 2 6 10 Notice that this returns a vector, not a matrix.\nSimilarly, if you want the entire third column, you leave the row spot empty:\nmat[, 3] ## [1] 9 10 11 12 This is also a vector, not a matrix.\nYou can access more than one column or more than one row if you like. This will give you a new matrix.\nmat[, 2:3] ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10 ## [3,] 7 11 ## [4,] 8 12 You can subset both rows and columns:\nmat[1:2, 2:3] ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10 We can convert matrices into data frames using the function as.data.frame:\nas.data.frame(mat) ## V1 V2 V3 ## 1 1 5 9 ## 2 2 6 10 ## 3 3 7 11 ## 4 4 8 12 You can also use single square brackets ([) to access rows and columns of a data frame:\ndata(\u0026quot;murders\u0026quot;) murders[25, 1] ## [1] \u0026quot;Mississippi\u0026quot; murders[2:3, ] ## state abb region population total ## 2 Alaska AK West 710231 19 ## 3 Arizona AZ West 6392017 232 TRY IT\nLoad the US murders dataset.  library(dslabs) data(murders) Use the function str to examine the structure of the murders object. Which of the following best describes the variables represented in this data frame?\nThe 51 states. The murder rates for all 50 states and DC. The state name, the abbreviation of the state name, the state’s region, and the state’s population and total number of murders for 2010. str shows no relevant information.  What are the column names used by the data frame for these five variables?\n Use the accessor $ to extract the state abbreviations and assign them to the object a. What is the class of this object?\n Now use the square brackets to extract the state abbreviations and assign them to the object b. Use the identical function to determine if a and b are the same.\n We saw that the region column stores a factor. You can corroborate this by typing:\n  class(murders$region) With one line of code, use the function levels and length to determine the number of regions defined by this dataset.\nThe function table takes a vector and returns the frequency of each element. You can quickly see how many states are in each region by applying this function. Use this function in one line of code to create a table of states per region.     Vectors In R, the most basic objects available to store data are vectors. As we have seen, complex datasets can usually be broken down into components that are vectors. For example, in a data frame, each column is a vector. Here we learn more about this important class.\nCreating vectors We can create vectors using the function c, which stands for concatenate. We use c to concatenate entries in the following way:\ncodes \u0026lt;- c(380, 124, 818) codes ## [1] 380 124 818 We can also create character vectors. We use the quotes to denote that the entries are characters rather than variable names.\ncountry \u0026lt;- c(\u0026quot;italy\u0026quot;, \u0026quot;canada\u0026quot;, \u0026quot;egypt\u0026quot;) In R you can also use single quotes:\ncountry \u0026lt;- c(\u0026#39;italy\u0026#39;, \u0026#39;canada\u0026#39;, \u0026#39;egypt\u0026#39;) But be careful not to confuse the single quote ’ with the back quote, which shares a keyboard key with ~.\nBy now you should know that if you type:\ncountry \u0026lt;- c(italy, canada, egypt) you receive an error because the variables italy, canada, and egypt are not defined. If we do not use the quotes, R looks for variables with those names and returns an error.\n Names Sometimes it is useful to name the entries of a vector. For example, when defining a vector of country codes, we can use the names to connect the two:\ncodes \u0026lt;- c(italy = 380, canada = 124, egypt = 818) codes ## italy canada egypt ## 380 124 818 The object codes continues to be a numeric vector:\nclass(codes) ## [1] \u0026quot;numeric\u0026quot; but with names:\nnames(codes) ## [1] \u0026quot;italy\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;egypt\u0026quot; If the use of strings without quotes looks confusing, know that you can use the quotes as well:\ncodes \u0026lt;- c(\u0026quot;italy\u0026quot; = 380, \u0026quot;canada\u0026quot; = 124, \u0026quot;egypt\u0026quot; = 818) codes ## italy canada egypt ## 380 124 818 There is no difference between this function call and the previous one. This is one of the many ways in which R is quirky compared to other languages.\nWe can also assign names using the names functions:\ncodes \u0026lt;- c(380, 124, 818) country \u0026lt;- c(\u0026quot;italy\u0026quot;,\u0026quot;canada\u0026quot;,\u0026quot;egypt\u0026quot;) names(codes) \u0026lt;- country codes ## italy canada egypt ## 380 124 818  Sequences Another useful function for creating vectors generates sequences:\nseq(1, 10) ## [1] 1 2 3 4 5 6 7 8 9 10 The first argument defines the start, and the second defines the end which is included. The default is to go up in increments of 1, but a third argument lets us tell it how much to jump by:\nseq(1, 10, 2) ## [1] 1 3 5 7 9 If we want consecutive integers, we can use the following shorthand:\n1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 When we use these functions, R produces integers, not numerics, because they are typically used to index something:\nclass(1:10) ## [1] \u0026quot;integer\u0026quot; However, if we create a sequence including non-integers, the class changes:\nclass(seq(1, 10, 0.5)) ## [1] \u0026quot;numeric\u0026quot;  Subsetting We use square brackets to access specific elements of a vector. For the vector codes we defined above, we can access the second element using:\ncodes[2] ## canada ## 124 You can get more than one entry by using a multi-entry vector as an index:\ncodes[c(1,3)] ## italy egypt ## 380 818 The sequences defined above are particularly useful if we want to access, say, the first two elements:\ncodes[1:2] ## italy canada ## 380 124 If the elements have names, we can also access the entries using these names. Below are two examples.\ncodes[\u0026quot;canada\u0026quot;] ## canada ## 124 codes[c(\u0026quot;egypt\u0026quot;,\u0026quot;italy\u0026quot;)] ## egypt italy ## 818 380   Coercion In general, coercion is an attempt by R to be flexible with data types. When an entry does not match the expected, some of the prebuilt R functions try to guess what was meant before throwing an error. This can also lead to confusion. Failing to understand coercion can drive programmers crazy when attempting to code in R since it behaves quite differently from most other languages in this regard. Let’s learn about it with some examples.\nWe said that vectors must be all of the same type. So if we try to combine, say, numbers and characters, you might expect an error:\nx \u0026lt;- c(1, \u0026quot;canada\u0026quot;, 3) But we don’t get one, not even a warning! What happened? Look at x and its class:\nx ## [1] \u0026quot;1\u0026quot; \u0026quot;canada\u0026quot; \u0026quot;3\u0026quot; class(x) ## [1] \u0026quot;character\u0026quot; R coerced the data into characters. It guessed that because you put a character string in the vector, you meant the 1 and 3 to actually be character strings \"1\" and “3”. The fact that not even a warning is issued is an example of how coercion can cause many unnoticed errors in R.\nR also offers functions to change from one type to another. For example, you can turn numbers into characters with:\nx \u0026lt;- 1:5 y \u0026lt;- as.character(x) y ## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5\u0026quot; You can turn it back with as.numeric:\nas.numeric(y) ## [1] 1 2 3 4 5 This function is actually quite useful since datasets that include numbers as character strings are common.\nNot availables (NA) This “topic” seems to be wholly unappreciated and it has been our experience that students often panic when encountering an NA. This often happens when a function tries to coerce one type to another and encounters an impossible case. In such circumstances, R usually gives us a warning and turns the entry into a special value called an NA (for “not available”). For example:\nx \u0026lt;- c(\u0026quot;1\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;3\u0026quot;) as.numeric(x) ## Warning: NAs introduced by coercion ## [1] 1 NA 3 R does not have any guesses for what number you want when you type b, so it does not try.\nWhile coercion is a common case leading to NAs, you’ll see them in nearly every real-world dataset. Most often, you will encounter the NAs as a stand-in for missing data. Again, this a common problem in real-world datasets and you need to be aware that it will come up.\n  Sorting Now that we have mastered some basic R knowledge (ha!), let’s try to gain some insights into the safety of different states in the context of gun murders.\nsort Say we want to rank the states from least to most gun murders. The function sort sorts a vector in increasing order. We can therefore see the largest number of gun murders by typing:\nlibrary(dslabs) data(murders) sort(murders$total) ## [1] 2 4 5 5 7 8 11 12 12 16 19 21 22 27 32 ## [16] 36 38 53 63 65 67 84 93 93 97 97 99 111 116 118 ## [31] 120 135 142 207 219 232 246 250 286 293 310 321 351 364 376 ## [46] 413 457 517 669 805 1257 However, this does not give us information about which states have which murder totals. For example, we don’t know which state had 1257.\n order The function order is closer to what we want. It takes a vector as input and returns the vector of indexes that sorts the input vector. This may sound confusing so let’s look at a simple example. We can create a vector and sort it:\nx \u0026lt;- c(31, 4, 15, 92, 65) sort(x) ## [1] 4 15 31 65 92 Rather than sort the input vector, the function order returns the index that sorts input vector:\nindex \u0026lt;- order(x) x[index] ## [1] 4 15 31 65 92 This is the same output as that returned by sort(x). If we look at this index, we see why it works:\nx ## [1] 31 4 15 92 65 order(x) ## [1] 2 3 1 5 4 The second entry of x is the smallest, so order(x) starts with 2. The next smallest is the third entry, so the second entry is 3 and so on.\nHow does this help us order the states by murders? First, remember that the entries of vectors you access with $ follow the same order as the rows in the table. For example, these two vectors containing state names and abbreviations, respectively, are matched by their order:\nmurders$state[1:6] ## [1] \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;Arkansas\u0026quot; \u0026quot;California\u0026quot; ## [6] \u0026quot;Colorado\u0026quot; murders$abb[1:6] ## [1] \u0026quot;AL\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;CO\u0026quot; This means we can order the state names by their total murders. We first obtain the index that orders the vectors according to murder totals and then index the state names vector:\nind \u0026lt;- order(murders$total) murders$abb[ind] ## [1] \u0026quot;VT\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;WV\u0026quot; \u0026quot;NE\u0026quot; ## [16] \u0026quot;OR\u0026quot; \u0026quot;DE\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;WA\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;DC\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;MA\u0026quot; ## [31] \u0026quot;MS\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;IL\u0026quot; \u0026quot;GA\u0026quot; ## [46] \u0026quot;MI\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;CA\u0026quot; According to the above, California had the most murders.\n max and which.max If we are only interested in the entry with the largest value, we can use max for the value:\nmax(murders$total) ## [1] 1257 and which.max for the index of the largest value:\ni_max \u0026lt;- which.max(murders$total) murders$state[i_max] ## [1] \u0026quot;California\u0026quot; For the minimum, we can use min and which.min in the same way.\nDoes this mean California is the most dangerous state? In an upcoming section, we argue that we should be considering rates instead of totals. Before doing that, we introduce one last order-related function: rank.\n rank Although not as frequently used as order and sort, the function rank is also related to order and can be useful. For any given vector it returns a vector with the rank of the first entry, second entry, etc., of the input vector. Here is a simple example:\nx \u0026lt;- c(31, 4, 15, 92, 65) rank(x) ## [1] 3 1 2 5 4 To summarize, let’s look at the results of the three functions we have introduced:\n## Registered S3 method overwritten by \u0026#39;webshot\u0026#39;: ## method from ## print.webshot webshot2   original  sort  order  rank      31  4  2  3    4  15  3  1    15  31  1  2    92  65  5  5    65  92  4  4      Beware of recycling Another common source of unnoticed errors in R is the use of recycling. We saw that vectors are added elementwise. So if the vectors don’t match in length, it is natural to assume that we should get an error. But we don’t. Notice what happens:\nx \u0026lt;- c(1,2,3) y \u0026lt;- c(10, 20, 30, 40, 50, 60, 70) x+y ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 11 22 33 41 52 63 71 We do get a warning, but no error. For the output, R has recycled the numbers in x. Notice the last digit of numbers in the output.\nTRY IT\nFor these exercises we will use the US murders dataset. Make sure you load it prior to starting.\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;) Use the $ operator to access the population size data and store it as the object pop. Then use the sort function to redefine pop so that it is sorted. Finally, use the [ operator to report the smallest population size.\n Now instead of the smallest population size, find the index of the entry with the smallest population size. Hint: use order instead of sort.\n We can actually perform the same operation as in the previous exercise using the function which.min. Write one line of code that does this.\n Now we know how small the smallest state is and we know which row represents it. Which state is it? Define a variable states to be the state names from the murders data frame. Report the name of the state with the smallest population.\n You can create a data frame using the data.frame function. Here is a quick example:\n  temp \u0026lt;- c(35, 88, 42, 84, 81, 30) city \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;, \u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;) city_temps \u0026lt;- data.frame(name = city, temperature = temp) Use the rank function to determine the population rank of each state from smallest population size to biggest. Save these ranks in an object called ranks, then create a data frame with the state name and its rank. Call the data frame my_df.\nRepeat the previous exercise, but this time order my_df so that the states are ordered from least populous to most populous. Hint: create an object ind that stores the indexes needed to order the population values. Then use the bracket operator [ to re-order each column in the data frame.\n The na_example vector represents a series of counts. You can quickly examine the object using:\n  data(\u0026quot;na_example\u0026quot;) str(na_example) ## int [1:1000] 2 1 3 2 1 3 1 4 3 2 ... However, when we compute the average with the function mean, we obtain an NA:\nmean(na_example) ## [1] NA The is.na function returns a logical vector that tells us which entries are NA. Assign this logical vector to an object called ind and determine how many NAs does na_example have.\nNow compute the average again, but only for the entries that are not NA. Hint: remember the ! operator.     Vector arithmetics California had the most murders, but does this mean it is the most dangerous state? What if it just has many more people than any other state? We can quickly confirm that California indeed has the largest population:\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;) murders$state[which.max(murders$population)] ## [1] \u0026quot;California\u0026quot; with over 37 million inhabitants. It is therefore unfair to compare the totals if we are interested in learning how safe the state is. What we really should be computing is the murders per capita. The reports we describe in the motivating section used murders per 100,000 as the unit. To compute this quantity, the powerful vector arithmetic capabilities of R come in handy.\nRescaling a vector In R, arithmetic operations on vectors occur element-wise. For a quick example, suppose we have height in inches:\ninches \u0026lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70) and want to convert to centimeters. Notice what happens when we multiply inches by 2.54:\ninches * 2.54 ## [1] 175.26 157.48 167.64 177.80 177.80 185.42 170.18 185.42 170.18 177.80 In the line above, we multiplied each element by 2.54. Similarly, if for each entry we want to compute how many inches taller or shorter than 69 inches, the average height for males, we can subtract it from every entry like this:\ninches - 69 ## [1] 0 -7 -3 1 1 4 -2 4 -2 1  Two vectors If we have two vectors of the same length, and we sum them in R, they will be added entry by entry as follows:\n\\[ \\begin{pmatrix} a\\\\ b\\\\ c\\\\ d \\end{pmatrix} + \\begin{pmatrix} e\\\\ f\\\\ g\\\\ h \\end{pmatrix} = \\begin{pmatrix} a +e\\\\ b + f\\\\ c + g\\\\ d + h \\end{pmatrix} \\]\nThe same holds for other mathematical operations, such as -, * and /.\nThis implies that to compute the murder rates we can simply type:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000 Once we do this, we notice that California is no longer near the top of the list. In fact, we can use what we have learned to order the states by murder rate:\nmurders$abb[order(murder_rate)] ## [1] \u0026quot;VT\u0026quot; \u0026quot;NH\u0026quot; \u0026quot;HI\u0026quot; \u0026quot;ND\u0026quot; \u0026quot;IA\u0026quot; \u0026quot;ID\u0026quot; \u0026quot;UT\u0026quot; \u0026quot;ME\u0026quot; \u0026quot;WY\u0026quot; \u0026quot;OR\u0026quot; \u0026quot;SD\u0026quot; \u0026quot;MN\u0026quot; \u0026quot;MT\u0026quot; \u0026quot;CO\u0026quot; \u0026quot;WA\u0026quot; ## [16] \u0026quot;WV\u0026quot; \u0026quot;RI\u0026quot; \u0026quot;WI\u0026quot; \u0026quot;NE\u0026quot; \u0026quot;MA\u0026quot; \u0026quot;IN\u0026quot; \u0026quot;KS\u0026quot; \u0026quot;NY\u0026quot; \u0026quot;KY\u0026quot; \u0026quot;AK\u0026quot; \u0026quot;OH\u0026quot; \u0026quot;CT\u0026quot; \u0026quot;NJ\u0026quot; \u0026quot;AL\u0026quot; \u0026quot;IL\u0026quot; ## [31] \u0026quot;OK\u0026quot; \u0026quot;NC\u0026quot; \u0026quot;NV\u0026quot; \u0026quot;VA\u0026quot; \u0026quot;AR\u0026quot; \u0026quot;TX\u0026quot; \u0026quot;NM\u0026quot; \u0026quot;CA\u0026quot; \u0026quot;FL\u0026quot; \u0026quot;TN\u0026quot; \u0026quot;PA\u0026quot; \u0026quot;AZ\u0026quot; \u0026quot;GA\u0026quot; \u0026quot;MS\u0026quot; \u0026quot;MI\u0026quot; ## [46] \u0026quot;DE\u0026quot; \u0026quot;SC\u0026quot; \u0026quot;MD\u0026quot; \u0026quot;MO\u0026quot; \u0026quot;LA\u0026quot; \u0026quot;DC\u0026quot; TRY IT\nPreviously we created this data frame:  temp \u0026lt;- c(35, 88, 42, 84, 81, 30) city \u0026lt;- c(\u0026quot;Beijing\u0026quot;, \u0026quot;Lagos\u0026quot;, \u0026quot;Paris\u0026quot;, \u0026quot;Rio de Janeiro\u0026quot;, \u0026quot;San Juan\u0026quot;, \u0026quot;Toronto\u0026quot;) city_temps \u0026lt;- data.frame(name = city, temperature = temp) Remake the data frame using the code above, but add a line that converts the temperature from Fahrenheit to Celsius. The conversion is \\(C = \\frac{5}{9} \\times (F - 32)\\).\nWrite code to compute the following sum \\(1+1/2^2 + 1/3^2 + \\dots 1/100^2\\)? Hint: thanks to Euler, we know it should be close to \\(\\pi^2/6\\).\n Compute the per 100,000 murder rate for each state and store it in the object murder_rate. Then compute the average murder rate for the US using the function mean. What is the average?\n     Indexing Indexing is a boring name for an important tool. R provides a powerful and convenient way of referencing specific elements of vectors. We can, for example, subset a vector based on properties of another vector. In this section, we continue working with our US murders example, which we can load like this:\nlibrary(dslabs) data(\u0026quot;murders\u0026quot;) Subsetting with logicals We have now calculated the murder rate using:\nmurder_rate \u0026lt;- murders$total / murders$population * 100000 Imagine you are moving from Italy where, according to an ABC news report, the murder rate is only 0.71 per 100,000. You would prefer to move to a state with a similar murder rate. Another powerful feature of R is that we can use logicals to index vectors. If we compare a vector to a single number, it actually performs the test for each entry. The following is an example related to the question above:\nind \u0026lt;- murder_rate \u0026lt; 0.71 If we instead want to know if a value is less or equal, we can use:\nind \u0026lt;- murder_rate \u0026lt;= 0.71 Note that we get back a logical vector with TRUE for each entry smaller than or equal to 0.71. To see which states these are, we can leverage the fact that vectors can be indexed with logicals.\nmurders$state[ind] ## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Iowa\u0026quot; \u0026quot;New Hampshire\u0026quot; \u0026quot;North Dakota\u0026quot; ## [5] \u0026quot;Vermont\u0026quot; In order to count how many are TRUE, the function sum returns the sum of the entries of a vector and logical vectors get coerced to numeric with TRUE coded as 1 and FALSE as 0. Thus we can count the states using:\nsum(ind) ## [1] 5  Logical operators Suppose we like the mountains and we want to move to a safe state in the western region of the country. We want the murder rate to be at most 1. In this case, we want two different things to be true. Here we can use the logical operator and, which in R is represented with \u0026amp;. This operation results in TRUE only when both logicals are TRUE. To see this, consider this example:\nTRUE \u0026amp; TRUE ## [1] TRUE TRUE \u0026amp; FALSE ## [1] FALSE FALSE \u0026amp; FALSE ## [1] FALSE For our example, we can form two logicals:\nwest \u0026lt;- murders$region == \u0026quot;West\u0026quot; safe \u0026lt;- murder_rate \u0026lt;= 1 and we can use the \u0026amp; to get a vector of logicals that tells us which states satisfy both conditions:\nind \u0026lt;- safe \u0026amp; west murders$state[ind] ## [1] \u0026quot;Hawaii\u0026quot; \u0026quot;Idaho\u0026quot; \u0026quot;Oregon\u0026quot; \u0026quot;Utah\u0026quot; \u0026quot;Wyoming\u0026quot;  which Suppose we want to look up California’s murder rate. For this type of operation, it is convenient to convert vectors of logicals into indexes instead of keeping long vectors of logicals. The function which tells us which entries of a logical vector are TRUE. So we can type:\nind \u0026lt;- which(murders$state == \u0026quot;California\u0026quot;) murder_rate[ind] ## [1] 3.374138  match If instead of just one state we want to find out the murder rates for several states, say New York, Florida, and Texas, we can use the function match. This function tells us which indexes of a second vector match each of the entries of a first vector:\nind \u0026lt;- match(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state) ind ## [1] 33 10 44 Now we can look at the murder rates:\nmurder_rate[ind] ## [1] 2.667960 3.398069 3.201360  %in% If rather than an index we want a logical that tells us whether or not each element of a first vector is in a second, we can use the function %in%. Let’s imagine you are not sure if Boston, Dakota, and Washington are states. You can find out like this:\nc(\u0026quot;Boston\u0026quot;, \u0026quot;Dakota\u0026quot;, \u0026quot;Washington\u0026quot;) %in% murders$state ## [1] FALSE FALSE TRUE Note that we will be using %in% often throughout the book.\nAdvanced: There is a connection between match and %in% through which. To see this, notice that the following two lines produce the same index (although in different order):\nmatch(c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;), murders$state) ## [1] 33 10 44 which(murders$state%in%c(\u0026quot;New York\u0026quot;, \u0026quot;Florida\u0026quot;, \u0026quot;Texas\u0026quot;)) ## [1] 10 33 44 EXERCISES\nStart by loading the library and data.\nlibrary(dslabs) data(murders) Compute the per 100,000 murder rate for each state and store it in an object called murder_rate. Then use logical operators to create a logical vector named low that tells us which entries of murder_rate are lower than 1.\n Now use the results from the previous exercise and the function which to determine the indices of murder_rate associated with values lower than 1.\n Use the results from the previous exercise to report the names of the states with murder rates lower than 1.\n Now extend the code from exercises 2 and 3 to report the states in the Northeast with murder rates lower than 1. Hint: use the previously defined logical vector low and the logical operator \u0026amp;.\n In a previous exercise we computed the murder rate for each state and the average of these numbers. How many states are below the average?\n Use the match function to identify the states with abbreviations AK, MI, and IA. Hint: start by defining an index of the entries of murders$abb that match the three abbreviations, then use the [ operator to extract the states.\n Use the %in% operator to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\n Extend the code you used in exercise 7 to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and vice versa, then which to obtain an index.\n     Videos      https://rstudio.cloud↩︎\n https://rafalab.github.io/dsbook/installing-r-rstudio.html↩︎\n http://abcnews.go.com/blogs/headlines/2012/12/us-gun-ownership-homicide-rate-higher-than-other-developed-countries/↩︎\n I’m especially partial to Puerto Rico.↩︎\n This is, without a doubt, my least favorite aspect of R. I’d even venture to call it stupid. The logic behind this pesky \u0026lt;- is a total mystery to me, but there is logic to avoiding =. But, you do you.↩︎\n This equals sign is the reasons we assign values with \u0026lt;-; then when arguments of a function are assigned values, we don’t end up with multiple equals signs. But… who cares.↩︎\n Whether you view this as a feature or a bug is a good indicator whether you’ll enjoy working with R.↩︎\n   ","date":1599091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599237941,"objectID":"9be773bd8dbb1773f9326846c039d666","permalink":"/content/00-content/","publishdate":"2020-09-03T00:00:00Z","relpermalink":"/content/00-content/","section":"content","summary":"Readings Guiding Question  Slides R basics Case study: US homicides by firearm The (very) basics Objects The workspace Functions Other prebuilt objects Variable names Saving your workspace Motivating scripts Commenting your code  Data types Data frames Examining an object The accessor: $ Vectors: numerics, characters, and logical Factors Lists Matrices  Vectors Creating vectors Names Sequences Subsetting  Coercion Not availables (NA)  Sorting sort order max and which.","tags":null,"title":"Welcome Back to R","type":"docs"},{"authors":null,"categories":null,"content":" Below is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon. Accordingly, you should visit this page frequently throughout the term.\nAs mentioned in the syllabus, the course is structured by topics; each week introduces a new topic. Moreover, every week is divided into three important sections that you should engage with.\nOverview The class is structured with three distinct bits. First, the Tuesday lecture will give an overview of the topic for the week. Next, the Thursday lecture will have a short, practical lecture and an activity which is designed to give you hands-on experience and a greater understanding of the broader material. Finally, you will complete weekly writings (short) and labs (also short; requiring coding in R). Out of class, you will complete readings and can watch (short) recorded lectures on the week’s topic. Note, though, that you are not required to view supplemental recorded videos.\n Content (): This page contains the readings and recorded lectures for the topic. These pages should be read completely. Lectures are not an exact replication of the written content; on the contrary, the lectures are intended to keep you focused on the high-level ideas, while the readings are broader and more comprehensive. Accordingly, lectures are shorter than the (often quite lengthy) written content.\n Examples (): This page the material that we will discuss in Thursday classes. In addition to teaching specific content, there are many more R code examples. These are intended as a useful reference to various functions that you will need when working on (nearly) weekly labs and your group project.\n Assignments (): This page contains the instructions for the weekly lab (1–3 brief tasks) and for the two mini projects + final project. Labs are due by 11:59 PM (Eastern) on the Monday after they’re posted.\n   Lab Hours (TA): Fridays 9:30 - 10:50 AM via Zoom The teaching assistant for this course (Anh Do; doanh@msu.edu) will host a (very short) supplemental lecture each week to help promote additional understanding. This will be followed by (or preceeded by) open Q\u0026amp;A about the week’s content. I highly encourage you to utilize this resource, especially if you struggle with basic R programming. Passcode: 242626.\n Link To TA Office Hours tl;dr: You should follow this general process (in order) each week:\n Do everything on the content () page before Tuesday Come to the lecture on Tuesday. While “in class” on Thursday, work through the example () page Complete the lab () and the weekly writing (assigned in class) before the next Tuesday. As needed, attend the lab hours hosted by the TA.      Programming Foundations Content Example Assignment   Week 0 (Re-) Introduction to R      Week 1 Programming Basics, the tidyverse, and Visualization      Week 2 Visualization II      Week 3 Visualization III       Data Analysis Foundations Content Example Assignment   Week 4 Probability and Statistics in R      Week 5 Linear Regression I      Week 6 Linear Regression II      End of Week 6  Project 1 Due        Week 7 Linear Regression III       Applications of Data Analysis Content Example Assignment   Week 8 Nonlinear Regression      Week 9 Bias vs Variance      Week 10 Classification         End of Week 10  Project 2 Due         Week 11 Applied Classification          Further Extensions Content Example Assignment   Week 12 The caret Package         Week 13 Wrangling (Large) Data          Conclusions Content Example Assignment   Finals Week Concluding Thoughts         Final Exam  Final Project Due          ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600098724,"objectID":"3e223d7ba58b0122b42458e4cf52e04c","permalink":"/schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/schedule/","section":"","summary":"Below is a roadmap for the semester. Note that this will inevitably change from the first day you access this course. However, whatever is listed below should be considered canon. Accordingly, you should visit this page frequently throughout the term.\nAs mentioned in the syllabus, the course is structured by topics; each week introduces a new topic. Moreover, every week is divided into three important sections that you should engage with.","tags":null,"title":"Schedule","type":"page"},{"authors":null,"categories":null,"content":"  What is This Course and Can / Should You Take It? What This Course is Not Success in this Course Course materials R and RStudio Online help  Evaluations and Grades Academic honesty  COVID-19 Miscellanea Contacting Me    Instructor  Prof. Ben Bushong  25A Marshall-Adams Hall  bbushong@msu.edu  @benbushong   Course details  Tuesday and Thursday  September – December, 2020  10:20 - 11:40 AM  Slack   Contacting me Please consider whether your question is short and concrete; if so, feel free to email me. If your question is deep, vague, interesting, or otherwise complex, please come to office hours or we can discuss in class. See syllabus for details.\n  What is This Course and Can / Should You Take It? Innovations in statistical learning have created many engineering breakthroughs. From real time voice recognition to automatic categorization (and in some cases production) of news stories, machine learning is transforming the way we live our lives. These techniques are, at their heart, novel ways to work with data, and therefore they should have implications for social science. This course explores the intersection of statistical learning (or machine learning) and social science and aims to answer two primary questions about these new techniques:\nHow does statistical learning work and what kinds of statistical guarantees can be made about the performance of statistical-learning algorithms?\n How can statistical learning be used to answer questions that interest social science researchers, such as testing theories or improving social policy?\n  In order to address these questions, we will cover so-called “standard” techniques such as supervised and unsupervised learning, statistical learning theory and nonparametric and Bayesian approaches. If it were up to me, this course would be titled “Statistical Learning for Social Scientists”—I believe this provides a more appropriate guide to the content of this course. And while this class will cover these novel statistical methodologies in some detail, it is not a substitute for the appropriate class in Computer Science or Statistics. Nor is this a class that teaches specific skills for the job market. Rather, this class will teach you to think about data analytics broadly. We will spend a great deal of time learning how to interpret the output of statistical learning algorithms and approaches, and will also spend a great deal of time on better understanding the basic ideas in statistical learning. This, of course, comes at some cost in terms of time spent on learning computational and/or programming skills.\nEnrollment for credit in this course is simply not suitable for those unprepared in or uninterested in elementary statistical theory no matter the intensity of interest in machine learning or “Big Data”. Really.\nYou will be required to understand elementary mathematics in this course and should have at least some exposure to statistical theory. The class is front-loaded technically: early lectures are more mathematically oriented, while later lectures are more applied.\nThe topics covered in this course are listed later in this document. I will assign readings sparingly from Introduction to Statistical Learning, henceforth referred to as ISL. This text is available for free online and, for those who like physical books, can be purchased for about $25. Importantly, the lectures deviate a fair bit from the reading, and thus you will rely on your course notes much more than you might in other classes.\nIf—after you have read this document and preferably after attending the first lecture—you have any questions about whether this course is appropriate for you, please come talk to me. Anybody is permitted to attend the lectures and I am delighted if people can benefit.\n What This Course is Not The focus of this course is conceptual. The goal is to create a working understanding of when and how tools from computer science and statistics can be profitably applied to problems in social science. Though students will be required to apply some of these techniques themselves, this course is not…\n…a replacement for EC420 or a course in causal inference.\nAs social scientists, we are most often concerned with causal inference in order to analyze and write policies. Statistical learning and the other methods we will discuss in this course are generally not well-suited to these problems, and while I’ll give a short overview of standard methods, this is only to build intuitions. Ultimately, this course has a different focus and you should still pursue standard methodological insights from your home departments.\n…a course on the computational aspects of the underlying methods.\nThere are many important innovations that have made machine learning techniques computationally feasible. We will not discuss these, as there are computer science courses better equipped to cover them. When appropriate, we will discuss whether something is computable, and we will even give rough approximations of the amount of time required (e.g. P vs NP). But we will not discuss how optimizers work or best practices in programming.\n…a primer on the nitty-gritty of how to use these tools or a way to pad your resume.\nThe mechanics of implementation, whether it be programming languages or learning to use APIs, will not be covered in any satisfying level of depth. Students will be expected to learn most of the programming skills on their own. Specifically, while there will be some material to remind you of basic R commands, this is not a good course for people who are simply looking to learn the mechanics of programming. This course is designed to get you to use both traditional analytics and, eventually, machine learning tools. We will do some review of basic programming, and you will have the opportunity to explore topics that interest you through a final project, but ultimately this is a course that largely focuses on the theoretical and practical aspects of statistical learning as applied to social science and not a class on programming.\nPerhaps most importantly, this course is an attempt to push undergraduate education toward the frontiers in social science. Accordingly, please allow some messiness. Some topics may be underdeveloped for a given person’s passions, but given the wide variety of technical skills and overall interests, this is a near certainty. Both the challenge and opportunity of this area comes from the fact that there is no fully developed, wholly unifying framework. Our collective struggle—me from teaching, you from learning—will ultimately bear fruit.\n Success in this Course I promise, you are equipped to succeed in this course.\nLearning R can be difficult at first. Like learning a new language—Spanish, French, or Chinese—it takes dedication and perseverance. Hadley Wickham (the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like) ggplot2—made this wise observation:\n It’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n Even experienced programmers (like me) find themselves bashing their heads against seemingly intractable errors.1 If you’re finding yourself bashing your head against a wall and not making progress, try the following. First, take a break. Sometimes you just need space to see an error. Next, talk to classmates. Finally, if you genuinely cannot see the solution, e-mail the TA. But, honestly, it’s probably just a typo.\n\n Course materials All of the readings and software in this class are free. There are free online version of all the textbooks and R / RStudio are free. (Don’t pay for RStudio.) We will reference outside readings and there exist paper versions of some “books”2 but you won’t need to buy anything3\nR and RStudio You will do all of your analysis with the open source (and free!) programming language R. You will use RStudio as the main program to access R. Think of R as an engine and RStudio as a car dashboard—R handles all the calculations produces the actual statistics and graphical output, while RStudio provides a nice interface for running R code.\nR is free, but it can sometimes be a pain to install and configure. To make life easier, you can (and should!) use the free RStudio.cloud service, which lets you run a full instance of RStudio in your web browser. This means you won’t have to install anything on your computer to get started with R. We recommend this for those who may be switching between computers and are trying to get some work done. That said, while RStudio.cloud is convenient, it can be slow and it is not designed to be able to handle larger datasets or more complicated analysis and graphics. You also can’t use your own custom fonts with RStudio.cloud.4 And, generally speaking, you should have (from the prerequisite course) sufficient experience to make your R work. If not, over the course of the semester, you’ll probably want to get around to installing R, RStudio, and other R packages on your computer and wean yourself off of RStudio.cloud. If you plan on making a career out of data science, you should consider this a necessary step.\nYou can find instructions for installing R, RStudio, and all the tidyverse packages here. And you may find some other goodies.\n Online help Data science and statistical programming can be difficult. Computers are stupid and little errors in your code can cause hours of headache (even if you’ve been doing this stuff for years!).\nFortunately there are tons of online resources to help you with this. Two of the most important are StackOverflow (a Q\u0026amp;A site with hundreds of thousands of answers to all sorts of programming questions) and RStudio Community (a forum specifically designed for people using RStudio and the tidyverse (i.e. you)).\nIf you use Twitter, post R-related questions and content with #rstats. The community there is exceptionally generous and helpful.\nSearching for help with R on Google can sometimes be tricky because the program name is, um, a single letter. Google is generally smart enough to figure out what you mean when you search for “r scatterplot”, but if it does struggle, try searching for “rstats” instead (e.g. “rstats scatterplot”). Likewise, whenever using a specific package, try searching for that package name instead of the letter “r” (e.g. “ggplot scatterplot”). Good, concise searches are generally more effective.\nFinally, we have a class “chatroom” (or whatever you call such things) at Slack where anyone in the class can ask questions and anyone can answer. I will monitor Slack regularly and will respond quickly. (It’s one of the rare Slack workspaces where I have notifications enabled!) Ask questions about the readings, exercises, and mini projects. You’ll likely have similar questions as your peers, and you’ll likely be able to answer other peoples’ questions too. I encourage using Slack for communications within your groups, but that is up to you.\n  Evaluations and Grades Your grade in this course will be based on attendance, labs, and a final project.\nThe general breakdown will be approximately 50% for both labs and weekly writings, and 50% for projects (see below for specific details). The primary focus of the course is a final project; this requires two “mini-projects” to ensure you’re making satisfactory progress. Assignment of numeric grades will follow the standard, where ties (e.g., 91.5%) are rounded to favor the student. Evaluations (read: grades) are designed not to deter anyone from taking this course who might otherwise be interested, but will be taken seriously.\nWeekly writings are intended to be an easy way to get some points. Labs will be short homework assignments that require you to do something practical using a basic statistical language. Support will be provided for the R language only, although I may present some examples in Python from time to time. You must have access to computing resources and the ability to program basic statistical analyses. As mentioned above, this course will not teach you how to program or how to write code in a specific language. If you are unprepared to do implement basic statistical coding, please take (or retake) PLS202. I highly encourage seeking coding advice from those who instruct computer science courses – it’s their job and they are better at it than I am. I’ll try to provide a good service, but I’m really not an expert in computer science.\nMore in-depth descriptions for all the assignments are on the assignments page. As the course progresses, the assignments themselves will be posted within that page.\nTo Recap:\n   Assignment Points Percent    Weekly Writings (12 × 10) 120 24%  Labs (10 × 15) 150 30%  Mini project 1 50 10%  Mini project 2 50 10%  Final project 130 26%  Total 500 —        Grade Range Grade Range    4.0 92–100% 2.0 72–76%  3.5 87–91% 1.5 67–72%  3.0 82-87% 1.0 62–67%  2.5 77–81% 0.0 bad–66%     Academic honesty Violation of MSU’s Spartan Code of Honor will result in a grade of 0.0 in the course. Moreover, I am required by MSU policy to report suspected cases of academic dishonesty for possible disciplinary action.5\n  COVID-19 Things are hard right now. You most likely know people who have lost their jobs, have tested positive for COVID-19, have been hospitalized, or perhaps have even died. You all have increased (or possibly decreased) work responsibilities and increased family care responsibilities. You might be caring for extra people right now, and you are likely facing uncertain job prospects.\nI’m fully committed to making sure that you learn everything you were hoping to learn from this class! I will make whatever accommodations I can to help you finish your exercises, do well on your projects, and learn and understand the class material. Under ordinary conditions, I am flexible and lenient with grading and course expectations when students face difficult challenges. Under pandemic conditions, that flexibility and leniency is intensified.\nIf you feel like you’re behind or not understanding everything, do not suffer in silence! Please contact me. I’m available through e-mail and Slack.\n Miscellanea All class notes will be posted on https://ssc442.netlify.app. D2L will be used sparingly and for submission of assignments.\nOffice Hours are Tues \u0026amp; Thur, 4:30 - 5:45 PM online via Zoom Please use my office hours (Passcode: GODUCKS). It would be remarkable if you didn’t need some assistance with the material, and I am here to help. One of the benefits of open office hours is to accommodate many students at once; if fellow students are “in my office”, please join in and feel very free to show up in groups. (This bit of the syllabus obviously less critical in the COVID Era). Office hours will move around a little bit throughout the semester to attempt to meet the needs of all students.\nIn addition to drop-in office hours, I always have sign-up office hours for advising and other purposes. They are online, linked from my web page. As a general rule, please first seek course-related help from the drop-in office hours. However, if my scheduled office hours are always infeasible for you, let me know, and then I may encourage you to make appointments with me. I ask that you schedule your studying so that you are prepared to ask questions during office hours – office hours are not a lecture and if you’re not prepared with questions we will end up awkwardly staring at each other for an hour until you leave.\nSome gentle requests regarding office hours and on contacting me. First, my office hours end sharply at the end, so don’t arrive 10 minutes before the scheduled end and expect a full session. Please arrive early if you have lengthy questions, or if you don’t want to risk not having time due to others’ questions. You are free to ask me some stuff by e-mail, (e.g. a typo or something on a handout), but please know e-mail sucks for answering many types of questions. “How do I do this lab?” or “What’s up with Python?” are short questions with long answers. Come to office hours.\n Contacting Me Email is a blessing and a curse. Instant communication is wonderful, but often email is the wrong medium to have a productive conversation about course material. Moreover, I get a lot of emails. This means that I am frequently triaging emails into two piles: “my house is burning down” and “everything else”. Your email is unlikely to make the former pile. So… asking questions about course material is always best done in-class or in office hours. Students always roll their eyes when professors say things like that, but it’s true that if you have a question, it’s very likely someone else has the same question. (An alternative here would be via Slack but again, in-class is still the best option.)\nThat said, email is still useful. If you’re going to use it, you should at least use if effectively. There’s a running joke in academia that professors only read an email until they find a question. They then respond to that question and ignore the rest of the email. I won’t do this, but I do think it is helpful to assume that the person on the receiving end of an email will operate this way. By keeping this in mind, you will write a much more concise and easy to understand email.\nSome general tips:\n Always include [SSC442] in your subject line (brackets included). Use a short but informative subject line. For example: [SSC442] Final Project Grading Use your University-supplied email for University business. This helps me know who you are. One topic, one email. If you have multiple things to discuss, and you anticipate followup replies, it is best to split them into two emails so that the threads do not get cluttered. Ask direct questions. If you’re asking multiple questions in one email, use a bulleted list. Don’t ask questions that are answered by reading the syllabus! This drives me nuts. I’ve also found that students are overly polite in emails. I suppose it may be intimidating to email a professor, and you should try to match the style that the professor prefers, but I view email for a course as a casual form of communication. Said another way: get to the point. Students often send an entire paragraph introducing themselves, but if you use your University email address, and add the course name in the subject, I will already know who you are. Here’s an example of a perfectly reasonable email:   Subject: [SSC442] Lab, Question 2, Typo\nHi Prof. Bushong,\nThere seems to be a typo in the Lab on Question 2. The problem says to use a column of data that doesn’t seem to exist. Can you correct this or which should we use?\nThanks, Student McStudentFace\n    By the end of the course, you will realize that 1) I make many many many errors; 2) that I frequently cannot remember a command or the correct syntax; and 3) that none of this matters too much in the big picture because I know the broad approaches I’m trying to take and I know how to Google stuff. Learn from my idiocy.↩︎\n wtf is a book?!↩︎\n If you’ve got money to burn, you can buy me Red Bull.↩︎\n This bothers me way more than it should.↩︎\n So just don’t cheat or plagiarize. This is an easy problem to avoid.↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600975650,"objectID":"e4d5a4a79239f08c6ad0d7cbf1be756c","permalink":"/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"What is This Course and Can / Should You Take It? What This Course is Not Success in this Course Course materials R and RStudio Online help  Evaluations and Grades Academic honesty  COVID-19 Miscellanea Contacting Me    Instructor  Prof. Ben Bushong  25A Marshall-Adams Hall  bbushong@msu.edu  @benbushong   Course details  Tuesday and Thursday  September – December, 2020  10:20 - 11:40 AM  Slack   Contacting me Please consider whether your question is short and concrete; if so, feel free to email me.","tags":null,"title":"Syllabus","type":"page"}]